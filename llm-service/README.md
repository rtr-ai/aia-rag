# LLM Service

This service is used for communicating with LLMs (Large Language Model).
Upoon starting the service, it fetches the LLM models from Ollama repository if requires and creates a vectore store index from manually annotated chunks.

## Endpoints

## `/chat`

This endpoint allows Promting an LLM, where a user can send a prompt and receive a response. The service processes the prompt by querying an index for relevant information, then interacting promting the Model to generate and stream a response. The events are streaming using Server Side Events.

**Method**: `POST`  
**Request Body**:  
```json
{
  "prompt": "string",
  "model": "string"  // Optional: Specify a model if different from the default
}
```

**Response**:  
Streams a series of events, each in the following format:
```json
{
  "content": "string",  
  "type": "heartbeat" | "assistant" | "error" | "sources"
}
```

- **"heartbeat"**: Sent initially to indicate the start of the stream.
- **"assistant"**: Response from the model to the prompt. Here, the content field contains tokens generated by LLM and not complete response.
- **"error"**: Any error encountered during the process.
- **"sources"**: Sources retrieved from the index service to supplement the response.

## Configuration

### Ports

- The service binds to `127.0.0.1:8085` on the host machine and forwards traffic to port `8000` in the container. This means the service is available at `http://127.0.0.1:8085`.

### Environment Variables

The service is configured with several important environment variables:

- **`HOST`**: Set to `0.0.0.0` to make the service accessible on all network interfaces within the container.
  
- **`OLLAMA_HOST`**: The hostname for the Ollama service, which is set to `ollama`. This allows communication with the Ollama service inside the Docker network.

- **`EMBEDDING_MODELS`**: A comma-separated list of embedding models. The first model in the list will be used as the default. Default is `bge-m3`. You can find available models at [Ollama Embedding Models](https://ollama.com/search?c=embedding).

- **`LLM_MODELS`**: A comma-separated list of Large Language Models (LLMs). The first model in the list will be used as the default. `llama3.1:8b-instruct-fp16 ` is used as the default LLM. You can find available models at [Ollama LLM Models](https://ollama.com/search).

- **`TEMPERATURE`**: A float value between 0 and 1 which determines the balance between predictability and creativity in generated text (higher value means higher creativity). 

- **`CONTEXT_WINDOW`**: The context window of LLM model in tokens. Default is 8000.

- **`ALLOWED_ORIGINS`**: A comma-separated list of allowed origins for Cross-Origin Resource Sharing (CORS).

### Volumes

- **`./data/aia_chunks_index.json:/app/data/chunks.json`**: This volume mounts a local file (`aia_chunks_index.json`) from the host machine to the container at `/app/data/chunks.json`. This file is used to generate vectore store index from manually annotated chunks.

---

## Prerequisites

Before running this configuration, ensure that the following are set up:

1. **Ollama Service**: The service `ollamaoo` must be running and accessible via the Docker network.
2. **Models**: Ensure that the required models (`embedding models` and `LLM models`) are available and properly configured in Ollama.
3. **Annotated JSON**  is mounted at /app/data/chunks.json inside of the container.
---

## Running the Service

To start the `llm-service` use the docker-compose file provided.
