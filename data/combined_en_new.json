{
  "id": "1",
  "date_created": 1733416387074,
  "last_modified": 1733416387074,
  "chunks": [
    {
      "id": "4a4e5d76-5052-4da8-b716-fbb0c5316035",
      "title": "AI Service Desk FAQ: What does RTR's AI Service Desk do?",
      "content": "# What does RTR's AI Service Desk do?\n\nThe AI Service Desk at RTR acts as a point of contact and information hub and is available to the Austrian AI ecosystem in preparation for the European AI Act. The following tasks are at the centre of this:\n\n- A low-threshold, accessible service for information and support on the regulatory framework for the use and development of AI\n- Promoting the development and exchange of knowledge on AI, including through specialist events and studies\n- Support for media companies to help them use AI systems in a responsible and controlled manner\n- Support for the high-calibre AI Advisory Board, which advises the AI Service Desk and the Federal Government on current developments in AI.\n\nWe recommend that you make use of the information offered by the AI Service Desk at RTR.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "7401fefb-6095-47f0-a00e-78aac879499e",
        "09b711a2-571c-4a78-9b84-60675ea83b5e",
        "0b34a175-21c7-4d02-9c4e-4380537fff16"
      ],
      "parameters": []
    },
    {
      "id": "f63dfa65-68ef-47e1-a339-e0adfc260fb0",
      "title": "AI Service Desk FAQ: Why is AI regulated?",
      "content": "# Why is AI regulated? \n\nAI-systems are developing rapidly and also affects sensitive areas such as health, security and fundamental rights. With the AI Act, the EU is introducing comprehensive legal rules to minimise the risks in these areas. The AI Act is also intended to protect the rule of law, democracy and the environment.\n\nAnother aim of the AI Act is to create standardised rules for affected parties throughout the EU. It also aims to eliminate legal uncertainties in order to motivate companies to participate in progress and innovation through artificial intelligence.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "be26e0cd-4d28-42f6-8560-20e6911c4c4f",
        "b833c1d7-ad46-4548-a2c6-63f671c1d211",
        "aa44ef37-ff65-4237-9ed5-b34174aa9c6a",
        "6ead0916-b1d0-4ee7-a131-b86ac144d0ac",
        "59e64fd2-e26e-426d-be65-48f42d265850",
        "10c70d26-f011-44f0-89db-011879d8401c",
        "0edca1d5-9879-4157-bd56-130035f6204f"
      ],
      "parameters": []
    },
    {
      "id": "bdaff5ed-d37e-4686-a1d9-f4b59cfb83ec",
      "title": "AI Service Desk FAQ: Who does the AI regulation apply to?",
      "content": "# Who does the AI regulation apply to?\n\nThe AI Act is adopted as a regulation and is therefore directly applicable in the member states and regulates both the private and public sectors. Companies in and outside the EU are affected if they place AI systems on the market in the EU or if individuals in the EU are affected by them. This ranges from pure providers of tools that utilise artificial intelligence to developers of high-risk AI systems. ",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "55e8665d-afb5-46d1-b258-64f964b08d09",
        "f9b21785-6f87-4438-8810-147304eb4d7e",
        "a010e5cb-6b93-499f-ad13-0a86a4c6239a"
      ],
      "parameters": []
    },
    {
      "id": "81c0b657-fac0-4b76-a86e-bfff22f75a63",
      "title": "AI Service Desk FAQ: When does the AI Act apply?",
      "content": "# When does the AI Act apply?\nFollowing its adoption by the European Parliament and the Council, the AI Act will enter into force on the twentieth day following its publication in the Official Journal. It will then be fully applicable 24 months after its entry into force, in accordance with the following staged procedure:\n\n* 6 months after entry into force: Prohibited practices may no longer be used\n* 12 months: The obligations relating to \"General Purpose AI\" become applicable\n* 24 months: All other provisions of the AI Act become applicable, including the obligations for high-risk systems set out in Annex III (list of high-risk use cases). Those in Annex I are still exempt at this time;\n* 36 months: The obligations for high-risk systems set out in Annex I (list of Union harmonisation legislation) become applicable.\n\nMore on the time frame of the AI Act",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "e0ff4862-edad-42af-8c72-7ac7ab5c25b7",
        "7c5fc4fc-05c2-4781-9924-fc7ef6ca50ef"
      ],
      "parameters": []
    },
    {
      "id": "7f48a1de-637b-45ca-a910-f7a73ad90904",
      "title": "AI Service Desk FAQ: Which four risk levels are AI systems categorised into?",
      "content": "# Which four risk levels are AI systems categorised into?\n\n* Unacceptable risk\n* High risk\n* Limited risk\n* Minimal or no risk\n\nMore in the risk levels of AI systems",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "a6e85038-89e8-4790-9457-cb624ca42c00",
        "6b073c5f-3422-4b34-b139-b8645e89b463",
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "42b80e11-733d-441e-98e1-d79837892537",
        "16b5ff49-dbab-4e7e-861a-7e1d4cd7cb03",
        "8a27e104-58f9-4f9d-8bbb-aff055c16634",
        "283d70b2-97f7-4252-9190-378e90b707bd",
        "e87ad418-9936-47c0-b4ec-99f31c41a0a8",
        "24b748de-eebc-404f-8b36-d12eed0a6660",
        "85555ab3-0021-4e99-96f7-b8d2182b43f7"
      ],
      "parameters": []
    },
    {
      "id": "c778be50-e474-4992-ad36-c564a8bdad2c",
      "title": "AI Service Desk FAQ: How do I determine the risk of an AI system?",
      "content": "# How do I determine the risk of an AI system?\n\nThe categorisation depends on the intended use and the application modalities of the AI system. The AI Act lists the prohibited practices and use cases of high-risk AI systems (Annex I and III). The EU Commission is authorised to extend the list of high-risk AI systems. In doing so, it takes account of market and technological developments and ensures consistency. AI systems that carry out profiling, i.e. the creation of personality profiles of natural persons, are always considered high-risk. ",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "416816a1-c29f-4c2c-b7f4-015e60a43042",
        "88111b7f-ede1-45cc-bedc-9aa57dc012a2",
        "7e595036-3aa8-4edf-b26f-0627760fb44e",
        "f6e922ce-0538-4d92-8c32-a0a750198c89",
        "b5274646-0b45-4227-ad44-f5afd431413c",
        "24b540c7-6f65-42f3-80e6-2a3ec3e86999",
        "56f9d88f-1824-4420-b01b-a3d855f3bbaf",
        "b740c960-f13f-4b09-95d9-3443b6e19b26",
        "a6e85038-89e8-4790-9457-cb624ca42c00",
        "6b073c5f-3422-4b34-b139-b8645e89b463",
        "283d70b2-97f7-4252-9190-378e90b707bd",
        "e87ad418-9936-47c0-b4ec-99f31c41a0a8",
        "24b748de-eebc-404f-8b36-d12eed0a6660",
        "85555ab3-0021-4e99-96f7-b8d2182b43f7"
      ],
      "parameters": []
    },
    {
      "id": "be2475ee-b7ff-4afe-9eeb-97028800cf97",
      "title": "AI Service Desk FAQ: What obligations do providers of high-risk AI systems have?",
      "content": "# What obligations do providers of high-risk AI systems have?\n\nThe natural or legal person, authority, institution or other body that develops a high-risk AI system or has it developed and also places it on the market or puts it into operation under its own name or brand has the most extensive obligations. They must ensure that the requirements placed on high-risk AI systems are met. The obligations include, among others:\n\n* Establishment of risk management systems;\n* Fulfilment of data governance requirements;\n* Documentation obligations in technical terms;\n* Recording obligations;\n* Transparency obligations in relation to users;\n* Sufficient implementation of human monitoring tools;\n* Ensuring accuracy, robustness and cyber security. \n\nMore on the provider obligations",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "181144f1-acf1-42ef-a1a2-aca3c1a52b7d",
        "6d7d2b9c-9224-4f54-ad18-671918597d0c",
        "69b883ca-3e2d-46ed-b797-e2c62832a376",
        "355be535-f654-47e6-9608-95369e856b37",
        "40622ac2-1630-4d8b-a8a6-0aadd9c20248",
        "e36b6aab-d7e1-44ce-8d20-d6bee82bff7d",
        "67156c3d-d006-41f0-87e3-725cd933dbdb",
        "91bb9b2c-b571-4042-8143-f399e538ffe2",
        "3be859ed-68c0-4253-864e-73cecf5b5500",
        "3ebbd833-32ab-4a37-87be-8bae31dbef92",
        "03a48ad3-88c0-41d1-be59-9032cdfa8840",
        "1d3e3007-9327-4f7b-ba9b-44bd93c2f4ae",
        "25407f28-0e07-4101-a714-b193ca14783e",
        "2aedd016-7002-471a-af6e-131b4f9f1f54",
        "316fe982-49dc-4467-a230-7b526feb2812",
        "53ee5174-71e9-4bc8-81f2-251f7a1bc278",
        "817c9521-a9db-4149-99df-86ce3f02b215",
        "9032a3a6-c85d-43bc-b8da-fb8d7a40e48c",
        "550c995f-0a8e-4714-b73e-9418fb982eed",
        "3b55e0a7-53c5-443d-a628-f905ea635201",
        "c9ddca61-d229-4d40-b57a-f7c249377ecb",
        "f13362fc-1e4e-4ac2-80ec-5def8ff64f2a",
        "3c1ccff0-1d4e-4217-9828-90e5bb65d611",
        "63c8bc3d-ba5f-4134-97e6-4294cf1e2697"
      ],
      "parameters": []
    },
    {
      "id": "c7be8215-ad70-4bd9-a020-4e5efeff7cca",
      "title": "AI Service Desk FAQ: How will the AI Act be enforced?",
      "content": "# How will the AI Act be enforced?\n\nEach Member State shall establish or designate at least one notifying authority and at least one market surveillance authority as national competent authorities for the purposes of this Regulation. Those national competent authorities shall exercise their powers independently, impartially and without bias.  \n\nIn addition, the Commission has set up a new European AI Office within the Commission to monitor general purpose AI models.\n\nThere will also be an AI Board, a Scientific Panel and an Advisory Forum, which will have an advisory and supportive function.\n\nMore on the authorities and bodies at EU-level",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "63ae5b47-b44b-4b4b-82ac-5fa868177399",
        "e0492e6c-e987-41c3-b664-775286020859",
        "e684a05a-dce1-41e7-9743-fd5b1b778be8",
        "bf998658-e660-43ca-8849-a1cc9c46930c",
        "6efc5cc4-f5b5-4581-bc5d-10d2aeca4486",
        "af44a0ad-8425-4b78-8b03-21093d6fc548",
        "50b2ea2a-b39a-49b3-86b1-4b89a8b9dbf6"
      ],
      "parameters": []
    },
    {
      "id": "e3368f8d-8bb5-46c1-873f-4af231bce512",
      "title": "AI Service Desk FAQ: What sanctions are there for violations?",
      "content": "# What sanctions are there for violations?\n\nIf AI-systems are placed on the market or put into service that do not comply with the requirements of the Regulation, Member States must lay down effective, proportionate and dissuasive penalties, including fines, and notify them to the Commission.\n\nCertain threshold values are defined in the ordinance for this purpose:\n\n* up to EUR 35 million or 7 per cent of the previous year's total global turnover (whichever is higher) for violations of prohibited practices or breaches of data requirements;\n* up to EUR 15 million or 3 per cent of the previous year's total global turnover for breaches of other requirements or obligations under the Regulation, including breaches of the provisions for general purpose AI models;\n* up to EUR 7.5 million or 1.5 per cent of the total worldwide turnover of the previous year for false, incomplete or misleading information in requested information to notified bodies and competent national authorities;\n* For all categories of infringements, the threshold would be the lower of the two amounts for SMEs and the higher for other companies.\n\nIn order to harmonise national rules and procedures for setting fines, the Commission will draw up guidelines based on the Committee's recommendations.\n\nAs the EU institutions, bodies, offices and agencies should lead by example, they will also be subject to the rules and possible sanctions. The European Data Protection Supervisor will be authorised to impose fines on them.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "61300139-468e-4f24-871e-61a39036b15f",
        "09a1468c-9fc4-44ee-a8c1-fd07084ea0d8",
        "de889fca-779f-4b70-8ae2-166b1f752102",
        "984548c3-63d4-4d21-99dc-c4f542c22ab3",
        "48edb908-8ded-493b-aa2d-f7864af2bd26",
        "c837084f-afdb-431e-b95a-b1d3bdacff9e"
      ],
      "parameters": []
    },
    {
      "id": "0ca3b298-b05b-490f-aded-9c6476a13580",
      "title": "AI Service Desk FAQ: I am affected by an infringement of the regulations. What can I do?",
      "content": "# I am affected by an infringement of the regulations. What can I do?\n\nThe AI Act provides for the right of natural and legal persons to lodge a complaint with a national authority. On this basis, national authorities can initiate market surveillance in accordance with the procedures of the market surveillance regulations.\n\nIn addition, the proposed AI Liability Directive aims to provide individuals seeking compensation for harm caused by high-risk AI systems with effective means to identify potentially liable persons and to secure relevant evidence for a claim for compensation. To this end, the proposed Directive provides for the disclosure of evidence of certain high-risk AI systems suspected of having caused harm.\n\nIn addition, the Product Liability Directive, which is currently being revised, will ensure that people who are killed, injured or suffer material damage in the Union as a result of a defective product will receive compensation. It is clarified that AI systems and products containing AI systems are also covered by the existing rules.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "43ec355f-40dd-4ebb-be43-6f6618d28b69",
        "1c919b24-516b-4255-979e-f4cbc0da447e"
      ],
      "parameters": []
    },
    {
      "id": "de47fa95-324c-4d75-89c4-ddb875707ef3",
      "title": "AI Service Desk FAQ: Must all employees undergo training, or is it required only for those directly working with AI systems?",
      "content": "# Must all employees undergo training, or is it required only for those directly working with AI systems?\n\nArticle 4 of the AI Act mandates that providers and deployers implement appropriate “measures” to ensure that their staff as well as third parties operating on their behalf such as service providers, possess a sufficient level of AI literacy if they are involved in the deployment or use of AI within the organization.\n\nThe obligation to acquire AI literacy applies to the following groups:\n\n* Individuals responsible for AI development\n* Individuals responsible for the operation of AI systems\n* Employees within the company who utilize AI systems\n\nThe specific training requirements depend on the AI system or AI model in use, its associated risk level, as well as the employees' technical expertise, experience, education, and training. Additionally, the requirements will be influenced by the specific context in which the AI systems are deployed. Different standards will apply to executives, project teams, and trainees.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "09b711a2-571c-4a78-9b84-60675ea83b5e",
        "21ada015-1d50-4ddb-97d5-33b540e849cb",
        "f9b21785-6f87-4438-8810-147304eb4d7e"
      ],
      "parameters": []
    },
    {
      "id": "cf7337ff-5e03-49da-beec-ed1fafaf88b7",
      "title": "AI Service Desk FAQ: Are there specific minimum requirements for training content (e.g. technical or regulatory aspects)?",
      "content": "# Are there specific minimum requirements for training content (e.g. technical or regulatory aspects)?\n\nThe AI Act does not specify particular minimum requirements; however, the competencies to be conveyed may include the following:\n\n* Familiarity with the organization’s corporate strategy, relevant policies, and designated points of contact.\n* Fundamental digital competencies (e.g. in accordance with DigiComp 2.3 AT) (https://www.digitalekompetenzen.gv.at/kompetenzen/Kompetenzmodell.html)\n  * Digital transformation in the workplace\n  * Searching for and critically evaluating information\n  * Utilizing digital technologies in daily work tasks\n* Understanding of AI and its areas of application, including\n  * Functionality and practical examples of AI use\n  * Opportunities for innovation through AI and its potential to facilitate work processes\n  * Opportunities for innovation through AI and its potential to facilitate work processes\n* Guidance for the use of AI systems implemented within the organization, including:\n  * User training, opportunities for knowledge sharing, and internal best practices\n  * For example, prompting workshops for the effective use of text-based AI\n  * The Austrian Economic Chamber provides an overview of widely used AI systems: https://www.wko.at/digitalisierung/ki-loesungen-fuer-die-praxis \n* Ethical aspects\n* Legal elements, including:\n* Awareness of relevant legal aspects, such as data protection, labor law, and intellectual property rights\n\nBest Practices for fostering AI literacy include:\n\n* Regular reassessment of deployed AI systems, including periodic re-evaluation of potential AI use cases based on the latest technological advancement.\n* Interdisciplinary oversight involving representatives from key areas such as technology, law, compliance, IT security, human resources, and employee representation bodies, depending on the organization's size and structure. Practice-oriented learning, incorporating systematic testing and evaluation of new AI systems in relation to internal organizational use cases, with a structured approach to gathering and integrating feedback.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [],
      "parameters": []
    },
    {
      "id": "8ff10fba-b55e-4c99-b089-7971806ebee8",
      "title": "AI Service Desk FAQ: What is deep learning, and what are neural networks?",
      "content": "# What is deep learning, and what are neural networks?\n\nDeep learning is a specialized subfield of machine learning. In this area, artificial neural networks are capable of learning from vast amounts of unstructured data and recognizing complex patterns. Training these networks requires significant computational power.\n\nArtificial neural networks consist of artificial neurons, whose structure is inspired by biological neurons. These artificial neurons are arranged in hierarchical layers that are interconnected. The architecture includes an input layer, multiple hidden layers, and an output layer. The numerous layers can make it challenging to understand the decision-making process of an AI model. The term \"deep\" learning derives from the use of deep (multi-layered) neural networks.\n\nTypical application areas include speech recognition in automatic translation, the conversion of spoken language to text, and use in voice assistants (e.g., Siri or Alexa). Other examples include predicting market trends in the financial sector and autonomous driving. Deep fakes are videos, audio recordings, or images created or manipulated using deep learning to resemble real people, objects, places, facilities, or events, which may falsely appear authentic or truthful to an observer. The term \"deep fake\" itself is a combination of \"deep learning\" and \"fake.\"\n\nThus, deep learning is a powerful and highly advanced technology with a broad range of applications. However, it also presents challenges in terms of data and computing requirements, as well as the explainability and traceability of models.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [],
      "parameters": []
    },
    {
      "id": "73d6d9b2-be6d-4359-8896-2c0277c9026b",
      "title": "AI Service Desk FAQ: What is an \"AI system\"?",
      "content": "# What is an \"AI system\"?\n\nThe legal definition of the AI Act is relevant for the legal treatment of artificial intelligence. It represents the gateway to the scope of application of the regulation. The definition is according to Art. 3 no. 3 AIA as follows: \n\n‘AI system’ means a machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments.\n\nThe intention of the Union legislator is not to cover simpler traditional software applications or programming approaches that are based exclusively on rules defined by natural persons for the automatic execution of processes. ",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "0b34a175-21c7-4d02-9c4e-4380537fff16",
        "7401fefb-6095-47f0-a00e-78aac879499e"
      ],
      "parameters": []
    },
    {
      "id": "780d67fe-08bc-4789-a2f1-df4d30ef1de3",
      "title": "AI Service Desk FAQ: What is generative AI?",
      "content": "# What is generative AI?\n\nGenerative AI are AI systems that make it possible to generate new information, including text, audio and images, based on user input. Due to the wide range of applications, such AI systems are used in a wide variety of contexts, such as for translations, answering questions and chatbots. ",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "42b80e11-733d-441e-98e1-d79837892537",
        "c9a0bc19-9064-435f-a8d1-73a20a754192"
      ],
      "parameters": []
    },
    {
      "id": "7e5b0065-efed-4c70-933f-0da191a391ef",
      "title": "AI Service Desk FAQ: What is a \"prompt\"?",
      "content": "# What is a \"prompt\"?\n\nThe English term \"prompt\" is used in IT to describe an instruction to a user to make an input. Generative AI works by entering \"prompts\". To generate an image, text or video (output), the AI system needs an input. Depending on the AI system, a prompt can be text-, image- or audio-based. A text-based prompt can consist of words, special characters and numbers, e.g: \"A picture of 3 cats sitting on the windowsill sleeping.\"\n\nThe importance of prompts has already led to the development of prompt marketplaces.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [],
      "parameters": []
    },
    {
      "id": "8f0aacde-27ab-460f-a731-735f3ea78739",
      "title": "AI Service Desk FAQ: What is a \"RAG\" system and how does it work?",
      "content": "# What is a \"RAG\" system and how does it work?\n\nThe abbreviation \"RAG\" stands for Retrieval Augmented Generation. RAG systems combine two techniques, namely information retrieval and text generation. This means that large language models (LLMs) can be expanded to include an additional external knowledge source (e.g. a database) without the LLM having to be retrained with this additional data. The initial letters RAG stand for the individual steps that a RAG system goes through to generate an answer to a user query.\n\nFor the retrieval of relevant information, the respective enquiry is compared with the external knowledge source. For example, in response to the question \"What is the budget overview for the last quarter?\", the system retrieves the relevant documents from the external knowledge source (quarterly reports, etc.). The next step is to augment the user enquiry with the previously identified relevant documents from the external knowledge source. Only in this augmented form is the enquiry then transferred to the LLM for the generation of the response.\n\nRAG systems offer advantages over pure LLM systems. For example, it is not necessary to retrain an LLM at great expense if the knowledge base is to be expanded. The answers from a RAG system are more precise and relevant.  In addition, the external knowledge source can limit so-called \"hallucinations\", which occur when relevant information is not available in the knowledge base of the LLM system. \n\nRAG systems make it possible, for example, to efficiently search and utilize internal knowledge bases. Other application examples for RAG systems include chatbots in customer support, product recommendations in online retail or knowledge management in companies.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [],
      "parameters": []
    },
    {
      "id": "c961ad84-cbd5-4b03-9ab7-3a7593855c66",
      "title": "AI Service Desk FAQ: What are Large Language Models (LLMs)?",
      "content": "# What are Large Language Models (LLMs)?\n\n\"Large Language Models\" are computational linguistic language models that generate texts. In a given context, the next word is selected on the basis of a probability previously defined in the algorithm. These models are called \"large\" in relation to the scope of their training data and the number of parameters. To exaggerate, it is said that these models are trained with the \"entire Internet\".\n\nLLMs are based on the so-called transformer model, a special type of artificial neural network. They therefore fall into the area of deep learning. The areas of application are diverse: creating texts, answering questions (chatbots, virtual assistants), generating code, creating content for marketing and websites and translating between different languages, to name just a few possibilities.  The best-known examples of large language models include the GPT model series from OpenAI, Meta LLama and the Mistral series from Mistral AI.\n\nUnder certain circumstances, LLMs can be categorised as general purpose AIs. In any case, it should be noted that they are not perfect. They also require human supervision, as they can sometimes make mistakes or pose challenges in terms of ethics and fairness.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [],
      "parameters": []
    },
    {
      "id": "b2735701-dd85-4e80-a648-baff0e065612",
      "title": "AI Service Desk FAQ: Is a large amount of data always needed to train AI models?",
      "content": "#Is a large amount of data always needed to train AI models?\n\nThe amount of data required to train an AI model can vary greatly and depends on several factors. It is difficult to give general figures, as this depends heavily on the use case, the complexity of the model and the specific requirements of the project. However, some rough guidelines and examples can be helpful to get a feel for it. A rule of thumb is that you need at least 10 to 100 times more training data sets than model parameters to train a good generalising model.  For a simple task such as email spam filtering, where a classification with only a few classes takes place, a few hundred to a thousand emails may be sufficient to train the model.\n\nAn example of a moderate task is the classification of handwritten digits with the MNIST dataset. The training dataset consists of 60,000 images of handwritten digits (0-9), plus a test dataset with 10,000 images.\n\nComplex tasks can require data sets ranging in size from hundreds of thousands to millions of training data. One example of this is the classification of objects in high-resolution images using deep learning and neural networks. A concrete application example of this is the recognition and diagnosis of diseases on medical images such as X-rays, CT scans and MRI images. Considerably more training data is required by large language models (LLMs).\n\nThe LLama-3 model published by Meta as open source was trained with 15T tokens of text, i.e. 15 trillion tokens (T stands for \"trillion\", i.e. 1,000,000,000,000). Finally, petabytes of data are analysed during the development and testing of self-driving cars, as Tesla and NVIDIA, for example, have announced (1 Petabyte (PB) equals to 1,000,000 Gigabyte (GB)).\n\nSo, you can see that this question cannot be answered with a general \"yes\" or \"no\" and always depends on the specific use case.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [],
      "parameters": []
    },
    {
      "id": "aa88e918-3fa9-4093-85a6-8dea72a863e7",
      "title": "AI Service Desk FAQ: What does it mean when an AI system \"hallucinates\"?",
      "content": "# What does it mean when an AI system \"hallucinates\"?\n\nIf an AI model generates information that is not based on training data or real facts, it is said to be \"hallucinating\". Such hallucinations are particularly familiar from large language models (LLMs). They can appear to be real, plausible answers, but are actually incorrect or unreliable. Depending on the context of the query, different forms of hallucinations can be distinguished. \n\nDepending on whether the reference parameter is data that has been provided to the AI system or real, verifiable knowledge, the first case is referred to as fidelity and the second as factuality. Hallucinations can occur for various reasons. If the training data is incomplete, incorrect or distorted, the AI model can draw false conclusions. Generative AI models that make predictions from probabilities can hallucinate when they try to give logical or coherent answers. \n\nHowever, as the most probable answer may not always be correct, how can this problem be addressed? Factual hallucinations can be limited by retrieval augmented generation (RAG). This involves adding an additional external knowledge source (e.g. a database) from which relevant documents or information are identified and extracted on the basis of the respective user enquiry. This retrieval component is the basis for the subsequent generation component of the RAG system. Other strategies include improved training data and the development of mechanisms to validate and verify generated information through external sources. \n\nLast but not least, awareness raising and user training are important. It is important to critically scrutinise the answers of an AI system and, if possible, validate them with reliable sources. This is especially true for important or sensitive information.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [],
      "parameters": []
    },
    {
      "id": "81a9f3e6-f4ca-4e67-a8ae-771d3ffe3b01",
      "title": "AI Service Desk FAQ: Do contents (texts or images) created with a generative AI have to be labelled?",
      "content": "# Do contents (texts or images) created with a generative AI have to be labelled?\n\nIf a company uses an AI offered on the market, i.e. the company is an deployer and not a provider within the meaning of the AI Act (\"AIA\"), the following applies: Without prejudice to other transparency obligations under Union or national law, deployers of an AI system generating or manipulating image, sound or video content that is a deep fake, must disclose that the content was artificially generated or manipulated in accordance with Art. 50 para. 4 AIA. \n\nA \"deep fake\" within the meaning of the AI Act is an AI-generated or manipulated image, audio or video content that resembles existing persons, objects, places, entities or events and would falsely appear to a person to be authentic or truthful (see Art. 3 para. 60 AIA). For example, a video of a politically active person giving an interview that appears to be real but it is not the case.\n\nIf the images generated are not \"deep fakes\", you are not subject to any transparency obligations, i.e. you do not have to identify AI-generated images as such. There are also exceptions to the transparency obligation for artistic, creative, satirical or fictional depictions (Art. 50 para. 4 AIA).\n\nThe transparency obligations do not apply to AI-generated and manipulated texts if they have been checked by a human or if there is an editorial manager. If this is not the case and the texts are of public interest, they must be disclosed as AI-generated.\n\nIf labelling is required, this must be done in a clear and unambiguous manner and must comply with the applicable accessibility requirements (see Art. 50 para. 5 AIA).\n\nIt should also be noted that the AI Act is gradually unfolding its obligations; although it came into force on 1 August 2024, there are transitional arrangements. The provisions on transparency obligations are mandatory from 2 August 2026. ",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "be8e58b9-7976-45e8-9b6e-ae5d17d5f7cb",
        "42b80e11-733d-441e-98e1-d79837892537",
        "aedc6b30-3ec3-4277-a8aa-bcc97fad2474",
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6",
        "0b34a175-21c7-4d02-9c4e-4380537fff16",
        "b757b57f-4015-401c-8003-852ba5aaefc0",
        "55614e42-478f-480a-a5c3-2ecb6c402441"
      ],
      "parameters": []
    },
    {
      "id": "64ec31a3-f49d-403d-8234-753a1abb695d",
      "title": "AI Service Desk FAQ: Can I be considered a provider if I develop an AI system only for internal use?",
      "content": "# Can I be considered a provider if I develop an AI system only for internal use?\n\nAccording to Article 3(3) of the AI Act (AIA), a \"provider\" is defined as a natural or legal person, public authority, agency or other body that develops an AI system or a general-purpose AI model or that has an AI system or a general-purpose AI model developed and places it on the market or puts the AI system into service under its own name or trademark, whether for payment or free of charge.\n\n\"Putting into service\" is defined in Article 3(11) AIA as the supply of an AI system for first use directly to the deployer or for own use in the Union for its intended purpose.\n\nIf a provider develops or has developed an AI system or general-purpose AI model and puts it into service for own use, the provider obligations still apply. Any \"naming\" of the AI system is irrelevant.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6",
        "0b34a175-21c7-4d02-9c4e-4380537fff16"
      ],
      "parameters": []
    },
    {
      "id": "d8d0a3eb-4dfd-4643-a565-aacee640088d",
      "title": "AI Service Desk FAQ: Is there already a competent authority in connection with [sector] in Austria?",
      "content": "# Is there already a competent authority in connection with [sector] in Austria?\n\nThe designation of national authorities responsible for implementation must be finalized by August 2, 2025 (https://www.rtr.at/rtr/service/ki-servicestelle/ai-act/Zeitplan.en.html).\n\nCurrently, there is no national implementation of the responsibilities outlined in the AI Act in Austria. This applies to both market surveillance authorities and the notifying authority.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "2d1207bb-4da9-4b6c-abee-bdd5de6d80e7",
        "577d5a3c-36b6-48b6-a3fd-27b111efe5fd",
        "0d1ee002-929c-4cd0-99ba-0e8f530307b5",
        "ecc6bef2-71ac-41e5-a335-c79864647312"
      ],
      "parameters": []
    },
    {
      "id": "c3154f04-5972-4c94-bb49-33de0cc3c6c2",
      "title": "AI Service Desk FAQ: I am developing my own AI system, which I will use exclusively internally. The system will never be made available to other market participants. The system would be classified as a high-risk AI system under Annex III. Am I subject to regulation?",
      "content": "# I am developing my own AI system, which I will use exclusively internally. The system will never be made available to other market participants. The system would be classified as a high-risk AI system under Annex III. Am I subject to regulation?\n\nAnnex III does not specify any particular division of tasks or roles but defines which AI systems are considered high-risk under Article 6(2). A company that develops and puts its own AI system into service assumes both the role of the provider and the role of the deployer. As such, the obligations of both the provider (https://www.rtr.at/rtr/service/ki-servicestelle/ai-act/Provider_obligations.en.html) and the deployer (https://www.rtr.at/rtr/service/ki-servicestelle/ai-act/Deployer_obligations.en.html) apply. ",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "416816a1-c29f-4c2c-b7f4-015e60a43042",
        "88111b7f-ede1-45cc-bedc-9aa57dc012a2",
        "f6e922ce-0538-4d92-8c32-a0a750198c89",
        "7e595036-3aa8-4edf-b26f-0627760fb44e",
        "b5274646-0b45-4227-ad44-f5afd431413c",
        "24b540c7-6f65-42f3-80e6-2a3ec3e86999",
        "56f9d88f-1824-4420-b01b-a3d855f3bbaf",
        "b740c960-f13f-4b09-95d9-3443b6e19b26",
        "6b073c5f-3422-4b34-b139-b8645e89b463",
        "16b5ff49-dbab-4e7e-861a-7e1d4cd7cb03",
        "070576ca-434c-4246-a1bf-ba25d2a45285"
      ],
      "parameters": []
    },
    {
      "id": "26604e5a-bcad-4972-8e6f-a2007c1d4761",
      "title": "AI Service Desk FAQ: How are corporate groups regulated when AI systems are developed internally and used by other entities within the same group?",
      "content": "# How are corporate groups regulated when AI systems are developed internally and used by other entities within the same group?\n\nIf a high-risk AI system is developed in one subsidiary of the group and used in another subsidiary, the obligations for the provider and the deployer will depend on the specific circumstances of each case. ",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6",
        "0b34a175-21c7-4d02-9c4e-4380537fff16"
      ],
      "parameters": []
    },
    {
      "id": "e3a3782a-9936-4a26-902c-d07af5bfb88e",
      "title": "AI Service Desk FAQ: I plan to source AI-generated videos from a provider and publish them on my platform. What are my obligations under the AI Act?",
      "content": "# I plan to source AI-generated videos from a provider and publish them on my platform. What are my obligations under the AI Act?\n\nIf you, as a business, use third-party AI systems without making any modifications to them, you are generally classified as a deployer within the AI value chain, meaning the obligations for deployers apply to you. In the case of AI-generated videos, this is considered a limited-risk AI system, for which transparency obligations will apply starting on August 2, 2026.\n\nThis means that if your videos are classified as \"Deep Fakes\" under the AI Act, you will be subject to a labelling requirement. Otherwise, no additional obligations under the AI Act apply to the videos. For further information, please visit our website: https://www.rtr.at/rtr/service/ki-servicestelle/ai-act/Transparency_obligations.en.html",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "42b80e11-733d-441e-98e1-d79837892537",
        "e0ff4862-edad-42af-8c72-7ac7ab5c25b7",
        "7c5fc4fc-05c2-4781-9924-fc7ef6ca50ef",
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6",
        "0b34a175-21c7-4d02-9c4e-4380537fff16",
        "b757b57f-4015-401c-8003-852ba5aaefc0",
        "55614e42-478f-480a-a5c3-2ecb6c402441"
      ],
      "parameters": []
    },
    {
      "id": "8fea4ada-f7cc-4b5f-92b7-495865c95467",
      "title": "AI Service Desk FAQ: I plan to source AI-generated photos from a provider and publish them on my platform. Is this permissible under copyright law?",
      "content": "# I plan to source AI-generated photos from a provider and publish them on my platform. Is this permissible under copyright law?\n\nIf you, as a business, use third-party AI systems without making any modifications to them, you are generally classified as a deployer within the AI value chain, meaning the obligations for deployers apply to you. In the case of AI-generated videos, this is considered a limited-risk AI system, for which transparency obligations will apply starting on August 2, 2026.\n\nThis means that if your videos are classified as \"Deep Fakes\" under the AI Act, you will be subject to a labelling requirement. Otherwise, no additional obligations under the AI Act apply to the videos. For further information, please visit our website: https://www.rtr.at/rtr/service/ki-servicestelle/ai-act/Transparency_obligations.en.html",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "7bed8e5e-3585-48d6-889d-68089e0581a4",
        "e4baece6-7335-43c7-bc93-770f04b64211",
        "b908b1fb-c1a0-41d0-85cd-96d3d195d360",
        "0a6d98df-01bf-4a5c-b316-f87ecb2b8f17",
        "be8e58b9-7976-45e8-9b6e-ae5d17d5f7cb",
        "42b80e11-733d-441e-98e1-d79837892537"
      ],
      "parameters": []
    },
    {
      "id": "146330a8-dcb0-4f24-a51b-6c90354ad5e3",
      "title": "AI Service Desk FAQ: Are there any obligations when creating videos with AI avatars or digital twins?",
      "content": "# Are there any obligations when creating videos with AI avatars or digital twins?\n\nIf you, as a business operator, use third-party AI systems without making any changes to them, you are generally classified as an deployer in the AI value chain, i.e. the deployer obligations apply to you. In the case of AI-generated videos and audio, the AI system is considered to be of limited risk, and transparency obligations will apply from 2 August 2026.\n\nThe respective obligations that apply depend on whether these avatars are photorealistic or illustrative.\n\nThis means that if your videos are ‘deep fakes’ within the meaning of the AI Act (for example, because the avatar is supposed to represent you photorealistically as a person), you are subject to a disclosure requirement. This also applies to illustrative avatars that reproduce your voice in an AI-generated way, even though you did not speak this text.\n\nOtherwise, the videos are not subject to any special obligations under the AI Act, since it is clear from illustrative avatars that it is not the actual person who is supposedly reproducing the content.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6",
        "0b34a175-21c7-4d02-9c4e-4380537fff16",
        "b757b57f-4015-401c-8003-852ba5aaefc0",
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "42b80e11-733d-441e-98e1-d79837892537",
        "63f69c82-a393-43d5-bf47-0ea7b61acc5f",
        "0a83841c-2931-4e56-a212-b73e3ab410f1",
        "dfe0e9c1-22e5-4ba8-ad77-a65c24cee14d"
      ],
      "parameters": []
    },
    {
      "id": "fd982bdd-fd66-49a7-9fb1-5551eb202552",
      "title": "AI Service Desk FAQ: We would like to use an AI to review application documents. Are we always subject to the legal requirements of Art 6 para 2 in conjunction with Annex III AI Act? When do the exceptions of Art 6 para 3 AI Act apply?",
      "content": "# We would like to use an AI to review application documents. Are we always subject to the legal requirements of Art 6 para 2 in conjunction with Annex III AI Act? When do the exceptions of Art 6 para 3 AI Act apply?\n\nIn principle, AI systems in relation to employment and personnel management fall within the scope of Annex III in conjunction with Art 6 (2) of the AI Act and are therefore categorised as high-risk systems in the risk level system of the AI Act.\n\nAccording to Art 6 para 3 AI Act, an AI system listed in Annex III is not considered a high-risk AI system if it does not pose a significant risk to the health, safety or fundamental rights of natural persons by, among other things, not significantly influencing the outcome of decision-making. This is particularly the case if the AI system is intended to perform a narrow procedural task (Art 6 para 3 lit a AI Act). The exception may also apply if the AI system is intended to improve the result of a previously completed human activity (Art. 6 para. 3 lit. b AI Act) or to recognise decision-making patterns or deviations from such patterns This includes restructuring data, reclassifying documents according to categories, recognising duplicates, etc. The AI system should assume a preparatory role and under no circumstances carry out assessments itself.\n\nIt should also be noted that, even if exceptions are accepted, there is a registration obligation under Art. 49 AI Act. (Recital 53)",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "0b34a175-21c7-4d02-9c4e-4380537fff16",
        "88111b7f-ede1-45cc-bedc-9aa57dc012a2",
        "d89b42c0-fb0b-40c3-a99d-40aae0d9dea9",
        "6b073c5f-3422-4b34-b139-b8645e89b463",
        "16b5ff49-dbab-4e7e-861a-7e1d4cd7cb03",
        "be8e58b9-7976-45e8-9b6e-ae5d17d5f7cb",
        "ee5c5bfc-ded7-4010-bdae-d396f7798630"
      ],
      "parameters": []
    },
    {
      "id": "30977cbc-3e3a-469b-823d-7859b1113c51",
      "title": "AI Service Desk FAQ: I am interested in utilizing AI regulatory sandboxes. When will this be possible, and how do they function?",
      "content": "# I am interested in utilizing AI regulatory sandboxes. When will this be possible, and how do they function?\n\nAI regulatory sandboxes allow for supervised testing under real-world conditions.\n\nArt. 57 of the AIA requires Member States to ensure that their competent authorities establish at least one AI regulatory sandbox at the national level, which must be operational by 2 August 2026. This regulatory sandbox may also be established in collaboration with the competent authorities of other Member States. The European Commission may provide technical support, guidance, and tools to facilitate the establishment and operation of AI regulatory sandboxes.\n\nWithin AI regulatory sandboxes, developers have access to a controlled environment to test and refine new AI systems for a specified period before they are released to the market. The testing process is governed by a plan agreed upon by both the developers and the authorities, detailing the testing procedures.\n\nThe primary benefit of these sandboxes is the support and oversight provided by the competent authorities, which helps to mitigate risks. This includes ensuring compliance with fundamental rights, health and safety standards, and other legal requirements. Additionally, guidelines will be available to clarify regulatory expectations and requirements.\n\nDevelopers may request written documentation of the activities conducted within the sandbox. Furthermore, the authorities will prepare a report summarizing the activities performed and the results obtained, which developers can use in the context of the conformity assessment process.\n\nThe establishment of AI regulatory sandboxes aims to contribute to the following objectives:\n\n* improving legal certainty\n* supporting the sharing of best practices through cooperation with the authorities involved in the AI regulatory sandbox\n* fostering innovation and competitiveness and facilitating the development of an AI ecosystem;\n* contributing to evidence-based regulatory learning;\n* Facilitating and accelerating access of AI systems to the single market.\n\nDevelopers participating in the AI regulatory sandboxes remain liable under applicable EU and national liability laws for any damages caused to third parties as a result of testing conducted within the sandbox.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "9bacce57-261f-4716-adfa-6f5d904a65b4",
        "8c75b44d-51ac-48e9-b7d7-45c0362eaefb",
        "0af8ee0f-22ec-4daa-92dd-589a6b0f94f1",
        "ca5614f2-4cf4-460e-addd-b5609143a0cd"
      ],
      "parameters": []
    },
    {
      "id": "0e7c7bb9-4047-4ca7-80d7-09d34f8783b1",
      "title": "AI Service Desk FAQ: Our company is based in the EU and intends to develop high-risk AI systems that will be deployed exclusively outside the EU. Does the AI Act apply to these AI systems?",
      "content": "# Our company is based in the EU and intends to develop high-risk AI systems that will be deployed exclusively outside the EU. Does the AI Act apply to these AI systems?\n\nGenerally, the applicability of the AI Act (AIA) follows the \"marketplace principle,\" meaning that the regulations apply based on the market where the product is made available, regardless of the company's location. Specifically, the AIA applies if the AI system or AI model is placed on the market or put into service within an EU Member State (Art. 2(1)(a) AIA).\n\nIn this case, the company is indeed based in the EU but plans to develop high-risk AI systems intended solely for distribution and use outside the EU, thus targeting a different market. Therefore, the AIA does not apply to providers who exclusively export a product.\n\nHowever, the company may still be required to comply with operator obligations since it is based in the EU (Art. 2(1)(b) AIA). This obligation would apply only if the company also operates the AI system within the EU. As this is not the case here, the AIA does not apply.\n\nFurthermore, according to Art. 2(1)(c) AIA, the AI system must not produce outcomes that are subsequently used within the Union. If the system's use remains exclusively outside the EU, the AIA does not apply.\n\nIt should be noted, however, that AIA provisions may still apply according to Art. 2(1)(g) AIA if there are affected persons located within the EU. The rights of affected persons, as outlined in Art. 86 AIA and Recital 22, stipulate that data must have been lawfully collected within the EU. Therefore, additional legal provisions—including those from other legislation regarding the lawful collection of data, particularly from affected natural persons to develop this AI system—must still be observed.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "55e8665d-afb5-46d1-b258-64f964b08d09",
        "38517ea5-282c-498e-8559-1df8a9f2efdb"
      ],
      "parameters": []
    },
    {
      "id": "0d7f4977-182d-4253-899a-b8044ab8c652",
      "title": "AI Service Desk FAQ: We would like to use an AI to review application documents. Are we always subject to the legal requirements of Art 6 para 2 in conjunction with Annex III AI Act? When do the exceptions of Art 6 para 3 AI Act apply?",
      "content": "# We would like to use an AI to review application documents. Are we always subject to the legal requirements of Art 6 para 2 in conjunction with Annex III AI Act? When do the exceptions of Art 6 para 3 AI Act apply?\n\nIn principle, AI systems in relation to employment and personnel management fall within the scope of Annex III in conjunction with Art 6 (2) of the AI Act and are therefore categorised as high-risk systems in the risk level system of the AI Act.\n\nAccording to Art 6 para 3 AI Act, an AI system listed in Annex III is not considered a high-risk AI system if it does not pose a significant risk to the health, safety or fundamental rights of natural persons by, among other things, not significantly influencing the outcome of decision-making. This is particularly the case if the AI system is intended to perform a narrow procedural task (Art 6 para 3 lit a AI Act). The exception may also apply if the AI system is intended to improve the result of a previously completed human activity (Art. 6 para. 3 lit. b AI Act) or to recognise decision-making patterns or deviations from such patterns This includes restructuring data, reclassifying documents according to categories, recognising duplicates, etc. The AI system should assume a preparatory role and under no circumstances carry out assessments itself.\n\nIt should also be noted that, even if exceptions are accepted, there is a registration obligation under Art. 49 AI Act. (Recital 53)",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463",
        "7e595036-3aa8-4edf-b26f-0627760fb44e"
      ],
      "parameters": []
    },
    {
      "id": "bdd6f602-f1a1-4eff-916c-3d6962c88204",
      "title": "AI Service Desk FAQ: May I use ChatGPT as a formulation aid for various activities (e.g. writing a book or creating various documents etc.) Do I have to worry about any copyright consequences?",
      "content": "# May I use ChatGPT as a formulation aid for various activities (e.g. writing a book or creating various documents etc.) Do I have to worry about any copyright consequences?\n\nCopyright law protects peculiar intellectual creations of a creator defined in Section 10 of the Austrian Copyright Act (UrhG), who can only be a natural person. Based on this legal definition, an AI cannot be an author according to the current legal situation. You will usually find out whether the AI provider has separate claims in its general terms and conditions or other contractual agreements.\n\nHowever, this does not mean that there can be no copyright protection. For example, if you use an AI whose training may already have used copyright-protected content, the AI-generated output could also enjoy copyright protection. Some AI providers already restrict the training data to licensed content. In addition, some AI providers already guarantee the unobjectionable use of the generated content with a guarantee of indemnification and indemnity. It would be advisable to examine the specific warranties agreed under private law. Providers of GPAI models, such as a system for the synthetic generation of text, images and videos, must provide a copyright compliance strategy (Art. 53 para. 1 lit. c AIA) in accordance with the obligations of the AI Act from 2 August 2025 - these topics will generally be dealt with in more detail in this strategy.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [],
      "parameters": []
    },
    {
      "id": "d950c78a-f414-4640-9236-7f7800dbd8e7",
      "title": "AI Service Desk FAQ: Is a licence plate recognition system (ANPR) an AI system? If so, which risk level would it be assigned to?",
      "content": "# Is a licence plate recognition system (ANPR) an AI system? If so, which risk level would it be assigned to?\n\nAn automatic number plate recognition system (ANPR), which reads out the corresponding numbers and letters from photos of a licence plate and then makes this data available for further processing steps (in other words: the date of receipt is the photo of a licence plate, the date of issue is the licence plate read out) is, in our opinion, to be classified as a machine-supported system, which also has a certain degree of autonomy, even if manual corrections are made in individual cases. Whether this OCR system (or the licence plate recognition system as a whole - ANPR) is an AI system within the meaning of the AI Act cannot be definitively answered without a precise technical analysis. If the OCR system is based on algorithms that include machine learning in order to improve the accuracy of character recognition, among other things, it can be assumed that it is an AI system within the meaning of Art 3(1) of the AI Act. This would not be unusual, as many OCR systems are based on neural networks.\n\nThe consequences that follow the use of the ANPR can also be relevant for an assessment. For example: Are these access restrictions, exit permits or the calculation of parking fees? How are these systems interconnected? Are automatic decisions made? Does it instruct a machine to open the barrier automatically, for example?\n\nEven if this is an AI system, it is unlikely to be a high-risk application unless there are special use cases and it is used in a way that interferes with the fundamental rights and freedoms of natural persons, e.g. authorisation for an emergency doctor to enter a hospital, etc., or it is used as a security component in road traffic or as part of border controls. We would also like to point out that the regulations on high-risk AI systems (Art 6 in conjunction with Annex III) will enter into force on 2 August 2026. Until then, guidelines from the European Commission are also expected, which will allow a more detailed categorisation. Guidelines on the definition of an \"AI system\" have already been published and are available on our website at https://ki.rtr.at",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "7401fefb-6095-47f0-a00e-78aac879499e"
      ],
      "parameters": []
    },
    {
      "id": "4c59fc2e-ffda-4312-bde4-5869f4250dce",
      "title": "AI Service Desk FAQ: Does the use of a voice bot by companies have to be disclosed? Are there any regulations on this in the AI Act?",
      "content": "# Does the use of a voice bot by companies have to be disclosed? Are there any regulations on this in the AI Act?\n\nThe AI Act defines transparency obligations for AI systems in Art. 50, which come into force on 2 August 2026: https://www.rtr.at/rtr/service/ki-servicestelle/ai-act/Zeitplan.de.html\n\nSpecifically, providers of AI systems must ensure, in accordance with Art. 50 para. 1 AI Act, that AI systems intended for direct interaction with natural persons are designed and developed in such a way that the natural persons concerned are informed that they are interacting with an AI system, unless this is obvious from the perspective of a reasonably informed, attentive and reasonable natural person based on the circumstances and context of use. In other words, if a conversation is conducted with an AI, it would have to be disclosed that an AI is being interacted with.\n\nAI systems that generate or manipulate deepfake audio content must disclose that the content has been artificially generated or manipulated.\n\nThere is no such disclosure obligation for audio content that is not a deepfake (Art. 50 para. 4 subpara. 1 AIA). Whether a deepfake exists must always be assessed on a case-by-case basis - this would be the case, for example, if a known human voice is generated by the AI.\n\nThe AI Act defines the term \"deepfake\" as follows (Art. 3 para. 60 AIA): \"an AI-generated or manipulated image, sound or video content that resembles real persons, objects, places, facilities or events and would falsely appear to a person to be genuine or truthful\".\n\nFurther information can be found on our website: https://www.rtr.at/rtr/service/ki-servicestelle/ai-act/Transparenzpflichten.de.html",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "986b540c-851f-4387-b2d1-e9b5d9bed869"
      ],
      "parameters": []
    },
    {
      "id": "d6262dea-f475-4de3-9b68-d55068abe47b",
      "title": "AI Service Desk FAQ: As an entrepreneur using an AI system that is intended for direct interaction with natural persons, what do I have to consider? What obligations do I have to fulfil?",
      "content": "# As an entrepreneur using an AI system that is intended for direct interaction with natural persons, what do I have to consider? What obligations do I have to fulfil?\n\nProviders of AI systems that are intended for direct interaction with natural persons are subject to a transparency obligation under Art 50 para 1 AI Act. These AI systems must therefore be designed and developed in such a way that the natural persons concerned are informed that they are interacting with an AI system (unless the AI interaction is obvious from the point of view of an informed, attentive and reasonable natural person due to the circumstances and context of use).\n\nAI systems that are authorised for the legal detection, prevention and investigation or prosecution of criminal offences are exempt from the transparency obligation under Art 50 (1) AI Act.\n\nHowever, if you are categorised as an operator of such an AI system in the AI value chain, you are exempt from the above-mentioned transparency obligation.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "986b540c-851f-4387-b2d1-e9b5d9bed869"
      ],
      "parameters": []
    },
    {
      "id": "a9f296a9-f363-42d0-a4a2-540958409e4a",
      "title": "AI Service Desk FAQ: What obligations do I have to fulfil as an entrepreneur if I develop and resell an AI system that generates synthetic audio, image, video or text content?",
      "content": "# What obligations do I have to fulfil as an entrepreneur if I develop and resell an AI system that generates synthetic audio, image, video or text content?\n\nProviders of AI systems (including GPAI models) that generate synthetic content (audio, image, video or text) must ensure that the output of the AI is labelled in a machine-readable format and recognisable as artificially generated or manipulated. Exempt from this labelling requirement are AI systems that only have a supporting function, that do not change the input data provided or its semantics and that are legally permitted for the detection, prevention, investigation or prosecution of criminal offences.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "986b540c-851f-4387-b2d1-e9b5d9bed869"
      ],
      "parameters": []
    },
    {
      "id": "80e3c1a0-4654-47b1-a100-aa83bfd8ac58",
      "title": "AI Service Desk FAQ: What obligations do I have to observe as an entrepreneur if I have synthetic audio, image, video or text content generated by an AI?",
      "content": "# What obligations do I have to observe as an entrepreneur if I have synthetic audio, image, video or text content generated by an AI?\n\nThe operators of an AI system that generates or manipulates text content and/or generates or manipulates image, sound or video content that is a deep fake must disclose that the content has been artificially generated or manipulated in accordance with Art 50 para 4 AI Act. This excludes the use of the above-mentioned outputs for the detection, prevention, investigation and prosecution of criminal offences.\n\nA \"deep fake\" within the meaning of the AI Act is an image, sound or video content generated or manipulated by an AI that resembles real persons, objects, places, facilities or events and would falsely appear to a person to be genuine or truthful (see Art. 3 para. 60 AIA).\n\nIf it is obvious that the artificially created or manipulated image, sound or video content is part of an artistic, creative, satirical, fictional or analogue work or programme, the transparency obligation is limited to disclosing the presence of artificially created and manipulated content in such a way that the presentation or enjoyment of the work is not impaired.\n\nWith regard to text outputs, it should be noted that the above-mentioned transparency obligation only applies if the generated or manipulated text (output of the AI system) is used to inform the public about matters of public interest or has not been subject to human review or editorial control.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "986b540c-851f-4387-b2d1-e9b5d9bed869"
      ],
      "parameters": []
    },
    {
      "id": "67558b14-55fe-439d-98ee-1e4e559bc1e3",
      "title": "AI Service Desk FAQ: Technical Documentation AI Act Chatbot",
      "content": "The AI Act Chatbot of the AI Service Desk uses ‘Mistral Small’ from the French company Mistral as its large language model.\n‘Arctic Embed 2’ from Snowflake is used as the embedding model.\nThe AI Act Chatbot is operated on a root server at Hetzner in Germany. An NVIDIA RTX 4000 ADA graphics card is used.\nThe source code is published as open source on Github and linked at the bottom of the page.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [],
      "parameters": []
    },
    {
      "id": "e419d565-51f0-41b6-92b6-c80be68c2956",
      "title": "AI-Service Desk: Actors, AI value chain",
      "content": "###Overview: the actors in the AI Act\n\nThe AI Act provides for many roles in the AI value chain. The various players are by no means unknown, and the EU legislator orientated itself in many respects on EU product law standards (see, for example, the Product Safety Regulation and the Medical Devices Regulation).\n\nThe actors in the AI Act include, in accordance with Art. 3 (8):\n\nProvider;\nProduct manufacturer;\nAuthorised representative;\nImporter;\nDistributor;\nDeployer.\nThere are also \"users\" and \"affected persons\", but these are not referred to as actors within the meaning of the AI Act.\n",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "10c70d26-f011-44f0-89db-011879d8401c",
        "b757b57f-4015-401c-8003-852ba5aaefc0",
        "63f69c82-a393-43d5-bf47-0ea7b61acc5f",
        "89b37334-a989-4eca-bb8e-55165382038d",
        "116d04ba-85cc-42e0-9e8e-ac7f1460a22d",
        "d5cb7544-49fd-4991-8ab4-23d8dab0ff08",
        "dfe0e9c1-22e5-4ba8-ad77-a65c24cee14d",
        "a513a6fc-8abc-4dd4-b7b9-1b2c4520a19c"
      ],
      "parameters": []
    },
    {
      "id": "0a3fb82e-b170-41b1-bce4-6271864f0212",
      "title": "AI-Service Desk: Actors, placing on the market",
      "content": "### Placing on the market, making available, commissioning\n\nThe various players take on different activities in the AI value chain. A distinction is made between \"placing on the market\", \"making available on the market\" and \"putting into service\", which are also legally defined in the AI Act. These terms are not new, as they already exist in EU product law.\n\nAccording to Art. 3(9) of the AIA, \"placing on the market\" means \"the first making available of an AI system or a GPAI modelon the Union market.\" \n\nAccording to the definition, placing on the market also includes \"making available on the market\", which in turn means \"the supply of an AI system or a GPAI model for distribution or use on the Union market in the course of a commercial activity, whether in return for payment or free of charge (Art. 3 (10) AIA).\n\nThe difference between \"placing on the market\" and \"making available\" lies in the fact that placing on the market means making available on the market for the first time. Within the EU, the placing on the market is usually carried out by the manufacturer of the product or, in the case of the AI system/GPAI model the provider; for products or AI systems outside the EU, it is the importer. Once the initial provision has taken place, the further making available (colloquially referred to as \"distribution\") is usually carried out by the distributor, who takes over typical activities such as sales, storage, transport (e.g. in the case of high-risk AI systems integrated into physical products), customer support or maintenance.\n\nThis form of supply chain primarily represents the traditional way of a supply chain, where the product finds its way from the manufacturer via the importer and (intermediate) distributor to the final customer. Distance selling has also led to the development of new forms of distribution. Providers and distributors from third countries can offer their products on the Union market via online interfaces, which also allows providers in particular to exclude certain players (e.g. distributors). This form of sales plays a particularly important role for digital products such as software applications, which can be purchased via a simple download.\n\nThe Union legislator has also reacted to the changes in sales channels in the Market Surveillance Regulation (Recital 15): If a product is offered online or through other means of distance sales, the product should be considered to have been made available on the market if the offer for sale is targeted at end-users in the Union. Whether such an offer for sale is made to end-users is a case-by-case analysis and shall take into account criteria such as the geographical areas to which dispatch is possible, the languages available, used for the offer or order and the means of payment. The mere accessibility of the operator's or the intermediaries' website is not in itself sufficient to presume the making available on the Union market.\n\nAccording to Article 3(11) of the AIA, \"putting into service\" means \"the supply of an AI system for first use directly to the deployer or for own use in the Union for its intended purpose\". In short, \"intended purpose\" means the use for which an AI system is intended or promoted according to the provider (see Art. 3(12) AIA).\n",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6"
      ],
      "parameters": []
    },
    {
      "id": "18cba04a-f269-4f84-b97c-fbbbe2ef300f",
      "title": "AI-Service Desk: Actors, individual players",
      "content": "### The individual players\n\nDepending on which actor and which AI system or GPAI models is involved, different far-reaching obligations must be complied with. It is therefore of fundamental importance to differentiate between the individual addressees of the obligation. In certain situations, an actor can also fulfil several roles simultaneously. The following explanations and diagram are intended to provide an overview.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6"
      ],
      "parameters": []
    },
    {
      "id": "4353602b-f09f-41e8-b609-864a7ea1a448",
      "title": "AI-Service Desk: Actors, provider",
      "content": "### Who is the provider?\n\nIn Art. 3 (3) AIA, a provider is defined as follows:\n\na natural or legal person, public authority, agency or other body that develops an AI system or a general-purpose AI model or that has an AI system or a general-purpose AI model developed and places it on the market or puts the AI system into service under its own name or trademark, whether for payment or free of charge\nThe provider is the link in the AI value chain that develops the AI system or GPAI model. Typically, these are persons and institutions involved in the individual development phases - data preparation, model training, evaluation and optimisation - of AI systems or GPAI models.\n\nSince the development of an AI system or GPAI model usually involves several professional disciplines, several specialists such as data scientists, engineers, domain experts and designers often work together. A provider is therefore also considered to be a provider if it has the AI system or a GPAI model developed (in other words, \"commissions\" it) in order to subsequently place it on the market or put it into service under its own name or trademark, whether for a payment or free of charge. Providers may also outsource certain tools, services, components or processes, such as training, retraining, testing and evaluation of models, integration into software or other aspects of model development. However, the obligations in connection with the AI Act apply more broadly to providers.\n\nNote: In order to ensure that the AI Act is also applied in these situations, the obligations must be contractually regulated. In this regard, the AI Office is authorised to draw up and make available model contractual provisions (Art. 25 para. 4 AI Act).\n\nA provider is also referred to when it integrates an AI model into its AI system or GPAI system. It is irrelevant whether the model is provided and vertically integrated by the provider itself or comes from someone else. In this case, the term \"downstream provider\" is used (see Art. 3 para. 68 AIA)\n\nThe rule in Art. 25 para. 1 AIA allows also treats importers, distributors, deployers and other third parties to be treated as providers. They assume this dual role if they:\n\na high-risk AI system that has already been placed on the market or put into operation is labelled with their name or trademark, irrespective of contractual agreements stipulating that the obligations are otherwise allocated. The respective actor thus acts as the provider of an AI system, even though it was not developed by them (\"quasi-manufacturer\"). \nmake a material change to a high-risk AI system that has already been placed on the market or put into operation in such a way that it remains a high-risk AI system within the meaning of Art. 6 AIA.\nA substantial modification is deemed to be made if it changes an AI system after it has been placed on the market or put into service in such a way that this was not provided for or planned in the original conformity assessment and thereby impairs the conformity of the AI system or leads to a change in the intended purpose for which the AI system was assessed, Art. 3 no. 23 AI Act.\nmodification of the intended purpose of an AI System, including a GPAI System that has not been classified as high-risk and has already been placed on the market or put into service, in such a way that the AI System concerned becomes a high-risk AI System within the meaning of Art. 6 AIA.\nTo summarise: When an actor adds its name to an AI system, significantly modifies it or changes its intended purpose after it has already been placed on the market or put into operation, it is considered a quasi-manufacturer and thus a provider with corresponding obligations.\n\nThe application of Art. 25 para. 1 AIA in the case of deployers is summarised in the chart below.\n\n\n",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "63f69c82-a393-43d5-bf47-0ea7b61acc5f",
        "6b073c5f-3422-4b34-b139-b8645e89b463",
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6"
      ],
      "parameters": []
    },
    {
      "id": "125e0d9e-7308-4567-a0ec-8a82fa1b0d33",
      "title": "AI-Service Desk: Actors, product manufacturer",
      "content": "### Who is the product manufacturer?\n\nAlthough product manufacturers are listed as actors, there is no corresponding definition in the AI Act. As Annex I of the AI Act refers to EU product law standards, the term product manufacturer in the AI Act refers to the manufacturer of the respective product.\n\nFor example, a manufacturer within the meaning of the Lifts Directive is \"any natural or legal person who manufactures a safety component for lifts or has such a component designed or manufactured, and markets it under his own name or trade mark\" (Article 2(8) of the Lifts Directive).\n\nIf an AI system is built into a product as a safety component, the product manufacturer must fulfil the obligations of a provider of an AI system set out in the AI Act. In these cases, the product manufacturer is treated as a provider.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6"
      ],
      "parameters": []
    },
    {
      "id": "aecae742-1970-41a4-aaa5-7d2389209d72",
      "title": "AI-Service Desk: Actors, authorized representative",
      "content": "### Who is the authorised representative?\n\nPursuant to Art 3(5) AIA, an authorised representative is defined as follows:\n\na natural or legal person located or established in the Union who has received and accepted a written mandate from a provider of an AI system or a general-purpose AI model to, respectively, perform and carry out on its behalf the obligations and procedures established by this Regulation.\nAn authorised representative is necessary if the provider of the AI system or the GPAI model is not established within the Union. In order to ensure compliance with the requirements of Union law for AI systems or GPAI models, the appointment of an authorised representative is mandatory. The authorised representative ensures compliance with the obligations set out in the AI Act on behalf of the providers of the AI system or GPAI model and also conducts proceedings. This obligation does not apply to providers of exempt open source models and systems within the meaning of Art. 2 para. 12 AIA.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "55e8665d-afb5-46d1-b258-64f964b08d09",
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6"
      ],
      "parameters": []
    },
    {
      "id": "fe38e634-50f1-4fcd-972e-428ed0b00086",
      "title": "AI-Service Desk: Actors, importer",
      "content": "### Who is the importer?\n\nAccording to Art. 3(6) AIA, an importer is defined as follows:\n\na natural or legal person located or established in the Union that places on the market an AI-system that bears the name or trademark of a natural or legal person established in a third country.\nThe importer is the link in the AI value chain that places on the market an AI system from a provider outside the EU. The importer therefore only plays a role for AI systems from third countries. In certain situations, the role of the importer may coincide with that of the distributor, namely when the importer purchases the AI system from the provider and makes it available at the same time.\n\nUnder certain circumstances, the importer may also be treated as a provider (see Art. 25 para. 1 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "63f69c82-a393-43d5-bf47-0ea7b61acc5f",
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6"
      ],
      "parameters": []
    },
    {
      "id": "5258a087-d836-41b0-b6ce-6df5f13bedc9",
      "title": "AI-Service Desk: Actors, distributor",
      "content": "### Who is a distributor?\n\nA distributor is legally defined as follows pursuant to Art. 3(7) AIA:\n\na natural or legal person in the supply chain, other than the provider or the importer, that makes an AI system available on the Union market.\nThe distributor is the person in the AI value chain who makes available an AI system on the Union market. The mere making available comes after the placing on the market in terms of time.\n\nUnder certain circumstances, the distributor may also be treated as a provider (see Art. 25 para. 1 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "63f69c82-a393-43d5-bf47-0ea7b61acc5f",
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6"
      ],
      "parameters": []
    },
    {
      "id": "54902079-8fec-477d-81b8-12abdae3a221",
      "title": "AI-Service Desk: Actors, deployer",
      "content": "### Who is the deployer?\n\nAccording to Art. 3(4) AIA, a deployer is defined as follows:\n\na natural or legal person, public authority, agency or other body using an AI system under its authority, except the AI system is used in the course of a personal non-professional activity.\nThe deployer is the link in the AI value chain that uses an AI system under its own responsibility. The institutions, bodies, authorities and agencies of the Union are mentioned as examples (recital 23). In the AI value chain, they are downstream of the providers; they implement the AI system in practice. The deployer is the person, authority, etc. that decides whether and how an AI system is used and is also responsible for this. The purely private use of AI systems is excluded from the definition of deployer.\n\nUnder certain circumstances, the deployer may also be treated as a provider (see Art. 25 para. 1 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "a010e5cb-6b93-499f-ad13-0a86a4c6239a",
        "63f69c82-a393-43d5-bf47-0ea7b61acc5f",
        "89b37334-a989-4eca-bb8e-55165382038d",
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6"
      ],
      "parameters": []
    },
    {
      "id": "356bb808-a31d-4d14-9273-1da6952a312a",
      "title": "AI-Service Desk: Actors, user",
      "content": "### Who is a user?\n\nThe term \"user\" appears in the AI Act, but is not legally defined. The term user is mentioned in various places in the AI Act, e.g. in Annex XIII as \"end-users\" and \"business users\" or in various recitals (see recital 16: \"to allow users\"; recital 102: \"software and data, including models [...] that users can freely access, use, modify and redistribute them\", \"if it allows users\"). In the version approved by Parliament , this term was partially equated with the operator.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "d605b91c-4f8d-4ba9-85fa-6dfbd07016e5",
        "b908b1fb-c1a0-41d0-85cd-96d3d195d360"
      ],
      "parameters": []
    },
    {
      "id": "7336e729-b4b4-4470-8d51-db20970705cc",
      "title": "AI-Service Desk: Actors, \"affected\" persons",
      "content": "###  \"Affected\" persons\n\nDepending on the type of AI system, the use may affect persons other than the operator (recital 13 AIA). This refers to those persons who are the subject to the use of the AI system or who are otherwise affected by the use of an AI system.\n\nExamples of this can also be found in the AI Act, such as the use of a chatbot or generating synthetic audio, image, video or text content such as deepfakes (see Art. 50 para. 1 and 2 AIA), the use of an emotion recognition system or a system for biometric categorisation (see Art. 50 para. 3 AIA), the use of a real-time biometric remote identification system (Art. 5 para. 2 AIA) or the commissioning or use of a high-risk AI system in the workplace (Art. 26 para. 7 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "b757b57f-4015-401c-8003-852ba5aaefc0",
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "54d92cd3-752d-4303-b8e9-f8c7efac0e64"
      ],
      "parameters": []
    },
    {
      "id": "9be93838-5d82-4573-a63f-81a708c77fa7",
      "title": "AI-Service Desk: Authorities & Bodies, AI-Office",
      "content": "##  What is the European Artificial Intelligence Office (\"AI Office\") and what does it do?\n\nhe AI Office was established by a Commission decision and is part of the Commission's Directorate-General for Communication Networks, Content and Technologies (DG CNCT).  Through the AI Office, the Commission is supposed to develop the Union's expertise and capabilities in the field of artificial intelligence (Art. 64 AIA). The tasks assigned will be carried out by civil servants already entrusted with these tasks. The tasks assigned to the AI Office can therefore already be performed.\n\nThe AI Office performs a wide range of tasks. These are laid down in the AI Act on the one hand and specified in the Commission decision on the other. In carrying out its tasks, the AI Office must cooperate with relevant stakeholders (science, AI developers, civil society, social partners, etc.), EU and national authorities and bodies.\n\nThe tasks include, among others:\n\nMonitoring the implementation and application of the AI Act in general. This includes in particular the following tasks:\nAssisting the Commission in the preparation of relevant Commission decisions, and implementing and delegated acts; preparing guidance and guidelines to support the practical implementation of the AI Act; preparing standardisation requests, evaluating existing standards and drafting common specifications;\nContributing to the provision of technical support, advice and tools for the establishment and operation of AI regulatory sandboxes;\nConducting evaluations and reviews and preparing reports\ncoordinating the establishment of an effective governance system, including by preparing the set-up of advisory bodies at Union level;\nProviding the Secretariat for the AI Board and its subgroups, as well as providing administrative support to the Advisory Forum and the Scientific Panel;\nEncouraging and facilitating the drawing up of codes of practices and codes of conducts .\n\n \n\nMonitoring the implementation and application of the AI Act in relation to GPAI. This includes in particular the following tasks:\nMarket surveillance authority for GPAI schemes where the model and scheme originate from the same provider (Art 75 AIA);\nDeveloping of tools, methods and benchmarks to assess the capabilities of GPAI, especially for very large GPAI with systemic risks;\nMonitoring the emergence of unforeseen risks stemming from GPAI models, including by responding to alerts from the Scientific Panel;\nCoordinating the supervision and enforcement of legislation for which the Commission has supervisory and enforcement powers (e.g. Digital Services Act or Digital Markets Act);\nSupporting the implementation of the rules on prohibited AI practices and high-risk AI systems in coordination with the relevant bodies responsible under sectoral legislation, including facilitating the exchange of information and collaboration between national authorities, collecting notifications and establishing information platforms and databases, in particular when a GPAI model or system is integrated into a high-risk AI system.\n\n \n\nCooperation with stakeholders, monitoring the implementation and application of the AI Act;\nInternational cooperation with third countries and international organisations to ensure a contribution to a strategic, coherent and effective Union approach to AI in coordination with Member States and in line with Union positions and policies;\nPromoting cross-sectoral cooperation within the Commission;\nSupporting the development, deployment, roll-out and use of trustworthy AI systems and applications that bring societal and economic benefits and contribute to the Union's competitiveness and economic growth. In particular, the promotion of innovation ecosystems through cooperation with relevant public and private stakeholders and the startup community;\nMarket monitoring of AI markets and technologies.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "e0492e6c-e987-41c3-b664-775286020859",
        "63ae5b47-b44b-4b4b-82ac-5fa868177399"
      ],
      "parameters": []
    },
    {
      "id": "73b4b412-4437-4a0a-a3de-75077b666509",
      "title": "AI-Service Desk: Authorities & Bodies, AI-Board",
      "content": "## What is the European Artificial Intelligence Board (AI Board) and what does it do?\n\nThe AI Board is made up of one representative from each Member State, the European Data Protection Supervisor and the AI Office. The European Data Protection Supervisor is acting as an observer and the AI Office is not participating in votes. Other national and European authorities, bodies or experts may be invited for relevant topics. The meeting is chaired by a representative of a Member State (Art. 65 para. 2 AIA).\n\nRepresentatives are elected by the member states for three years and can be re-elected once. The following criteria must be considered when electing the representative (Art. 65 para. 4 AIA):\n\nThe representative must have the relevant national competences and power to contribute to the tasks of the AI Board;\nThe representative must have the power to facilitate the implementation of this regulation in their Member State, for example by collecting relevant data;\nThe representative must be designated as the single contact point between the Member State and the AI Board; where appropriate, the representative should also serve as a single contact point for stakeholders in their Member State.\n\nThe objectivity and impartiality of the AI Board must be guaranteed.\n\nThe AI Board has according to Art. 66 AIA the following tasks, among others:\n\nContributing to the coordination between the national competent authorities for the application of this Regulation and supporting the market surveillance authorities\nCooperating with other Union institutions, bodies, offices and agencies and competent authorities of third countries and international organisations\nCollecting and exchanging technical and regulatory expertise and best practices among Member States;\nAdvising on the implementation of this Regulation, in particular with regard to the enforcement of the rules on GPAI;\nContributing to the harmonisation of administrative practices (conformity assessment procedures, regulatory sandboxes and real-life testing) in the Member States;\nProviding recommendations and written opinions on all relevant issues related to the implementation and application of this Regulation.\n\nThe AI Board consists of two standing sub-groups to provide cooperation and exchange among market surveillance authorities and the notifying authorities. Further standing or temporary sub-groups may be established to fulfil the tasks.\n\nThe AI Office shall provide the Secretariat for the AI Board.\n\nThe Advisory Forum should not be confused with the AI Advisory Board (often also referred to as the “KI-Beirat”) in Austria!",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "bf998658-e660-43ca-8849-a1cc9c46930c",
        "e684a05a-dce1-41e7-9743-fd5b1b778be8",
        "4035dbfb-54eb-4161-9b2d-4974bd7440ba"
      ],
      "parameters": []
    },
    {
      "id": "e8636770-3900-45b1-a25d-a32616c51a42",
      "title": "AI-Service Desk: Authorities & Bodies, Advisory Forum",
      "content": "## What is the Advisory Forum and what does it do?\n\nThe Advisory Forum serves according to Art. 67 AIA as an advisory body on technical matters for the Commission and the AI Board and consists of stakeholders from industry, start-ups, the SME sector, academia, think tanks and society. The composition is determined by the Commission, which must ensure a balance between commercial and non-commercial interests. \n\nThe Fundamental Rights Agency, the European Union Agency for Cybersecurity (FRA), the European Union Agency for Cybersecurity (ENISA), the European Committee for Electrotechnical Standardisation (CENELEC) and the European Telecommunications Standards Institute (ETSI) are permanent members of the advisory body.\n\nThe members must have proven expertise in the field of artificial intelligence. The term of office is two years and can be extended by a maximum of four years. It meets at least twice a year and must draw up an annual activity report, which must be published.\n\nThe Advisory Forum is according to Art. 67 para. 8 AIA responsible for the preparation of:\n\nOpinions;\nRecommendations;\nWritten contributions.\n\nStanding or temporary sub-groups may be formed to fulfil these tasks.\n\nThe Advisory Forum should not be confused with the AI Advisory Board (often referred to as “KI-Beirat”) in Austria!",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "199c6e64-698e-4e3f-a806-9ff9d364cc85",
        "957b417e-f553-4553-aa90-daea77a9a036"
      ],
      "parameters": []
    },
    {
      "id": "7f776c36-7b5f-47d2-b5bc-43cf3bc74d3e",
      "title": "AI-Service Desk: Authorities & Bodies, Scientific Panel",
      "content": "## What is the Scientific Panel of Independent Experts and what does it do?\n\nAccording to Art. 68 para. 1 AIA the Commission shall set up a Scientific Panel of independent experts by means of an implementing act to support the enforcement measures under this Regulation.\n\nThe Commission shall select the members on the basis of up-to-date scientific or technical expertise in the field of artificial intelligence necessary for the tasks assigned to the Scientific Panel. The number of members shall be determined by the Commission in consultation with the AI Board on the basis of the required demand. When making appointments, it takes into account a balanced gender and geographical representation.\n\nExperts must fulfil the following criteria (Art. 68 para. 2 AIA):\n\nParticular expertise and competence as well as scientific or technical expertise in the field of Artificial Intelligence;\nIndependence from any provider of AI systems or GPAI models or systems;\nAbility to carry out the activities diligently, accurately and objectively.\n\nThe Scientific Panel provides the following advisory and support services (Art. 68 para. 3 AIA):\n\nImplementation and enforcement of this Regulation, esp. in relation to GPAI models, including in particular the following activities:\nAlerting the AI Office of possible systemic risks associated with GPAI models at Union level;\nContributing to the development of tools and methodologies to assess the capabilities of GPAI models, including through benchmarks;\nAdvising on the classification of GPAI models with systemic risk;\nAdvice on the classification of different GPAI models and systems;\nContributing to the development of tools and templates.\nSupporting the work of market surveillance authorities at their request;\nSupporting cross-border market surveillance activities;\nSupporting the AI Office in carrying out its duties in the context of the \"safeguard clause procedure\" to Art 81 AIA.\n\nThe members of the Scientific Panel fulfil their tasks independently, impartially, and objectively. They guarantee the confidentiality of the information and data they receive in the performance of their duties and activities. Each member submits a declaration of interests, which is made publicly available.\n\nThe AI Office implements systems and procedures to actively manage and prevent potential conflicts of interest.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "6efc5cc4-f5b5-4581-bc5d-10d2aeca4486",
        "47710b63-0230-41a5-80af-2581299ac9d6",
        "af44a0ad-8425-4b78-8b03-21093d6fc548"
      ],
      "parameters": []
    },
    {
      "id": "8ce6bcf0-99b5-4801-a15c-e73a5c5355f4",
      "title": "AI-Service Desk: Provider obligations, generally",
      "content": "### Die einzelnen Akteure und deren Verpflichtungen\n\nAs already stated in the recitals of the AI Act, given the nature and complexity of the AI value chain and in line with the new legal framework, it is essential to ensure legal certainty and facilitate compliance with this Regulation. Therefore, the roles and specific obligations of the relevant actors along the value chain need to be clarified. The core of the AI Act concerns obligations related to AI systems or AI models according to their risk classification.\n\nIn certain situations, different roles in the AI value chain can coincide in one person or organisation. The following constellations are possible, for example:\n\nA provider of AI models or GPAI models may also be a provider of an AI system (Art. 75 AIA).\nAn importer can also assume the role of a trader (see recital 83)\n",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "09b711a2-571c-4a78-9b84-60675ea83b5e",
        "41caa9c8-36d6-4917-badd-540c9137f2cf",
        "0a83841c-2931-4e56-a212-b73e3ab410f1",
        "63f69c82-a393-43d5-bf47-0ea7b61acc5f",
        "89b37334-a989-4eca-bb8e-55165382038d"
      ],
      "parameters": []
    },
    {
      "id": "e1c5a1de-3024-4178-a191-283ad1525b65",
      "title": "AI-Service Desk: Provider obligations, AI literacy",
      "content": "### What obligations do providers have?\n\nIn general, providers of AI systems of any kind have a duty to ensure that their staff and other persons entrusted with the AI systems have a sufficient level of AI competence (Art. 4 AIA). This includes the skills, knowledge and understanding that enable providers, considering their respective rights and obligations under the AI Act, to make an informed deployment of AI systems and to be aware of the opportunities and risks of AI and the potential harm it can cause (Art. 3 No. 56 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "09b711a2-571c-4a78-9b84-60675ea83b5e",
        "21ada015-1d50-4ddb-97d5-33b540e849cb",
        "f9b21785-6f87-4438-8810-147304eb4d7e",
        "550c995f-0a8e-4714-b73e-9418fb982eed",
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6"
      ],
      "parameters": []
    },
    {
      "id": "f707c5a0-6873-4780-802e-51814906b167",
      "title": "AI-Service Desk: Provider obligations, High risk AI Systems",
      "content": "#### High-risk AI systems\n\nThese are AI systems that pose a high risk. They are not prohibited, but in some cases far-reaching obligations must be complied with. The obligations of providers of high-risk AI systems are standardised in Art. 16 of the AIA. The obligations include ensuring compliance with the requirements for high-risk AI systems in accordance with Art. 16 letter a in conjunction with Chapter III Section 2:",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "86806ab4-b869-4e14-8e9f-39249fac5a50",
        "d5118a3b-1529-4362-9b1b-c99a8dd748ec",
        "0fa0541d-b4e5-48ce-b134-aff5e22c8eec"
      ],
      "parameters": []
    },
    {
      "id": "0d630708-202b-4ce7-84f0-930e77637a94",
      "title": "AI-Service Desk: Provider obligations, Risk management system",
      "content": "-   Risk management system\n    \nA risk management system must be established, implemented, documented and maintained (see Art. 9 para. 1 AIA). Risk management is understood as a continuous iterative, i.e. repetitive, process throughout the entire life cycle. The provider of high-risk AI systems is subject to risk assessment and mitigation obligations.\n\nKnown and reasonably foreseeable risks to health, safety or fundamental rights must be identified, estimated and evaluated. In addition to possible risks in accordance with the intended purpose of the AI system, reasonably foreseeable misuse must also be assessed. Risks that only become apparent after placing on the market, in accordance with Art. 72 AIA, must also be assessed. Based on the risks identified, \"appropriate and targeted\" risk management measures must be taken.\n\nThe risk management system also requires that high-risk AI systems are tested throughout the entire development process (e.g. through tests under real conditions outside of AI real laboratories in accordance with Art. 60 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "6d7d2b9c-9224-4f54-ad18-671918597d0c",
        "1d3e3007-9327-4f7b-ba9b-44bd93c2f4ae",
        "25407f28-0e07-4101-a714-b193ca14783e",
        "cbfbb51e-9675-4e61-b006-d153a588f1e1",
        "5b7ccaab-a361-4b2e-bf6c-67a90fd684eb"
      ],
      "parameters": []
    },
    {
      "id": "9cf28dba-9ed8-4cfb-ada5-5eb80160e94e",
      "title": "AI-Service Desk: Provider obligations, Data and data governance",
      "content": "-   Data and data governance\n    \nAI systems are generally based AI models. If the AI models used in high-risk AI systems have been trained with data, training, validation and testing data sets must be developed that meet the quality requirements of Art. 10 para. 2 to 5 AIA (see Art. 10 para. 1 AIA). Among other things, these data sets must be assessed for availability, quantity and suitability, possible distortions must be checked (bias), and data gaps or deficiencies must be identified.\n\nData sets must be sufficiently representative, to the best extent possible error-free and complete and must be used that are typical of the geographical, contextual, behavioural or functional conditions under which the high-risk AI system is intended to be used; e.g. autonomous driving systems - these systems must be strongly prepared for safety-critical decisions and function in a wide range of geographical and contextual situations (e.g. unfavourable weather conditions).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "69b883ca-3e2d-46ed-b797-e2c62832a376",
        "2aedd016-7002-471a-af6e-131b4f9f1f54",
        "316fe982-49dc-4467-a230-7b526feb2812",
        "53ee5174-71e9-4bc8-81f2-251f7a1bc278"
      ],
      "parameters": []
    },
    {
      "id": "aea615d8-72d9-4ae1-9dac-351e485258fc",
      "content": "-"
    },
    {
      "id": "65ecc4bf-ac73-4860-a566-328ac49a6c41",
      "title": "AI-Service Desk: Provider obligations, Technical documentation",
      "content": "- Technical documentation\n    \n The technical documentation in accordance with Art. 11 AIA must be prepared before placing on the market or commissioning a high-risk AI system. The minimum details for technical documentation are listed in Annex IV. The technical documentation must be prepared in such a way that it provides evidence that the requirements for high-risk AI systems are met. It must enable competent authorities and notified bodies to assess the compliance of the AI-system with those requirements.\n\nSMEs, including start-ups, may provide the elements of technical documentation listed in Annex IV in a simplified manner.\n\nWith regard to devices listed in Annex I Section A of the harmonisation regulations, a single set of technical documentation shall be created that also covers the requirements of the AI Act in addition to the general documentation. For example, the Medical Device Regulation also standardises the obligation for device manufacturers to prepare technical documentation (Art. 10 para. 4 MDR).\n\nTo facilitate compliance with this requirement, TÜV AUSTRIA has published a best practice guideline coordinated with RTR.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "355be535-f654-47e6-9608-95369e856b37",
        "817c9521-a9db-4149-99df-86ce3f02b215"
      ],
      "parameters": []
    },
    {
      "id": "4de289ef-a262-4a5c-a5da-b85a80d53dc7",
      "title": "AI-Service Desk: Provider obligations, Record-keeping- \"Logging\"",
      "content": "-   Record-keeping- \"Logging\"\n    \nHigh-risk AI systems must be technically designed and developed in such a way that they allow the automatic recording of events - so-called \"logging\" (see Art. 12 AIA). This logging is used for documentation purposes, in particular to determine whether the high-risk AI system presents a risk within the meaning of Art. 79 para. 1 AIA or whether a \"substantial modification\" has been made.\n\nThe high-risk AI systems (remote biometric identification systems) referred to in Annex III(1)(a) of the AIA must fulfil specific logging functions.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "40622ac2-1630-4d8b-a8a6-0aadd9c20248",
        "817c9521-a9db-4149-99df-86ce3f02b215",
        "069d1fdf-6329-4927-865f-862b02fbc7c1",
        "90ac7181-45af-41f1-817b-5a51223d7825"
      ],
      "parameters": []
    },
    {
      "id": "23225ccc-255f-4ebf-841a-ba906dbf896d",
      "title": "AI-Service Desk: Provider obligations, Human supervision",
      "content": "-   Human supervision\n    \nHigh-risk AI systems must be designed and developed in such a way that they can be effectively overseen by natural persons for the duration of their use (see Art. 14 AIA). The purpose of this requirement is to prevent or minimise risks to health, safety and fundamental rights, as it cannot be ruled out that risks may persist despite compliance with all the requirements of Chapter III Section 2.\n\nAppropriate supervisory measures must be taken in accordance with the risks, the degree of autonomy and the context of use of the high-risk AI system. These can be precautions of a technical nature that are built into the high-risk AI system and/or precautions that are to be implemented by the deployer.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "67156c3d-d006-41f0-87e3-725cd933dbdb",
        "550c995f-0a8e-4714-b73e-9418fb982eed"
      ],
      "parameters": []
    },
    {
      "id": "9a15e8cd-db82-492d-8897-c2b4ef6d9039",
      "title": "AI-Service Desk: Provider obligations, Accuracy, robustness and cyber security",
      "content": "-   Accuracy, robustness and cyber security\n    \nHigh-risk AI systems must be designed and developed in such a way to achieve an appropriate level of accuracy, robustness and cybersecurity throughout their lifecycle (see Art. 15 AIA).\n\n“Accuracy” refers to the extent to which a model's predictions or classifications match the actual data. It is a measure of how well the model is able to make the correct predictions.\n\n“Robustness” describes the resilience of high-risk AI systems; these must be as resilient as possible to errors, faults, inconsistencies or unexpected situations that may occur within the system or the environment in which the system operates, in particular due to its interaction with natural persons or other systems (Recital 75).\n\n“Cybersecurity” plays a critical role in ensuring that AI systems are resilient to attempts by malicious third parties to exploit the systems' vulnerabilities to alter their use, behaviour, performance or compromise their security features (Recital 76).\n\nThe requirements for accuracy, robustness and cyber security are largely technical aspects, which is why these measurements are to be ensured using benchmarks and measurement methods to be developed. In addition to technical measures, organisational measures must also be taken.\n\nPossible measures include backup or disruption security plans (see Art. 15 para. 4 subpara. 2 AIA), measures to minimise the risk of so-called \"feedback loops\" and measures (see Art. 15 para. 4 subpara. 3 AIA) to prevent attacks that attempt to manipulate the training data set (\"data poisoning\") or pre-trained components used in training (\"model poisoning\"), or to carry out attacks that attempt to manipulate the training data set (\"data poisoning\") or pre-trained components used during training (\"model poisoning\"), input data intended to mislead the AI model into making errors (\"adversarial examples\" or \"model evasions\") (see Art. 17 para. 5 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "91bb9b2c-b571-4042-8143-f399e538ffe2",
        "3b55e0a7-53c5-443d-a628-f905ea635201",
        "c9ddca61-d229-4d40-b57a-f7c249377ecb",
        "f13362fc-1e4e-4ac2-80ec-5def8ff64f2a",
        "3c1ccff0-1d4e-4217-9828-90e5bb65d611",
        "63c8bc3d-ba5f-4134-97e6-4294cf1e2697",
        "86806ab4-b869-4e14-8e9f-39249fac5a50"
      ],
      "parameters": []
    },
    {
      "id": "2ef9a966-cbd1-46a6-b276-cf7bfbbff1df",
      "title": "AI-Service Desk: Provider obligations, Quality management",
      "content": "-   Quality management\n    \n Providers of high-risk AI systems must set up a quality management system that ensures compliance with this Ordinance. Rules, procedures and instructions must be documented (see Art. 17 AIA). Among other things, this system must contain a concept of how regulatory provisions and conformity assessment procedures are to be complied with.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "e3935b15-c152-49db-9b69-2166080067c1",
        "da5fac2f-a0db-40a4-8e4c-03c6125fa585"
      ],
      "parameters": []
    },
    {
      "id": "acd0edef-f67f-4d8f-9c99-76bc6ebe0092",
      "title": "AI-Service Desk: Provider obligations, Labelling",
      "content": "-   Labelling\n    \nProviders of high-risk AI systems must indicate their name, registered trade name or trade mark and contact address on the AI system itself or, if this is not possible, on its packaging or accompanying documentation (see Art. 16 letter b AIA)",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "86806ab4-b869-4e14-8e9f-39249fac5a50",
        "d5118a3b-1529-4362-9b1b-c99a8dd748ec",
        "0fa0541d-b4e5-48ce-b134-aff5e22c8eec"
      ],
      "parameters": []
    },
    {
      "id": "4052397b-e75f-463d-8380-06bb7be621a1",
      "title": "AI-Service Desk: Provider obligations, Retention of certain documents and log events ",
      "content": "-   Retention of certain documents and log events \n    \n For a period of ten years from placing on the market or putting into service, keeping of documents such as the technical documentation in accordance with Art. 11 AIA, the documentation within the meaning of Art. 18 AIA, documentation on changes authorised by the notified bodies, any decisions and other documents issued by the notified bodies and the EU Declaration of Conformity in accordance with Art. 47 AIA (see Art. 18 AIA).\n\nThe automatically generated logs must be retained for a period of six months in accordance with Art. 12 para. 1 AIA (see Art. 19 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "b4faa5d7-2c2b-43c9-b793-47d6579e2fc8",
        "817c9521-a9db-4149-99df-86ce3f02b215",
        "da5fac2f-a0db-40a4-8e4c-03c6125fa585",
        "40622ac2-1630-4d8b-a8a6-0aadd9c20248",
        "45c3bd82-bba0-434b-96d1-a4036c68969c"
      ],
      "parameters": []
    },
    {
      "id": "006419f7-d820-4171-aa0a-bc6e78eff100",
      "title": "AI-Service Desk: Provider obligations, Registration of the AI system ",
      "content": "-   Registration of the AI system \n    \nBefore placing on the market or putting into service high-risk AI systems within the meaning of Annex III (with the exception of point 2: Critical infrastructure), the provider must register the high-risk AI system in the EU database within the meaning of Art. 71 AIA (see Art. 49 para. 1 AIA)",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "bea96906-be50-4eb8-9cbe-cf07816645c5",
        "e85b8add-b4ef-4422-bafa-b843f3f02662",
        "90893d61-25fd-4ae1-8232-5e4cdb5675e9"
      ],
      "parameters": []
    },
    {
      "id": "ee52640d-4f93-446a-9a94-aa8b1225ddb9",
      "title": "AI-Service Desk: Provider obligations, Co-operation with the competent authorities",
      "content": "-   Co-operation with the competent authorities\n    \nUpon reasoned request by a competent authority, providers of high-risk AI systems must provide all information and documentation, including the automatically generated logs, if they have access to them (see Art. 21 AIA)",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "ff73c457-0b7d-4690-9417-d68464a484ea"
      ],
      "parameters": []
    },
    {
      "id": "be9d0595-f380-4ba4-9b06-207fb395a8e4",
      "title": "AI-Service Desk: Provider obligations, Conformity assessment, declaration and labelling",
      "content": "-   Conformity assessment, declaration and labelling\n    \n The provider of high-risk AI systems must ensure that a conformity assessment procedure is carried out (Art. 43 AIA). Depending on the high-risk AI system in question, this can be carried out on the basis of an internal inspection or with the involvement of a notified body. Furthermore, a declaration of conformity assessment must be issued (Art. 47 AIA) and a CE marking must be affixed to the AI system itself or, if this is not possible, on its packaging or in the accompanying documentation (Art. 48 AIA). ",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "aeae4d53-6d81-4c49-843d-b93704615c05",
        "53305eb0-be55-40af-8070-94d5214e877c",
        "9819f1c8-9e45-4ff4-8edf-575415272e9f",
        "dffe5a54-c8df-4391-9a4c-7180f3dbda14",
        "33677500-8ade-4c9b-96ec-eff3f9ffb701",
        "c7c4a6e2-92a4-40d5-9243-1d698685d1a0",
        "90893d61-25fd-4ae1-8232-5e4cdb5675e9",
        "5668652e-ee66-42a3-9cd2-d8f70595e52e"
      ],
      "parameters": []
    },
    {
      "id": "d44da4c3-33ab-48fa-85e2-f47de3562bb7",
      "title": "AI-Service Desk: Provider obligations, Obligation to report serious incidents to the competent authorities",
      "content": "-   Obligation to report serious incidents to the competent authorities\n    \nIn the event of serious incidents involving high-risk AI systems placed on the market, the provider must notify the national market surveillance authority where the incident occurred. A notification must be made immediately after the causal link or the reasonable likelihood of such a link has been established, but in any case no later than 15 days after becoming aware of the serious incident (see Art. 73 AIA). There are stricter time requirements for this basic rule for certain incidents.\n\nAccording to Article 3(49) of the AIA, a serious incident refers to incidents or malfunctions of an AI system that directly or indirectly results in death of a person or serious harm to a person’s health, serious and irreversible disruption to the management and operation of critical infrastructure, infringements of obligations under Union law intended to protect fundamental rights, or serious harm to property or the environment.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "fbe4d324-9497-4865-b73a-72e761e651b7",
        "3ccde775-96a9-422b-87da-43e4cd707e7f",
        "ecc6bef2-71ac-41e5-a335-c79864647312",
        "8371d991-3132-4b4f-b0b4-0cecd37c2b99"
      ],
      "parameters": []
    },
    {
      "id": "2d2d6ae0-e189-43c2-b5a4-ce6e409335b9",
      "content": "-"
    },
    {
      "id": "a9c29f95-f8f9-494c-bf52-f6bdc0b0b126",
      "title": "AI-Service Desk: Provider obligations, Ensuring accessibility requirements are met",
      "content": "- Ensuring accessibility requirements are met\n    \nSpecifically, the accessibility requirements of Directives (EU) 2016/2102 and (EU) 2019/882 must be met (see Art. 16 letter l AIA). According to Annex I Section I of Directive (EU) 2019/882, this concerns specific requirements for the provision of information, the design of user interfaces and functionality and also support services such as help desks, call centres, etc.).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "86806ab4-b869-4e14-8e9f-39249fac5a50",
        "d5118a3b-1529-4362-9b1b-c99a8dd748ec",
        "0fa0541d-b4e5-48ce-b134-aff5e22c8eec"
      ],
      "parameters": []
    },
    {
      "id": "649ea5ee-b256-46c3-9332-53e42dcce5d1",
      "title": "AI-Service Desk: Provider obligations, If necessary, appointment of an authorised representative",
      "content": "-   If necessary, appointment of an authorised representative\n    \nProviders established in third countries must appoint an authorised representative established in the Union before providing the high-risk AI system in accordance with Art. 22 AIA. Providers are obliged to enable the authorised representative to perform their tasks.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "19b32d95-81d7-4097-8ca6-ca2c6c0358df",
        "86e3f5bd-eef6-4109-ab46-aedd548f4a71"
      ],
      "parameters": []
    },
    {
      "id": "97de0d49-8dfd-445b-becb-01576d557e09",
      "title": "AI-Service Desk: Provider obligations, If necessary, corrective measures",
      "content": "-   If necessary, corrective measures\n    \nIf a provider of high-risk AI systems considers or has reason to consider that a high-risk AI system that has already been placed on the market or put into service does not comply with the AI Act, corrective action must be taken immediately (see Art. 20). This primarily means establishing compliance, but it can also mean withdrawing, disabling or recalling the AI system. At the same time, downstream actors (deployers, authorised representatives, importers) must also be informed.\n\nIf the high-risk AI-system presents a risk within the meaning of Art. 79 para. 1 of the AIA and the provider becomes aware of this, the provider - together with the deployer, if applicable - shall notify the market surveillance authorities, including the notified body, if applicable. ",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "d74611d3-9a18-4ee9-86e8-953a959a7350",
        "069d1fdf-6329-4927-865f-862b02fbc7c1",
        "577d5a3c-36b6-48b6-a3fd-27b111efe5fd",
        "491d84d6-b754-4408-8bbd-21c789451f0f",
        "ecc6bef2-71ac-41e5-a335-c79864647312",
        "1c919b24-516b-4255-979e-f4cbc0da447e",
        "90ac7181-45af-41f1-817b-5a51223d7825"
      ],
      "parameters": []
    },
    {
      "id": "020bdcf5-11bc-470e-afb9-6045f8cd50dd",
      "title": "AI-Service Desk: Provider obligations, AI systems with \"limited\" risk",
      "content": "#### AI systems with \"limited\" risk\n\nIn Art. 50 AIA, the AI Act lists certain AI systems that present a limited risk. The risk can be minimised by means of certain transparency obligations. They can be summarised under the category \"Transparency towards downstream actors\".\n\nThe provider has the following transparency obligations with regard to the following AI systems:",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "42b80e11-733d-441e-98e1-d79837892537"
      ],
      "parameters": []
    },
    {
      "id": "cba8a6f7-4035-4e48-9cde-de0d62edfa78",
      "title": "AI-Service Desk: Provider obligations, AI systems with direct interaction",
      "content": "##### AI systems that interact directly with natural persons (e.g. chatbots)\n\nSuch AI systems must be designed and developed in such a way that the natural persons concerned are informed that they are interacting with an AI system. Exceptions to this are cases where this is obvious from the circumstances and context of use.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "42b80e11-733d-441e-98e1-d79837892537"
      ],
      "parameters": []
    },
    {
      "id": "4e59e48e-bfc6-4ccc-9979-12ba3220dcec",
      "title": "AI-Service Desk: Provider obligations, generating of contents with AI Systems",
      "content": "##### AI systems that generate or manipulate image, audio or video content (e.g. deepfakes)\n\nAI systems (including GPAI systems) that generate synthetic or manipulate image, audio or video content shall be designed and developed in such a way that the outputs can be output in a machine-readable format and recognised as artificially generated or manipulated.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "42b80e11-733d-441e-98e1-d79837892537"
      ],
      "parameters": []
    },
    {
      "id": "5841b6df-de62-452c-8a19-f2b6391feb5d",
      "title": "AI-Service Desk: Provider obligations, AI systems with \"minimal\" risk",
      "content": "##### AI systems with \"minimal\" risk\n\nNo mandatory requirements are standardised for AI systems with minimal risk. Only the obligation for \"AI competence\" pursuant to Art. 4 AIA also applies to such AI systems. Otherwise, compliance with the Code of Practices is encouraged, but this is voluntary. ",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "42b80e11-733d-441e-98e1-d79837892537",
        "6ead0916-b1d0-4ee7-a131-b86ac144d0ac",
        "21ada015-1d50-4ddb-97d5-33b540e849cb",
        "f9b21785-6f87-4438-8810-147304eb4d7e",
        "550c995f-0a8e-4714-b73e-9418fb982eed"
      ],
      "parameters": []
    },
    {
      "id": "74f0a1e0-89f6-47f5-b65e-8fc4327d62d7",
      "title": "AI-Service Desk: Provider obligations, Technical documentation",
      "content": "#### GPAI models\n\nIn the case of GPAI models, providers must fulfil the following obligations in accordance with Art. 53, 54 AIA:\n\n-    Technical documentation\n    \n   Providers of GPAI models shall prepare and update the technical documentation of the model, including its training and testing procedure and the results of its evaluation, containing at least the information listed in Annex XI (Art. 53 para. 1 letter a sentence 1 AIA).\n\nThis obligation does not apply to providers of exempt open source models within the meaning of Art. 2 para. 12 AIA, unless it is a GPAI model with systemic risk.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "fe6a85d3-98f4-4ac0-9055-3b4dad052553",
        "7bed8e5e-3585-48d6-889d-68089e0581a4",
        "e4baece6-7335-43c7-bc93-770f04b64211",
        "b908b1fb-c1a0-41d0-85cd-96d3d195d360",
        "0a6d98df-01bf-4a5c-b316-f87ecb2b8f17",
        "b5f38203-47bc-4aba-a5fb-0270a939ad11",
        "867f6662-88e1-490c-9191-9ba91afbd52d"
      ],
      "parameters": []
    },
    {
      "id": "3960ac09-8d97-4baf-a897-eac5f554a03e",
      "title": "AI-Service Desk: Provider obligations, Co-operation with the competent authorities",
      "content": "-   Co-operation with the competent authorities\n    \n At the request of a competent authority or the AI Office, providers of GPAI models must make the above-mentioned technical documentation available (Art. 53 para. 1 letter a sentence 2 AIA).\n\nIn general, providers of GPAI models must \"cooperate with the competent national authorities, including the Commission where necessary, in the exercise of their responsibilities and powers under this Regulation\" (see Art. 53 para.3 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "7bed8e5e-3585-48d6-889d-68089e0581a4",
        "e4baece6-7335-43c7-bc93-770f04b64211",
        "b908b1fb-c1a0-41d0-85cd-96d3d195d360",
        "0a6d98df-01bf-4a5c-b316-f87ecb2b8f17",
        "b5f38203-47bc-4aba-a5fb-0270a939ad11",
        "867f6662-88e1-490c-9191-9ba91afbd52d"
      ],
      "parameters": []
    },
    {
      "id": "00c8f4c7-c9c8-4a8d-8c17-5bb005aeb14b",
      "title": "AI-Service Desk: Provider obligations, Transparency",
      "content": "-   Transparency and provision of information for providers of AI systems\n\nProviders of GPAI models shall prepare and update certain information and documentation for downstream providers of AI systems that intend to integrate this model into their AI systems and make it available to the providers of AI systems. The technical documentation shall fulfil the minimum requirements set out in Annex XII.\n\nThis information and documentation must enable providers of AI systems to \"have a good understanding\" of the capabilities and limitations of the GPAI model and also enable them to fulfil their obligations under the AI Act (Art. 53 para. 1 letter b AIA)\n\nThis obligation does not apply to providers of exempt open source models within the meaning of Art. 2 para. 12 AIA, unless it is a GPAI model with systemic risk.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "7bed8e5e-3585-48d6-889d-68089e0581a4",
        "e4baece6-7335-43c7-bc93-770f04b64211",
        "b908b1fb-c1a0-41d0-85cd-96d3d195d360",
        "0a6d98df-01bf-4a5c-b316-f87ecb2b8f17",
        "b5f38203-47bc-4aba-a5fb-0270a939ad11",
        "867f6662-88e1-490c-9191-9ba91afbd52d",
        "55e8665d-afb5-46d1-b258-64f964b08d09"
      ],
      "parameters": []
    },
    {
      "id": "92de995d-6ec6-4518-aefe-5e81d76b7db3",
      "title": "AI-Service Desk: Provider obligations,  compliance with EU copyright law",
      "content": "-   Policy of compliance with EU copyright law\nProviders of GPAI models must have a policy for complying with EU copyright law and related rights, including through state-of-the-art technologies. This includes, in particular, the identification of and compliance with a reservation of rights asserted under Art. 4 para. 3 of the Copyright Directive (see Art. 53 para 1 letter c of the AIA).\n\nNote: Art. 3 of the Copyright Directive (Directive (EU) 2019/790) standardises that text and data mining (\"TDM\") may be carried out in principle.  However, this right can be restricted by a reservation of the rights holders in accordance with Art. 4 para. 3 Copyright Directive in such a way that only research institutions and cultural heritage institutions are permitted to operate TDM.\n\nTDM is a collective term for various procedures that make it possible to search and analyse large quantities of texts or data from various perspectives. In Austria, this is implemented in Section 42h UrhG. Furthermore, the content used for the training of the GPAI model must be published in accordance with a template provided by the AI Office (see Art. 53 para. 1 letter d AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "09b711a2-571c-4a78-9b84-60675ea83b5e",
        "21ada015-1d50-4ddb-97d5-33b540e849cb",
        "f9b21785-6f87-4438-8810-147304eb4d7e",
        "550c995f-0a8e-4714-b73e-9418fb982eed",
        "7bed8e5e-3585-48d6-889d-68089e0581a4",
        "e4baece6-7335-43c7-bc93-770f04b64211",
        "b908b1fb-c1a0-41d0-85cd-96d3d195d360",
        "0a6d98df-01bf-4a5c-b316-f87ecb2b8f17",
        "b5f38203-47bc-4aba-a5fb-0270a939ad11",
        "867f6662-88e1-490c-9191-9ba91afbd52d"
      ],
      "parameters": []
    },
    {
      "id": "8380352c-9c45-4369-b85f-2458cb560103",
      "title": "AI-Service Desk: Provider obligations, representatives",
      "content": "-  Appointment of an authorised representative\n\nIf the provider of a GPAI model is established in a third country, the provider is obliged to appoint an authorised representative within the Union before placing the model on the market (see Art. 54 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "49e88b0c-2141-4832-bc84-ee8b5fef7d4f",
        "86e3f5bd-eef6-4109-ab46-aedd548f4a71"
      ],
      "parameters": []
    },
    {
      "id": "7d2ce9a7-cb03-4c28-aaa2-28e0f63262bf",
      "title": "AI-Service Desk: Provider obligations, GPAI models with systemic risk",
      "content": "#### GPAI models with systemic risk\n\nIf the GPAI model is one with systemic risk, providers must fulfil the obligations set out in Art. 55 AIA in addition to those set out in Art. 53 and 54 AIA:\n\n-   Risk management\n\nThe obligations under Art. 55 (1) (a) and (b) can be summarised under the heading \"risk management\". Accordingly, providers of GPAI models must carry out a model evaluation in accordance with standardised protocols and instruments. This also includes the performance and documentation of attack tests in order to identify and minimise systemic risks. In addition, potential systemic risks at Union level - including their sources - that may stem from the development, placing on the market or use of GPAI models with systemic risk must be assessed and mitigated.\n    \n-   Obligation to report serious incidents to the competent authorities\n\nSerious incidents must be documented and reported immediately to the AI Office and, if necessary, to the competent national authorities.\n    \n-   Cybersecurity\n\nGPAI model providers must ensure an \"adequate level\" of cybersecurity and the physical infrastructure of the model (see Art. 55 para. 1 letter d). When protecting cybersecurity in the context of systemic risks associated with malicious use or malicious attacks, due consideration should be given to unintentional model data loss, unauthorised deployment, circumvention of security measures and protection against cyber-attacks, unauthorised access or model theft.\n\nThis protection could be facilitated by securing model weights, algorithms, servers and datasets, e.g. through operational security measures for information security, specific cybersecurity strategies, appropriate technical and established solutions, and physical and cyber access controls appropriate to the circumstances and associated risks (see Recital 115).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "49e88b0c-2141-4832-bc84-ee8b5fef7d4f",
        "16cd04eb-0068-4a58-9062-a5e44f5f15a0",
        "7bed8e5e-3585-48d6-889d-68089e0581a4",
        "7b5cb5db-20b5-4e2c-8079-565b6f264eaf",
        "86e3f5bd-eef6-4109-ab46-aedd548f4a71",
        "89e675f5-d397-4279-80b8-8d3635280832",
        "e4baece6-7335-43c7-bc93-770f04b64211",
        "b908b1fb-c1a0-41d0-85cd-96d3d195d360",
        "0a6d98df-01bf-4a5c-b316-f87ecb2b8f17",
        "b5f38203-47bc-4aba-a5fb-0270a939ad11",
        "867f6662-88e1-490c-9191-9ba91afbd52d"
      ],
      "parameters": []
    },
    {
      "id": "856e2b50-28e3-4e59-afb8-e0b942ae265d",
      "title": "AI-Service Desk: Provider obligations, obligations of product manufacturers",
      "content": "### What obligations do product manufacturers have?\n\nIn the case of high-risk AI systems that are safety components of products covered by the Union harmonisation legislation listed in Annex I Section A, the product manufacturer is deemed to be the provider of the high-risk AI system in accordance with Art. 25 para. 3 of the AIA and is subject to the obligations of a provider in accordance with Art. 16 of the AIA in the two cases below:\n\nThe high-risk AI system is placed on the market together with the product under the name or trade mark of the product manufacturer;\nthe high-risk AI system is put into serivce under the name or trade mark of the product manufacturer after the product has been placed on the market.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "63f69c82-a393-43d5-bf47-0ea7b61acc5f",
        "86806ab4-b869-4e14-8e9f-39249fac5a50",
        "0a83841c-2931-4e56-a212-b73e3ab410f1",
        "89b37334-a989-4eca-bb8e-55165382038d",
        "4123b06a-0100-490b-9631-66e3f5f66ba2"
      ],
      "parameters": []
    },
    {
      "id": "268f2a84-8cd3-49f7-9690-f547936af47a",
      "title": "AI Service Desk: Deployer obligations, generally",
      "content": "### What obligations do deployers have?\n\nJust like providers, deployers of AI systems are also obliged to ensure that, when using any type of AI system, the staff deployed and other persons entrusted with the AI systems must have a sufficient level of AI literacy (Art. 4 in conjunction with Art. 3 No. 56 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "09b711a2-571c-4a78-9b84-60675ea83b5e",
        "21ada015-1d50-4ddb-97d5-33b540e849cb",
        "f9b21785-6f87-4438-8810-147304eb4d7e",
        "550c995f-0a8e-4714-b73e-9418fb982eed"
      ],
      "parameters": []
    },
    {
      "id": "2b4e2670-f415-4bb2-ae30-8e1951e9f535",
      "title": "AI Service Desk: Deployer obligations, High-risk AI Systems",
      "content": "#### High-risk AI systems\n\nThe obligations of deployers of high-risk AI systems are regulated in Art. 26 AIA. The following obligations must be complied with:",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "54d92cd3-752d-4303-b8e9-f8c7efac0e64",
        "c7976729-6c7e-451a-8143-a8a52257644d"
      ],
      "parameters": []
    },
    {
      "id": "725eccf5-6895-43c3-bd02-fd28a674168f",
      "title": "AI Service Desk: Deployer obligations, High-risk AI Systems, instructions for use",
      "content": "-   Use of the high-risk AI system according to the instructions for use\n\nIn accordance with Art. 13 para. 2 AIA, providers must provide instructions for use and make them available to deployers. Deployers of high-risk AI systems must take the necessary technical and organisational measures to ensure that high-risk AI systems are used in accordance with the attached instructions for use (see Art. 26 para. 1 AIA).\n\nIf the deployers have control over input data, they must ensure that it is in view of the intended purpose of the high-risk AI system and must be sufficiently representative (see Art. 26 para. 4 AIA).\n\nThis obligation does not affect other obligations of the deployer under Union or national law (see Art. 26 para. 3 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "e36b6aab-d7e1-44ce-8d20-d6bee82bff7d",
        "54d92cd3-752d-4303-b8e9-f8c7efac0e64",
        "c7976729-6c7e-451a-8143-a8a52257644d",
        "b43808c6-abf1-4af0-8243-6e7e62e8e30b",
        "7188729e-4dec-4ecf-a3b9-8494a9fc716e",
        "90669d84-c0e3-4ecb-9c0f-f8bbef4679aa",
        "4441c4d9-747f-4f05-8fc8-b67033b9273f"
      ],
      "parameters": []
    },
    {
      "id": "2629f5ca-f950-440a-96be-44e71d8d2a85",
      "title": "AI Service Desk: Deployer obligations, High-risk AI Systems, human oversight",
      "content": "-   Human oversight\n\nProviders are responsible for the basic implementation of human oversight tools (see Art. 16 letter a in conjunction with Art. 14 AIA), and deployers are subsequently obliged to assign human oversight to natural persons who have the necessary competence, training, and authorisation (see Art. 26 para. 2 AIA). Deployers are also obliged to provide these natural persons with the necessary support.\n\nThis does not affect other obligations of the deployer under Union or national law (see Art. 26 para. 3 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "86806ab4-b869-4e14-8e9f-39249fac5a50",
        "67156c3d-d006-41f0-87e3-725cd933dbdb",
        "54d92cd3-752d-4303-b8e9-f8c7efac0e64",
        "550c995f-0a8e-4714-b73e-9418fb982eed",
        "d5118a3b-1529-4362-9b1b-c99a8dd748ec",
        "0fa0541d-b4e5-48ce-b134-aff5e22c8eec"
      ],
      "parameters": []
    },
    {
      "id": "7dd66929-d63c-4bac-bec0-a41ec1644573",
      "title": "AI Service Desk: Deployer obligations, High-risk AI Systems, Monitoring the AI system",
      "content": "-   Monitoring the AI system \n\nPursuant to Art. 26 para. 5 subpara. 1 AIA, deployers must monitor the operation of the high-risk AI system used on the basis of the attached instructions for use and, if necessary, inform providers in accordance with Art. 72 AIA (\"post-market monitoring\").\n\nFor deployers that are financial institutions and subject to the relevant financial services law, the requirements on internal governance arrangements, processes and mechanisms apply (see Art. 26 para. 5 subpara. 2 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "54d92cd3-752d-4303-b8e9-f8c7efac0e64",
        "5b7ccaab-a361-4b2e-bf6c-67a90fd684eb",
        "c7976729-6c7e-451a-8143-a8a52257644d",
        "3ccde775-96a9-422b-87da-43e4cd707e7f",
        "d685b8fd-f03f-4563-8a9a-41aaf6ee30ad"
      ],
      "parameters": []
    },
    {
      "id": "c7c652b8-5799-4a4c-b21d-63f64e0323c2",
      "content": "-"
    },
    {
      "id": "2b42aac4-945f-40fb-9f90-7cee39dbbc74",
      "title": "AI Service Desk: Deployer obligations, High-risk AI Systems, Reporting of serious incidents",
      "content": "- Reporting of serious incidents\n\nIf there is reason to consider that the high-risk AI system presents a risk within the meaning of Art. 79 AIA, or if a serious incident has been identified, the deployer has reporting obligations towards the provider, importer/distributor and the competent market surveillance authorities (see Art. 26 para. 5 subpara. 1 AIA).\n\nIf there is reason to assume that a high-risk AI system presents a risk within the meaning of Art. 79 para. 1 AIA, the deployer shall suspend the use of that system.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "069d1fdf-6329-4927-865f-862b02fbc7c1",
        "54d92cd3-752d-4303-b8e9-f8c7efac0e64",
        "c7976729-6c7e-451a-8143-a8a52257644d",
        "b43808c6-abf1-4af0-8243-6e7e62e8e30b",
        "577d5a3c-36b6-48b6-a3fd-27b111efe5fd"
      ],
      "parameters": []
    },
    {
      "id": "e3e8d669-8420-4865-9fd8-0ad760b09242",
      "title": "AI Service Desk: Deployer obligations, High-risk AI Systems, Record-keeping",
      "content": "-   Record-keeping\n\nDeployers must keep the automatically generated logs of high-risk AI systems for at least 6 months (unless otherwise stipulated by applicable Union law such as the GDPR), provided that they are under their control (see Art. 26 para. 6 subpara. 1 AIA).\n\nDeployers that are financial institutions and subject to the relevant financial services law must keep the logs as part of these documentation requirements (see Art. 26 para. 6 subpara. 2 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "c7976729-6c7e-451a-8143-a8a52257644d",
        "54d92cd3-752d-4303-b8e9-f8c7efac0e64"
      ],
      "parameters": []
    },
    {
      "id": "fb82ef44-9f09-40c7-bad5-4656bebeb483",
      "title": "AI Service Desk: Deployer obligations, High-risk AI Systems, data protection",
      "content": "-  If relevant, data protection impact assessment in accordance with Art. 35 GDPR\n\nDeployers of high-risk AI systems may use the information (\"instructions for use\") provided by the providers in accordance with Art. 13 AIA to fulfil their obligation to carry out a data protection impact assessment in accordance with Art. 35 GDPR or Article 27 of Directive (EU) 2016/680 (see Art. 26 para.9 AIA).).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "e36b6aab-d7e1-44ce-8d20-d6bee82bff7d",
        "9032a3a6-c85d-43bc-b8da-fb8d7a40e48c",
        "54d92cd3-752d-4303-b8e9-f8c7efac0e64",
        "c7976729-6c7e-451a-8143-a8a52257644d"
      ],
      "parameters": []
    },
    {
      "id": "44e0a04a-089c-40ec-a3bc-86f4015339f3",
      "title": "AI Service Desk: Deployer obligations, High-risk AI Systems, Co-operation with competent national authorities",
      "content": "-   Co-operation with competent national authorities\n\nWhen using high-risk AI systems in accordance with Annex III, natural persons affected by a decision or in the case of such decisions where the AI system in question provides support must be informed of the use of the high-risk AI system in accordance with Art. 26 para. 11 AIA. In the context of law enforcement, Art. 13 Directive 2016/680 applies, for example to AI systems that are used for admission to educational institutions or for filtering applicants for job advertisements.\n\nThese obligations apply without prejudice to the transparency obligations pursuant to Art. 50 AIA.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "54d92cd3-752d-4303-b8e9-f8c7efac0e64",
        "c7976729-6c7e-451a-8143-a8a52257644d"
      ],
      "parameters": []
    },
    {
      "id": "0a86d1c5-9a31-4783-842f-6086786f0867",
      "title": "AI Service Desk: Deployer obligations, High-risk AI Systems, transparency",
      "content": "-   Transparency towards downstream players\n\nWhen using high-risk AI systems in accordance with Annex III, natural persons affected by a decision or in the case of such decisions where the AI system in question provides support must be informed of the use of the high-risk AI system in accordance with Art. 26 para. 11 AIA. In the context of law enforcement, Art. 13 Directive 2016/680 applies, for example to AI systems that are used for admission to educational institutions or for filtering applicants for job advertisements.\n\nThese obligations apply without prejudice to the transparency obligations pursuant to Art. 50 AIA.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "54d92cd3-752d-4303-b8e9-f8c7efac0e64",
        "c7976729-6c7e-451a-8143-a8a52257644d",
        "90669d84-c0e3-4ecb-9c0f-f8bbef4679aa",
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "42b80e11-733d-441e-98e1-d79837892537"
      ],
      "parameters": []
    },
    {
      "id": "7b61393c-3edc-4b21-a63a-67ef711aa4bb",
      "title": "AI Service Desk: Deployer obligations, High-risk AI Systems, individual rights",
      "content": "-   Right to an explanation of the decision-making process in individual cases\n\nAccording to Art. 86 AIA, individuals subject to a decision taken by the deployer based on the output of a high-risk AI system listed in Annex III (except point 2: Critical Infrastructure) that has legal implications or similarly significantly affects them in a way that they consider to have an adverse impact on their health, safety or fundamental rights shall have the right to receive a clear and meaningful explanation from the deployer on the role of the AI system in the decision-making process and on the main elements of the decision taken.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "cb445557-7bf2-48a2-bb97-bcaf072934fd",
        "af6b62de-8c37-48f7-b7f7-5bcdb2b0bf39",
        "1c919b24-516b-4255-979e-f4cbc0da447e"
      ],
      "parameters": []
    },
    {
      "id": "f7521e56-add5-4d83-a21e-e9c641ccf8d8",
      "title": "AI Service Desk: Deployer obligations, specific High-risk AI Systems",
      "content": "The following obligations only apply to certain deployers when using specific high-risk AI systems:\n\n-   Employers who use high-risk AI systems in the workplace: Duty to inform the employee representative body\n\nIn addition to the employees concerned, employers must also inform the employee representatives of the planned use of a high-risk AI system in the workplace prior to the putting into service or using of such an AI system (see Art. 26 para. 7). According to Annex III para. 4 letter a and b AIA, this includes AI systems intended to be used for the recruitment or selection of natural persons, in particular to place targeted job advertisements, screen or filter applications and evaluate applicants, as well as AI systems intended to be used for decisions affecting the terms and conditions of employment, promotions and terminations of employment contracts, for the assignment of tasks based on individual behaviour or personal characteristics or traits, or for the monitoring and evaluation of the performance and behaviour of persons in such relationships.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "54d92cd3-752d-4303-b8e9-f8c7efac0e64",
        "c7976729-6c7e-451a-8143-a8a52257644d",
        "b43808c6-abf1-4af0-8243-6e7e62e8e30b"
      ],
      "parameters": []
    },
    {
      "id": "e3612b37-75ae-4a48-a2d5-0c86a8d41255",
      "title": "AI Service Desk: Deployer obligations, obligation to register",
      "content": "-   If EU institutions, bodies, offices and agencies are deployers: Obligation to register\n\nEU institutions, bodies, offices and agencies that are deployers of high-risk AI systems must register the AI system used in accordance with Art. 49 AIA (see Art. 26 para. 8 AIA). If the high-risk AI system intended for use is not registered in the EU database referred to in Art. 71, they shall refrain from using it and also inform the provider or distributor of this.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "1d1f418c-443a-4e77-b70a-5415bad671e6",
        "bea96906-be50-4eb8-9cbe-cf07816645c5",
        "e85b8add-b4ef-4422-bafa-b843f3f02662",
        "90893d61-25fd-4ae1-8232-5e4cdb5675e9"
      ],
      "parameters": []
    },
    {
      "id": "455a7f0e-e6cf-4241-892f-098bec6464d2",
      "title": "AI Service Desk: Deployer obligations, authorisation requiry",
      "content": "-   If used for subsequent remote biometric identification: authorisation required from a judicial or administrative authority\n\nIf deployers (ergo law enforcement authorities) use high-risk AI systems for post-remote biometric identification in the context of investigations for the targeted search of a person suspected of or convicted of committing a criminal offence, they must obtain authorisation in advance or immediately, at the latest within 48 hours, from a judicial or administrative authority whose decision is subject to judicial review in accordance with Art. 26 para. 10 AIA.\n\nIf the requested authorisation is refused, the use of the high-risk AI system in question must be discontinued with immediate effect and any personal data associated with the use of this system must be deleted. Any use of such high-risk AI systems shall be documented in the relevant police file and made available to the competent market surveillance authority and the national data protection authority - except for sensitive operational data - upon request. Annual reports must also be submitted to these authorities.\n\nThe use of such AI systems in an untargeted way and without any connection to a criminal offence or the search for a specific missing person is prohibited.\n\nThis does not apply to the initial identification of a potential suspect on the basis of objective and verifiable facts that are directly related to the offence.\n\nMember States remain free to adopt stricter legislation on the use of AI systems for post-remote biometric identification.\n\nThis Article is without prejudice to the application of Directive (EU) 2016/680.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "54d92cd3-752d-4303-b8e9-f8c7efac0e64"
      ],
      "parameters": []
    },
    {
      "id": "9cf8614f-614f-4a97-a52f-93a37e60c327",
      "title": "AI Service Desk: Deployer obligations, fundamental rights impact assesment",
      "content": "- If public and private organisations providing public services/partly private institutions: Preparation of a fundamental rights impact assessment \n\nAccording to Art. 27 AIA, entities governed by public law and private entities providing public services must carry out a fundamental rights impact assessment when using (applies to the first use) a high-risk AI system in accordance with Art. 6 para. 2 in conjunction with Annex III (except point 2: Critical Infrastructure) and deployers of high-risk AI systems must carry out a fundamental rights impact assessment when using high-risk AI systems in accordance with Annex III point 5 letter b (credit scoring and creditworthiness assessment of natural persons) and (c) (risk assessment and pricing in relation to natural persons in the case of life and health insurance).\n\nThe following aspects must be taken into account:\n\na description of the deployer's procedures in which the high-risk AI system is used in accordance with its intended purpose;\na description of the time period and frequency within which each high-risk AI system is to be used;\nthe categories of natural persons and groups of persons who could be affected by its use in the specific context;\nthe specific risks of harm that could have an impact on the categories of natural persons or groups of persons identified in accordance with point c of this paragraph, taking into account the information provided by the provider in accordance with Article 13;\na description of the implementation of human oversight measures in accordance with the instructions for use;\nthe measures to be taken if these risks materialise, including internal governance and complaint mechanisms. \n\nIf these obligations are already covered by the data protection impact assessment pursuant to Art. 35 GDPR or Art. 27 of Directive (EU) 2016/680, the fundamental rights assessment supplements the data protection impact assessment within the meaning of the AIA.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "1a8cb301-fedf-4e50-a05e-2d3ad00b5264",
        "6b073c5f-3422-4b34-b139-b8645e89b463",
        "90ac7181-45af-41f1-817b-5a51223d7825"
      ],
      "parameters": []
    },
    {
      "id": "b48bb3d6-986f-4161-a486-48216c35347a",
      "title": "AI Service Desk: Deployer obligations, limited risk",
      "content": "#### AI systems with \"limited\" risk\n\nArt. 50 AIA lists certain AI systems that pose a limited risk, as the risk can be minimised by means of certain transparency obligations. The following transparency obligations apply to the deployers of the following AI systems:\n\n- Emotion recognition systems or AI systems for biometric categorisation\n\nWithout prejudice to other transparency obligations resulting from Union or national law, natural persons exposed to the operation of such systems must be informed about the operation of an emotion recognition system or an AI system for biometric categorisation (see Art. 50 para. 3 AIA). Personal data shall be processed in accordance with data protection regulations. This does not apply to authorised AI systems for the detection, prevention and investigation of criminal offences, provided that appropriate safeguards are in place to protect the rights and freedoms of third parties.\n\nThe information must be provided in a clear and distinguishable manner at the latest at the time of the first interaction or suspension and must comply with the applicable accessibility requirements (see Art. 50 para. 5 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "be8e58b9-7976-45e8-9b6e-ae5d17d5f7cb",
        "42b80e11-733d-441e-98e1-d79837892537",
        "55614e42-478f-480a-a5c3-2ecb6c402441"
      ],
      "parameters": []
    },
    {
      "id": "16b18429-94ae-4d83-80ed-907e2e552fe5",
      "title": "AI Service Desk: Deployer obligations, GPAI-Modells",
      "content": "-   AI systems that generate or manipulate text, image, sound or video content\n\nWithout prejudice to other EU or national transparency obligations, deployers of an AI system that generates or manipulates text, image, sound or video content that is a deep fake must disclose that the content was artificially generated or manipulated in accordance with Art. 50 para. 4 AIA. Use for the detection, prevention, investigation and prosecution of criminal offences is excluded.\n\nA \"deep fake\" within the meaning of the AI Act is an image, sound or video content generated or manipulated by an AI that resembles existing persons, objects, places, entities or events and would falsely appear to a person to be authentic or truthful (see Art. 3 para. 60 AIA).\n\nIf it is obvious that the artificially created or manipulated image, sound or video content is part of an artistic, creative, satirical, fictional or analogue work or programme, the transparency obligation is limited to disclosing the existence of artificially created and manipulated content in such a way that the presentation or enjoyment of the work is not hampered.\n\nThe transparency obligations do not apply to generated and manipulated texts if this text has been checked by a human and there is an editor responsible.\n\nThe information must be provided in a clear and distinguishable manner at the latest at the time of the first interaction or suspension and must comply with the applicable accessibility requirements (see Art. 50 para. 5 AIA).\n  ",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "42b80e11-733d-441e-98e1-d79837892537",
        "55614e42-478f-480a-a5c3-2ecb6c402441",
        "71a61842-dac1-4a64-b31a-cf6c68220d51"
      ],
      "parameters": []
    },
    {
      "id": "b2fb0350-3a4b-4eee-89c6-6baf1455e250",
      "title": "AI Service Desk: Deployer obligations, minimal risk",
      "content": "#### AI systems with \"minimal risk\"\n\nThere are no mandatory requirements for AI systems with \"minimal\" risk. Only the obligation for \"AI competence\" pursuant to Art. 4 AIA also applies to such AI systems. Compliance with Code of Practices is encouraged but voluntary.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "09b711a2-571c-4a78-9b84-60675ea83b5e",
        "21ada015-1d50-4ddb-97d5-33b540e849cb",
        "f9b21785-6f87-4438-8810-147304eb4d7e",
        "6f3bcb0e-023a-4fae-b8ae-aff6dfd8c929"
      ],
      "parameters": []
    },
    {
      "id": "ecfd2b0e-fbbd-4551-8c94-4eaef2c02ec5",
      "title": "AI Service Desk: AI literacy, generally",
      "content": "## What is \"AI literacy\"?\n\nOn February 2, 2025, the first set of provisions of the AI Act will enter into application, including the requirement for AI literacy as outlined in Article 4 of the AIA. According to Article 4 AIA, the following obligation applies uniformly across all AI systems, models, and risk categories.\n\nArt. 4 AIA: Providers and deployers of AI systems shall take measures to ensure, to their best extent, a sufficient level of AI literacy of their staff and other persons dealing with the operation and use of AI systems on their behalf, taking into account their technical knowledge, experience, education and training and the context the AI systems are to be used in, and considering the persons or groups of persons on whom the AI systems are to be used.\nThe term \"AI literacy\" referenced in the text of Regulation and in the title of Article 4 AIA is further elaborated in Article 3, item 56 AIA. The following sections detail the relationships and distinctions between these two provisions.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "6f3bcb0e-023a-4fae-b8ae-aff6dfd8c929",
        "09b711a2-571c-4a78-9b84-60675ea83b5e",
        "21ada015-1d50-4ddb-97d5-33b540e849cb",
        "f9b21785-6f87-4438-8810-147304eb4d7e"
      ],
      "parameters": []
    },
    {
      "id": "1b027697-3d10-4487-9e5c-0a9be1158e3f",
      "title": "AI Service Desk: AI literacy, definition",
      "content": "### Definition of \"AI literacy\" according to Article 3, Item 56 AIA\n\nAccording to Article 3, item 56 AIA, \"AI literacy\" is defined as:\n\n[the] skills, knowledge and understanding that allow providers, deployers and affected persons, taking into account their respective rights and obligations in the context of this Regulation, to make an informed deployment of AI systems, as well as to gain awareness about the opportunities and risks of AI and possible harm it can cause.\nThe concept of AI literacy encompasses, in an abstract sense, the necessary skills required to navigate and succeed in the digital landscape through the effective use of AI systems.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "6f3bcb0e-023a-4fae-b8ae-aff6dfd8c929"
      ],
      "parameters": []
    },
    {
      "id": "5a9f9935-184c-45e2-8864-b91d0b030612",
      "title": "AI Service Desk: AI literacy, AI value chain",
      "content": "### Provider, deployer und affected persons\n\nAI literacy is applicable to all relevant stakeholders within the AI value chain, depending on their roles throughout the value creation process (see Recital 20). Different competencies are naturally required at various stages of this process. For example, providers of high-risk AI systems must possess a thorough understanding of the technical specifics of AI during the development phase to ensure the creation of AI that is both safe and consistent with European values.\n\nPursuant to Article 4 of the AIA, deployers and providers are required to implement \"measures\" to ensure that their staff, as well as any other individuals involved in the operation and use of AI systems on their behalf, possess an adequate level of AI literacy. The nature of these measures depends on the specific AI system or model employed and its associated risk level. It is essential to take into account the technical knowledge, experience, training, and education of the employees, as well as the context in which the AI systems are deployed and the individuals or groups they are intended to serve. AI literacy is inherently interdisciplinary, encompassing not only technical expertise but also legal and ethical considerations (see Recital 20 AIA). For example, providers involved in the development of a chatbot will naturally address different concerns than an operator who merely implements such a system within their organization.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "09b711a2-571c-4a78-9b84-60675ea83b5e",
        "21ada015-1d50-4ddb-97d5-33b540e849cb",
        "f9b21785-6f87-4438-8810-147304eb4d7e",
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6",
        "6f3bcb0e-023a-4fae-b8ae-aff6dfd8c929"
      ],
      "parameters": []
    },
    {
      "id": "0018756c-5b2f-46b7-abb9-8a5e3ffc3054",
      "title": "AI Service Desk: AI literacy, use case",
      "content": "### Illustration using an example on the topic of data protection and security:\n    \nA provider of a chatbot must ensure during the development process that user-entered data is stored and processed securely (e.g., data encryption, security updates, etc.). An operator of such a chatbot, who makes this system available to their employees, must ensure that no personal data or trade secrets are unlawfully transferred to the provider as a third party. This may include measures such as the operator using \"on-premises\" solutions, implementing necessary contractual safeguards, and/or providing appropriate training for employees on using the chatbot to ensure that such data entries are avoided (see also information from the Austrian Data Protection Authority regarding AI and data protection).\n\nGiven the varied applications and configurations of AI systems, the measures required under Article 4 AIA can differ significantly. There is no universal approach to determining the specific actions necessary to meet the requirements of Article 4 AIA. This also means that not all companies are equally affected by Article 4 AIA, nor is it necessary for every employee to possess the same level of AI literacy. For instance, if a company allows its entire staff to use chatbots like ChatGPT, it must implement appropriate and recurring training sessions (including for new hires) for the whole workforce. Conversely, if an \"AI tool\" is limited to use within the HR or communications department, the training can be focused on a smaller group. The depth and frequency of training may vary accordingly, as deploying AI systems with limited risk may require different measures than those needed for high-risk AI systems.\n\nIt is important to note that, unlike the GDPR, Article 4 AIA does not mandate the appointment of an \"AI officer.\" The decision to provide training for employees or to hire personnel with AI expertise for the implementation of AI strategies is left to the discretion of each individual company. Suitable approaches should be tailored to each specific case. Given that the process is not rigid, it is advisable to incorporate AI literacy as a continuous component of professional development and training programs.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "09b711a2-571c-4a78-9b84-60675ea83b5e",
        "21ada015-1d50-4ddb-97d5-33b540e849cb",
        "f9b21785-6f87-4438-8810-147304eb4d7e",
        "6f3bcb0e-023a-4fae-b8ae-aff6dfd8c929"
      ],
      "parameters": []
    },
    {
      "id": "26497c14-895e-4da7-bf4a-018625e3e739",
      "title": "AI Service Desk: AI literacy, subjects",
      "content": "The definition of the term \"AI literacy\" explicitly includes the positive requirement to understand the opportunities presented by AI, enabling the identification of potential value-adding applications.\n\nThe following groups are subject to the obligation for AI literacy:\n\n-Individuals involved in the development of AI\n-Individuals responsible for the operation of AI systems\n-Individuals within a company who utilize AI systems.\n\nThe AI Act does not specify the nature of the training measures to be implemented. These may include internal training sessions, external consultations, or in-house courses.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "09b711a2-571c-4a78-9b84-60675ea83b5e",
        "21ada015-1d50-4ddb-97d5-33b540e849cb",
        "f9b21785-6f87-4438-8810-147304eb4d7e",
        "6f3bcb0e-023a-4fae-b8ae-aff6dfd8c929"
      ],
      "parameters": []
    },
    {
      "id": "14294dba-dcf9-475a-8f96-d58137d64c43",
      "title": "AI Service Desk: AI literacy, Penalties",
      "content": "### Penalties\n\nAlthough the AI Act itself does not prescribe administrative penalties for non-compliance with Article 4 AIA, non-compliance may lead to consequences. A lack of employee training is generally attributable to the employer under § 1313a of the Austrian Civil Code, even outside the scope of the AI Act. Article 4 of the AI Act serves to clarify the duty of care that businesses must exercise with respect to AI. Thus, if damages occur due to insufficient AI literacy, Article 4 AIA establishes that there was an obligation to provide appropriate training.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "09b711a2-571c-4a78-9b84-60675ea83b5e",
        "21ada015-1d50-4ddb-97d5-33b540e849cb",
        "f9b21785-6f87-4438-8810-147304eb4d7e",
        "6f3bcb0e-023a-4fae-b8ae-aff6dfd8c929"
      ],
      "parameters": []
    },
    {
      "id": "f3529f8b-f673-4bc4-aa17-3022e4814d31",
      "title": "AI Service Desk: AI literacy, further information",
      "content": "### Further information\nAI literacy is often associated with digital competence, and the two topics are indeed closely related. This is evident in the fact that AI literacy is integrated into various skills areas and sub-competencies. Moreover, AI literacy builds upon the foundation of digital skills. To successfully apply and develop AI systems and models, digital competencies are also required.\n\nNational initiatives: https://www.digitalaustria.gv.at/Strategien/DKO-Digitale-Kompetenzoffensive.html\n\nEuropean Commission: DigComp 2.2: The Digital Competence Framework for Citizens\n\nEuropean Commission: Digital skills",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "09b711a2-571c-4a78-9b84-60675ea83b5e",
        "21ada015-1d50-4ddb-97d5-33b540e849cb",
        "f9b21785-6f87-4438-8810-147304eb4d7e",
        "6f3bcb0e-023a-4fae-b8ae-aff6dfd8c929"
      ],
      "parameters": []
    },
    {
      "id": "b2ad8ee0-3563-4261-b77f-e51e8f14ae46",
      "title": "AI Service Desk: Risk levels AI models, GPAI",
      "content": "# risk levels of AI models\n\n### General Purpose AI Models (\"GPAI Models\")\n\nThe EU law-making bodies initially focussed on the regulation of AI systems that were developed for a more or less specific purpose (e.g. autonomous driving). Since AI tools such as ChatGPT & Co also reached the general public, the focus then turned to \"AI models with a general purpose\".",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "55614e42-478f-480a-a5c3-2ecb6c402441"
      ],
      "parameters": []
    },
    {
      "id": "244539de-d09c-4047-8f24-0970ea22e691",
      "title": "AI Service Desk: Risk levels AI models, GPAI definition",
      "content": "### What are General Purpose AI Models?\n\nGeneral purpose AI models – also referred to as foundation models - are AI models (not to be confused with AI systems, see recitals 97) that can handle a wide range of tasks rather than being optimised for a specific task or application. These models are often able to process large amounts of unstructured data such as text, images, audio and video and also perform tasks such as classification, generation and prediction. Due to their flexibility and adaptability, these models are used in a variety of cases and different industries. They also often form the basis for fine-tuning specific AI systems. Examples of GPAI models include the large language models (LLM) based on transformer models from OpenAI (\"GPT-3.5/GPT-4\"), Meta (\"LLama\") or Mistral (\"Mixtral\"). However, GPAI models are not limited to language models; other models, for example for classification, can also fall under this definition.\n\nIn order to address possible risks which are specific to GPAI models (such as undesirable results, copyright and data protection violations during development), providers are subject of documentation and information obligations (see Art. 53 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "7bed8e5e-3585-48d6-889d-68089e0581a4",
        "e4baece6-7335-43c7-bc93-770f04b64211",
        "b908b1fb-c1a0-41d0-85cd-96d3d195d360",
        "0a6d98df-01bf-4a5c-b316-f87ecb2b8f17",
        "55614e42-478f-480a-a5c3-2ecb6c402441"
      ],
      "parameters": []
    },
    {
      "id": "a544c6f9-a5c0-44a9-8767-64afdfdda1e8",
      "title": "AI Service Desk: Risk levels AI models, GPAI model vs AI System",
      "content": "### Are GPAI models and AI systems the same?\n\nA clear distinction needs to be made between the terms \"AI systems\" and \"AI models\". Not all AI models fall within the scope of the AI Act, only GPAI models. Although GPAI models can be part of an AI system, they do not form an AI system in isolation. For a GPAI model to become an AI system, additional components - such as a user interface - must be added. In this case we speak of a general-purpose AI system (or GPAI system) in accordance with Art 3 para. 66 AIA.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "0b34a175-21c7-4d02-9c4e-4380537fff16",
        "7401fefb-6095-47f0-a00e-78aac879499e",
        "55614e42-478f-480a-a5c3-2ecb6c402441"
      ],
      "parameters": []
    },
    {
      "id": "83722d3a-a3fb-4f8b-9733-9a4c583b3754",
      "title": "AI Service Desk: Risk levels AI models, GPAI model vs GenAI",
      "content": "### Is GPAI and generative AI the same?\n\nGPAI and generative AI are similar concepts, but they are not exactly the same. GPAI models are designed for a wide range of applications, whereas generative AI systems focus on generating text, images, videos, music, and other content. (GPAI-Systems such as ChatGPT for gnenerating texts, Midjourney or Dall-E 2 for generating images and videos, etc.). Generative AI is therefore a specific sub-area of GPAI models. \n\nIn short, GPAI models are general concepts for a wide range of applications, while generative AI systems refer specifically to the ability to generate data or content.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "0b34a175-21c7-4d02-9c4e-4380537fff16",
        "55614e42-478f-480a-a5c3-2ecb6c402441"
      ],
      "parameters": []
    },
    {
      "id": "35d4c6fc-05c7-4377-8737-52ee79a7441b",
      "title": "AI Service Desk: Risk levels AI models, GPAI model with systemic risk",
      "content": "### When does a GPAI model pose systemic risks?\n\nGPAI models with systemic risks have a special status. This is when the AI system has capabilities \"high-impact capabilities\". By \"systemic risk\", the Union legislator refers to risks that are specific to a GPAI model that has high impact capabilities (Art. 3 para. 65 subpara. 1 AIA). This is considered to be the case when the capabilities of the GPAI model in question match to or exceed the capabilities recorded in the most advanced GPAI models. These models have a certain reach or have actual or reasonably foreseeable negative effects on public health, safety, public security, fundamental rights or society, which have a significant impact on the Union market that can propagated at scale across the value chain (see Art. 3(65)(2) of the AEOI).\n\n\nA systemic risk GPAI model is one that meets one of the following criteria:\n\n-It has high-impact capabilities evaluated on the basis of appropriate technical tools and methodologies, including indicators and benchmarks. Providers are subject to reporting requirements if these criteria are met (e.g. this is assumed when the amount of compute used,  measured in floating point operations [‘FLOPs’], is above1025);\n\n-There is an ex officio decision by the Commission or a qualified warning by the Scientific Panel that the GPAI model has capabilities or impacts equivalent to the above criteria.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "0b34a175-21c7-4d02-9c4e-4380537fff16",
        "18f063a4-31d6-40a9-b45a-da3f602f920f",
        "8543b14a-dfa5-4fd3-9673-e854faf06935",
        "aa588b7b-71e5-42c1-9adc-5aaa8688320a",
        "c9a0bc19-9064-435f-a8d1-73a20a754192",
        "55614e42-478f-480a-a5c3-2ecb6c402441"
      ],
      "parameters": []
    },
    {
      "id": "c119b51c-2e95-494b-9e83-7ceead178087",
      "title": "AI Service Desk: Risk levels AI models, FLOP",
      "content": "#### Excursus: What actually is a FLOP?\n\nThe AI Act defines a floating point operation (“FLOP”) in Art. 3 (67) as any arithmetic operation with floating point numbers. This includes basic arithmetic operations such as addition and multiplication. A simple calculation such as ‘42 * 42 + 17.32’ would therefore be two FLOPs (42*42; 1,764 + 17.32).\n\nThe AI Act uses the number of floating point operations that were necessary in the training phase as a substitute for the power of a model. The AIA therefore assumes that the currently defined threshold of 1025 FLOPs (written out: 10,000,000,000,000,000,000,000,000,000,000,000 arithmetic operations) results in a model with a high degree of efficiency. Current open source models already exceed this limit: The LLama-3 model 405B published by Meta as open source in July 2024 achieves 3.8 x 1025 FLOPS in the training performance used.\n\nTo categorise this figure to categorise this number: A current smartphone chip currently manages an order of magnitude of several 1012 FLOPs per second (‘teraflop/s’), a current home graphics card more than forty times that. Current data centre GPUs already achieve almost 2000 teraflops per second for simple simple computing operations. A current smartphone would have to calculate for 100,000 years of continuous computing to reach the limit of 1025 operations.\n\nThe ratios are summarised in the graphic below.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "55614e42-478f-480a-a5c3-2ecb6c402441"
      ],
      "parameters": []
    },
    {
      "id": "6fc17179-dc09-41b2-acdc-542b045be24c",
      "content": "Die Relationen sind in der untenstehenden Grafik zusammengefasst."
    },
    {
      "id": "80cd63d8-b1ac-4a11-9e9e-8bb628596d72",
      "title": "AI Service Desk: Risk levels AI models, systemic risk",
      "content": "In order to assess whether a GPAI model poses a systemic risk, the following parameters in accordance with Annex XIII must be taken into account:\n\n-the number of parameters of the model;\n-the quality or size of the data set, for example measured through tokens;\n-the amount of computation used for training the model, measured in FLOPs indicated by \n  a combination of other variables such as estimated cost of training, estimated time \n  required for training or estimated energy consumption for the training;\n  the input and output modalities of the model, e.g., text-to-text (large language models), \n  text-to-image, multi-modality, and the state-of-the-art thresholds for determining high-i \n  impact capabilities for each modality, as well as the specific type of inputs and outputs \n  (e.g., biological sequences);\n-the benchmarks and evaluations of the model's capabilities, including consideration the \n number of tasks it can perform without additional training, its adaptability to learn new, \n distinct tasks, its degree of autonomy and scalability, and the tools it has access to;\n-whether it has a high impact on the internal market in terms of its reach, which shall be \n presumed when it has been made available to at least 10 000 registered business users \n established in the Union;\n-The number of registered end-users.\n\n\nDue to the risk potential, the providers of GPAI models with systemic risks are subject to obligations that go beyond Art 53 AIA. In particular, providers must take measures to identify, assess and mitigate systemic risks (see Art. 55 AIA).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "7bed8e5e-3585-48d6-889d-68089e0581a4",
        "16cd04eb-0068-4a58-9062-a5e44f5f15a0",
        "89e675f5-d397-4279-80b8-8d3635280832",
        "55614e42-478f-480a-a5c3-2ecb6c402441"
      ],
      "parameters": []
    },
    {
      "id": "c8096ef4-8680-44ed-b282-f02c6434e936",
      "title": "AI Service Desk: Risk levels AI systems generally",
      "content": "# Risk levels of AI systems\n\n### AI systems and their categorisation into four risk levels\n\nThe AI Act pursues a risk-based approach to introduce a proportionate and effective binding set of rules for AI systems. AI systems are categorised according to their risk potential as unacceptable, high, low, and minimal risk. The AI Act defines risk as \" the combination of the probability of harm and the severity of that harm\" to public interests (health, safety, fundamental rights, including democracy, the rule of law and environmental protection) and individual interests. Damage can be material or immaterial in nature. It covers physical, psychological, social, or economic damage.\n\nThe General Purpose AI Models (GPAI) take a special position in this categorisation.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "8371d991-3132-4b4f-b0b4-0cecd37c2b99",
        "283d70b2-97f7-4252-9190-378e90b707bd",
        "e87ad418-9936-47c0-b4ec-99f31c41a0a8",
        "24b748de-eebc-404f-8b36-d12eed0a6660",
        "85555ab3-0021-4e99-96f7-b8d2182b43f7",
        "a6e85038-89e8-4790-9457-cb624ca42c00",
        "6b073c5f-3422-4b34-b139-b8645e89b463",
        "986b540c-851f-4387-b2d1-e9b5d9bed869"
      ],
      "parameters": []
    },
    {
      "id": "479fd058-034c-4a38-b806-38bf6965c0b7",
      "title": "AI Service Desk: Risk levels AI systems, Unacceptable risk",
      "content": "### Unacceptable risk\n\nSome practices in connection with AI systems pose too high a risk in terms of the probability of harm occurring and the extent of harm to individual or public interests, which is why they are prohibited.\n\nThese include according to Article 5 AI Act:\n\n-AI systems that manipulate human behaviour in order to circumvent human free will;\n-AI systems that are used to exploit people's weaknesses (due to their age, disability, \n social or economic situation);\n-AI systems that make assessments of natural persons based on social behaviour or \n personal characteristics (social scoring);\n-Risk assessment systems that use profiling to assess the risk or predict whether a natural \n person will commit a criminal offence (predictive policing);\n Untargeted extraction of facial images from the internet or CCTV footage to create facial \n recognition databases;\n-Emotion recognition in the workplace and in educational institutions (with the exception \n of AI systems for medical [e.g. therapeutic use] or security purposes);\n-Biometric categorisation systems to draw or determine conclusions about sensitive \n information (e.g. political, religious or philosophical beliefs, sexual orientation, race);\n-Use of real-time remote biometric recognition systems in publicly accessible spaces for \n law enforcement purposes (with some exceptions such as certain victims or missing \n children, perpetrators of certain offences, etc.).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "16b5ff49-dbab-4e7e-861a-7e1d4cd7cb03",
        "9ef138b6-76a2-493c-a1c6-31522cbe8a79",
        "db043755-0b3e-49ed-9a8c-bdd479acfa09",
        "c3e85c74-274f-4f56-a4da-990cbf4c0ee7",
        "57634ab2-0a60-4452-a940-edfae9ab9d14",
        "cc52311b-4b54-409f-894a-919d3d7b931b",
        "5d100c53-2098-421b-b3fe-4a40e61bee4f",
        "c0d0abea-63e2-4ed1-b273-a1882356a3a7",
        "1d0188f8-8269-474c-aa50-c34765fa8aeb",
        "263930a6-b8af-4ad8-a6bd-ada082c8e8e5",
        "757068d7-b3cb-45d4-9e8a-cb78aad7a2a3",
        "8eb3b772-a8b9-4ca0-bc85-92977cee0861",
        "641fe645-ae39-4cdb-a206-d34d77981d2f",
        "e97971b9-9a7c-49f3-9fe7-3d17ed71fc7a",
        "82237262-5992-4506-b339-958fcac3761c",
        "917725ae-e2f2-4cb4-aa3c-4d530d7373c9",
        "dc094e74-9f86-4a1e-ba5b-2fb56b333cb5",
        "27c3ba49-be38-4d63-856a-35c3e11faa5f",
        "283d70b2-97f7-4252-9190-378e90b707bd",
        "e87ad418-9936-47c0-b4ec-99f31c41a0a8",
        "24b748de-eebc-404f-8b36-d12eed0a6660",
        "85555ab3-0021-4e99-96f7-b8d2182b43f7",
        "a6e85038-89e8-4790-9457-cb624ca42c00"
      ],
      "parameters": []
    },
    {
      "id": "32ec33e4-0b93-498d-bf92-8f561ac77fbd",
      "title": "AI Service Desk: Risk levels AI systems, High-risk AI systems, Annex I",
      "content": "### High-risk AI systems\n\nAs the name suggests, high-risk AI systems according to Article 6 AI Act pose a high risk in terms of the probability of damage occurring and the extent of damage to individual or public interests. However, high-risk AI systems are not prohibited per se; placing on the market or putting into services is only permitted under compliance with certain requirements. Such AI systems are listed in Annex I and III of the AI Act, among others:\n\nAnnex I - AI system is itself the product or is intended as a safety component of a product in the following areas regulated by Union law.\n\nSection A - List of Union harmonisation legislation based on the New Legislative Framework:\n\n-Machinery - Directive 2006/42/EC (will be repealed with effect from 14 January 2027 and \n replaced by Regulation (EU) 2023/1230)\n-Toys - Directive 2009/48/EC (a new proposal for a regulation is being negotiated)\n Recreational craft and personal watercraft - Directive 2013/53/EU\n-Lifts - Directive 2014/33/EU\n-Equipment and protective systems intended for use in potentially explosive atmospheres \n Directive 2014/34/EU\n-Radio equipment - Directive 2014/53/EU\n-Pressure equipment - Directive 2014/68/EU\n-Cableways - Regulation (EU) 2016/424\n-Personal protective equipment - Regulation (EU) 2016/425\n-Burning gaseous fuels - Regulation (EU) 2016/426\n-Medical devices - Regulation (EU) 2017/745\n-In vitro diagnostic medical devices - Regulation (EU) 2017/746\n\nSection B - List of other Union harmonisation legislation:\n\n-Civil aviation - Regulation (EC) No 300/2008 and Regulation (EU) 2018/1139\n-Two- or three-wheel vehicles and quadricycles - Regulation (EU) No 168/2013\n-Agricultural and forestry vehicles - Regulation (EU) No 167/2013\n-Marine equipment - Directive 2014/90/EU\n-Interoperability of the railway system - Directive (EU) 2016/797\n-Motor vehicles and their trailers, and systems, components and separate technical units intended for such vehicles - Regulation (EU) 2018/858 and Regulation (EU) 2019/2144 \n\n",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463",
        "814206ed-2f5c-4c1c-91fd-616f0a128cd3"
      ],
      "parameters": []
    },
    {
      "id": "d6b1d3b4-a6fc-460d-adac-3810b3fd4b9a",
      "title": "AI Service Desk: Risk levels AI systems, High-risk AI systems, Annex  III",
      "content": "\nAnnex III - AI system depending on area of use:\n\n-Biometrics, in so far as their use is permitted under relevant Union or national law;\n-Safety components in critical infrastructure;\n-Education and vocational training;\n-Employment, workers management and access to self-employment;\n-Access to and enjoyment of essential private services and essential public services and benefits;\n -Law enforcement, in so far as their use is permitted under relevant Union or national law;\n-Migration, asylum and border control management, in so far as their use is permitted under relevant Union or national law;\n-Administration of justice and democratic processes.\n",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "416816a1-c29f-4c2c-b7f4-015e60a43042",
        "88111b7f-ede1-45cc-bedc-9aa57dc012a2",
        "f6e922ce-0538-4d92-8c32-a0a750198c89",
        "7e595036-3aa8-4edf-b26f-0627760fb44e",
        "b5274646-0b45-4227-ad44-f5afd431413c",
        "24b540c7-6f65-42f3-80e6-2a3ec3e86999",
        "56f9d88f-1824-4420-b01b-a3d855f3bbaf",
        "b740c960-f13f-4b09-95d9-3443b6e19b26"
      ],
      "parameters": []
    },
    {
      "id": "a9bf3bf8-40da-4c63-a908-e12625e1e946",
      "title": "AI Service Desk: Risk levels AI systems, critical infrastructure and safety components",
      "content": "#### Critical Infrastructure and safety components\n\nAI systems used in critical infrastructure may be considered high-risk AI systems under Article 6(2) in conjunction with Annex III, Item 2 of the AI Act (AIA). Specifically, an AI system is classified as a high-risk AI system if it is used as a safety component:\n\n-in the management and operation\n-of critical digital infrastructure,\n-road traffic, or\n-in the supply of water, gas, heating or electricity.\n\nFor the assessment, the key questions are what constitutes \"critical infrastructure\" and what qualifies as a \"safety component.\"\n\nIn general, critical infrastructure is part of a critical entity. According to the definition in Article 3(62) AIA, the term \"critical infrastructure\" refers to Article 2(4) of Directive (EU) 2022/2557 (\"Directive on the Resilience of Critical Entities,\" \"CER\"). According to Articles 2(1), 2(4), and 2(5) CER, a \"critical entity\"—whether a public or private entity—must be designated as such by the respective Member State.\n\nThe corresponding \"critical infrastructure\" includes:\n\n-Assets, facilities, equipment, networks, or systems, or\n-Parts of an asset, facility, equipment, network, or system,\n-that are necessary for the provision of an essential service. \n\nA service is deemed essential if it is crucial for:\n\nThe maintenance of vital societal functions,\n-Critical economic activities,\n-Public health and safety, or\n-The environment.\n\nArticle 2 of the Delegated Regulation 2023/2450 of the European Commission, issued pursuant to CER, provides a non-exhaustive list of essential services.\n\nThe term \"safety component\" is defined in Article 3(14) AIA as follows:\n\nA component of a product or of an AI system which fulfils a safety function for that product or AI system, or the failure or malfunctioning of which endangers the health and safety of persons or property.\n\nRegarding critical infrastructure, the co-legislators provide further clarification in Recital 55 AIA:\n\n(55) […] Safety components of critical infrastructure, including critical digital infrastructure, are systems used to directly protect the physical integrity of critical infrastructure or the health and safety of persons and property but which are not necessary in order for the system to function. The failure or malfunctioning of such components might directly lead to risks to the physical integrity of critical infrastructure and thus to risks to health and safety of persons and property. Components intended to be used solely for cybersecurity purposes should not qualify as safety components. Examples of safety components of such critical infrastructure may include systems for monitoring water pressure or fire alarm controlling systems in cloud computing centres.\n\nWhether an exception applies under one of the grounds for exemption outlined in Article 6(3) AIA will depend on the specific circumstances of each case.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463",
        "88111b7f-ede1-45cc-bedc-9aa57dc012a2",
        "d89b42c0-fb0b-40c3-a99d-40aae0d9dea9",
        "ee5c5bfc-ded7-4010-bdae-d396f7798630"
      ],
      "parameters": []
    },
    {
      "id": "032907d6-0290-44ab-bee5-be05248e3ae2",
      "title": "AI Service Desk: Risk levels AI systems, limited risk",
      "content": "### AI systems with \"limited\" risk\n\nAI systems with \"limited\" risk are AI systems whose risk can be minimised through transparency. Such AI systems are not prohibited; providers and deployers are mainly subject to transparency obligations, such as informing persons that they are interacting with an AI system or that content has been artificially generated. AI systems with \"limited\" risk include the following systems in accordance with Article 50 AI Act:\n\n-AI systems that interact directly with natural persons (e.g. chatbots);\n-AI systems that generate or manipulate image, audio, text or video content, also known as generative AI (this is to be distinguished from deepfakes for manipulating human behaviour, which are prohibited);\n-Use of biometric categorisation and emotion recognition systems (a distinction must be made between these and AI systems, which are prohibited!).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "42b80e11-733d-441e-98e1-d79837892537"
      ],
      "parameters": []
    },
    {
      "id": "d9cb5958-fede-437d-a0f6-ad55c6d6db50",
      "title": "AI Service Desk: Risk levels AI systems, minimal risk",
      "content": "### AI systems with \"minimal\" or no risk\n\nAll other AI systems are classified as those with \"minimal\" or no risk. They are not subject to any specific obligations under the AI Act; compliance with Codes of Practices is recommended but voluntary.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "7cf70e13-b212-45e3-bf7b-66d860a91315",
        "b39b09ad-8deb-4ec1-bfae-a098c9425de2"
      ],
      "parameters": []
    },
    {
      "id": "63523a36-401e-4c90-82f2-be96588697ba",
      "title": "AI Service Desk: Sanctions",
      "content": "# Sanctions\n\nTo ensure that the prohibitions and obligations set out in the AI Act are also complied with, regulations are needed to ensure enforcement. In the form known at EU level, Member States must ensure that the sanctions provided for must be \"effective, proportionate and dissuasive\". Possible forms include sanctions and other enforcement measures such as warnings and non-monetary measures. The imposition of fines is an essential part of this package of sanctions. The offences and the maximum fines are specified by the AIA.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "09a1468c-9fc4-44ee-a8c1-fd07084ea0d8",
        "de889fca-779f-4b70-8ae2-166b1f752102",
        "61300139-468e-4f24-871e-61a39036b15f",
        "c837084f-afdb-431e-b95a-b1d3bdacff9e",
        "48edb908-8ded-493b-aa2d-f7864af2bd26"
      ],
      "parameters": []
    },
    {
      "id": "80bfe49e-2622-477d-9750-5eb45f903f38",
      "title": "AI Service Desk: Sanctions, fines",
      "content": "The following maximum fines may be imposed for the following violations (see Art. 99, 101 AIA):  \n\n-Up to EUR 35 million or 7 per cent of the previous year's total worldwide turnover (whichever is higher) for non-compliance with the prohibited practices;\n-Up to EUR 15 million or 3 per cent of the previous year's total global turnover (whichever is higher) for breaches of obligations that conformity assessment bodies and the relevant stakeholders must comply with;\n  -Providers of high-risk AI systems, AI systems with limited risk, GPAI \n    models;\n  -Deployers of high-risk AI systems and AI systems with limited risk;\n  -Authorised representative;\n  -Importer;\n  -distributor;\n-Up to EUR 7.5 million or 1.5 per cent of the previous year's total worldwide turnover (whichever is higher) for providing incorrect, incomplete or misleading information to conformity assessment bodies or competent national authorities in response to their request for information.\n\nAs the EU institutions, bodies, offices and agencies should lead by example, they are also subject to the rules and possible sanctions. The following maximum fines can be imposed for the following infringements (see Art. 100 AIA):\n\n-Up to EUR 1.5 million for non-compliance with prohibited practices;\n-Up to EUR 750 000 in case of non-compliance of the AI-system with requirements or obligations laid down in this Regulation.\n\n\nWho is authorised to impose fines depends on who has been delegated supervision. In principle, it is national authorities that are authorised to impose fines (see Art. 99 AIA). In the case of providers of GPAI models, the Commission may impose fines (see Art. 101 AIA); in the case of violations of the AIA by EU institutions, bodies, offices and agencies, the European Data Protection Supervisor is authorised to do so (see Art. 100 AIA). \n\nWhen considering whether and/or to what extent a fine should be imposed, the following aspects, among others, should be taken into account (see Art. 99 para. 7 AIA):\n\n-the nature, gravity and duration of the infringement and its consequences, taking into account the purpose of the AI system and, where applicable, the number of persons concerned and the extent of the harm suffered by them;\n-whether the same operator has already been fined by other market surveillance authorities for the same infringement or has been fined by other authorities for infringements of Union or national law where those infringements result from the same act or omission constituting a relevant infringement of this Regulation;\n-Size, annual turnover and market share of the actor who committed the offence;\n-Intentional or negligent nature of the offence;\netc.\n\nIn the case of EU institutions, bodies, offices and agencies, special reasons must be considered (see Art. 100 para. 1 AIA for more details).",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "61300139-468e-4f24-871e-61a39036b15f",
        "de889fca-779f-4b70-8ae2-166b1f752102",
        "c837084f-afdb-431e-b95a-b1d3bdacff9e",
        "48edb908-8ded-493b-aa2d-f7864af2bd26",
        "09a1468c-9fc4-44ee-a8c1-fd07084ea0d8"
      ],
      "parameters": []
    },
    {
      "id": "d71c298c-6a5a-4562-a514-2cdd57d72029",
      "title": "AI Service Desk: Transparency obligations",
      "content": "# Information, disclosure and labelling obligations\n\nIn the \"limited risk\" risk level, information, disclosure and labelling obligations are imposed on providers and deployers of AI systems. The specific provisions can be found in Art. 50 AIA.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "42b80e11-733d-441e-98e1-d79837892537"
      ],
      "parameters": []
    },
    {
      "id": "5068565f-3628-4022-83bb-b39a1fa52110",
      "title": "AI Service Desk: Transparency obligations, direct interactions",
      "content": "### AI systems designed for direct interactions\n\n**Providers of AI systems that are intended for direct interaction with natural persons must design and develop them in such a way that the natural persons concerned are informed that they are interacting with an AI system (Art. 50 para. 1 AIA). Chatbots typically fall into this category. The provider of a chatbot system must design it in such a way that it is made clear in conversations that an interaction with an AI is taking place.\n\nException: The use of an AI system is obvious from the perspective of a reasonably well-informed, observant and circumspect natural person due to the circumstances and context of use. E.g. virtual assistance systems that interact with their users through voice commands such as Siri (Apple) or Alexa (Amazon)",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "42b80e11-733d-441e-98e1-d79837892537",
        "7401fefb-6095-47f0-a00e-78aac879499e"
      ],
      "parameters": []
    },
    {
      "id": "a4835586-f359-456e-833a-89d5d54259ec",
      "title": "AI Service Desk: Transparency obligations, generating synthetic content, provider",
      "content": "### AI systems generating synthetic content\n\nProviders of AI systems, including general purpose AI systems, that generate synthetic audio, image, video or text content shall ensure that the output of the AI system is labelled in a machine-readable format and detectable as artificially generated or manipulated (Art. 50 para. 2 AIA). The labelling must be carried out using technical solutions, such as watermarks, metadata identifiers, cryptographic methods to prove the origin and authenticity of the content, logging methods, fingerprints or other techniques, or a combination of such techniques depending on the circumstances (see recital 133). Typically, this category includes AI-powered text generators such as ChatGPT and AI-powered image and video generators Midjourney or DALL-E, etc. The labelling must be machine-readable; there is no labelling requirement for providers intended for human viewers.\n\nException 1: AI systems that perform a supporting function for standard editing or that do not significantly change the input data provided by the deployer or its semantics. E.g. small-scale \"generative fill\" in image processing programmes.\n\nException 2: AI systems that are legally authorised for the detection, prevention or investigation of criminal offences.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "aedc6b30-3ec3-4277-a8aa-bcc97fad2474",
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "7401fefb-6095-47f0-a00e-78aac879499e",
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6"
      ],
      "parameters": []
    },
    {
      "id": "e2f421fa-1fc0-47b8-9769-ca6042253dcd",
      "title": "AI Service Desk: Transparency obligations, generating synthetic content, deepfakes",
      "content": "**Deployers also have a number of disclosure obligations. In the case of AI systems that generate or manipulate deepfake image, sound or video content, it must be disclosed that the content has been artificially generated or manipulated. There is no such disclosure obligation for image, sound and video content that is not deepfake. (Art. 50 para. 4 subpara. 1 AIA)\n\nException 1: Use for the detection, prevention, investigation or prosecution of criminal offences is permitted by law.\n\nException 2: Where the image, sound or video content is part of an obviously artistic, creative, satirical, fictional or analogous work or programme, the transparency obligations set out in this paragraph shall be limited to disclosing the presence of such generated or manipulated content in an appropriate manner that does not impair the presentation or enjoyment of the work. E.g. exaggerated satirical portrayal of persons of public interest.\n",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "b757b57f-4015-401c-8003-852ba5aaefc0",
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "42b80e11-733d-441e-98e1-d79837892537",
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6",
        "71a61842-dac1-4a64-b31a-cf6c68220d51"
      ],
      "parameters": []
    },
    {
      "id": "9366d9d8-b619-4196-9503-fe6f97bf038b",
      "title": "AI Service Desk: Transparency obligations, generating synthetic content, text generating",
      "content": "**Deployers of an AI system that generates or manipulates text that is published with the purpose of informing the public about matters of public interest must disclose that the text was artificially generated or manipulated (Art. 50 para. 4 subpara. 2 AIA). AI-generated texts that are not published or that are not used to inform the public about matters of public interest are not subject to a disclosure obligation.\n\nException 1: Artificially generated text content is subject to human review or editorial control and a natural or legal person bears editorial responsibility for the publication of the content. E.g. traditional media owner (newspaper publisher)\n\nException 2: Use for the detection, prevention, investigation or prosecution of criminal offences is permitted by law. ",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "b757b57f-4015-401c-8003-852ba5aaefc0",
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "42b80e11-733d-441e-98e1-d79837892537",
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6"
      ],
      "parameters": []
    },
    {
      "id": "f142935d-789b-4aa9-b531-57d800014ffe",
      "title": "AI Service Desk: Transparency obligations, generating synthetic image/video/audio",
      "content": "#### Is a synthetic image/video/audio also a deepfake?\n\nAlthough there are overlaps between the terms synthetic content and deepfake, a distinction must be made with regard to the legal consequences.\n\nThe AIA defines the term \"deepfake\" as follows (Art. 3 no. 60 AIA):\n\nan AI-generated or manipulated image, sound or video content that resembles real persons, objects, places, facilities or events and would falsely appear to a person to be real or truthful\nThe term \"deepfake\" is a portmanteau word - i.e. a word that consists of at least two word segments - and is made up of the words \"deep learning\" and \"fake\". To summarise, it refers to media content that appears realistic but has not actually taken place. The areas of application are wide-ranging, both positive and negative.\n\nPositive examples:\n\n-Entertainment and media: Deepfakes can be used in films and video games to create impressive special effects. Music videos can also be created using deepfake technology to achieve visually appealing effects.\n-Forensics: Incidents can be reconstructed or visualised that would otherwise be difficult to depict due to a lack of videos or images.\n\n\nNegative examples:\n\n-Disinformation and fake news: False information can be spread using deepfake technology by presenting personalities in videos who do or say something that they have never done or said.\n-Cybersecurity and data protection: Criminals can use deepfakes to create fraudulent videos or calls to deceive people or commit identity-related crimes.\n-Misuse and blackmail: Deepfake technology can be misused to create compromising images or videos of innocent people (e.g. production of pornographic content)\n \n\nSynthetic audio, image or video content is all content that has not been generated by humans (see recital 133). The term therefore goes further than a deepfake. An AI-generated cartoon, for example, is synthetic image content, but not a deepfake because it is not realistic. An AI-generated video in which, for example, an actual politician is simulated in front of parliament in an interview situation and talks about political agendas is synthetic video content that is also a deepfake. Such situations take place on a daily basis and can therefore be mistaken for real by citizens.\n\nHowever, whether a deepfake actually exists must always be investigated on a case-by-case basis!\n\nTo summarise: Every deepfake is a synthetic image/video/audio, but not every synthetic image/video/audio is also a deepfake.\n\n ",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "aedc6b30-3ec3-4277-a8aa-bcc97fad2474",
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "42b80e11-733d-441e-98e1-d79837892537",
        "71a61842-dac1-4a64-b31a-cf6c68220d51"
      ],
      "parameters": []
    },
    {
      "id": "fd81d40a-c22c-4aff-a4d2-ddc6dbc37550",
      "title": "AI Service Desk: Transparency obligations, emotion recognition",
      "content": "### AI systems designed for emotion recognition\n\nDeployers of emotion recognition systems or a system for biometric categorisation shall inform the natural persons concerned about the operation of the system (Art. 50 para. 3 AIA).\n\nException: AI systems that are legally authorised for the detection, prevention or investigation of criminal offences, provided that appropriate safeguards are in place to protect the rights and freedoms of third parties.",
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "relevantChunksIds": [
        "986b540c-851f-4387-b2d1-e9b5d9bed869",
        "42b80e11-733d-441e-98e1-d79837892537",
        "f3f24184-1d9d-4f5c-af39-7853beef1c1b",
        "87c3670b-a636-4431-aadd-9cfe780035b0"
      ],
      "parameters": []
    },
    {
      "chunk_idx": 0,
      "id": "b833c1d7-ad46-4548-a2c6-63f671c1d211",
      "title": "Recital 1",
      "relevantChunksIds": [
        "be26e0cd-4d28-42f6-8560-20e6911c4c4f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(1) The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development, the placing on the market, the putting into service and the use of artificial intelligence systems (AI systems) in the Union, in accordance with Union values, to promote the uptake of human centric and trustworthy artificial intelligence (AI) while ensuring a high level of protection of health, safety, fundamental rights as enshrined in the Charter of Fundamental Rights of the European Union (the ‘Charter’), including democracy, the rule of law and environmental protection, to protect against the harmful effects of AI systems in the Union, and to support innovation. This Regulation ensures the free movement, cross-border, of AI-based goods and services, thus preventing Member States from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by this Regulation.",
      "original_content": "(1) Zweck dieser Verordnung ist es, das Funktionieren des Binnenmarkts zu verbessern, indem ein einheitlicher Rechtsrahmen insbesondere für die Entwicklung, das Inverkehrbringen, die Inbetriebnahme und die Verwendung von Systemen künstlicher Intelligenz (KI-Systeme) in der Union im Einklang mit den Werten der Union festgelegt wird, um die Einführung von menschenzentrierter und vertrauenswürdiger künstlicher Intelligenz (KI) zu fördern und gleichzeitig ein hohes Schutzniveau in Bezug auf Gesundheit, Sicherheit und der in der Charta der Grundrechte der Europäischen Union („Charta“) verankerten Grundrechte, einschließlich Demokratie, Rechtsstaatlichkeit und Umweltschutz, sicherzustellen, den Schutz vor schädlichen Auswirkungen von KI-Systemen in der Union zu gewährleisten und gleichzeitig die Innovation zu unterstützen. Diese Verordnung gewährleistet den grenzüberschreitenden freien Verkehr KI-gestützter Waren und Dienstleistungen, wodurch verhindert wird, dass die Mitgliedstaaten die Entwicklung, Vermarktung und Verwendung von KI-Systemen beschränken, sofern dies nicht ausdrücklich durch diese Verordnung erlaubt wird."
    },
    {
      "chunk_idx": 1,
      "id": "aa44ef37-ff65-4237-9ed5-b34174aa9c6a",
      "title": "Recital 2",
      "relevantChunksIds": [
        "be26e0cd-4d28-42f6-8560-20e6911c4c4f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(2) This Regulation should be applied in accordance with the values of the Union enshrined as in the Charter, facilitating the protection of natural persons, undertakings, democracy, the rule of law and environmental protection, while boosting innovation and employment and making the Union a leader in the uptake of trustworthy AI.",
      "original_content": "(2) Diese Verordnung sollte im Einklang mit den in der Charta verankerten Werten der Union angewandt werden, den Schutz von natürlichen Personen, Unternehmen, Demokratie und Rechtsstaatlichkeit sowie der Umwelt erleichtern und gleichzeitig Innovation und Beschäftigung fördern und der Union eine Führungsrolle bei der Einführung vertrauenswürdiger KI verschaffen."
    },
    {
      "chunk_idx": 2,
      "id": "10c70d26-f011-44f0-89db-011879d8401c",
      "title": "Recital 3",
      "relevantChunksIds": [
        "be26e0cd-4d28-42f6-8560-20e6911c4c4f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(3) AI systems can be easily deployed in a large variety of sectors of the economy and many parts of society, including across borders, and can easily circulate throughout the Union. Certain Member States have already explored the adoption of national rules to ensure that AI is trustworthy and safe and is developed and used in accordance with fundamental rights obligations. Diverging national rules may lead to the fragmentation of the internal market and may decrease legal certainty for operators that develop, import or use AI systems. A consistent and high level of protection throughout the Union should therefore be ensured in order to achieve trustworthy AI, while divergences hampering the free circulation, innovation, deployment and the uptake of AI systems and related products and services within the internal market should be prevented by laying down uniform obligations for operators and guaranteeing the uniform protection of overriding reasons of public interest and of rights of persons throughout the internal market on the basis of Article 114 of the Treaty on the Functioning of the European Union (TFEU). To the extent that this Regulation contains specific rules on the protection of individuals with regard to the processing of personal data concerning restrictions of the use of AI systems for remote biometric identification for the purpose of law enforcement, of the use of AI systems for risk assessments of natural persons for the purpose of law enforcement and of the use of AI systems of biometric categorisation for the purpose of law enforcement, it is appropriate to base this Regulation, in so far as those specific rules are concerned, on Article 16 TFEU. In light of those specific rules and the recourse to Article 16 TFEU, it is appropriate to consult the European Data Protection Board.",
      "original_content": "(3) KI-Systeme können problemlos in verschiedenen Bereichen der Wirtschaft und Gesellschaft, auch grenzüberschreitend, eingesetzt werden und in der gesamten Union verkehren. Einige Mitgliedstaaten haben bereits die Verabschiedung nationaler Vorschriften in Erwägung gezogen, damit KI vertrauenswürdig und sicher ist und im Einklang mit den Grundrechten entwickelt und verwendet wird. Unterschiedliche nationale Vorschriften können zu einer Fragmentierung des Binnenmarkts führen und können die Rechtssicherheit für Akteure, die KI-Systeme entwickeln, einführen oder verwenden, beeinträchtigen. Daher sollte in der gesamten Union ein einheitlich hohes Schutzniveau sichergestellt werden, um eine vertrauenswürdige KI zu erreichen, wobei Unterschiede, die den freien Verkehr, Innovationen, den Einsatz und die Verbreitung von KI-Systemen und damit zusammenhängenden Produkten und Dienstleistungen im Binnenmarkt behindern, vermieden werden sollten, indem den Akteuren einheitliche Pflichten auferlegt werden und der gleiche Schutz der zwingenden Gründe des Allgemeininteresses und der Rechte von Personen im gesamten Binnenmarkt auf der Grundlage des Artikels 114 des Vertrags über die Arbeitsweise der Europäischen Union (AEUV) gewährleistet wird. Soweit diese Verordnung konkrete Vorschriften zum Schutz von Einzelpersonen im Hinblick auf die Verarbeitung personenbezogener Daten enthält, mit denen die Verwendung von KI-Systemen zur biometrischen Fernidentifizierung zu Strafverfolgungszwecken, die Verwendung von KI-Systemen für die Risikobewertung natürlicher Personen zu Strafverfolgungszwecken und die Verwendung von KI-Systemen zur biometrischen Kategorisierung zu Strafverfolgungszwecken eingeschränkt wird, ist es angezeigt, diese Verordnung in Bezug auf diese konkreten Vorschriften auf Artikel 16 AEUV zu stützen. Angesichts dieser konkreten Vorschriften und des Rückgriffs auf Artikel 16 AEUV ist es angezeigt, den Europäischen Datenschutzausschuss zu konsultieren."
    },
    {
      "chunk_idx": 3,
      "id": "6ead0916-b1d0-4ee7-a131-b86ac144d0ac",
      "title": "Recital 4",
      "relevantChunksIds": [
        "be26e0cd-4d28-42f6-8560-20e6911c4c4f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(4) AI is a fast evolving family of technologies that contributes to a wide array of economic, environmental and societal benefits across the entire spectrum of industries and social activities. By improving prediction, optimising operations and resource allocation, and personalising digital solutions available for individuals and organisations, the use of AI can provide key competitive advantages to undertakings and support socially and environmentally beneficial outcomes, for example in healthcare, agriculture, food safety, education and training, media, sports, culture, infrastructure management, energy, transport and logistics, public services, security, justice, resource and energy efficiency, environmental monitoring, the conservation and restoration of biodiversity and ecosystems and climate change mitigation and adaptation.",
      "original_content": "(4) KI bezeichnet eine Reihe von Technologien, die sich rasant entwickeln und zu vielfältigem Nutzen für Wirtschaft, Umwelt und Gesellschaft über das gesamte Spektrum industrieller und gesellschaftlicher Tätigkeiten hinweg beitragen. Durch die Verbesserung der Vorhersage, die Optimierung der Abläufe, Ressourcenzuweisung und die Personalisierung digitaler Lösungen, die Einzelpersonen und Organisationen zur Verfügung stehen, kann die Verwendung von KI Unternehmen wesentliche Wettbewerbsvorteile verschaffen und zu guten Ergebnissen für Gesellschaft und Umwelt führen, beispielsweise in den Bereichen Gesundheitsversorgung, Landwirtschaft, Lebensmittelsicherheit, allgemeine und berufliche Bildung, Medien, Sport, Kultur, Infrastrukturmanagement, Energie, Verkehr und Logistik, öffentliche Dienstleistungen, Sicherheit, Justiz, Ressourcen- und Energieeffizienz, Umweltüberwachung, Bewahrung und Wiederherstellung der Biodiversität und der Ökosysteme sowie Klimaschutz und Anpassung an den Klimawandel."
    },
    {
      "chunk_idx": 4,
      "id": "0edca1d5-9879-4157-bd56-130035f6204f",
      "title": "Recital 5",
      "relevantChunksIds": [
        "be26e0cd-4d28-42f6-8560-20e6911c4c4f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(5) At the same time, depending on the circumstances regarding its specific application, use, and level of technological development, AI may generate risks and cause harm to public interests and fundamental rights that are protected by Union law. Such harm might be material or immaterial, including physical, psychological, societal or economic harm.",
      "original_content": "(5) Gleichzeitig kann KI je nach den Umständen ihrer konkreten Anwendung und Nutzung sowie der technologischen Entwicklungsstufe Risiken mit sich bringen und öffentliche Interessen und grundlegende Rechte schädigen, die durch das Unionsrecht geschützt sind. Ein solcher Schaden kann materieller oder immaterieller Art sein, einschließlich physischer, psychischer, gesellschaftlicher oder wirtschaftlicher Schäden."
    },
    {
      "chunk_idx": 5,
      "id": "27cb6f5f-8336-46c3-a0bd-07955c3bd714",
      "title": "Recital 6",
      "relevantChunksIds": [
        "be26e0cd-4d28-42f6-8560-20e6911c4c4f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(6) Given the major impact that AI can have on society and the need to build trust, it is vital for AI and its regulatory framework to be developed in accordance with Union values as enshrined in Article 2 of the Treaty on European Union (TEU), the fundamental rights and freedoms enshrined in the Treaties and, pursuant to Article 6 TEU, the Charter. As a prerequisite, AI should be a human-centric technology. It should serve as a tool for people, with the ultimate aim of increasing human well-being.",
      "original_content": "(6) Angesichts der großen Auswirkungen, die KI auf die Gesellschaft haben kann, und der Notwendigkeit, Vertrauen aufzubauen, ist es von entscheidender Bedeutung, dass KI und ihr Regulierungsrahmen im Einklang mit den in Artikel 2 des Vertrags über die Europäische Union (EUV) verankerten Werten der Union, den in den Verträgen und, nach Artikel 6 EUV, der Charta verankerten Grundrechten und -freiheiten entwickelt werden. Voraussetzung sollte sein, dass KI eine menschenzentrierte Technologie ist. Sie sollte den Menschen als Instrument dienen und letztendlich das menschliche Wohlergehen verbessern."
    },
    {
      "chunk_idx": 6,
      "id": "fde668f5-fe30-48cc-b961-be101bc4e1a7",
      "title": "Recital 7",
      "relevantChunksIds": [
        "be26e0cd-4d28-42f6-8560-20e6911c4c4f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(7) In order to ensure a consistent and high level of protection of public interests as regards health, safety and fundamental rights, common rules for high-risk AI systems should be established. Those rules should be consistent with the Charter, non-discriminatory and in line with the Union’s international trade commitments. They should also take into account the European Declaration on Digital Rights and Principles for the Digital Decade and the Ethics guidelines for trustworthy AI of the High-Level Expert Group on Artificial Intelligence (AI HLEG).",
      "original_content": "(7) Um ein einheitliches und hohes Schutzniveau in Bezug auf öffentliche Interessen im Hinblick auf Gesundheit, Sicherheit und Grundrechte zu gewährleisten, sollten für alle Hochrisiko-KI-Systeme gemeinsame Vorschriften festgelegt werden. Diese Vorschriften sollten mit der Charta im Einklang stehen, nichtdiskriminierend sein und mit den internationalen Handelsverpflichtungen der Union vereinbar sein. Sie sollten auch die Europäische Erklärung zu den digitalen Rechten und Grundsätzen für die digitale Dekade und die Ethikleitlinien für vertrauenswürdige KI der hochrangigen Expertengruppe für künstliche Intelligenz berücksichtigen."
    },
    {
      "chunk_idx": 7,
      "id": "8b015b72-9483-4675-bf5a-02c2ddc3a267",
      "title": "Recital 8",
      "relevantChunksIds": [
        "be26e0cd-4d28-42f6-8560-20e6911c4c4f",
        "2c00d4ce-c731-4696-9693-eda24b9eaf27",
        "e7a7f354-d2ea-480e-80eb-6f29beca4569"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(8) A Union legal framework laying down harmonised rules on AI is therefore needed to foster the development, use and uptake of AI in the internal market that at the same time meets a high level of protection of public interests, such as health and safety and the protection of fundamental rights, including democracy, the rule of law and environmental protection as recognised and protected by Union law. To achieve that objective, rules regulating the placing on the market, the putting into service and the use of certain AI systems should be laid down, thus ensuring the smooth functioning of the internal market and allowing those systems to benefit from the principle of free movement of goods and services. Those rules should be clear and robust in protecting fundamental rights, supportive of new innovative solutions, enabling a European ecosystem of public and private actors creating AI systems in line with Union values and unlocking the potential of the digital transformation across all regions of the Union. By laying down those rules as well as measures in support of innovation with a particular focus on small and medium enterprises (SMEs), including startups, this Regulation supports the objective of promoting the European human-centric approach to AI and being a global leader in the development of secure, trustworthy and ethical AI as stated by the European Council (5), and it ensures the protection of ethical principles, as specifically requested by the European Parliament (6).",
      "original_content": "(8) Daher ist ein Rechtsrahmen der Union mit harmonisierten Vorschriften für KI erforderlich, um die Entwicklung, Verwendung und Verbreitung von KI im Binnenmarkt zu fördern und gleichzeitig ein hohes Schutzniveau in Bezug auf öffentliche Interessen wie etwa Gesundheit und Sicherheit und den Schutz der durch das Unionsrecht anerkannten und geschützten Grundrechte, einschließlich der Demokratie, der Rechtsstaatlichkeit und des Umweltschutzes, zu gewährleisten. Zur Umsetzung dieses Ziels sollten Vorschriften für das Inverkehrbringen, die Inbetriebnahme und die Verwendung bestimmter KI-Systeme festgelegt werden, um das reibungslose Funktionieren des Binnenmarkts zu gewährleisten, sodass diesen Systemen der Grundsatz des freien Waren- und Dienstleistungsverkehrs zugutekommen kann. Diese Regeln sollten klar und robust sein, um die Grundrechte zu schützen, neue innovative Lösungen zu unterstützen und ein europäisches Ökosystem öffentlicher und privater Akteure zu ermöglichen, die KI-Systeme im Einklang mit den Werten der Union entwickeln, und um das Potenzial des digitalen Wandels in allen Regionen der Union zu erschließen. Durch die Festlegung dieser Vorschriften sowie durch Maßnahmen zur Unterstützung der Innovation mit besonderem Augenmerk auf kleinen und mittleren Unternehmen (KMU), einschließlich Start-up-Unternehmen, unterstützt diese Verordnung das vom Europäischen Rat formulierte Ziel, das europäische menschenzentrierte KI-Konzept zu fördern und bei der Entwicklung einer sicheren, vertrauenswürdigen und ethisch vertretbaren KI weltweit eine Führungsrolle einzunehmen (Fußnote 5), und sorgt für den vom Europäischen Parlament ausdrücklich geforderten Schutz von Ethikgrundsätzen (Fußnote 6). Fußnote 5: Europäischer Rat, Außerordentliche Tagung des Europäischen Rates (1. und 2. Oktober 2020) – Schlussfolgerungen, EUCO 13/20, 2020, S. 6., Fußnote 6: Entschließung des Europäischen Parlaments vom 20. Oktober 2020 mit Empfehlungen an die Kommission zu dem Rahmen für die ethischen Aspekte von künstlicher Intelligenz, Robotik und damit zusammenhängenden Technologien, 2020/2012 (INL)."
    },
    {
      "chunk_idx": 8,
      "id": "59e64fd2-e26e-426d-be65-48f42d265850",
      "title": "Recital 9",
      "relevantChunksIds": [
        "be26e0cd-4d28-42f6-8560-20e6911c4c4f",
        "55e8665d-afb5-46d1-b258-64f964b08d09",
        "e7a7f354-d2ea-480e-80eb-6f29beca4569"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(9) Harmonised rules applicable to the placing on the market, the putting into service and the use of high-risk AI systems should be laid down consistently with Regulation (EC) No 765/2008 of the European Parliament and of the Council (7), Decision No 768/2008/EC of the European Parliament and of the Council (8) and Regulation (EU) 2019/1020 of the European Parliament and of the Council (9) (New Legislative Framework). The harmonised rules laid down in this Regulation should apply across sectors and, in line with the New Legislative Framework, should be without prejudice to existing Union law, in particular on data protection, consumer protection, fundamental rights, employment, and protection of workers, and product safety, to which this Regulation is complementary. As a consequence, all rights and remedies provided for by such Union law to consumers, and other persons on whom AI systems may have a negative impact, including as regards the compensation of possible damages pursuant to Council Directive 85/374/EEC (10) remain unaffected and fully applicable. Furthermore, in the context of employment and protection of workers, this Regulation should therefore not affect Union law on social policy and national labour law, in compliance with Union law, concerning employment and working conditions, including health and safety at work and the relationship between employers and workers. This Regulation should also not affect the exercise of fundamental rights as recognised in the Member States and at Union level, including the right or freedom to strike or to take other action covered by the specific industrial relations systems in Member States as well as the right to negotiate, to conclude and enforce collective agreements or to take collective action in accordance with national law. This Regulation should not affect the provisions aiming to improve working conditions in platform work laid down in a Directive of the European Parliament and of the Council on improving working conditions in platform work. Moreover, this Regulation aims to strengthen the effectiveness of such existing rights and remedies by establishing specific requirements and obligations, including in respect of the transparency, technical documentation and record-keeping of AI systems. Furthermore, the obligations placed on various operators involved in the AI value chain under this Regulation should apply without prejudice to national law, in compliance with Union law, having the effect of limiting the use of certain AI systems where such law falls outside the scope of this Regulation or pursues legitimate public interest objectives other than those pursued by this Regulation. For example, national labour law and law on the protection of minors, namely persons below the age of 18, taking into account the UNCRC General Comment No 25 (2021) on children’s rights in relation to the digital environment, insofar as they are not specific to AI systems and pursue other legitimate public interest objectives, should not be affected by this Regulation.",
      "original_content": "(9) Es sollten harmonisierte Vorschriften für das Inverkehrbringen, die Inbetriebnahme und die Verwendung von Hochrisiko-KI-Systemen im Einklang mit der Verordnung (EG) Nr. 765/2008 des Europäischen Parlaments und des Rates (Fußnote 7), dem Beschluss Nr. 768/2008/EG des Europäischen Parlaments und des Rates (Fußnote 8) und der Verordnung (EU) 2019/1020 des Europäischen Parlaments und des Rates (Fußnote 9) („neuer Rechtsrahmen“) festgelegt werden. Die in dieser Verordnung festgelegten harmonisierten Vorschriften sollten in allen Sektoren gelten und sollten im Einklang mit dem neuen Rechtsrahmen bestehendes Unionsrecht, das durch diese Verordnung ergänzt wird, unberührt lassen, insbesondere in den Bereichen Datenschutz, Verbraucherschutz, Grundrechte, Beschäftigung, Arbeitnehmerschutz und Produktsicherheit. Daher bleiben alle Rechte und Rechtsbehelfe, die für Verbraucher und andere Personen, auf die sich KI-Systeme negativ auswirken können, gemäß diesem Unionsrecht vorgesehen sind, auch in Bezug auf einen möglichen Schadenersatz gemäß der Richtlinie 85/374/EWG des Rates (Fußnote 10) unberührt und in vollem Umfang anwendbar. Darüber hinaus und unter Einhaltung des Unionsrechts in Bezug auf Beschäftigungs- und Arbeitsbedingungen, einschließlich des Gesundheitsschutzes und der Sicherheit am Arbeitsplatz sowie der Beziehungen zwischen Arbeitgebern und Arbeitnehmern sollte diese Verordnung daher — was Beschäftigung und den Schutz von Arbeitnehmern angeht — das Unionsrecht im Bereich der Sozialpolitik und die nationalen Arbeitsrechtsvorschriften nicht berühren. Diese Verordnung sollte auch die Ausübung der in den Mitgliedstaaten und auf Unionsebene anerkannten Grundrechte, einschließlich des Rechts oder der Freiheit zum Streik oder zur Durchführung anderer Maßnahmen, die im Rahmen der spezifischen Systeme der Mitgliedstaaten im Bereich der Arbeitsbeziehungen vorgesehen sind, sowie das Recht, im Einklang mit nationalem Recht Kollektivvereinbarungen auszuhandeln, abzuschließen und durchzusetzen oder kollektive Maßnahmen zu ergreifen, nicht beeinträchtigen. Diese Verordnung sollte die in einer Richtlinie des Europäischen Parlaments und des Rates zur Verbesserung der Arbeitsbedingungen in der Plattformarbeit enthaltenen Bestimmungen nicht berühren. Darüber hinaus zielt diese Verordnung darauf ab, die Wirksamkeit dieser bestehenden Rechte und Rechtsbehelfe zu stärken, indem bestimmte Anforderungen und Pflichten, auch in Bezug auf die Transparenz, die technische Dokumentation und das Führen von Aufzeichnungen von KI-Systemen, festgelegt werden. Ferner sollten die in dieser Verordnung festgelegten Pflichten der verschiedenen Akteure, die an der KI-Wertschöpfungskette beteiligt sind, unbeschadet der nationalen Rechtsvorschriften unter Einhaltung des Unionsrechts angewandt werden, wodurch die Verwendung bestimmter KI-Systeme begrenzt wird, wenn diese Rechtsvorschriften nicht in den Anwendungsbereich dieser Verordnung fallen oder mit ihnen andere legitime Ziele des öffentlichen Interesses verfolgt werden als in dieser Verordnung. So sollten etwa die nationalen arbeitsrechtlichen Vorschriften und die Rechtsvorschriften zum Schutz Minderjähriger, nämlich Personen unter 18 Jahren, unter Berücksichtigung der Allgemeinen Bemerkung Nr. 25 (2021) des UNCRC über die Rechte der Kinder im digitalen Umfeld von dieser Verordnung unberührt bleiben, sofern sie nicht spezifisch KI-Systeme betreffen und mit ihnen andere legitime Ziele des öffentlichen Interesses verfolgt werden. Fußnote 7: Verordnung (EG) Nr. 765/2008 des Europäischen Parlaments und des Rates vom 9. Juli 2008 über die Vorschriften für die Akkreditierung und zur Aufhebung der Verordnung (EWG) Nr. 339/93 des Rates (ABl. L 218 vom 13.8.2008, S. 30)., Fußnote 8: Beschluss Nr. 768/2008/EG des Europäischen Parlaments und des Rates vom 9. Juli 2008 über einen gemeinsamen Rechtsrahmen für die Vermarktung von Produkten und zur Aufhebung des Beschlusses 93/465/EWG des Rates (ABl. L 218 vom 13.8.2008, S. 82)., Fußnote 9: Verordnung (EU) 2019/1020 des Europäischen Parlaments und des Rates vom 20. Juni 2019 über Marktüberwachung und die Konformität von Produkten sowie zur Änderung der Richtlinie 2004/42/EG und der Verordnungen (EG) Nr. 765/2008 und (EU) Nr. 305/2011 (ABl. L 169 vom 25.6.2019, S. 1)., Fußnote 10: Richtlinie 85/374/EWG des Rates vom 25. Juli 1985 zur Angleichung der Rechts- und Verwaltungsvorschriften der Mitgliedstaaten über die Haftung für fehlerhafte Produkte (ABl. L 210 vom 7.8.1985, S. 29)."
    },
    {
      "chunk_idx": 9,
      "id": "53f9c5fc-d089-4233-9a15-16964e40e3c5",
      "title": "Recital 10",
      "relevantChunksIds": [
        "55e8665d-afb5-46d1-b258-64f964b08d09"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(10) The fundamental right to the protection of personal data is safeguarded in particular by Regulations (EU) 2016/679 (11) and (EU) 2018/1725 (12) of the European Parliament and of the Council and Directive (EU) 2016/680 of the European Parliament and of the Council (13). Directive 2002/58/EC of the European Parliament and of the Council (14) additionally protects private life and the confidentiality of communications, including by way of providing conditions for any storing of personal and non-personal data in, and access from, terminal equipment. Those Union legal acts provide the basis for sustainable and responsible data processing, including where data sets include a mix of personal and non-personal data. This Regulation does not seek to affect the application of existing Union law governing the processing of personal data, including the tasks and powers of the independent supervisory authorities competent to monitor compliance with those instruments. It also does not affect the obligations of providers and deployers of AI systems in their role as data controllers or processors stemming from Union or national law on the protection of personal data in so far as the design, the development or the use of AI systems involves the processing of personal data. It is also appropriate to clarify that data subjects continue to enjoy all the rights and guarantees awarded to them by such Union law, including the rights related to solely automated individual decision-making, including profiling. Harmonised rules for the placing on the market, the putting into service and the use of AI systems established under this Regulation should facilitate the effective implementation and enable the exercise of the data subjects’ rights and other remedies guaranteed under Union law on the protection of personal data and of other fundamental rights.",
      "original_content": "(10) Das Grundrecht auf Schutz personenbezogener Daten wird insbesondere durch die Verordnungen (EU) 2016/679 (Fußnote 11) und (EU) 2018/1725 (Fußnote 12) des Europäischen Parlaments und des Rates und die Richtlinie (EU) 2016/680 des Europäischen Parlaments und des Rates (Fußnote 13) gewahrt. Die Richtlinie 2002/58/EG des Europäischen Parlaments und des Rates (Fußnote 14) schützt darüber hinaus die Privatsphäre und die Vertraulichkeit der Kommunikation, auch durch Bedingungen für die Speicherung personenbezogener und nicht personenbezogener Daten auf Endgeräten und den Zugang dazu. Diese Rechtsakte der Union bieten die Grundlage für eine nachhaltige und verantwortungsvolle Datenverarbeitung, auch wenn Datensätze eine Mischung aus personenbezogenen und nicht-personenbezogenen Daten enthalten. Diese Verordnung soll die Anwendung des bestehenden Unionsrechts zur Verarbeitung personenbezogener Daten, einschließlich der Aufgaben und Befugnisse der unabhängigen Aufsichtsbehörden, die für die Überwachung der Einhaltung dieser Instrumente zuständig sind, nicht berühren. Sie lässt ferner die Pflichten der Anbieter und Betreiber von KI-Systemen in ihrer Rolle als Verantwortliche oder Auftragsverarbeiter, die sich aus dem Unionsrecht oder dem nationalen Recht über den Schutz personenbezogener Daten ergeben, unberührt, soweit die Konzeption, die Entwicklung oder die Verwendung von KI-Systemen die Verarbeitung personenbezogener Daten umfasst. Ferner sollte klargestellt werden, dass betroffene Personen weiterhin über alle Rechte und Garantien verfügen, die ihnen durch dieses Unionsrecht gewährt werden, einschließlich der Rechte im Zusammenhang mit der ausschließlich automatisierten Entscheidungsfindung im Einzelfall und dem Profiling. Harmonisierte Vorschriften für das Inverkehrbringen, die Inbetriebnahme und die Verwendung von KI-Systemen, die im Rahmen dieser Verordnung festgelegt werden, sollten die wirksame Durchführung erleichtern und die Ausübung der Rechte betroffener Personen und anderer Rechtsbehelfe, die im Unionsrecht über den Schutz personenbezogener Daten und anderer Grundrechte garantiert sind, ermöglichen. Fußnote 11: Verordnung (EU) 2016/679 des Europäischen Parlaments und des Rates vom 27. April 2016 zum Schutz natürlicher Personen bei der Verarbeitung personenbezogener Daten, zum freien Datenverkehr und zur Aufhebung der Richtlinie 95/46/EG (Datenschutz-Grundverordnung) (ABl. L 119 vom 4.5.2016, S. 1)., Fußnote 12: Verordnung (EU) 2018/1725 des Europäischen Parlaments und des Rates vom 23. Oktober 2018 zum Schutz natürlicher Personen bei der Verarbeitung personenbezogener Daten durch die Organe, Einrichtungen und sonstigen Stellen der Union, zum freien Datenverkehr und zur Aufhebung der Verordnung (EG) Nr. 45/2001 und des Beschlusses Nr. 1247/2002/EG (ABl. L 295 vom 21.11.2018, S. 39)., Fußnote 13: Richtlinie (EU) 2016/680 des Europäischen Parlaments und des Rates vom 27. April 2016 zum Schutz natürlicher Personen bei der Verarbeitung personenbezogener Daten durch die zuständigen Behörden zum Zwecke der Verhütung, Ermittlung, Aufdeckung oder Verfolgung von Straftaten oder der Strafvollstreckung sowie zum freien Datenverkehr und zur Aufhebung des Rahmenbeschlusses 2008/977/JI des Rates (ABl. L 119 vom 4.5.2016, S. 89)., Fußnote 14: Richtlinie 2002/58/EG des Europäischen Parlaments und des Rates vom 12. Juli 2002 über die Verarbeitung personenbezogener Daten und den Schutz der Privatsphäre in der elektronischen Kommunikation (Datenschutzrichtlinie für elektronische Kommunikation) (ABl. L 201 vom 31.7.2002, S. 37)."
    },
    {
      "chunk_idx": 10,
      "id": "c99d2066-4a8b-4bc4-85b0-b7d0c948cc56",
      "title": "Recital 11",
      "relevantChunksIds": [
        "55e8665d-afb5-46d1-b258-64f964b08d09"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(11) This Regulation should be without prejudice to the provisions regarding the liability of providers of intermediary services as set out in Regulation (EU) 2022/2065 of the European Parliament and of the Council (15).",
      "original_content": "(11) Diese Verordnung sollte die Bestimmungen über die Verantwortlichkeit der Anbieter von Vermittlungsdiensten gemäß der Verordnung (EU) 2022/2065 des Europäischen Parlaments und des Rates (Fußnote 15) unberührt lassen. Fußnote 15: Verordnung (EU) 2022/2065 des Europäischen Parlaments und des Rates vom 19. Oktober 2022 über einen Binnenmarkt für digitale Dienste und zur Änderung der Richtlinie 2000/31/EG (Gesetz über digitale Dienste) (ABl. L 277 vom 27.10.2022, S. 1)."
    },
    {
      "chunk_idx": 11,
      "id": "0b34a175-21c7-4d02-9c4e-4380537fff16",
      "title": "Recital 12",
      "relevantChunksIds": [
        "7401fefb-6095-47f0-a00e-78aac879499e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(12) The notion of ‘AI system’ in this Regulation should be clearly defined and should be closely aligned with the work of international organisations working on AI to ensure legal certainty, facilitate international convergence and wide acceptance, while providing the flexibility to accommodate the rapid technological developments in this field. Moreover, the definition should be based on key characteristics of AI systems that distinguish it from simpler traditional software systems or programming approaches and should not cover systems that are based on the rules defined solely by natural persons to automatically execute operations. A key characteristic of AI systems is their capability to infer. This capability to infer refers to the process of obtaining the outputs, such as predictions, content, recommendations, or decisions, which can influence physical and virtual environments, and to a capability of AI systems to derive models or algorithms, or both, from inputs or data. The techniques that enable inference while building an AI system include machine learning approaches that learn from data how to achieve certain objectives, and logic- and knowledge-based approaches that infer from encoded knowledge or symbolic representation of the task to be solved. The capacity of an AI system to infer transcends basic data processing by enabling learning, reasoning or modelling. The term ‘machine-based’ refers to the fact that AI systems run on machines. The reference to explicit or implicit objectives underscores that AI systems can operate according to explicit defined objectives or to implicit objectives. The objectives of the AI system may be different from the intended purpose of the AI system in a specific context. For the purposes of this Regulation, environments should be understood to be the contexts in which the AI systems operate, whereas outputs generated by the AI system reflect different functions performed by AI systems and include predictions, content, recommendations or decisions. AI systems are designed to operate with varying levels of autonomy, meaning that they have some degree of independence of actions from human involvement and of capabilities to operate without human intervention. The adaptiveness that an AI system could exhibit after deployment, refers to self-learning capabilities, allowing the system to change while in use. AI systems can be used on a stand-alone basis or as a component of a product, irrespective of whether the system is physically integrated into the product (embedded) or serves the functionality of the product without being integrated therein (non-embedded).",
      "original_content": "(12) Der Begriff „KI-System“ in dieser Verordnung sollte klar definiert und eng mit der Tätigkeit internationaler Organisationen abgestimmt werden, die sich mit KI befassen, um Rechtssicherheit, mehr internationale Konvergenz und hohe Akzeptanz sicherzustellen und gleichzeitig Flexibilität zu bieten, um den raschen technologischen Entwicklungen in diesem Bereich Rechnung zu tragen. Darüber hinaus sollte die Begriffsbestimmung auf den wesentlichen Merkmalen der KI beruhen, die sie von einfacheren herkömmlichen Softwaresystemen und Programmierungsansätzen abgrenzen, und sollte sich nicht auf Systeme beziehen, die auf ausschließlich von natürlichen Personen definierten Regeln für das automatische Ausführen von Operationen beruhen. Ein wesentliches Merkmal von KI-Systemen ist ihre Fähigkeit, abzuleiten. Diese Fähigkeit bezieht sich auf den Prozess der Erzeugung von Ausgaben, wie Vorhersagen, Inhalte, Empfehlungen oder Entscheidungen, die physische und digitale Umgebungen beeinflussen können, sowie auf die Fähigkeit von KI-Systemen, Modelle oder Algorithmen oder beides aus Eingaben oder Daten abzuleiten. Zu den Techniken, die während der Gestaltung eines KI-Systems das Ableiten ermöglichen, gehören Ansätze für maschinelles Lernen, wobei aus Daten gelernt wird, wie bestimmte Ziele erreicht werden können, sowie logik- und wissensgestützte Konzepte, wobei aus kodierten Informationen oder symbolischen Darstellungen der zu lösenden Aufgabe abgeleitet wird. Die Fähigkeit eines KI-Systems, abzuleiten, geht über die einfache Datenverarbeitung hinaus, indem Lern-, Schlussfolgerungs- und Modellierungsprozesse ermöglicht werden. Die Bezeichnung „maschinenbasiert“ bezieht sich auf die Tatsache, dass KI-Systeme von Maschinen betrieben werden. Durch die Bezugnahme auf explizite oder implizite Ziele wird betont, dass KI-Systeme gemäß explizit festgelegten Zielen oder gemäß impliziten Zielen arbeiten können. Die Ziele des KI-Systems können sich — unter bestimmten Umständen — von der Zweckbestimmung des KI-Systems unterscheiden. Für die Zwecke dieser Verordnung sollten Umgebungen als Kontexte verstanden werden, in denen KI-Systeme betrieben werden, während die von einem KI-System erzeugten Ausgaben verschiedene Funktionen von KI-Systemen widerspiegeln, darunter Vorhersagen, Inhalte, Empfehlungen oder Entscheidungen. KI-Systeme sind mit verschiedenen Graden der Autonomie ausgestattet, was bedeutet, dass sie bis zu einem gewissen Grad unabhängig von menschlichem Zutun agieren und in der Lage sind, ohne menschliches Eingreifen zu arbeiten. Die Anpassungsfähigkeit, die ein KI-System nach Inbetriebnahme aufweisen könnte, bezieht sich auf seine Lernfähigkeit, durch sie es sich während seiner Verwendung verändern kann. KI-Systeme können eigenständig oder als Bestandteil eines Produkts verwendet werden, unabhängig davon, ob das System physisch in das Produkt integriert (eingebettet) ist oder der Funktion des Produkts dient, ohne darin integriert zu sein (nicht eingebettet)."
    },
    {
      "chunk_idx": 12,
      "id": "b757b57f-4015-401c-8003-852ba5aaefc0",
      "title": "Recital 13",
      "relevantChunksIds": [
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(13) The notion of ‘deployer’ referred to in this Regulation should be interpreted as any natural or legal person, including a public authority, agency or other body, using an AI system under its authority, except where the AI system is used in the course of a personal non-professional activity. Depending on the type of AI system, the use of the system may affect persons other than the deployer.",
      "original_content": "(13) Der in dieser Verordnung verwendete Begriff „Betreiber“ sollte als eine natürliche oder juristische Person, einschließlich Behörden, Einrichtungen oder sonstiger Stellen, die ein KI-System unter ihrer Befugnis verwenden, verstanden werden, es sei denn das KI-System wird im Rahmen einer persönlichen und nicht beruflichen Tätigkeit verwendet. Je nach Art des KI-Systems kann sich dessen Verwendung auf andere Personen als den Betreiber auswirken."
    },
    {
      "chunk_idx": 13,
      "id": "d1298add-efff-452d-ba5a-f730964d638a",
      "title": "Recital 14",
      "relevantChunksIds": [
        "87c3670b-a636-4431-aadd-9cfe780035b0",
        "7f7015ca-f41b-45bb-97f0-d06734d9b6b0",
        "d605b91c-4f8d-4ba9-85fa-6dfbd07016e5",
        "e3935b15-c152-49db-9b69-2166080067c1"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(14) The notion of ‘biometric data’ used in this Regulation should be interpreted in light of the notion of biometric data as defined in Article 4, point (14) of Regulation (EU) 2016/679, Article 3, point (18) of Regulation (EU) 2018/1725 and Article 3, point (13) of Directive (EU) 2016/680. Biometric data can allow for the authentication, identification or categorisation of natural persons and for the recognition of emotions of natural persons.",
      "original_content": "(14) Der in dieser Verordnung verwendete Begriff „biometrische Daten“ sollte im Sinne des Begriffs „biometrische Daten“ nach Artikel 4 Nummer 14 der Verordnung (EU) 2016/679, Artikel 3 Nummer 18 der Verordnung (EU) 2018/1725 und Artikel 3 Nummer 13 der Richtlinie (EU) 2016/680 ausgelegt werden. Biometrische Daten können die Authentifizierung, Identifizierung oder Kategorisierung natürlicher Personen und die Erkennung von Emotionen natürlicher Personen ermöglichen."
    },
    {
      "chunk_idx": 14,
      "id": "7f7015ca-f41b-45bb-97f0-d06734d9b6b0",
      "title": "Recital 15",
      "relevantChunksIds": [
        "87c3670b-a636-4431-aadd-9cfe780035b0",
        "d1298add-efff-452d-ba5a-f730964d638a",
        "d605b91c-4f8d-4ba9-85fa-6dfbd07016e5",
        "e3935b15-c152-49db-9b69-2166080067c1"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(15) The notion of ‘biometric identification’ referred to in this Regulation should be defined as the automated recognition of physical, physiological and behavioural human features such as the face, eye movement, body shape, voice, prosody, gait, posture, heart rate, blood pressure, odour, keystrokes characteristics, for the purpose of establishing an individual’s identity by comparing biometric data of that individual to stored biometric data of individuals in a reference database, irrespective of whether the individual has given its consent or not. This excludes AI systems intended to be used for biometric verification, which includes authentication, whose sole purpose is to confirm that a specific natural person is the person he or she claims to be and to confirm the identity of a natural person for the sole purpose of having access to a service, unlocking a device or having security access to premises.",
      "original_content": "(15) Der Begriff „biometrische Identifizierung“ sollte gemäß dieser Verordnung als automatische Erkennung physischer, physiologischer und verhaltensbezogener menschlicher Merkmale wie Gesicht, Augenbewegungen, Körperform, Stimme, Prosodie, Gang, Haltung, Herzfrequenz, Blutdruck, Geruch, charakteristischer Tastenanschlag zum Zweck der Überprüfung der Identität einer Person durch Abgleich der biometrischen Daten der entsprechenden Person mit den in einer Datenbank gespeicherten biometrischen Daten definiert werden, unabhängig davon, ob die Einzelperson ihre Zustimmung dazu gegeben hat oder nicht. Dies umfasst keine KI-Systeme, die bestimmungsgemäß für die biometrische Verifizierung, wozu die Authentifizierung gehört, verwendet werden sollen, deren einziger Zweck darin besteht, zu bestätigen, dass eine bestimmte natürliche Person die Person ist, für die sie sich ausgibt, sowie zur Bestätigung der Identität einer natürlichen Person zu dem alleinigen Zweck Zugang zu einem Dienst zu erhalten, ein Gerät zu entriegeln oder Sicherheitszugang zu Räumlichkeiten zu erhalten."
    },
    {
      "chunk_idx": 15,
      "id": "d605b91c-4f8d-4ba9-85fa-6dfbd07016e5",
      "title": "Recital 16",
      "relevantChunksIds": [
        "7f7015ca-f41b-45bb-97f0-d06734d9b6b0",
        "e3935b15-c152-49db-9b69-2166080067c1",
        "d1298add-efff-452d-ba5a-f730964d638a",
        "87c3670b-a636-4431-aadd-9cfe780035b0"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(16) The notion of ‘biometric categorisation’ referred to in this Regulation should be defined as assigning natural persons to specific categories on the basis of their biometric data. Such specific categories can relate to aspects such as sex, age, hair colour, eye colour, tattoos, behavioural or personality traits, language, religion, membership of a national minority, sexual or political orientation. This does not include biometric categorisation systems that are a purely ancillary feature intrinsically linked to another commercial service, meaning that the feature cannot, for objective technical reasons, be used without the principal service, and the integration of that feature or functionality is not a means to circumvent the applicability of the rules of this Regulation. For example, filters categorising facial or body features used on online marketplaces could constitute such an ancillary feature as they can be used only in relation to the principal service which consists in selling a product by allowing the consumer to preview the display of the product on him or herself and help the consumer to make a purchase decision. Filters used on online social network services which categorise facial or body features to allow users to add or modify pictures or videos could also be considered to be ancillary feature as such filter cannot be used without the principal service of the social network services consisting in the sharing of content online.",
      "original_content": "(16) Der Begriff „biometrischen Kategorisierung“ sollte im Sinne dieser Verordnung die Zuordnung natürlicher Personen auf der Grundlage ihrer biometrischen Daten zu bestimmten Kategorien bezeichnen. Diese bestimmten Kategorien können Aspekte wie Geschlecht, Alter, Haarfarbe, Augenfarbe, Tätowierungen, Verhaltens- oder Persönlichkeitsmerkmale, Sprache, Religion, Zugehörigkeit zu einer nationalen Minderheit, sexuelle oder politische Ausrichtung betreffen. Dies gilt nicht für Systeme zur biometrischen Kategorisierung, bei denen es sich um eine reine Nebenfunktion handelt, die untrennbar mit einem anderen kommerziellen Dienst verbunden ist, d. h. die Funktion kann aus objektiven technischen Gründen nicht ohne den Hauptdienst verwendet werden und die Integration dieses Merkmals oder dieser Funktion dient nicht dazu, die Anwendbarkeit der Vorschriften dieser Verordnung zu umgehen. Beispielsweise könnten Filter zur Kategorisierung von Gesichts- oder Körpermerkmalen, die auf Online-Marktplätzen verwendet werden, eine solche Nebenfunktion darstellen, da sie nur im Zusammenhang mit der Hauptdienstleistung verwendet werden können, die darin besteht, ein Produkt zu verkaufen, indem es dem Verbraucher ermöglicht wird, zu sehen, wie das Produkt an seiner Person aussieht, und ihm so zu helfen, eine Kaufentscheidung zu treffen. Filter, die in sozialen Netzwerken eingesetzt werden und Gesichts- oder Körpermerkmale kategorisieren, um es den Nutzern zu ermöglichen, Bilder oder Videos hinzuzufügen oder zu verändern, können ebenfalls als Nebenfunktion betrachtet werden, da ein solcher Filter nicht ohne die Hauptdienstleistung sozialer Netzwerke verwendet werden kann, die in der Weitergabe von Online-Inhalten besteht."
    },
    {
      "chunk_idx": 16,
      "id": "e3935b15-c152-49db-9b69-2166080067c1",
      "title": "Recital 17",
      "relevantChunksIds": [
        "d1298add-efff-452d-ba5a-f730964d638a",
        "7f7015ca-f41b-45bb-97f0-d06734d9b6b0",
        "d605b91c-4f8d-4ba9-85fa-6dfbd07016e5",
        "87c3670b-a636-4431-aadd-9cfe780035b0"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(17) The notion of ‘remote biometric identification system’ referred to in this Regulation should be defined functionally, as an AI system intended for the identification of natural persons without their active involvement, typically at a distance, through the comparison of a person’s biometric data with the biometric data contained in a reference database, irrespectively of the particular technology, processes or types of biometric data used. Such remote biometric identification systems are typically used to perceive multiple persons or their behaviour simultaneously in order to facilitate significantly the identification of natural persons without their active involvement. This excludes AI systems intended to be used for biometric verification, which includes authentication, the sole purpose of which is to confirm that a specific natural person is the person he or she claims to be and to confirm the identity of a natural person for the sole purpose of having access to a service, unlocking a device or having security access to premises. That exclusion is justified by the fact that such systems are likely to have a minor impact on fundamental rights of natural persons compared to the remote biometric identification systems which may be used for the processing of the biometric data of a large number of persons without their active involvement. In the case of ‘real-time’ systems, the capturing of the biometric data, the comparison and the identification occur all instantaneously, near-instantaneously or in any event without a significant delay. In this regard, there should be no scope for circumventing the rules of this Regulation on the ‘real-time’ use of the AI systems concerned by providing for minor delays. ‘Real-time’ systems involve the use of ‘live’ or ‘near-live’ material, such as video footage, generated by a camera or other device with similar functionality. In the case of ‘post’ systems, in contrast, the biometric data has already been captured and the comparison and identification occur only after a significant delay. This involves material, such as pictures or video footage generated by closed circuit television cameras or private devices, which has been generated before the use of the system in respect of the natural persons concerned.",
      "original_content": "(17) Der in dieser Verordnung verwendete Begriff „biometrisches Fernidentifizierungssystem“ sollte funktional definiert werden als KI-System, das dem Zweck dient, natürliche Personen ohne ihre aktive Einbeziehung in der Regel aus der Ferne durch Abgleich der biometrischen Daten einer Person mit den in einer Referenzdatenbank gespeicherten biometrischen Daten zu identifizieren, unabhängig davon, welche Technologie, Verfahren oder Arten biometrischer Daten dazu verwendet werden. Diese biometrischen Fernidentifizierungssysteme werden in der Regel zur zeitgleichen Erkennung mehrerer Personen oder ihrer Verhaltensweisen verwendet, um die Identifizierung natürlicher Personen ohne ihre aktive Einbeziehung erheblich zu erleichtern. Dies umfasst keine KI-Systeme, die bestimmungsgemäß für die biometrische Verifizierung, wozu die Authentifizierung gehört, verwendet werden sollen, deren einziger Zweck darin besteht, zu bestätigen, dass eine bestimmte natürliche Person die Person ist, für die sie sich ausgibt, sowie zur Bestätigung der Identität einer natürlichen Person zu dem alleinigen Zweck Zugang zu einem Dienst zu erhalten, ein Gerät zu entriegeln oder Sicherheitszugang zu Räumlichkeiten zu erhalten. Diese Ausnahme wird damit begründet, dass diese Systeme im Vergleich zu biometrischen Fernidentifizierungssystemen, die zur Verarbeitung biometrischer Daten einer großen Anzahl von Personen ohne ihre aktive Einbeziehung verwendet werden können, geringfügige Auswirkungen auf die Grundrechte natürlicher Personen haben dürften. Bei „Echtzeit-Systemen“ erfolgen die Erfassung der biometrischen Daten, der Abgleich und die Identifizierung zeitgleich, nahezu zeitgleich oder auf jeden Fall ohne erhebliche Verzögerung. In diesem Zusammenhang sollte es keinen Spielraum für eine Umgehung der Bestimmungen dieser Verordnung über die „Echtzeit-Nutzung“ der betreffenden KI-Systeme geben, indem kleinere Verzögerungen vorgesehen werden. „Echtzeit-Systeme“ umfassen die Verwendung von „Live-Material“ oder „Near-live-Material“ wie etwa Videoaufnahmen, die von einer Kamera oder einem anderen Gerät mit ähnlicher Funktion erzeugt werden. Bei Systemen zur nachträglichen Identifizierung hingegen wurden die biometrischen Daten schon zuvor erfasst und der Abgleich und die Identifizierung erfolgen erst mit erheblicher Verzögerung. Dabei handelt es sich um Material wie etwa Bild- oder Videoaufnahmen, die von Video-Überwachungssystemen oder privaten Geräten vor der Anwendung des Systems auf die betroffenen natürlichen Personen erzeugt wurden."
    },
    {
      "chunk_idx": 17,
      "id": "f3f24184-1d9d-4f5c-af39-7853beef1c1b",
      "title": "Recital 18",
      "relevantChunksIds": [
        "87c3670b-a636-4431-aadd-9cfe780035b0"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(18) The notion of ‘emotion recognition system’ referred to in this Regulation should be defined as an AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data. The notion refers to emotions or intentions such as happiness, sadness, anger, surprise, disgust, embarrassment, excitement, shame, contempt, satisfaction and amusement. It does not include physical states, such as pain or fatigue, including, for example, systems used in detecting the state of fatigue of professional pilots or drivers for the purpose of preventing accidents. This does also not include the mere detection of readily apparent expressions, gestures or movements, unless they are used for identifying or inferring emotions. Those expressions can be basic facial expressions, such as a frown or a smile, or gestures such as the movement of hands, arms or head, or characteristics of a person’s voice, such as a raised voice or whispering.",
      "original_content": "(18) Der in dieser Verordnung verwendete Begriff „Emotionserkennungssystem“ sollte als ein KI-System definiert werden, das dem Zweck dient, Emotionen oder Absichten natürlicher Personen auf der Grundlage ihrer biometrischen Daten festzustellen oder daraus abzuleiten. In diesem Begriff geht es um Emotionen oder Absichten wie Glück, Trauer, Wut, Überraschung, Ekel, Verlegenheit, Aufregung, Scham, Verachtung, Zufriedenheit und Vergnügen. Dies umfasst nicht physische Zustände wie Schmerz oder Ermüdung, einschließlich beispielsweise Systeme, die zur Erkennung des Zustands der Ermüdung von Berufspiloten oder -fahrern eigesetzt werden, um Unfälle zu verhindern. Es geht dabei auch nicht um die bloße Erkennung offensichtlicher Ausdrucksformen, Gesten und Bewegungen, es sei denn, sie werden zum Erkennen oder Ableiten von Emotionen verwendet. Bei diesen Ausdrucksformen kann es sich um einfache Gesichtsausdrücke wie ein Stirnrunzeln oder ein Lächeln oder um Gesten wie Hand-, Arm- oder Kopfbewegungen oder um die Stimmmerkmale einer Person handeln, wie eine erhobene Stimme oder ein Flüstern."
    },
    {
      "chunk_idx": 18,
      "id": "2ddd69ca-7f98-4767-b858-31234db4b6f6",
      "title": "Recital 19",
      "relevantChunksIds": [
        "87c3670b-a636-4431-aadd-9cfe780035b0"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(19) For the purposes of this Regulation the notion of ‘publicly accessible space’ should be understood as referring to any physical space that is accessible to an undetermined number of natural persons, and irrespective of whether the space in question is privately or publicly owned, irrespective of the activity for which the space may be used, such as for commerce, for example, shops, restaurants, cafés; for services, for example, banks, professional activities, hospitality; for sport, for example, swimming pools, gyms, stadiums; for transport, for example, bus, metro and railway stations, airports, means of transport; for entertainment, for example, cinemas, theatres, museums, concert and conference halls; or for leisure or otherwise, for example, public roads and squares, parks, forests, playgrounds. A space should also be classified as being publicly accessible if, regardless of potential capacity or security restrictions, access is subject to certain predetermined conditions which can be fulfilled by an undetermined number of persons, such as the purchase of a ticket or title of transport, prior registration or having a certain age. In contrast, a space should not be considered to be publicly accessible if access is limited to specific and defined natural persons through either Union or national law directly related to public safety or security or through the clear manifestation of will by the person having the relevant authority over the space. The factual possibility of access alone, such as an unlocked door or an open gate in a fence, does not imply that the space is publicly accessible in the presence of indications or circumstances suggesting the contrary, such as. signs prohibiting or restricting access. Company and factory premises, as well as offices and workplaces that are intended to be accessed only by relevant employees and service providers, are spaces that are not publicly accessible. Publicly accessible spaces should not include prisons or border control. Some other spaces may comprise both publicly accessible and non-publicly accessible spaces, such as the hallway of a private residential building necessary to access a doctor’s office or an airport. Online spaces are not covered, as they are not physical spaces. Whether a given space is accessible to the public should however be determined on a case-by-case basis, having regard to the specificities of the individual situation at hand.",
      "original_content": "(19) Für die Zwecke dieser Verordnung sollte der Begriff „öffentlich zugänglicher Raum“ so verstanden werden, dass er sich auf einen einer unbestimmten Anzahl natürlicher Personen zugänglichen physischen Ort bezieht, unabhängig davon, ob er sich in privatem oder öffentlichem Eigentum befindet, unabhängig von den Tätigkeiten, für die der Ort verwendet werden kann; dazu zählen Bereiche wie etwa für Gewerbe, etwa Geschäfte, Restaurants, Cafés, für Dienstleistungen, etwa Banken, berufliche Tätigkeiten, Gastgewerbe, für Sport, etwa Schwimmbäder, Fitnessstudios, Stadien, für Verkehr, etwa Bus- und U-Bahn-Haltestellen, Bahnhöfe, Flughäfen, Transportmittel, für Unterhaltung, etwa Kinos, Theater, Museen, Konzert- und Konferenzsäle oder für Freizeit oder Sonstiges, etwa öffentliche Straßen und Plätze, Parks, Wälder, Spielplätze. Ein Ort sollte auch als öffentlich zugänglich eingestuft werden, wenn der Zugang, unabhängig von möglichen Kapazitäts- oder Sicherheitsbeschränkungen, bestimmten im Voraus festgelegten Bedingungen unterliegt, die von einer unbestimmten Anzahl von Personen erfüllt werden können, etwa durch den Kauf eines Fahrscheins, die vorherige Registrierung oder die Erfüllung eines Mindestalters. Dahingegen sollte ein Ort nicht als öffentlich zugänglich gelten, wenn der Zugang auf natürliche Personen beschränkt ist, die entweder im Unionsrecht oder im nationalen Recht, das direkt mit der öffentlichen Sicherheit zusammenhängt, oder im Rahmen einer eindeutigen Willenserklärung der Person, die die entsprechende Befugnis über den Ort ausübt, bestimmt und festgelegt werden. Die tatsächliche Zugangsmöglichkeit allein, etwa eine unversperrte Tür oder ein offenes Zauntor, bedeutet nicht, dass der Ort öffentlich zugänglich ist, wenn aufgrund von Hinweisen oder Umständen das Gegenteil nahegelegt wird (etwa Schilder, die den Zugang verbieten oder einschränken). Unternehmens- und Fabrikgelände sowie Büros und Arbeitsplätze, die nur für die betreffenden Mitarbeiter und Dienstleister zugänglich sein sollen, sind Orte, die nicht öffentlich zugänglich sind. Justizvollzugsanstalten und Grenzkontrollbereiche sollten nicht zu den öffentlich zugänglichen Orten zählen. Einige andere Gebiete können sowohl öffentlich zugängliche als auch nicht öffentlich zugängliche Orte umfassen, etwa die Gänge eines privaten Wohngebäudes, deren Zugang erforderlich ist, um zu einer Arztpraxis zu gelangen, oder Flughäfen. Online-Räume werden nicht erfasst, da es sich nicht um physische Räume handelt. Ob ein bestimmter Raum öffentlich zugänglich ist, sollte jedoch von Fall zu Fall unter Berücksichtigung der Besonderheiten der jeweiligen individuellen Situation entschieden werden."
    },
    {
      "chunk_idx": 19,
      "id": "21ada015-1d50-4ddb-97d5-33b540e849cb",
      "title": "Recital 20",
      "relevantChunksIds": [
        "09b711a2-571c-4a78-9b84-60675ea83b5e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(20) In order to obtain the greatest benefits from AI systems while protecting fundamental rights, health and safety and to enable democratic control, AI literacy should equip providers, deployers and affected persons with the necessary notions to make informed decisions regarding AI systems. Those notions may vary with regard to the relevant context and can include understanding the correct application of technical elements during the AI system’s development phase, the measures to be applied during its use, the suitable ways in which to interpret the AI system’s output, and, in the case of affected persons, the knowledge necessary to understand how decisions taken with the assistance of AI will have an impact on them. In the context of the application this Regulation, AI literacy should provide all relevant actors in the AI value chain with the insights required to ensure the appropriate compliance and its correct enforcement. Furthermore, the wide implementation of AI literacy measures and the introduction of appropriate follow-up actions could contribute to improving working conditions and ultimately sustain the consolidation, and innovation path of trustworthy AI in the Union. The European Artificial Intelligence Board (the ‘Board’) should support the Commission, to promote AI literacy tools, public awareness and understanding of the benefits, risks, safeguards, rights and obligations in relation to the use of AI systems. In cooperation with the relevant stakeholders, the Commission and the Member States should facilitate the drawing up of voluntary codes of conduct to advance AI literacy among persons dealing with the development, operation and use of AI.",
      "original_content": "(20) Um den größtmöglichen Nutzen aus KI-Systemen zu ziehen und gleichzeitig die Grundrechte, Gesundheit und Sicherheit zu wahren und eine demokratische Kontrolle zu ermöglichen, sollte die KI-Kompetenz Anbieter, Betreiber und betroffene Personen mit den notwendigen Konzepten ausstatten, um fundierte Entscheidungen über KI-Systeme zu treffen. Diese Konzepte können in Bezug auf den jeweiligen Kontext unterschiedlich sein und das Verstehen der korrekten Anwendung technischer Elemente in der Entwicklungsphase des KI-Systems, der bei seiner Verwendung anzuwendenden Maßnahmen und der geeigneten Auslegung der Ausgaben des KI-Systems umfassen sowie — im Falle betroffener Personen — das nötige Wissen, um zu verstehen, wie sich mithilfe von KI getroffene Entscheidungen auf sie auswirken werden. Im Zusammenhang mit der Anwendung dieser Verordnung sollte die KI-Kompetenz allen einschlägigen Akteuren der KI-Wertschöpfungskette die Kenntnisse vermitteln, die erforderlich sind, um die angemessene Einhaltung und die ordnungsgemäße Durchsetzung der Verordnung sicherzustellen. Darüber hinaus könnten die umfassende Umsetzung von KI-Kompetenzmaßnahmen und die Einführung geeigneter Folgemaßnahmen dazu beitragen, die Arbeitsbedingungen zu verbessern und letztlich die Konsolidierung und den Innovationspfad vertrauenswürdiger KI in der Union unterstützen. Ein Europäisches Gremium für Künstliche Intelligenz (im Folgenden „KI-Gremium“) sollte die Kommission dabei unterstützen, KI-Kompetenzinstrumente sowie die Sensibilisierung und Aufklärung der Öffentlichkeit in Bezug auf die Vorteile, Risiken, Schutzmaßnahmen, Rechte und Pflichten im Zusammenhang mit der Nutzung von KI-Systeme zu fördern. In Zusammenarbeit mit den einschlägigen Interessenträgern sollten die Kommission und die Mitgliedstaaten die Ausarbeitung freiwilliger Verhaltenskodizes erleichtern, um die KI-Kompetenz von Personen, die mit der Entwicklung, dem Betrieb und der Verwendung von KI befasst sind, zu fördern."
    },
    {
      "chunk_idx": 20,
      "id": "f9b21785-6f87-4438-8810-147304eb4d7e",
      "title": "Recital 21",
      "relevantChunksIds": [
        "55e8665d-afb5-46d1-b258-64f964b08d09",
        "09b711a2-571c-4a78-9b84-60675ea83b5e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(21) In order to ensure a level playing field and an effective protection of rights and freedoms of individuals across the Union, the rules established by this Regulation should apply to providers of AI systems in a non-discriminatory manner, irrespective of whether they are established within the Union or in a third country, and to deployers of AI systems established within the Union.",
      "original_content": "(21) Um gleiche Wettbewerbsbedingungen und einen wirksamen Schutz der Rechte und Freiheiten von Einzelpersonen in der gesamten Union zu gewährleisten, sollten die in dieser Verordnung festgelegten Vorschriften in nichtdiskriminierender Weise für Anbieter von KI-Systemen — unabhängig davon, ob sie in der Union oder in einem Drittland niedergelassen sind — und für Betreiber von KI-Systemen, die in der Union niedergelassen sind, gelten."
    },
    {
      "chunk_idx": 21,
      "id": "38517ea5-282c-498e-8559-1df8a9f2efdb",
      "title": "Recital 22",
      "relevantChunksIds": [
        "55e8665d-afb5-46d1-b258-64f964b08d09"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(22) In light of their digital nature, certain AI systems should fall within the scope of this Regulation even when they are not placed on the market, put into service, or used in the Union. This is the case, for example, where an operator established in the Union contracts certain services to an operator established in a third country in relation to an activity to be performed by an AI system that would qualify as high-risk. In those circumstances, the AI system used in a third country by the operator could process data lawfully collected in and transferred from the Union, and provide to the contracting operator in the Union the output of that AI system resulting from that processing, without that AI system being placed on the market, put into service or used in the Union. To prevent the circumvention of this Regulation and to ensure an effective protection of natural persons located in the Union, this Regulation should also apply to providers and deployers of AI systems that are established in a third country, to the extent the output produced by those systems is intended to be used in the Union. Nonetheless, to take into account existing arrangements and special needs for future cooperation with foreign partners with whom information and evidence is exchanged, this Regulation should not apply to public authorities of a third country and international organisations when acting in the framework of cooperation or international agreements concluded at Union or national level for law enforcement and judicial cooperation with the Union or the Member States, provided that the relevant third country or international organisation provides adequate safeguards with respect to the protection of fundamental rights and freedoms of individuals. Where relevant, this may cover activities of entities entrusted by the third countries to carry out specific tasks in support of such law enforcement and judicial cooperation. Such framework for cooperation or agreements have been established bilaterally between Member States and third countries or between the European Union, Europol and other Union agencies and third countries and international organisations. The authorities competent for supervision of the law enforcement and judicial authorities under this Regulation should assess whether those frameworks for cooperation or international agreements include adequate safeguards with respect to the protection of fundamental rights and freedoms of individuals. Recipient national authorities and Union institutions, bodies, offices and agencies making use of such outputs in the Union remain accountable to ensure their use complies with Union law. When those international agreements are revised or new ones are concluded in the future, the contracting parties should make utmost efforts to align those agreements with the requirements of this Regulation.",
      "original_content": "(22) Angesichts ihres digitalen Charakters sollten bestimmte KI-Systeme in den Anwendungsbereich dieser Verordnung fallen, selbst wenn sie in der Union weder in Verkehr gebracht noch in Betrieb genommen oder verwendet werden. Dies ist beispielsweise der Fall, wenn ein in der Union niedergelassener Akteur bestimmte Dienstleistungen an einen in einem Drittland niedergelassenen Akteur im Zusammenhang mit einer Tätigkeit vergibt, die von einem KI-System ausgeübt werden soll, das als hochriskant einzustufen wäre. Unter diesen Umständen könnte das von dem Akteur in einem Drittland betriebene KI-System Daten verarbeiten, die rechtmäßig in der Union erhoben und aus der Union übertragen wurden, und dem vertraglichen Akteur in der Union die aus dieser Verarbeitung resultierende Ausgabe dieses KI-Systems liefern, ohne dass dieses KI-System dabei in der Union in Verkehr gebracht, in Betrieb genommen oder verwendet würde. Um die Umgehung dieser Verordnung zu verhindern und einen wirksamen Schutz in der Union ansässiger natürlicher Personen zu gewährleisten, sollte diese Verordnung auch für Anbieter und Betreiber von KI-Systemen gelten, die in einem Drittland niedergelassen sind, soweit beabsichtigt wird, die von diesem System erzeugte Ausgabe in der Union zu verwenden. Um jedoch bestehenden Vereinbarungen und besonderen Erfordernissen für die künftige Zusammenarbeit mit ausländischen Partnern, mit denen Informationen und Beweismittel ausgetauscht werden, Rechnung zu tragen, sollte diese Verordnung nicht für Behörden eines Drittlands und internationale Organisationen gelten, wenn sie im Rahmen der Zusammenarbeit oder internationaler Übereinkünfte tätig werden, die auf Unionsebene oder nationaler Ebene für die Zusammenarbeit mit der Union oder den Mitgliedstaaten im Bereich der Strafverfolgung und der justiziellen Zusammenarbeit geschlossen wurden, vorausgesetzt dass dieses Drittland oder diese internationale Organisationen angemessene Garantien in Bezug auf den Schutz der Grundrechte und -freiheiten von Einzelpersonen bieten. Dies kann gegebenenfalls Tätigkeiten von Einrichtungen umfassen, die von Drittländern mit der Wahrnehmung bestimmter Aufgaben zur Unterstützung dieser Zusammenarbeit im Bereich der Strafverfolgung und der justiziellen Zusammenarbeit betraut wurden. Ein solcher Rahmen für die Zusammenarbeit oder für Übereinkünfte wurde bilateral zwischen Mitgliedstaaten und Drittländern oder zwischen der Europäischen Union, Europol und anderen Agenturen der Union und Drittländern und internationalen Organisationen erarbeitet. Die Behörden, die nach dieser Verordnung für die Aufsicht über die Strafverfolgungs- und Justizbehörden zuständig sind, sollten prüfen, ob diese Rahmen für die Zusammenarbeit oder internationale Übereinkünfte angemessene Garantien in Bezug auf den Schutz der Grundrechte und -freiheiten von Einzelpersonen enthalten. Empfangende nationale Behörden und Organe, Einrichtungen sowie sonstige Stellen der Union, die diese Ausgaben in der Union verwenden, sind weiterhin dafür verantwortlich, sicherzustellen, dass ihre Verwendung mit Unionsrecht vereinbar ist. Wenn diese internationalen Übereinkünfte überarbeitet oder wenn künftig neue Übereinkünfte geschlossen werden, sollten die Vertragsparteien größtmögliche Anstrengungen unternehmen, um diese Übereinkünfte an die Anforderungen dieser Verordnung anzugleichen."
    },
    {
      "chunk_idx": 22,
      "id": "a010e5cb-6b93-499f-ad13-0a86a4c6239a",
      "title": "Recital 23",
      "relevantChunksIds": [
        "55e8665d-afb5-46d1-b258-64f964b08d09"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(23) This Regulation should also apply to Union institutions, bodies, offices and agencies when acting as a provider or deployer of an AI system.",
      "original_content": "(23) Diese Verordnung sollte auch für Organe, Einrichtungen und sonstige Stellen der Union gelten, wenn sie als Anbieter oder Betreiber eines KI-Systems auftreten."
    },
    {
      "chunk_idx": 23,
      "id": "a035c99d-ea59-4c78-87db-94e1b89e1980",
      "title": "Recital 24",
      "relevantChunksIds": [
        "55e8665d-afb5-46d1-b258-64f964b08d09"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(24) If, and insofar as, AI systems are placed on the market, put into service, or used with or without modification of such systems for military, defence or national security purposes, those should be excluded from the scope of this Regulation regardless of which type of entity is carrying out those activities, such as whether it is a public or private entity. As regards military and defence purposes, such exclusion is justified both by Article 4(2) TEU and by the specificities of the Member States’ and the common Union defence policy covered by Chapter 2 of Title V TEU that are subject to public international law, which is therefore the more appropriate legal framework for the regulation of AI systems in the context of the use of lethal force and other AI systems in the context of military and defence activities. As regards national security purposes, the exclusion is justified both by the fact that national security remains the sole responsibility of Member States in accordance with Article 4(2) TEU and by the specific nature and operational needs of national security activities and specific national rules applicable to those activities. Nonetheless, if an AI system developed, placed on the market, put into service or used for military, defence or national security purposes is used outside those temporarily or permanently for other purposes, for example, civilian or humanitarian purposes, law enforcement or public security purposes, such a system would fall within the scope of this Regulation. In that case, the entity using the AI system for other than military, defence or national security purposes should ensure the compliance of the AI system with this Regulation, unless the system is already compliant with this Regulation. AI systems placed on the market or put into service for an excluded purpose, namely military, defence or national security, and one or more non-excluded purposes, such as civilian purposes or law enforcement, fall within the scope of this Regulation and providers of those systems should ensure compliance with this Regulation. In those cases, the fact that an AI system may fall within the scope of this Regulation should not affect the possibility of entities carrying out national security, defence and military activities, regardless of the type of entity carrying out those activities, to use AI systems for national security, military and defence purposes, the use of which is excluded from the scope of this Regulation. An AI system placed on the market for civilian or law enforcement purposes which is used with or without modification for military, defence or national security purposes should not fall within the scope of this Regulation, regardless of the type of entity carrying out those activities.",
      "original_content": "(24) Wenn und soweit KI-Systeme mit oder ohne Änderungen für Zwecke in den Bereichen Militär, Verteidigung oder nationale Sicherheit in Verkehr gebracht, in Betrieb genommen oder verwendet werden, sollten sie vom Anwendungsbereich dieser Verordnung ausgenommen werden, unabhängig von der Art der Einrichtung, die diese Tätigkeiten ausübt, etwa ob es sich um eine öffentliche oder private Einrichtung handelt. In Bezug auf die Zwecke in den Bereichen Militär und Verteidigung gründet sich die Ausnahme sowohl auf Artikel 4 Absatz 2 EUV als auch auf die Besonderheiten der Verteidigungspolitik der Mitgliedstaaten und der in Titel V Kapitel 2 EUV abgedeckten gemeinsamen Verteidigungspolitik der Union, die dem Völkerrecht unterliegen, was daher den geeigneteren Rechtsrahmen für die Regulierung von KI-Systemen im Zusammenhang mit der Anwendung tödlicher Gewalt und sonstigen KI-Systemen im Zusammenhang mit Militär- oder Verteidigungstätigkeiten darstellt. In Bezug auf die Zwecke im Bereich nationale Sicherheit gründet sich die Ausnahme sowohl auf die Tatsache, dass die nationale Sicherheit gemäß Artikel 4 Absatz 2 EUV weiterhin in die alleinige Verantwortung der Mitgliedstaaten fällt, als auch auf die besondere Art und die operativen Bedürfnisse der Tätigkeiten im Bereich der nationalen Sicherheit und der spezifischen nationalen Vorschriften für diese Tätigkeiten. Wird ein KI-System, das für Zwecke in den Bereichen Militär, Verteidigung oder nationale Sicherheit entwickelt, in Verkehr gebracht, in Betrieb genommen oder verwendet wird, jedoch vorübergehend oder ständig für andere Zwecke verwendet, etwa für zivile oder humanitäre Zwecke oder für Zwecke der Strafverfolgung oder öffentlichen Sicherheit, so würde dieses System in den Anwendungsbereich dieser Verordnung fallen. In diesem Fall sollte die Einrichtung, die das KI-System für andere Zwecke als Zwecke in den Bereichen Militär, Verteidigung oder nationale Sicherheit verwendet, die Konformität des KI-Systems mit dieser Verordnung sicherstellen, es sei denn, das System entspricht bereits dieser Verordnung. KI-Systeme, die für einen ausgeschlossenen Zweck, nämlich Militär, Verteidigung oder nationale Sicherheit, und für einen oder mehrere nicht ausgeschlossene Zwecke, etwa zivile Zwecke oder Strafverfolgungszwecke, in Verkehr gebracht oder in Betrieb genommen werden, fallen in den Anwendungsbereich dieser Verordnung, und Anbieter dieser Systeme sollten die Einhaltung dieser Verordnung sicherstellen. In diesen Fällen sollte sich die Tatsache, dass ein KI-System in den Anwendungsbereich dieser Verordnung fällt, nicht darauf auswirken, dass Einrichtungen, die Tätigkeiten in den Bereichen nationale Sicherheit, Verteidigung oder Militär ausüben, KI-Systeme für Zwecke in den Bereichen nationale Sicherheit, Militär und Verteidigung verwenden können, unabhängig von der Art der Einrichtung, die diese Tätigkeiten ausübt, wobei die Verwendung vom Anwendungsbereich dieser Verordnung ausgenommen ist. Ein KI-System, das für zivile Zwecke oder Strafverfolgungszwecke in Verkehr gebracht wurde und mit oder ohne Änderungen für Zwecke in den Bereichen Militär, Verteidigung oder nationale Sicherheit verwendet wird, sollte nicht in den Anwendungsbereich dieser Verordnung fallen, unabhängig von der Art der Einrichtung, die diese Tätigkeiten ausübt."
    },
    {
      "chunk_idx": 24,
      "id": "51f69443-5b55-4fb2-b1d5-31137a66e681",
      "title": "Recital 25",
      "relevantChunksIds": [
        "55e8665d-afb5-46d1-b258-64f964b08d09"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(25) This Regulation should support innovation, should respect freedom of science, and should not undermine research and development activity. It is therefore necessary to exclude from its scope AI systems and models specifically developed and put into service for the sole purpose of scientific research and development. Moreover, it is necessary to ensure that this Regulation does not otherwise affect scientific research and development activity on AI systems or models prior to being placed on the market or put into service. As regards product-oriented research, testing and development activity regarding AI systems or models, the provisions of this Regulation should also not apply prior to those systems and models being put into service or placed on the market. That exclusion is without prejudice to the obligation to comply with this Regulation where an AI system falling into the scope of this Regulation is placed on the market or put into service as a result of such research and development activity and to the application of provisions on AI regulatory sandboxes and testing in real world conditions. Furthermore, without prejudice to the exclusion of AI systems specifically developed and put into service for the sole purpose of scientific research and development, any other AI system that may be used for the conduct of any research and development activity should remain subject to the provisions of this Regulation. In any event, any research and development activity should be carried out in accordance with recognised ethical and professional standards for scientific research and should be conducted in accordance with applicable Union law.",
      "original_content": "(25) Diese Verordnung sollte die Innovation fördern, die Freiheit der Wissenschaft achten und Forschungs- und Entwicklungstätigkeiten nicht untergraben. Daher müssen KI-Systeme und -Modelle, die eigens für den alleinigen Zweck der wissenschaftlichen Forschung und Entwicklung entwickelt und in Betrieb genommen werden, vom Anwendungsbereich der Verordnung ausgenommen werden. Ferner muss sichergestellt werden, dass sich die Verordnung nicht anderweitig auf Forschungs- und Entwicklungstätigkeiten zu KI-Systemen und -Modellen auswirkt, bevor diese in Verkehr gebracht oder in Betrieb genommen werden. Hinsichtlich produktorientierter Forschungs-, Test- und Entwicklungstätigkeiten in Bezug auf KI-Systeme oder -Modelle sollten die Bestimmungen dieser Verordnung auch nicht vor der Inbetriebnahme oder dem Inverkehrbringen dieser Systeme und Modelle gelten. Diese Ausnahme berührt weder die Pflicht zur Einhaltung dieser Verordnung, wenn ein KI-System, das in den Anwendungsbereich dieser Verordnung fällt, infolge solcher Forschungs- und Entwicklungstätigkeiten in Verkehr gebracht oder in Betrieb genommen wird, noch die Anwendung der Bestimmungen zu KI-Reallaboren und zu Tests unter Realbedingungen. Darüber hinaus sollte unbeschadet der Ausnahme in Bezug auf KI-Systeme, die eigens für den alleinigen Zweck der wissenschaftlichen Forschung und Entwicklung entwickelt und in Betrieb genommen werden, jedes andere KI-System, das für die Durchführung von Forschungs- und Entwicklungstätigkeiten verwendet werden könnte, den Bestimmungen dieser Verordnung unterliegen. In jedem Fall sollten jegliche Forschungs- und Entwicklungstätigkeiten gemäß anerkannten ethischen und professionellen Grundsätzen für die wissenschaftliche Forschung und unter Wahrung des geltenden Unionsrechts ausgeführt werden."
    },
    {
      "chunk_idx": 25,
      "id": "16b5ff49-dbab-4e7e-861a-7e1d4cd7cb03",
      "title": "Recital 26",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463",
        "a6e85038-89e8-4790-9457-cb624ca42c00",
        "e436d2fa-0d04-4566-931b-a1955edc7114",
        "7e829946-fb22-41cb-8900-9ff0f0a32229",
        "80b0fa01-7245-4610-a673-5d369d0e9f9a",
        "c9d80e98-9396-4526-ad4f-1e57ebf765f0",
        "239278ca-b87b-4431-b7c9-71fd1a54f9ff",
        "283d70b2-97f7-4252-9190-378e90b707bd",
        "e87ad418-9936-47c0-b4ec-99f31c41a0a8",
        "24b748de-eebc-404f-8b36-d12eed0a6660",
        "85555ab3-0021-4e99-96f7-b8d2182b43f7"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(26) In order to introduce a proportionate and effective set of binding rules for AI systems, a clearly defined risk-based approach should be followed. That approach should tailor the type and content of such rules to the intensity and scope of the risks that AI systems can generate. It is therefore necessary to prohibit certain unacceptable AI practices, to lay down requirements for high-risk AI systems and obligations for the relevant operators, and to lay down transparency obligations for certain AI systems.",
      "original_content": "(26) Um ein verhältnismäßiges und wirksames verbindliches Regelwerk für KI-Systeme einzuführen, sollte ein klar definierter risikobasierter Ansatz verfolgt werden. Bei diesem Ansatz sollten Art und Inhalt solcher Vorschriften auf die Intensität und den Umfang der Risiken zugeschnitten werden, die von KI-Systemen ausgehen können. Es ist daher notwendig, bestimmte inakzeptable Praktiken im Bereich der KI zu verbieten und Anforderungen an Hochrisiko-KI-Systeme und Pflichten für die betreffenden Akteure sowie Transparenzpflichten für bestimmte KI-Systeme festzulegen."
    },
    {
      "chunk_idx": 26,
      "id": "9ef138b6-76a2-493c-a1c6-31522cbe8a79",
      "title": "Recital 27",
      "relevantChunksIds": [
        "a6e85038-89e8-4790-9457-cb624ca42c00",
        "e436d2fa-0d04-4566-931b-a1955edc7114",
        "7e829946-fb22-41cb-8900-9ff0f0a32229",
        "80b0fa01-7245-4610-a673-5d369d0e9f9a",
        "c9d80e98-9396-4526-ad4f-1e57ebf765f0",
        "239278ca-b87b-4431-b7c9-71fd1a54f9ff",
        "283d70b2-97f7-4252-9190-378e90b707bd",
        "e87ad418-9936-47c0-b4ec-99f31c41a0a8",
        "24b748de-eebc-404f-8b36-d12eed0a6660",
        "85555ab3-0021-4e99-96f7-b8d2182b43f7"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(27) While the risk-based approach is the basis for a proportionate and effective set of binding rules, it is important to recall the 2019 Ethics guidelines for trustworthy AI developed by the independent AI HLEG appointed by the Commission. In those guidelines, the AI HLEG developed seven non-binding ethical principles for AI which are intended to help ensure that AI is trustworthy and ethically sound. The seven principles include human agency and oversight; technical robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; societal and environmental well-being and accountability. Without prejudice to the legally binding requirements of this Regulation and any other applicable Union law, those guidelines contribute to the design of coherent, trustworthy and human-centric AI, in line with the Charter and with the values on which the Union is founded. According to the guidelines of the AI HLEG, human agency and oversight means that AI systems are developed and used as a tool that serves people, respects human dignity and personal autonomy, and that is functioning in a way that can be appropriately controlled and overseen by humans. Technical robustness and safety means that AI systems are developed and used in a way that allows robustness in the case of problems and resilience against attempts to alter the use or performance of the AI system so as to allow unlawful use by third parties, and minimise unintended harm. Privacy and data governance means that AI systems are developed and used in accordance with privacy and data protection rules, while processing data that meets high standards in terms of quality and integrity. Transparency means that AI systems are developed and used in a way that allows appropriate traceability and explainability, while making humans aware that they communicate or interact with an AI system, as well as duly informing deployers of the capabilities and limitations of that AI system and affected persons about their rights. Diversity, non-discrimination and fairness means that AI systems are developed and used in a way that includes diverse actors and promotes equal access, gender equality and cultural diversity, while avoiding discriminatory impacts and unfair biases that are prohibited by Union or national law. Social and environmental well-being means that AI systems are developed and used in a sustainable and environmentally friendly manner as well as in a way to benefit all human beings, while monitoring and assessing the long-term impacts on the individual, society and democracy. The application of those principles should be translated, when possible, in the design and use of AI models. They should in any case serve as a basis for the drafting of codes of conduct under this Regulation. All stakeholders, including industry, academia, civil society and standardisation organisations, are encouraged to take into account, as appropriate, the ethical principles for the development of voluntary best practices and standards.",
      "original_content": "(27) Während der risikobasierte Ansatz die Grundlage für ein verhältnismäßiges und wirksames verbindliches Regelwerk bildet, muss auch auf die Ethikleitlinien für vertrauenswürdige KI von 2019 der von der Kommission eingesetzten unabhängigen hochrangigen Expertengruppe für künstliche Intelligenz verwiesen werden. In diesen Leitlinien hat die hochrangige Expertengruppe sieben unverbindliche ethische Grundsätze für KI entwickelt, die dazu beitragen sollten, dass KI vertrauenswürdig und ethisch vertretbar ist. Zu den sieben Grundsätzen gehören: menschliches Handeln und menschliche Aufsicht, technische Robustheit und Sicherheit, Privatsphäre und Daten-Governance, Transparenz, Vielfalt, Nichtdiskriminierung und Fairness, soziales und ökologisches Wohlergehen sowie Rechenschaftspflicht. Unbeschadet der rechtsverbindlichen Anforderungen dieser Verordnung und anderer geltender Rechtsvorschriften der Union tragen diese Leitlinien zur Gestaltung kohärenter, vertrauenswürdiger und menschenzentrierter KI bei im Einklang mit der Charta und den Werten, auf die sich die Union gründet. Nach den Leitlinien der hochrangigen Expertengruppe bedeutet „menschliches Handeln und menschliche Aufsicht“, dass ein KI-System entwickelt und als Instrument verwendet wird, das den Menschen dient, die Menschenwürde und die persönliche Autonomie achtet und so funktioniert, dass es von Menschen angemessen kontrolliert und überwacht werden kann. „Technische Robustheit und Sicherheit“ bedeutet, dass KI-Systeme so entwickelt und verwendet werden, dass sie im Fall von Schwierigkeiten robust sind und widerstandsfähig gegen Versuche, die Verwendung oder Leistung des KI-Systems so zu verändern, dass dadurch die unrechtmäßige Verwendung durch Dritte ermöglicht wird, und dass ferner unbeabsichtigte Schäden minimiert werden. „Privatsphäre und Daten-Governance“ bedeutet, dass KI-Systeme im Einklang mit den geltenden Vorschriften zum Schutz der Privatsphäre und zum Datenschutz entwickelt und verwendet werden und dabei Daten verarbeiten, die hohen Qualitäts- und Integritätsstandards genügen. „Transparenz“ bedeutet, dass KI-Systeme so entwickelt und verwendet werden, dass sie angemessen nachvollziehbar und erklärbar sind, wobei den Menschen bewusst gemacht werden muss, dass sie mit einem KI-System kommunizieren oder interagieren, und dass die Betreiber ordnungsgemäß über die Fähigkeiten und Grenzen des KI-Systems informieren und die betroffenen Personen über ihre Rechte in Kenntnis setzen müssen. „Vielfalt, Nichtdiskriminierung und Fairness“ bedeutet, dass KI-Systeme in einer Weise entwickelt und verwendet werden, die unterschiedliche Akteure einbezieht und den gleichberechtigten Zugang, die Geschlechtergleichstellung und die kulturelle Vielfalt fördert, wobei diskriminierende Auswirkungen und unfaire Verzerrungen, die nach Unionsrecht oder nationalem Recht verboten sind, verhindert werden. „Soziales und ökologisches Wohlergehen“ bedeutet, dass KI-Systeme in nachhaltiger und umweltfreundlicher Weise und zum Nutzen aller Menschen entwickelt und verwendet werden, wobei die langfristigen Auswirkungen auf den Einzelnen, die Gesellschaft und die Demokratie überwacht und bewertet werden. Die Anwendung dieser Grundsätze sollte, soweit möglich, in die Gestaltung und Verwendung von KI-Modellen einfließen. Sie sollten in jedem Fall als Grundlage für die Ausarbeitung von Verhaltenskodizes im Rahmen dieser Verordnung dienen. Alle Interessenträger, einschließlich der Industrie, der Wissenschaft, der Zivilgesellschaft und der Normungsorganisationen, werden aufgefordert, die ethischen Grundsätze bei der Entwicklung freiwilliger bewährter Verfahren und Normen, soweit angebracht, zu berücksichtigen."
    },
    {
      "chunk_idx": 27,
      "id": "db043755-0b3e-49ed-9a8c-bdd479acfa09",
      "title": "Recital 28",
      "relevantChunksIds": [
        "a6e85038-89e8-4790-9457-cb624ca42c00",
        "e436d2fa-0d04-4566-931b-a1955edc7114",
        "7e829946-fb22-41cb-8900-9ff0f0a32229",
        "80b0fa01-7245-4610-a673-5d369d0e9f9a",
        "c9d80e98-9396-4526-ad4f-1e57ebf765f0",
        "239278ca-b87b-4431-b7c9-71fd1a54f9ff",
        "283d70b2-97f7-4252-9190-378e90b707bd",
        "e87ad418-9936-47c0-b4ec-99f31c41a0a8",
        "24b748de-eebc-404f-8b36-d12eed0a6660",
        "85555ab3-0021-4e99-96f7-b8d2182b43f7"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(28) Aside from the many beneficial uses of AI, it can also be misused and provide novel and powerful tools for manipulative, exploitative and social control practices. Such practices are particularly harmful and abusive and should be prohibited because they contradict Union values of respect for human dignity, freedom, equality, democracy and the rule of law and fundamental rights enshrined in the Charter, including the right to non-discrimination, to data protection and to privacy and the rights of the child.",
      "original_content": "(28) Abgesehen von den zahlreichen nutzbringenden Verwendungsmöglichkeiten von KI kann diese Technologie auch missbraucht werden und neue und wirkungsvolle Instrumente für manipulative, ausbeuterische und soziale Kontrollpraktiken bieten. Solche Praktiken sind besonders schädlich und missbräuchlich und sollten verboten werden, weil sie im Widerspruch zu den Werten der Union stehen, nämlich der Achtung der Menschenwürde, Freiheit, Gleichheit, Demokratie und Rechtsstaatlichkeit sowie der in der Charta verankerten Grundrechte, einschließlich des Rechts auf Nichtdiskriminierung, Datenschutz und Privatsphäre sowie der Rechte des Kindes."
    },
    {
      "chunk_idx": 28,
      "id": "c3e85c74-274f-4f56-a4da-990cbf4c0ee7",
      "title": "Recital 29",
      "relevantChunksIds": [
        "283d70b2-97f7-4252-9190-378e90b707bd"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(29) AI-enabled manipulative techniques can be used to persuade persons to engage in unwanted behaviours, or to deceive them by nudging them into decisions in a way that subverts and impairs their autonomy, decision-making and free choices. The placing on the market, the putting into service or the use of certain AI systems with the objective to or the effect of materially distorting human behaviour, whereby significant harms, in particular having sufficiently important adverse impacts on physical, psychological health or financial interests are likely to occur, are particularly dangerous and should therefore be prohibited. Such AI systems deploy subliminal components such as audio, image, video stimuli that persons cannot perceive, as those stimuli are beyond human perception, or other manipulative or deceptive techniques that subvert or impair person’s autonomy, decision-making or free choice in ways that people are not consciously aware of those techniques or, where they are aware of them, can still be deceived or are not able to control or resist them. This could be facilitated, for example, by machine-brain interfaces or virtual reality as they allow for a higher degree of control of what stimuli are presented to persons, insofar as they may materially distort their behaviour in a significantly harmful manner. In addition, AI systems may also otherwise exploit the vulnerabilities of a person or a specific group of persons due to their age, disability within the meaning of Directive (EU) 2019/882 of the European Parliament and of the Council (16), or a specific social or economic situation that is likely to make those persons more vulnerable to exploitation such as persons living in extreme poverty, ethnic or religious minorities. Such AI systems can be placed on the market, put into service or used with the objective to or the effect of materially distorting the behaviour of a person and in a manner that causes or is reasonably likely to cause significant harm to that or another person or groups of persons, including harms that may be accumulated over time and should therefore be prohibited. It may not be possible to assume that there is an intention to distort behaviour where the distortion results from factors external to the AI system which are outside the control of the provider or the deployer, namely factors that may not be reasonably foreseeable and therefore not possible for the provider or the deployer of the AI system to mitigate. In any case, it is not necessary for the provider or the deployer to have the intention to cause significant harm, provided that such harm results from the manipulative or exploitative AI-enabled practices. The prohibitions for such AI practices are complementary to the provisions contained in Directive 2005/29/EC of the European Parliament and of the Council (17), in particular unfair commercial practices leading to economic or financial harms to consumers are prohibited under all circumstances, irrespective of whether they are put in place through AI systems or otherwise. The prohibitions of manipulative and exploitative practices in this Regulation should not affect lawful practices in the context of medical treatment such as psychological treatment of a mental disease or physical rehabilitation, when those practices are carried out in accordance with the applicable law and medical standards, for example explicit consent of the individuals or their legal representatives. In addition, common and legitimate commercial practices, for example in the field of advertising, that comply with the applicable law should not, in themselves, be regarded as constituting harmful manipulative AI-enabled practices.",
      "original_content": "(29) KI-gestützte manipulative Techniken können dazu verwendet werden, Personen zu unerwünschten Verhaltensweisen zu bewegen oder sie zu täuschen, indem sie in einer Weise zu Entscheidungen angeregt werden, die ihre Autonomie, Entscheidungsfindung und freie Auswahl untergräbt und beeinträchtigt. Das Inverkehrbringen, die Inbetriebnahme oder die Verwendung bestimmter KI-Systeme, die das Ziel oder die Auswirkung haben, menschliches Verhalten maßgeblich nachteilig zu beeinflussen, und große Schäden, insbesondere erhebliche nachteilige Auswirkungen auf die physische und psychische Gesundheit oder auf die finanziellen Interessen verursachen dürften, ist besonders gefährlich und sollte dementsprechend verboten werden. Solche KI-Systeme setzen auf eine unterschwellige Beeinflussung, beispielweise durch Reize in Form von Ton-, Bild- oder Videoinhalten, die für Menschen nicht erkennbar sind, da diese Reize außerhalb ihres Wahrnehmungsbereichs liegen, oder auf andere Arten manipulativer oder täuschender Beeinflussung, die ihre Autonomie, Entscheidungsfindung oder freie Auswahl in einer Weise untergraben und beeinträchtigen, die sich ihrer bewussten Wahrnehmung entzieht oder deren Einfluss — selbst wenn sie sich seiner bewusst sind — sie nicht kontrollieren oder widerstehen können. Dies könnte beispielsweise durch Gehirn-Computer-Schnittstellen oder virtuelle Realität erfolgen, da diese ein höheres Maß an Kontrolle darüber ermöglichen, welche Reize den Personen, insofern diese das Verhalten der Personen in erheblichem Maße schädlich beeinflussen können, angeboten werden. Ferner können KI-Systeme auch anderweitig die Vulnerabilität einer Person oder bestimmter Gruppen von Personen aufgrund ihres Alters oder einer Behinderung im Sinne der Richtlinie (EU) 2019/882 des Europäischen Parlaments und des Rates (Fußnote 16) oder aufgrund einer bestimmten sozialen oder wirtschaftlichen Situation ausnutzen, durch die diese Personen gegenüber einer Ausnutzung anfälliger werden dürften, beispielweise Personen, die in extremer Armut leben, und ethnische oder religiöse Minderheiten. Solche KI-Systeme können mit dem Ziel oder der Wirkung in Verkehr gebracht, in Betrieb genommen oder verwendet werden, das Verhalten einer Person in einer Weise wesentlich zu beeinflussen, die dieser Person oder einer anderen Person oder Gruppen von Personen einen erheblichen Schaden zufügt oder mit hinreichender Wahrscheinlichkeit zufügen wird, einschließlich Schäden, die sich im Laufe der Zeit anhäufen können, und sollten daher verboten werden. Diese Absicht, das Verhalten zu beeinflussen, kann nicht vermutet werden, wenn die Beeinflussung auf Faktoren zurückzuführen ist, die nicht Teil des KI-Systems sind und außerhalb der Kontrolle des Anbieters oder Betreibers liegen, d. h. Faktoren, die vom Anbieter oder Betreiber des KI-Systems vernünftigerweise nicht vorhergesehen oder gemindert werden können. In jedem Fall ist es nicht erforderlich, dass der Anbieter oder der Betreiber die Absicht haben, erheblichen Schaden zuzufügen, wenn dieser Schaden aufgrund von manipulativen oder ausbeuterischen KI-gestützten Praktiken entsteht. Das Verbot solcher KI-Praktiken ergänzt die Bestimmungen der Richtlinie 2005/29/EG des Europäischen Parlaments und des Rates (Fußnote 17); insbesondere sind unlautere Geschäftspraktiken, durch die Verbraucher wirtschaftliche oder finanzielle Schäden erleiden, unter allen Umständen verboten, unabhängig davon, ob sie durch KI-Systeme oder anderweitig umgesetzt werden. Das Verbot manipulativer und ausbeuterischer Praktiken gemäß dieser Verordnung sollte sich nicht auf rechtmäßige Praktiken im Zusammenhang mit medizinischen Behandlungen, etwa der psychologischen Behandlung einer psychischen Krankheit oder der physischen Rehabilitation, auswirken, wenn diese Praktiken gemäß den geltenden Rechtsvorschriften und medizinischen Standards erfolgen, z. B mit der ausdrücklichen Zustimmung der Einzelpersonen oder ihrer gesetzlichen Vertreter. Darüber hinaus sollten übliche und rechtmäßige Geschäftspraktiken, beispielsweise im Bereich der Werbung, die im Einklang mit den geltenden Rechtsvorschriften stehen, als solche nicht als schädliche manipulative KI-gestützten Praktiken gelten. Fußnote 16: Richtlinie (EU) 2019/882 des Europäischen Parlaments und des Rates vom 17. April 2019 über die Barrierefreiheitsanforderungen für Produkte und Dienstleistungen (ABl. L 151 vom 7.6.2019, S. 70)., Fußnote 17: Richtlinie 2005/29/EG des Europäischen Parlaments und des Rates vom 11. Mai 2005 über unlautere Geschäftspraktiken im binnenmarktinternen Geschäftsverkehr zwischen Unternehmen und Verbrauchern und zur Änderung der Richtlinie 84/450/EWG des Rates, der Richtlinien 97/7/EG, 98/27/EG und 2002/65/EG des Europäischen Parlaments und des Rates sowie der Verordnung (EG) Nr. 2006/2004 des Europäischen Parlaments und des Rates (Richtlinie über unlautere Geschäftspraktiken) (ABl. L 149 vom 11.6.2005, S. 22)."
    },
    {
      "chunk_idx": 29,
      "id": "57634ab2-0a60-4452-a940-edfae9ab9d14",
      "title": "Recital 30",
      "relevantChunksIds": [
        "a6e85038-89e8-4790-9457-cb624ca42c00"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(30) Biometric categorisation systems that are based on natural persons’ biometric data, such as an individual person’s face or fingerprint, to deduce or infer an individuals’ political opinions, trade union membership, religious or philosophical beliefs, race, sex life or sexual orientation should be prohibited. That prohibition should not cover the lawful labelling, filtering or categorisation of biometric data sets acquired in line with Union or national law according to biometric data, such as the sorting of images according to hair colour or eye colour, which can for example be used in the area of law enforcement.",
      "original_content": "(30) Systeme zur biometrischen Kategorisierung, die anhand der biometrischen Daten von natürlichen Personen, wie dem Gesicht oder dem Fingerabdruck einer Person, die politische Meinung, die Mitgliedschaft in einer Gewerkschaft, religiöse oder weltanschauliche Überzeugungen, die Rasse, das Sexualleben oder die sexuelle Ausrichtung einer Person erschließen oder ableiten, sollten verboten werden. Dieses Verbot sollte nicht für die rechtmäßige Kennzeichnung, Filterung oder Kategorisierung biometrischer Datensätze gelten, die im Einklang mit dem Unionsrecht oder dem nationalen Recht anhand biometrischer Daten erworben wurden, wie das Sortieren von Bildern nach Haar- oder Augenfarbe, was beispielsweise im Bereich der Strafverfolgung verwendet werden kann."
    },
    {
      "chunk_idx": 30,
      "id": "cc52311b-4b54-409f-894a-919d3d7b931b",
      "title": "Recital 31",
      "relevantChunksIds": [
        "283d70b2-97f7-4252-9190-378e90b707bd"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(31) AI systems providing social scoring of natural persons by public or private actors may lead to discriminatory outcomes and the exclusion of certain groups. They may violate the right to dignity and non-discrimination and the values of equality and justice. Such AI systems evaluate or classify natural persons or groups thereof on the basis of multiple data points related to their social behaviour in multiple contexts or known, inferred or predicted personal or personality characteristics over certain periods of time. The social score obtained from such AI systems may lead to the detrimental or unfavourable treatment of natural persons or whole groups thereof in social contexts, which are unrelated to the context in which the data was originally generated or collected or to a detrimental treatment that is disproportionate or unjustified to the gravity of their social behaviour. AI systems entailing such unacceptable scoring practices and leading to such detrimental or unfavourable outcomes should therefore be prohibited. That prohibition should not affect lawful evaluation practices of natural persons that are carried out for a specific purpose in accordance with Union and national law.",
      "original_content": "(31) KI-Systeme, die eine soziale Bewertung natürlicher Personen durch öffentliche oder private Akteure bereitstellen, können zu diskriminierenden Ergebnissen und zur Ausgrenzung bestimmter Gruppen führen. Sie können die Menschenwürde und das Recht auf Nichtdiskriminierung sowie die Werte der Gleichheit und Gerechtigkeit verletzen. Solche KI-Systeme bewerten oder klassifizieren natürliche Personen oder Gruppen natürlicher Personen in einem bestimmten Zeitraum auf der Grundlage zahlreicher Datenpunkte in Bezug auf ihr soziales Verhalten in verschiedenen Zusammenhängen oder aufgrund bekannter, vermuteter oder vorhergesagter persönlicher Eigenschaften oder Persönlichkeitsmerkmale. Die aus solchen KI-Systemen erzielte soziale Bewertung kann zu einer Schlechterstellung oder Benachteiligung bestimmter natürlicher Personen oder ganzer Gruppen natürlicher Personen in sozialen Kontexten, die in keinem Zusammenhang mit den Umständen stehen, unter denen die Daten ursprünglich erzeugt oder erhoben wurden, oder zu einer Schlechterstellung führen, die im Hinblick auf die Tragweite ihres sozialen Verhaltens unverhältnismäßig oder ungerechtfertigt ist. KI-Systeme, die solche inakzeptablen Bewertungspraktiken mit sich bringen und zu einer solchen Schlechterstellung oder Benachteiligung führen, sollten daher verboten werden. Dieses Verbot sollte nicht die rechtmäßigen Praktiken zur Bewertung natürlicher Personen berühren, die im Einklang mit dem Unionsrecht und dem nationalen Recht zu einem bestimmten Zweck durchgeführt werden."
    },
    {
      "chunk_idx": 31,
      "id": "5d100c53-2098-421b-b3fe-4a40e61bee4f",
      "title": "Recital 32",
      "relevantChunksIds": [
        "e436d2fa-0d04-4566-931b-a1955edc7114",
        "7e829946-fb22-41cb-8900-9ff0f0a32229",
        "80b0fa01-7245-4610-a673-5d369d0e9f9a",
        "c9d80e98-9396-4526-ad4f-1e57ebf765f0",
        "239278ca-b87b-4431-b7c9-71fd1a54f9ff",
        "a6e85038-89e8-4790-9457-cb624ca42c00"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(32) The use of AI systems for ‘real-time’ remote biometric identification of natural persons in publicly accessible spaces for the purpose of law enforcement is particularly intrusive to the rights and freedoms of the concerned persons, to the extent that it may affect the private life of a large part of the population, evoke a feeling of constant surveillance and indirectly dissuade the exercise of the freedom of assembly and other fundamental rights. Technical inaccuracies of AI systems intended for the remote biometric identification of natural persons can lead to biased results and entail discriminatory effects. Such possible biased results and discriminatory effects are particularly relevant with regard to age, ethnicity, race, sex or disabilities. In addition, the immediacy of the impact and the limited opportunities for further checks or corrections in relation to the use of such systems operating in real-time carry heightened risks for the rights and freedoms of the persons concerned in the context of, or impacted by, law enforcement activities.",
      "original_content": "(32) Die Verwendung von KI-Systemen zur biometrischen Echtzeit-Fernidentifizierung natürlicher Personen in öffentlich zugänglichen Räumen zu Strafverfolgungszwecken greift besonders in die Rechte und Freiheiten der betroffenen Personen ein, da sie die Privatsphäre eines großen Teils der Bevölkerung beeinträchtigt, ein Gefühl der ständigen Überwachung weckt und indirekt von der Ausübung der Versammlungsfreiheit und anderer Grundrechte abhalten kann. Technische Ungenauigkeiten von KI-Systemen, die für die biometrische Fernidentifizierung natürlicher Personen bestimmt sind, können zu verzerrten Ergebnissen führen und eine diskriminierende Wirkung haben. Solche möglichen verzerrten Ergebnisse und eine solche diskriminierende Wirkung sind von besonderer Bedeutung, wenn es um das Alter, die ethnische Herkunft, die Rasse, das Geschlecht oder Behinderungen geht. Darüber hinaus bergen die Unmittelbarkeit der Auswirkungen und die begrenzten Möglichkeiten weiterer Kontrollen oder Korrekturen im Zusammenhang mit der Verwendung solcher in Echtzeit betriebener Systeme erhöhte Risiken für die Rechte und Freiheiten der betreffenden Personen, die im Zusammenhang mit Strafverfolgungsmaßnahmen stehen oder davon betroffen sind."
    },
    {
      "chunk_idx": 32,
      "id": "c0d0abea-63e2-4ed1-b273-a1882356a3a7",
      "title": "Recital 33",
      "relevantChunksIds": [
        "a6e85038-89e8-4790-9457-cb624ca42c00",
        "e436d2fa-0d04-4566-931b-a1955edc7114",
        "7e829946-fb22-41cb-8900-9ff0f0a32229",
        "80b0fa01-7245-4610-a673-5d369d0e9f9a",
        "c9d80e98-9396-4526-ad4f-1e57ebf765f0",
        "239278ca-b87b-4431-b7c9-71fd1a54f9ff"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(33) The use of those systems for the purpose of law enforcement should therefore be prohibited, except in exhaustively listed and narrowly defined situations, where the use is strictly necessary to achieve a substantial public interest, the importance of which outweighs the risks. Those situations involve the search for certain victims of crime including missing persons; certain threats to the life or to the physical safety of natural persons or of a terrorist attack; and the localisation or identification of perpetrators or suspects of the criminal offences listed in an annex to this Regulation, where those criminal offences are punishable in the Member State concerned by a custodial sentence or a detention order for a maximum period of at least four years and as they are defined in the law of that Member State. Such a threshold for the custodial sentence or detention order in accordance with national law contributes to ensuring that the offence should be serious enough to potentially justify the use of ‘real-time’ remote biometric identification systems. Moreover, the list of criminal offences provided in an annex to this Regulation is based on the 32 criminal offences listed in the Council Framework Decision 2002/584/JHA (18), taking into account that some of those offences are, in practice, likely to be more relevant than others, in that the recourse to ‘real-time’ remote biometric identification could, foreseeably, be necessary and proportionate to highly varying degrees for the practical pursuit of the localisation or identification of a perpetrator or suspect of the different criminal offences listed and having regard to the likely differences in the seriousness, probability and scale of the harm or possible negative consequences. An imminent threat to life or the physical safety of natural persons could also result from a serious disruption of critical infrastructure, as defined in Article 2, point (4) of Directive (EU) 2022/2557 of the European Parliament and of the Council (19), where the disruption or destruction of such critical infrastructure would result in an imminent threat to life or the physical safety of a person, including through serious harm to the provision of basic supplies to the population or to the exercise of the core function of the State. In addition, this Regulation should preserve the ability for law enforcement, border control, immigration or asylum authorities to carry out identity checks in the presence of the person concerned in accordance with the conditions set out in Union and national law for such checks. In particular, law enforcement, border control, immigration or asylum authorities should be able to use information systems, in accordance with Union or national law, to identify persons who, during an identity check, either refuse to be identified or are unable to state or prove their identity, without being required by this Regulation to obtain prior authorisation. This could be, for example, a person involved in a crime, being unwilling, or unable due to an accident or a medical condition, to disclose their identity to law enforcement authorities.",
      "original_content": "(33) Die Verwendung solcher Systeme zu Strafverfolgungszwecken sollte daher untersagt werden, außer in erschöpfend aufgeführten und eng abgegrenzten Fällen, in denen die Verwendung unbedingt erforderlich ist, um einem erheblichen öffentlichen Interesse zu dienen, dessen Bedeutung die Risiken überwiegt. Zu diesen Fällen gehört die Suche nach bestimmten Opfern von Straftaten, einschließlich vermisster Personen, bestimmte Gefahren für das Leben oder die körperliche Unversehrtheit natürlicher Personen oder die Gefahr eines Terroranschlags sowie das Aufspüren oder Identifizieren von Tätern oder Verdächtigen in Bezug auf die in einem Anhang zu dieser Verordnung genannten Straftaten, sofern diese Straftaten in dem betreffenden Mitgliedstaat im Sinne des Rechts dieses Mitgliedstaats mit einer Freiheitsstrafe oder einer freiheitsentziehenden Maßregel der Sicherung im Höchstmaß von mindestens vier Jahren bedroht sind. Eine solche Schwelle für eine Freiheitsstrafe oder eine freiheitsentziehende Maßregel der Sicherung nach nationalem Recht trägt dazu bei, sicherzustellen, dass die Straftat schwerwiegend genug ist, um den Einsatz biometrischer Echtzeit-Fernidentifizierungssysteme möglicherweise zu rechtfertigen. Darüber hinaus beruht die im Anhang dieser Verordnung aufgeführte Liste der Straftaten auf den 32 im Rahmenbeschluss 2002/584/JI des Rates (Fußnote 18) aufgeführten Straftaten, unter Berücksichtigung der Tatsache, dass einige der Straftaten in der Praxis eher relevant sein können als andere, da der Rückgriff auf die biometrische Echtzeit-Fernidentifizierung für die konkrete Aufspürung oder Identifizierung eines Täters oder Verdächtigen in Bezug auf eine der verschiedenen aufgeführten Straftaten voraussichtlich in äußerst unterschiedlichem Maße erforderlich und verhältnismäßig sein könnte und da dabei die wahrscheinlichen Unterschiede in Schwere, Wahrscheinlichkeit und Ausmaß des Schadens oder möglicher negativer Folgen zu berücksichtigen sind. Eine unmittelbare Gefahr für das Leben oder die körperliche Unversehrtheit natürlicher Personen kann auch durch die schwerwiegende Störung einer kritischen Infrastruktur im Sinne des Artikels 2 Nummer 4 der Richtlinie (EU) 2022/2557 des Europäischen Parlaments und des Rates (Fußnote 19) entstehen, wenn die Störung oder Zerstörung einer solchen kritischen Infrastruktur zu einer unmittelbaren Gefahr für das Leben oder die körperliche Unversehrtheit einer Person führen würde, auch durch die schwerwiegende Beeinträchtigung der Bereitstellung der Grundversorgung für die Bevölkerung oder der Wahrnehmung der Kernfunktion des Staates. Darüber hinaus sollte diese Verordnung die Fähigkeit der Strafverfolgungs-, Grenzschutz-, Einwanderungs- oder Asylbehörden erhalten, gemäß den im Unionsrecht und im nationalen Recht für diesen Zweck festgelegten Bedingungen die Identität der betreffenden Person in ihrer Anwesenheit festzustellen. Insbesondere sollten Strafverfolgungs-, Grenzschutz-, Einwanderungs- oder Asylbehörden gemäß dem Unionsrecht oder dem nationalen Recht Informationssysteme verwenden können, um eine Person zu identifizieren, die während einer Identitätsfeststellung entweder verweigert, identifiziert zu werden, oder nicht in der Lage ist, ihre Identität anzugeben oder zu belegen, wobei gemäß dieser Verordnung keine vorherige Genehmigung erlangt werden muss. Dabei könnte es sich beispielsweise um eine Person handeln, die in eine Straftat verwickelt ist und nicht gewillt oder aufgrund eines Unfalls oder des Gesundheitszustands nicht in der Lage ist, den Strafverfolgungsbehörden ihre Identität offenzulegen. Fußnote18: Rahmenbeschluss 2002/584/JI des Rates vom 13. Juni 2002 über den Europäischen Haftbefehl und die Übergabeverfahren zwischen den Mitgliedstaaten (ABl. L 190 vom 18.7.2002, S. 1)., Fußnote 19: Richtlinie (EU) 2022/2557 des Europäischen Parlaments und des Rates vom 14. Dezember 2022 über die Resilienz kritischer Einrichtungen und zur Aufhebung der Richtlinie 2008/114/EG des Rates (ABl. L 333 vom 27.12.2022, S. 164)."
    },
    {
      "chunk_idx": 33,
      "id": "1d0188f8-8269-474c-aa50-c34765fa8aeb",
      "title": "Recital 34",
      "relevantChunksIds": [
        "a6e85038-89e8-4790-9457-cb624ca42c00",
        "e436d2fa-0d04-4566-931b-a1955edc7114",
        "7e829946-fb22-41cb-8900-9ff0f0a32229",
        "80b0fa01-7245-4610-a673-5d369d0e9f9a",
        "c9d80e98-9396-4526-ad4f-1e57ebf765f0",
        "239278ca-b87b-4431-b7c9-71fd1a54f9ff"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(34) In order to ensure that those systems are used in a responsible and proportionate manner, it is also important to establish that, in each of those exhaustively listed and narrowly defined situations, certain elements should be taken into account, in particular as regards the nature of the situation giving rise to the request and the consequences of the use for the rights and freedoms of all persons concerned and the safeguards and conditions provided for with the use. In addition, the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement should be deployed only to confirm the specifically targeted individual’s identity and should be limited to what is strictly necessary concerning the period of time, as well as the geographic and personal scope, having regard in particular to the evidence or indications regarding the threats, the victims or perpetrator. The use of the real-time remote biometric identification system in publicly accessible spaces should be authorised only if the relevant law enforcement authority has completed a fundamental rights impact assessment and, unless provided otherwise in this Regulation, has registered the system in the database as set out in this Regulation. The reference database of persons should be appropriate for each use case in each of the situations mentioned above.",
      "original_content": "(34) Um sicherzustellen, dass diese Systeme verantwortungsvoll und verhältnismäßig genutzt werden, ist es auch wichtig, festzulegen, dass in jedem dieser erschöpfend aufgeführten und eng abgegrenzten Fälle bestimmte Elemente berücksichtigt werden sollten, insbesondere in Bezug auf die Art des dem Antrag zugrunde liegenden Falls und die Auswirkungen der Verwendung auf die Rechte und Freiheiten aller betroffenen Personen sowie auf die für die Verwendung geltenden Schutzvorkehrungen und Bedingungen. Darüber hinaus sollte die Verwendung biometrischer Echtzeit-Fernidentifizierungssysteme in öffentlich zugänglichen Räumen für die Zwecke der Strafverfolgung nur eingesetzt werden, um die Identität der speziell betroffenen Person zu bestätigen und sollte sich hinsichtlich des Zeitraums und des geografischen und personenbezogenen Anwendungsbereichs auf das unbedingt Notwendige beschränken, wobei insbesondere den Beweisen oder Hinweisen in Bezug auf die Bedrohungen, die Opfer oder den Täter Rechnung zu tragen ist. Die Verwendung des biometrischen Echtzeit-Fernidentifizierungssystems in öffentlich zugänglichen Räumen sollte nur genehmigt werden, wenn die zuständige Strafverfolgungsbehörde eine Grundrechte-Folgenabschätzung durchgeführt und, sofern in dieser Verordnung nichts anderes bestimmt ist, das System gemäß dieser Verordnung in der Datenbank registriert hat. Die Personenreferenzdatenbank sollte für jeden Anwendungsfall in jedem der oben genannten Fälle geeignet sein."
    },
    {
      "chunk_idx": 34,
      "id": "263930a6-b8af-4ad8-a6bd-ada082c8e8e5",
      "title": "Recital 35",
      "relevantChunksIds": [
        "a6e85038-89e8-4790-9457-cb624ca42c00",
        "e436d2fa-0d04-4566-931b-a1955edc7114",
        "7e829946-fb22-41cb-8900-9ff0f0a32229",
        "80b0fa01-7245-4610-a673-5d369d0e9f9a",
        "c9d80e98-9396-4526-ad4f-1e57ebf765f0",
        "239278ca-b87b-4431-b7c9-71fd1a54f9ff"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(35) Each use of a ‘real-time’ remote biometric identification system in publicly accessible spaces for the purpose of law enforcement should be subject to an express and specific authorisation by a judicial authority or by an independent administrative authority of a Member State whose decision is binding. Such authorisation should, in principle, be obtained prior to the use of the AI system with a view to identifying a person or persons. Exceptions to that rule should be allowed in duly justified situations on grounds of urgency, namely in situations where the need to use the systems concerned is such as to make it effectively and objectively impossible to obtain an authorisation before commencing the use of the AI system. In such situations of urgency, the use of the AI system should be restricted to the absolute minimum necessary and should be subject to appropriate safeguards and conditions, as determined in national law and specified in the context of each individual urgent use case by the law enforcement authority itself. In addition, the law enforcement authority should in such situations request such authorisation while providing the reasons for not having been able to request it earlier, without undue delay and at the latest within 24 hours. If such an authorisation is rejected, the use of real-time biometric identification systems linked to that authorisation should cease with immediate effect and all the data related to such use should be discarded and deleted. Such data includes input data directly acquired by an AI system in the course of the use of such system as well as the results and outputs of the use linked to that authorisation. It should not include input that is legally acquired in accordance with another Union or national law. In any case, no decision producing an adverse legal effect on a person should be taken based solely on the output of the remote biometric identification system.",
      "original_content": "(35) Jede Verwendung biometrischer Echtzeit-Fernidentifizierungssysteme in öffentlich zugänglichen Räumen zu Strafverfolgungszwecken sollte einer ausdrücklichen spezifischen Genehmigung durch eine Justizbehörde oder eine unabhängige Verwaltungsbehörde eines Mitgliedstaats, deren Entscheidung rechtsverbindlich ist, unterliegen. Eine solche Genehmigung sollte grundsätzlich vor der Verwendung des KI-Systems zur Identifizierung einer Person oder mehrerer Personen eingeholt werden. Ausnahmen von dieser Regel sollten in hinreichend begründeten dringenden Fällen erlaubt sein, d. h. in Situationen, in denen es wegen der Notwendigkeit der Verwendung der betreffenden Systeme tatsächlich und objektiv unmöglich ist, vor dem Beginn der Verwendung des KI-Systems eine Genehmigung einzuholen. In solchen dringenden Fällen sollte die Verwendung des KI-Systems auf das absolut notwendige Mindestmaß beschränkt werden und angemessenen Schutzvorkehrungen und Bedingungen unterliegen, die im nationalen Recht festgelegt sind und im Zusammenhang mit jedem einzelnen dringenden Anwendungsfall von der Strafverfolgungsbehörde selbst präzisiert werden. Darüber hinaus sollte die Strafverfolgungsbehörde in solchen Fällen eine solche Genehmigung beantragen und die Gründe dafür angeben, warum sie sie nicht früher, unverzüglich, spätestens jedoch innerhalb von 24 Stunden beantragen konnte. Wird eine solche Genehmigung abgelehnt, sollte die Verwendung biometrischer Echtzeit-Identifizierungssysteme, die mit dieser Genehmigung verbunden sind, mit sofortiger Wirkung eingestellt werden, und alle Daten im Zusammenhang mit dieser Verwendung sollten verworfen und gelöscht werden. Diese Daten umfassen Eingabedaten, die von einem KI-System während der Nutzung eines solchen Systems direkt erfasst werden, sowie die Ergebnisse und Ausgaben der mit dieser Genehmigung verbundenen Verwendung. Daten, die im Einklang mit anderem Unionsrecht oder nationalem Recht rechtmäßig erworben wurden, sollten davon nicht betroffen sein. In jedem Fall darf keine Entscheidung mit nachteiligen Rechtsfolgen für eine Person allein auf der Grundlage der Ausgaben des biometrischen Fernidentifizierungssystems getroffen werden."
    },
    {
      "chunk_idx": 35,
      "id": "757068d7-b3cb-45d4-9e8a-cb78aad7a2a3",
      "title": "Recital 36",
      "relevantChunksIds": [
        "a6e85038-89e8-4790-9457-cb624ca42c00",
        "e436d2fa-0d04-4566-931b-a1955edc7114",
        "80b0fa01-7245-4610-a673-5d369d0e9f9a",
        "7e829946-fb22-41cb-8900-9ff0f0a32229",
        "c9d80e98-9396-4526-ad4f-1e57ebf765f0",
        "239278ca-b87b-4431-b7c9-71fd1a54f9ff"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(36) In order to carry out their tasks in accordance with the requirements set out in this Regulation as well as in national rules, the relevant market surveillance authority and the national data protection authority should be notified of each use of the real-time biometric identification system. Market surveillance authorities and the national data protection authorities that have been notified should submit to the Commission an annual report on the use of real-time biometric identification systems.",
      "original_content": "(36) Damit sie ihre Aufgaben im Einklang mit den Anforderungen dieser Verordnung und der nationalen Vorschriften erfüllen können, sollte die zuständige Marktüberwachungsbehörde und die nationale Datenschutzbehörde über jede Verwendung des biometrischen Echtzeit-Identifizierungssystems unterrichtet werden. Die Marktüberwachungsbehörden und die nationalen Datenschutzbehörden, die unterrichtet wurden, sollten der Kommission jährlich einen Bericht über die Verwendung biometrischer Echtzeit-Identifizierungssysteme vorlegen."
    },
    {
      "chunk_idx": 36,
      "id": "8eb3b772-a8b9-4ca0-bc85-92977cee0861",
      "title": "Recital 37",
      "relevantChunksIds": [
        "e436d2fa-0d04-4566-931b-a1955edc7114",
        "7e829946-fb22-41cb-8900-9ff0f0a32229",
        "80b0fa01-7245-4610-a673-5d369d0e9f9a",
        "c9d80e98-9396-4526-ad4f-1e57ebf765f0",
        "239278ca-b87b-4431-b7c9-71fd1a54f9ff",
        "a6e85038-89e8-4790-9457-cb624ca42c00"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(37) Furthermore, it is appropriate to provide, within the exhaustive framework set by this Regulation that such use in the territory of a Member State in accordance with this Regulation should only be possible where and in as far as the Member State concerned has decided to expressly provide for the possibility to authorise such use in its detailed rules of national law. Consequently, Member States remain free under this Regulation not to provide for such a possibility at all or to only provide for such a possibility in respect of some of the objectives capable of justifying authorised use identified in this Regulation. Such national rules should be notified to the Commission within 30 days of their adoption.",
      "original_content": "(37) Darüber hinaus ist es angezeigt, innerhalb des durch diese Verordnung vorgegebenen erschöpfenden Rahmens festzulegen, dass eine solche Verwendung im Hoheitsgebiet eines Mitgliedstaats gemäß dieser Verordnung nur möglich sein sollte, sofern der betreffende Mitgliedstaat in seinen detaillierten nationalen Rechtsvorschriften ausdrücklich vorgesehen hat, dass eine solche Verwendung genehmigt werden kann. Folglich steht es den Mitgliedstaaten im Rahmen dieser Verordnung frei, eine solche Möglichkeit generell oder nur in Bezug auf einige der in dieser Verordnung genannten Ziele, für die eine genehmigte Verwendung gerechtfertigt sein kann, vorzusehen. Solche nationalen Rechtsvorschriften sollten der Kommission spätestens 30 Tage nach ihrer Annahme mitgeteilt werden."
    },
    {
      "chunk_idx": 37,
      "id": "641fe645-ae39-4cdb-a206-d34d77981d2f",
      "title": "Recital 38",
      "relevantChunksIds": [
        "a6e85038-89e8-4790-9457-cb624ca42c00",
        "e436d2fa-0d04-4566-931b-a1955edc7114",
        "7e829946-fb22-41cb-8900-9ff0f0a32229",
        "80b0fa01-7245-4610-a673-5d369d0e9f9a",
        "c9d80e98-9396-4526-ad4f-1e57ebf765f0",
        "239278ca-b87b-4431-b7c9-71fd1a54f9ff"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(38) The use of AI systems for real-time remote biometric identification of natural persons in publicly accessible spaces for the purpose of law enforcement necessarily involves the processing of biometric data. The rules of this Regulation that prohibit, subject to certain exceptions, such use, which are based on Article 16 TFEU, should apply as lex specialis in respect of the rules on the processing of biometric data contained in Article 10 of Directive (EU) 2016/680, thus regulating such use and the processing of biometric data involved in an exhaustive manner. Therefore, such use and processing should be possible only in as far as it is compatible with the framework set by this Regulation, without there being scope, outside that framework, for the competent authorities, where they act for purpose of law enforcement, to use such systems and process such data in connection thereto on the grounds listed in Article 10 of Directive (EU) 2016/680. In that context, this Regulation is not intended to provide the legal basis for the processing of personal data under Article 8 of Directive (EU) 2016/680. However, the use of real-time remote biometric identification systems in publicly accessible spaces for purposes other than law enforcement, including by competent authorities, should not be covered by the specific framework regarding such use for the purpose of law enforcement set by this Regulation. Such use for purposes other than law enforcement should therefore not be subject to the requirement of an authorisation under this Regulation and the applicable detailed rules of national law that may give effect to that authorisation.",
      "original_content": "(38) Die Verwendung von KI-Systemen zur biometrischen Echtzeit-Fernidentifizierung natürlicher Personen in öffentlich zugänglichen Räumen zu Strafverfolgungszwecken erfordert zwangsläufig die Verarbeitung biometrischer Daten. Die Vorschriften dieser Verordnung, die vorbehaltlich bestimmter Ausnahmen eine solche Verwendung auf der Grundlage des Artikels 16 AEUV verbieten, sollten als Lex specialis in Bezug auf die in Artikel 10 der Richtlinie (EU) 2016/680 enthaltenen Vorschriften über die Verarbeitung biometrischer Daten gelten und somit die Verwendung und Verarbeitung der betreffenden biometrischen Daten umfassend regeln. Eine solche Verwendung und Verarbeitung sollte daher nur möglich sein, soweit sie mit dem in dieser Verordnung festgelegten Rahmen vereinbar ist, ohne dass es den zuständigen Behörden bei ihren Tätigkeiten zu Strafverfolgungszwecken Raum lässt, außerhalb dieses Rahmens solche Systeme zu verwenden und die damit verbundenen Daten aus den in Artikel 10 der Richtlinie (EU) 2016/680 aufgeführten Gründen zu verarbeiten. In diesem Zusammenhang soll diese Verordnung nicht als Rechtsgrundlage für die Verarbeitung personenbezogener Daten gemäß Artikel 8 der Richtlinie (EU) 2016/680 dienen. Die Verwendung biometrischer Echtzeit-Fernidentifizierungssysteme in öffentlich zugänglichen Räumen zu anderen Zwecken als der Strafverfolgung, auch durch zuständige Behörden, sollte jedoch nicht unter den in dieser Verordnung festgelegten spezifischen Rahmen für diese Verwendung zu Strafverfolgungszwecken fallen. Eine solche Verwendung zu anderen Zwecken als der Strafverfolgung sollte daher nicht der Genehmigungspflicht gemäß dieser Verordnung und den zu dieser Genehmigung anwendbaren detaillierten nationalen Rechtsvorschriften unterliegen."
    },
    {
      "chunk_idx": 38,
      "id": "e97971b9-9a7c-49f3-9fe7-3d17ed71fc7a",
      "title": "Recital 39",
      "relevantChunksIds": [
        "a6e85038-89e8-4790-9457-cb624ca42c00",
        "e436d2fa-0d04-4566-931b-a1955edc7114",
        "7e829946-fb22-41cb-8900-9ff0f0a32229",
        "80b0fa01-7245-4610-a673-5d369d0e9f9a",
        "c9d80e98-9396-4526-ad4f-1e57ebf765f0",
        "239278ca-b87b-4431-b7c9-71fd1a54f9ff"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(39) Any processing of biometric data and other personal data involved in the use of AI systems for biometric identification, other than in connection to the use of real-time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement as regulated by this Regulation, should continue to comply with all requirements resulting from Article 10 of Directive (EU) 2016/680. For purposes other than law enforcement, Article 9(1) of Regulation (EU) 2016/679 and Article 10(1) of Regulation (EU) 2018/1725 prohibit the processing of biometric data subject to limited exceptions as provided in those Articles. In the application of Article 9(1) of Regulation (EU) 2016/679, the use of remote biometric identification for purposes other than law enforcement has already been subject to prohibition decisions by national data protection authorities.",
      "original_content": "(39) Jede Verarbeitung biometrischer Daten und anderer personenbezogener Daten im Zusammenhang mit der Verwendung von KI-Systemen für die biometrische Identifizierung, ausgenommen im Zusammenhang mit der Verwendung biometrischer Echtzeit-Fernidentifizierungssysteme in öffentlich zugänglichen Räumen zu Strafverfolgungszwecken im Sinne dieser Verordnung, sollte weiterhin allen Anforderungen genügen, die sich aus Artikel 10 der Richtlinie (EU) 2016/680 ergeben. Für andere Zwecke als die Strafverfolgung ist die Verarbeitung biometrischer Daten gemäß Artikel 9 Absatz 1 der Verordnung (EU) 2016/679 und Artikel 10 Absatz 1 der Verordnung (EU) 2018/1725, vorbehaltlich der in diesen Artikeln vorgesehenen begrenzten Ausnahmefällen, verboten. In Anwendung des Artikels 9 Absatz 1 der Verordnung (EU) 2016/679 war die Verwendung biometrischer Fernidentifizierung zu anderen Zwecken als der Strafverfolgung bereits Gegenstand von Verbotsentscheidungen der nationalen Datenschutzbehörden."
    },
    {
      "chunk_idx": 39,
      "id": "e2d5f660-9b2c-4eba-ac74-adbd809c9700",
      "title": "Recital 40",
      "relevantChunksIds": [
        "a6e85038-89e8-4790-9457-cb624ca42c00",
        "e436d2fa-0d04-4566-931b-a1955edc7114",
        "7e829946-fb22-41cb-8900-9ff0f0a32229",
        "80b0fa01-7245-4610-a673-5d369d0e9f9a",
        "c9d80e98-9396-4526-ad4f-1e57ebf765f0",
        "239278ca-b87b-4431-b7c9-71fd1a54f9ff"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(40) In accordance with Article 6a of Protocol No 21 on the position of the United Kingdom and Ireland in respect of the area of freedom, security and justice, as annexed to the TEU and to the TFEU, Ireland is not bound by the rules laid down in Article 5(1), first subparagraph, point (g), to the extent it applies to the use of biometric categorisation systems for activities in the field of police cooperation and judicial cooperation in criminal matters, Article 5(1), first subparagraph, point (d), to the extent it applies to the use of AI systems covered by that provision, Article 5(1), first subparagraph, point (h), Article 5(2) to (6) and Article 26(10) of this Regulation adopted on the basis of Article 16 TFEU which relate to the processing of personal data by the Member States when carrying out activities falling within the scope of Chapter 4 or Chapter 5 of Title V of Part Three of the TFEU, where Ireland is not bound by the rules governing the forms of judicial cooperation in criminal matters or police cooperation which require compliance with the provisions laid down on the basis of Article 16 TFEU.",
      "original_content": "(40) Nach Artikel 6a des dem EUV und dem AEUV beigefügten Protokolls Nr. 21 über die Position des Vereinigten Königreichs und Irlands hinsichtlich des Raums der Freiheit, der Sicherheit und des Rechts sind die auf der Grundlage des Artikels 16 AEUV festgelegten Vorschriften in Artikel 5 Absatz 1 Unterabsatz 1 Buchstabe g, soweit er auf die Verwendung von Systemen zur biometrischen Kategorisierung für Tätigkeiten im Bereich der polizeilichen Zusammenarbeit und der justiziellen Zusammenarbeit in Strafsachen Anwendung findet, Artikel 5 Absatz 1 Buchstabe d, soweit sie auf die Verwendung von KI-Systemen nach der darin festgelegten Bestimmung Anwendung finden, sowie Artikel 5 Absatz 1 Unterabsatz 1 Buchstabe h, Artikel 5 Absätze 2 bis 6 und Artikel 26 Absatz 10 dieser Verordnung in Bezug auf die Verarbeitung personenbezogener Daten durch die Mitgliedstaaten im Rahmen der Ausübung von Tätigkeiten, die in den Anwendungsbereich des Dritten Teils Titel V Kapitel 4 oder 5 AEUV fallen, für Irland nicht bindend, wenn Irland nicht durch die Vorschriften gebunden ist, die die Formen der justiziellen Zusammenarbeit in Strafsachen oder der polizeilichen Zusammenarbeit regeln, in deren Rahmen die auf der Grundlage des Artikels 16 AEUV festgelegten Vorschriften eingehalten werden müssen."
    },
    {
      "chunk_idx": 40,
      "id": "fd8838c6-8253-40a4-b5e8-7f539ec45032",
      "title": "Recital 41",
      "relevantChunksIds": [
        "a6e85038-89e8-4790-9457-cb624ca42c00",
        "e436d2fa-0d04-4566-931b-a1955edc7114",
        "7e829946-fb22-41cb-8900-9ff0f0a32229",
        "80b0fa01-7245-4610-a673-5d369d0e9f9a",
        "c9d80e98-9396-4526-ad4f-1e57ebf765f0",
        "239278ca-b87b-4431-b7c9-71fd1a54f9ff"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(41) In accordance with Articles 2 and 2a of Protocol No 22 on the position of Denmark, annexed to the TEU and to the TFEU, Denmark is not bound by rules laid down in Article 5(1), first subparagraph, point (g), to the extent it applies to the use of biometric categorisation systems for activities in the field of police cooperation and judicial cooperation in criminal matters, Article 5(1), first subparagraph, point (d), to the extent it applies to the use of AI systems covered by that provision, Article 5(1), first subparagraph, point (h), (2) to (6) and Article 26(10) of this Regulation adopted on the basis of Article 16 TFEU, or subject to their application, which relate to the processing of personal data by the Member States when carrying out activities falling within the scope of Chapter 4 or Chapter 5 of Title V of Part Three of the TFEU.",
      "original_content": "(41) Nach den Artikeln 2 und 2a des dem EUV und dem AEUV beigefügten Protokolls Nr. 22 über die Position Dänemarks ist Dänemark durch die auf der Grundlage des Artikels 16 AEUV festgelegten Vorschriften in Artikel 5 Absatz 1 Unterabsatz 1 Buchstabe g soweit er auf die Verwendung von Systemen zur biometrischen Kategorisierung für Tätigkeiten im Bereich der polizeilichen Zusammenarbeit und der justiziellen Zusammenarbeit in Strafsachen Anwendung findet, Artikel 5 Absatz 1 Unterabsatz 1 Buchstaben d soweit sie auf die Verwendung von KI-Systemen nach der darin festgelegten Bestimmung Anwendung finden, sowie Artikel 5 Absatz 1 Unterabsatz 1 Buchstabe h, Artikel 5 Absätze 2 bis 6 und Artikel 26 Absatz 10 dieser Verordnung in Bezug auf die Verarbeitung personenbezogener Daten durch die Mitgliedstaaten im Rahmen der Ausübung von Tätigkeiten, die in den Anwendungsbereich des Dritten Teils Titel V Kapitel 4 oder 5 AEUV fallen, weder gebunden noch zu ihrer Anwendung verpflichtet."
    },
    {
      "chunk_idx": 41,
      "id": "82237262-5992-4506-b339-958fcac3761c",
      "title": "Recital 42",
      "relevantChunksIds": [
        "e87ad418-9936-47c0-b4ec-99f31c41a0a8"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(42) In line with the presumption of innocence, natural persons in the Union should always be judged on their actual behaviour. Natural persons should never be judged on AI-predicted behaviour based solely on their profiling, personality traits or characteristics, such as nationality, place of birth, place of residence, number of children, level of debt or type of car, without a reasonable suspicion of that person being involved in a criminal activity based on objective verifiable facts and without human assessment thereof. Therefore, risk assessments carried out with regard to natural persons in order to assess the likelihood of their offending or to predict the occurrence of an actual or potential criminal offence based solely on profiling them or on assessing their personality traits and characteristics should be prohibited. In any case, that prohibition does not refer to or touch upon risk analytics that are not based on the profiling of individuals or on the personality traits and characteristics of individuals, such as AI systems using risk analytics to assess the likelihood of financial fraud by undertakings on the basis of suspicious transactions or risk analytic tools to predict the likelihood of the localisation of narcotics or illicit goods by customs authorities, for example on the basis of known trafficking routes.",
      "original_content": "(42) Im Einklang mit der Unschuldsvermutung sollten natürliche Personen in der Union stets nach ihrem tatsächlichen Verhalten beurteilt werden. Natürliche Personen sollten niemals allein nach dem Verhalten beurteilt werden, das von einer KI auf der Grundlage ihres Profiling, ihrer Persönlichkeitsmerkmale oder -eigenschaften wie Staatsangehörigkeit, Geburtsort, Wohnort, Anzahl der Kinder, Schulden oder Art ihres Fahrzeugs vorhergesagt wird, ohne dass ein begründeter Verdacht besteht, dass diese Person an einer kriminellen Tätigkeit auf der Grundlage objektiver nachprüfbarer Tatsachen beteiligt ist, und ohne dass eine menschliche Überprüfung stattfindet. Daher sollten Risikobewertungen, die in Bezug auf natürliche Personen durchgeführt werden, um zu bewerten, ob sie straffällig werden oder um eine tatsächliche oder mögliche Straftat vorherzusagen, und dies ausschließlich auf dem Profiling dieser Personen oder der Bewertung ihrer Persönlichkeitsmerkmale und -eigenschaften beruht, verboten werden. In jedem Fall betrifft oder berührt dieses Verbot nicht Risikoanalysen, die nicht auf dem Profiling von Einzelpersonen oder auf Persönlichkeitsmerkmalen und -eigenschaften von Einzelpersonen beruhen, wie etwa KI-Systeme, die Risikoanalysen zur Bewertung der Wahrscheinlichkeit eines Finanzbetrugs durch Unternehmen auf der Grundlage verdächtiger Transaktionen durchführen, oder Risikoanalyseinstrumente einsetzen, um die Wahrscheinlichkeit vorherzusagen, dass Betäubungsmittel oder illegale Waren durch Zollbehörden, beispielsweise auf der Grundlage bekannter Schmuggelrouten, aufgespürt werden."
    },
    {
      "chunk_idx": 42,
      "id": "917725ae-e2f2-4cb4-aa3c-4d530d7373c9",
      "title": "Recital 43",
      "relevantChunksIds": [
        "24b748de-eebc-404f-8b36-d12eed0a6660"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(43) The placing on the market, the putting into service for that specific purpose, or the use of AI systems that create or expand facial recognition databases through the untargeted scraping of facial images from the internet or CCTV footage, should be prohibited because that practice adds to the feeling of mass surveillance and can lead to gross violations of fundamental rights, including the right to privacy.",
      "original_content": "(43) Das Inverkehrbringen, die Inbetriebnahme für diesen spezifischen Zweck oder die Verwendung von KI-Systemen, die Datenbanken zur Gesichtserkennung durch das ungezielte Auslesen von Gesichtsbildern aus dem Internet oder von Videoüberwachungsaufnahmen erstellen oder erweitern, sollte verboten werden, da dies das Gefühl der Massenüberwachung verstärkt und zu schweren Verstößen gegen die Grundrechte, einschließlich des Rechts auf Privatsphäre, führen kann."
    },
    {
      "chunk_idx": 43,
      "id": "dc094e74-9f86-4a1e-ba5b-2fb56b333cb5",
      "title": "Recital 44",
      "relevantChunksIds": [
        "85555ab3-0021-4e99-96f7-b8d2182b43f7"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(44) There are serious concerns about the scientific basis of AI systems aiming to identify or infer emotions, particularly as expression of emotions vary considerably across cultures and situations, and even within a single individual. Among the key shortcomings of such systems are the limited reliability, the lack of specificity and the limited generalisability. Therefore, AI systems identifying or inferring emotions or intentions of natural persons on the basis of their biometric data may lead to discriminatory outcomes and can be intrusive to the rights and freedoms of the concerned persons. Considering the imbalance of power in the context of work or education, combined with the intrusive nature of these systems, such systems could lead to detrimental or unfavourable treatment of certain natural persons or whole groups thereof. Therefore, the placing on the market, the putting into service, or the use of AI systems intended to be used to detect the emotional state of individuals in situations related to the workplace and education should be prohibited. That prohibition should not cover AI systems placed on the market strictly for medical or safety reasons, such as systems intended for therapeutical use.",
      "original_content": "(44) Im Hinblick auf die wissenschaftliche Grundlage von KI-Systemen, die darauf abzielen, Emotionen zu erkennen oder abzuleiten, bestehen ernsthafte Bedenken, insbesondere da sich Gefühlsausdrücke je nach Kultur oder Situation und selbst bei ein und derselben Person erheblich unterscheiden. Zu den größten Schwachstellen solcher Systeme gehört, dass sie beschränkt zuverlässig, nicht eindeutig und nur begrenzt verallgemeinerbar sind. Daher können KI-Systeme, die Emotionen oder Absichten natürlicher Personen auf der Grundlage ihrer biometrischen Daten erkennen oder ableiten, diskriminierende Ergebnisse hervorbringen und in die Rechte und Freiheiten der betroffenen Personen eingreifen. Angesichts des Machtungleichgewichts in den Bereichen Arbeit und Bildung in Verbindung mit dem intrusiven Charakter dieser Systeme können diese zu einer Schlechterstellung oder Benachteiligung bestimmter natürlicher Personen oder ganzer Gruppen führen. Daher sollte das Inverkehrbringen, die Inbetriebnahme oder die Verwendung von KI-Systemen, die den emotionalen Zustand von Einzelpersonen in Situationen, ableiten sollen, die mit dem Arbeitsplatz oder dem Bildungsbereich in Zusammenhang stehen, verboten werden. Dieses Verbot sollte nicht für KI-Systeme gelten, die ausschließlich aus medizinischen oder sicherheitstechnischen Gründen in Verkehr gebracht werden, wie z. B. Systeme, die für therapeutische Zwecke bestimmt sind."
    },
    {
      "chunk_idx": 44,
      "id": "27c3ba49-be38-4d63-856a-35c3e11faa5f",
      "title": "Recital 45",
      "relevantChunksIds": [
        "239278ca-b87b-4431-b7c9-71fd1a54f9ff"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(45) Practices that are prohibited by Union law, including data protection law, non-discrimination law, consumer protection law, and competition law, should not be affected by this Regulation.",
      "original_content": "(45) Praktiken, die nach Unionsrecht, einschließlich Datenschutzrecht, Nichtdiskriminierungsrecht, Verbraucherschutzrecht und Wettbewerbsrecht, verboten sind, sollten von dieser Verordnung nicht betroffen sein."
    },
    {
      "chunk_idx": 45,
      "id": "070576ca-434c-4246-a1bf-ba25d2a45285",
      "title": "Recital 46",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(46) High-risk AI systems should only be placed on the Union market, put into service or used if they comply with certain mandatory requirements. Those requirements should ensure that high-risk AI systems available in the Union or whose output is otherwise used in the Union do not pose unacceptable risks to important Union public interests as recognised and protected by Union law. On the basis of the New Legislative Framework, as clarified in the Commission notice ‘The “Blue Guide” on the implementation of EU product rules 2022’ (20), the general rule is that more than one legal act of Union harmonisation legislation, such as Regulations (EU) 2017/745 (21) and (EU) 2017/746 (22) of the European Parliament and of the Council or Directive 2006/42/EC of the European Parliament and of the Council (23), may be applicable to one product, since the making available or putting into service can take place only when the product complies with all applicable Union harmonisation legislation. To ensure consistency and avoid unnecessary administrative burdens or costs, providers of a product that contains one or more high-risk AI systems, to which the requirements of this Regulation and of the Union harmonisation legislation listed in an annex to this Regulation apply, should have flexibility with regard to operational decisions on how to ensure compliance of a product that contains one or more AI systems with all applicable requirements of the Union harmonisation legislation in an optimal manner. AI systems identified as high-risk should be limited to those that have a significant harmful impact on the health, safety and fundamental rights of persons in the Union and such limitation should minimise any potential restriction to international trade.",
      "original_content": "(46) Hochrisiko-KI-Systeme sollten nur dann auf dem Unionsmarkt in Verkehr gebracht, in Betrieb genommen oder verwendet werden, wenn sie bestimmte verbindliche Anforderungen erfüllen. Mit diesen Anforderungen sollte sichergestellt werden, dass Hochrisiko-KI-Systeme, die in der Union verfügbar sind oder deren Ausgabe anderweitig in der Union verwendet wird, keine unannehmbaren Risiken für wichtige öffentliche Interessen der Union bergen, wie sie im Unionsrecht anerkannt und geschützt sind. Auf der Grundlage des neuen Rechtsrahmens, wie in der Bekanntmachung der Kommission „Leitfaden für die Umsetzung der Produktvorschriften der EU 2022 (Blue Guide)“ (Fußnote 20) dargelegt, gilt als allgemeine Regel, dass mehr als ein Rechtsakt der Harmonisierungsrechtsvorschriften der Union wie die Verordnungen (EU) 2017/745 (Fußnote 21) und (EU) 2017/746 (Fußnote 22) des Europäischen Parlaments und des Rates oder die Richtlinie 2006/42/EG des Europäischen Parlaments und des Rates (Fußnote 23) auf ein Produkt anwendbar sein können, da die Bereitstellung oder Inbetriebnahme nur erfolgen kann, wenn das Produkt allen geltenden Harmonisierungsrechtsvorschriften der Union entspricht. Um Kohärenz zu gewährleisten und unnötigen Verwaltungsaufwand oder Kosten zu vermeiden, sollten die Anbieter eines Produkts, das ein oder mehrere Hochrisiko-KI-Systeme enthält, für die die Anforderungen dieser Verordnung und der in einem Anhang dieser Verordnung aufgeführten Harmonisierungsrechtsvorschriften der Union gelten, in Bezug auf operative Entscheidungen darüber flexibel sein, wie die Konformität eines Produkts, das ein oder mehrere Hochrisiko-KI-Systeme enthält, bestmöglich mit allen geltenden Anforderungen der Harmonisierungsrechtsvorschriften der Union sichergestellt werden kann. Als hochriskant sollten nur solche KI-Systeme eingestuft werden, die erhebliche schädliche Auswirkungen auf die Gesundheit, die Sicherheit und die Grundrechte von Personen in der Union haben, wodurch eine mögliche Beschränkung des internationalen Handels so gering wie möglich bleiben sollte. Fußnote 20: ABl. C 247 vom 29.6.2022, S. 1., Fußnote 21: Verordnung (EU) 2017/745 des Europäischen Parlaments und des Rates vom 5. April 2017 über Medizinprodukte, zur Änderung der Richtlinie 2001/83/EG, der Verordnung (EG) Nr. 178/2002 und der Verordnung (EG) Nr. 1223/2009 und zur Aufhebung der Richtlinien 90/385/EWG und 93/42/EWG des Rates (ABl. L 117 vom 5.5.2017, S. 1)., Fußnote 22: Verordnung (EU) 2017/746 des Europäischen Parlaments und des Rates vom 5. April 2017 über In-vitro-Diagnostika und zur Aufhebung der Richtlinie 98/79/EG und des Beschlusses 2010/227/EU der Kommission (ABl. L 117 vom 5.5.2017, S. 176)., Fußnote 23: Richtlinie 2006/42/EG des Europäischen Parlaments und des Rates vom 17. Mai 2006 über Maschinen und zur Änderung der Richtlinie 95/16/EG (ABl. L 157 vom 9.6.2006, S. 24)."
    },
    {
      "chunk_idx": 46,
      "id": "cfa2dd1a-cc63-4053-9da5-b7fddaa3ae9c",
      "title": "Recital 47",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463",
        "49563d87-4fce-4a1e-adc5-839e980a9965"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(47) AI systems could have an adverse impact on the health and safety of persons, in particular when such systems operate as safety components of products. Consistent with the objectives of Union harmonisation legislation to facilitate the free movement of products in the internal market and to ensure that only safe and otherwise compliant products find their way into the market, it is important that the safety risks that may be generated by a product as a whole due to its digital components, including AI systems, are duly prevented and mitigated. For instance, increasingly autonomous robots, whether in the context of manufacturing or personal assistance and care should be able to safely operate and performs their functions in complex environments. Similarly, in the health sector where the stakes for life and health are particularly high, increasingly sophisticated diagnostics systems and systems supporting human decisions should be reliable and accurate.",
      "original_content": "(47) KI-Systeme könnten nachteilige Auswirkungen auf die Gesundheit und Sicherheit von Personen haben, insbesondere wenn solche Systeme als Sicherheitsbauteile von Produkten zum Einsatz kommen. Im Einklang mit den Zielen der Harmonisierungsrechtsvorschriften der Union, die den freien Verkehr von Produkten im Binnenmarkt erleichtern und gewährleisten sollen, dass nur sichere und anderweitig konforme Produkte auf den Markt gelangen, ist es wichtig, dass die Sicherheitsrisiken, die ein Produkt als Ganzes aufgrund seiner digitalen Komponenten, einschließlich KI-Systeme, mit sich bringen kann, angemessen vermieden und gemindert werden. So sollten beispielsweise zunehmend autonome Roboter — sei es in der Fertigung oder in der persönlichen Assistenz und Pflege — in der Lage sein, sicher zu arbeiten und ihre Funktionen in komplexen Umgebungen zu erfüllen. Desgleichen sollten die immer ausgefeilteren Diagnosesysteme und Systeme zur Unterstützung menschlicher Entscheidungen im Gesundheitssektor, in dem die Risiken für Leib und Leben besonders hoch sind, zuverlässig und genau sein."
    },
    {
      "chunk_idx": 47,
      "id": "d027c4c8-3e9c-44bf-8ac9-72c2959acd00",
      "title": "Recital 48",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(48) The extent of the adverse impact caused by the AI system on the fundamental rights protected by the Charter is of particular relevance when classifying an AI system as high risk. Those rights include the right to human dignity, respect for private and family life, protection of personal data, freedom of expression and information, freedom of assembly and of association, the right to non-discrimination, the right to education, consumer protection, workers’ rights, the rights of persons with disabilities, gender equality, intellectual property rights, the right to an effective remedy and to a fair trial, the right of defence and the presumption of innocence, and the right to good administration. In addition to those rights, it is important to highlight the fact that children have specific rights as enshrined in Article 24 of the Charter and in the United Nations Convention on the Rights of the Child, further developed in the UNCRC General Comment No 25 as regards the digital environment, both of which require consideration of the children’s vulnerabilities and provision of such protection and care as necessary for their well-being. The fundamental right to a high level of environmental protection enshrined in the Charter and implemented in Union policies should also be considered when assessing the severity of the harm that an AI system can cause, including in relation to the health and safety of persons.",
      "original_content": "(48) Das Ausmaß der nachteiligen Auswirkungen des KI-Systems auf die durch die Charta geschützten Grundrechte ist bei der Einstufung eines KI-Systems als hochriskant von besonderer Bedeutung. Zu diesen Rechten gehören die Würde des Menschen, die Achtung des Privat- und Familienlebens, der Schutz personenbezogener Daten, die Freiheit der Meinungsäußerung und die Informationsfreiheit, die Versammlungs- und Vereinigungsfreiheit, das Recht auf Nichtdiskriminierung, das Recht auf Bildung, der Verbraucherschutz, die Arbeitnehmerrechte, die Rechte von Menschen mit Behinderungen, die Gleichstellung der Geschlechter, Rechte des geistigen Eigentums, das Recht auf einen wirksamen Rechtsbehelf und ein faires Gerichtsverfahren, das Verteidigungsrecht, die Unschuldsvermutung sowie das Recht auf eine gute Verwaltung. Es muss betont werden, dass Kinder — zusätzlich zu diesen Rechten — über spezifische Rechte verfügen, wie sie in Artikel 24 der Charta und im Übereinkommen der Vereinten Nationen über die Rechte des Kindes (UNCRC) — im Hinblick auf das digitale Umfeld weiter ausgeführt in der Allgemeinen Bemerkung Nr. 25 des UNCRC — verankert sind; in beiden wird die Berücksichtigung der Schutzbedürftigkeit der Kinder gefordert und ihr Anspruch auf den Schutz und die Fürsorge festgelegt, die für ihr Wohlergehen notwendig sind. Darüber hinaus sollte dem Grundrecht auf ein hohes Umweltschutzniveau, das in der Charta verankert ist und mit der Unionspolitik umgesetzt wird, bei der Bewertung der Schwere des Schadens, den ein KI-System unter anderem in Bezug auf die Gesundheit und Sicherheit von Personen verursachen kann, ebenfalls Rechnung getragen werden."
    },
    {
      "chunk_idx": 48,
      "id": "0b3d80d6-3316-4d5d-85a8-6477c568dfbe",
      "title": "Recital 49",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463",
        "aef2917e-0009-4636-8202-ef41bb6b33ff",
        "78ec7e62-bc3c-4239-afad-86ca34ac4746",
        "1b724be9-cb85-4ad6-97f8-2e4f22c482d3",
        "0dc12b68-7c9a-435e-91e6-cede7d6adcb9",
        "10ba704f-b5e6-4a7e-a2c8-7418525d1a4a",
        "b4d74881-6aab-4351-b7de-3b591441dd6f",
        "261f7b87-d60b-4708-b420-d2552cf3e5ca",
        "025462b7-9edf-4f37-b3a9-23f3c037f5db",
        "d788b690-5390-4f75-b1cb-08c2784a0b6f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(49) As regards high-risk AI systems that are safety components of products or systems, or which are themselves products or systems falling within the scope of Regulation (EC) No 300/2008 of the European Parliament and of the Council (24), Regulation (EU) No 167/2013 of the European Parliament and of the Council (25), Regulation (EU) No 168/2013 of the European Parliament and of the Council (26), Directive 2014/90/EU of the European Parliament and of the Council (27), Directive (EU) 2016/797 of the European Parliament and of the Council (28), Regulation (EU) 2018/858 of the European Parliament and of the Council (29), Regulation (EU) 2018/1139 of the European Parliament and of the Council (30), and Regulation (EU) 2019/2144 of the European Parliament and of the Council (31), it is appropriate to amend those acts to ensure that the Commission takes into account, on the basis of the technical and regulatory specificities of each sector, and without interfering with existing governance, conformity assessment and enforcement mechanisms and authorities established therein, the mandatory requirements for high-risk AI systems laid down in this Regulation when adopting any relevant delegated or implementing acts on the basis of those acts.",
      "original_content": "(49) In Bezug auf Hochrisiko-KI-Systeme, die Sicherheitsbauteile von Produkten oder Systemen oder selbst Produkte oder Systeme sind, die in den Anwendungsbereich der Verordnung (EG) Nr. 300/2008 des Europäischen Parlaments und des Rates (Fußnote 24), der Verordnung (EU) Nr. 167/2013 des Europäischen Parlaments und des Rates (Fußnote 25), der Verordnung (EU) Nr. 168/2013 des Europäischen Parlaments und des Rates (Fußnote 26), der Richtlinie 2014/90/EU des Europäischen Parlaments und des Rates (Fußnote 27), der Richtlinie (EU) 2016/797 des Europäischen Parlaments und des Rates (Fußnote 28), der Verordnung (EU) 2018/858 des Europäischen Parlaments und des Rates (Fußnote 29), der Verordnung (EU) 2018/1139 des Europäischen Parlaments und des Rates (Fußnote 30) und der Verordnung (EU) 2019/2144 des Europäischen Parlaments und des Rates (Fußnote 31) fallen, ist es angezeigt, diese Rechtsakte zu ändern, damit die Kommission — aufbauend auf den technischen und regulatorischen Besonderheiten des jeweiligen Sektors und ohne Beeinträchtigung bestehender Governance-, Konformitätsbewertungs- und Durchsetzungsmechanismen sowie der darin eingerichteten Behörden — beim Erlass von etwaigen delegierten Rechtsakten oder Durchführungsrechtsakten auf der Grundlage der genannten Rechtsakte die in der vorliegenden Verordnung festgelegten verbindlichen Anforderungen an Hochrisiko-KI-Systeme berücksichtigt. Fußnote 24: Verordnung (EG) Nr. 300/2008 des Europäischen Parlaments und des Rates vom 11. März 2008 über gemeinsame Vorschriften für die Sicherheit in der Zivilluftfahrt und zur Aufhebung der Verordnung (EG) Nr. 2320/2002 (ABl. L 97 vom 9.4.2008, S. 72)., Fußnote 25: Verordnung (EU) Nr. 167/2013 des Europäischen Parlaments und des Rates vom 5. Februar 2013 über die Genehmigung und Marktüberwachung von land- und forstwirtschaftlichen Fahrzeugen (ABl. L 60 vom 2.3.2013, S. 1)., Fußnote 26: Verordnung (EU) Nr. 168/2013 des Europäischen Parlaments und des Rates vom 15. Januar 2013 über die Genehmigung und Marktüberwachung von zwei- oder dreirädrigen und vierrädrigen Fahrzeugen (ABl. L 60 vom 2.3.2013, S. 52)., Fußnote 27: Richtlinie 2014/90/EU des Europäischen Parlaments und des Rates vom 23. Juli 2014 über Schiffsausrüstung und zur Aufhebung der Richtlinie 96/98/EG des Rates (ABl. L 257 vom 28.8.2014, S. 146)., Fußnote 28: Richtlinie (EU) 2016/797 des Europäischen Parlaments und des Rates vom 11. Mai 2016 über die Interoperabilität des Eisenbahnsystems in der Europäischen Union (ABl. L 138 vom 26.5.2016, S. 44)., Fußnote 29: Verordnung (EU) 2018/858 des Europäischen Parlaments und des Rates vom 30. Mai 2018 über die Genehmigung und die Marktüberwachung von Kraftfahrzeugen und Kraftfahrzeuganhängern sowie von Systemen, Bauteilen und selbstständigen technischen Einheiten für diese Fahrzeuge, zur Änderung der Verordnungen (EG) Nr. 715/2007 und (EG) Nr. 595/2009 und zur Aufhebung der Richtlinie 2007/46/EG (ABl. L 151 vom 14.6.2018, S. 1)., Fußnote 30: Verordnung (EU) 2018/1139 des Europäischen Parlaments und des Rates vom 4. Juli 2018 zur Festlegung gemeinsamer Vorschriften für die Zivilluftfahrt und zur Errichtung einer Agentur der Europäischen Union für Flugsicherheit sowie zur Änderung der Verordnungen (EG) Nr. 2111/2005, (EG) Nr. 1008/2008, (EU) Nr. 996/2010, (EU) Nr. 376/2014 und der Richtlinien 2014/30/EU und 2014/53/EU des Europäischen Parlaments und des Rates, und zur Aufhebung der Verordnungen (EG) Nr. 552/2004 und (EG) Nr. 216/2008 des Europäischen Parlaments und des Rates und der Verordnung (EWG) Nr. 3922/91 des Rates (ABl. L 212 vom 22.8.2018, S. 1)., Fußnote 31: Verordnung (EU) 2019/2144 des Europäischen Parlaments und des Rates vom 27. November 2019 über die Typgenehmigung von Kraftfahrzeugen und Kraftfahrzeuganhängern sowie von Systemen, Bauteilen und selbstständigen technischen Einheiten für diese Fahrzeuge im Hinblick auf ihre allgemeine Sicherheit und den Schutz der Fahrzeuginsassen und von ungeschützten Verkehrsteilnehmern, zur Änderung der Verordnung (EU) 2018/858 des Europäischen Parlaments und des Rates und zur Aufhebung der Verordnungen (EG) Nr. 78/2009, (EG) Nr. 79/2009 und (EG) Nr. 661/2009 des Europäischen Parlaments und des Rates sowie der Verordnungen (EG) Nr. 631/2009, (EU) Nr. 406/2010, (EU) Nr. 672/2010, (EU) Nr. 1003/2010, (EU) Nr. 1005/2010, (EU) Nr. 1008/2010, (EU) Nr. 1009/2010, (EU) Nr. 19/2011, (EU) Nr. 109/2011, (EU) Nr. 458/2011, (EU) Nr. 65/2012, (EU) Nr. 130/2012, (EU) Nr. 347/2012, (EU) Nr. 351/2012, (EU) Nr. 1230/2012 und (EU) 2015/166 der Kommission (ABl. L 325 vom 16.12.2019, S. 1)."
    },
    {
      "chunk_idx": 49,
      "id": "be8e58b9-7976-45e8-9b6e-ae5d17d5f7cb",
      "title": "Recital 50",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(50) As regards AI systems that are safety components of products, or which are themselves products, falling within the scope of certain Union harmonisation legislation listed in an annex to this Regulation, it is appropriate to classify them as high-risk under this Regulation if the product concerned undergoes the conformity assessment procedure with a third-party conformity assessment body pursuant to that relevant Union harmonisation legislation. In particular, such products are machinery, toys, lifts, equipment and protective systems intended for use in potentially explosive atmospheres, radio equipment, pressure equipment, recreational craft equipment, cableway installations, appliances burning gaseous fuels, medical devices, in vitro diagnostic medical devices, automotive and aviation.",
      "original_content": "(50) In Bezug auf KI-Systeme, die Sicherheitsbauteile von Produkten oder selbst Produkte sind, die in den Anwendungsbereich bestimmter, im Anhang dieser Verordnung aufgeführter Harmonisierungsrechtsvorschriften der Union fallen, ist es angezeigt, sie im Rahmen dieser Verordnung als hochriskant einzustufen, wenn das betreffende Produkt gemäß den einschlägigen Harmonisierungsrechtsvorschriften der Union dem Konformitätsbewertungsverfahren durch eine als Dritte auftretende Konformitätsbewertungsstelle unterzogen wird. Dabei handelt es sich insbesondere um Produkte wie Maschinen, Spielzeuge, Aufzüge, Geräte und Schutzsysteme zur bestimmungsgemäßen Verwendung in explosionsgefährdeten Bereichen, Funkanlagen, Druckgeräte, Sportbootausrüstung, Seilbahnen, Geräte zur Verbrennung gasförmiger Brennstoffe, Medizinprodukte, In-vitro-Diagnostika, Automobile und Flugzeuge."
    },
    {
      "chunk_idx": 50,
      "id": "44ee4963-2543-4ba3-8550-cf2a21c2224c",
      "title": "Recital 51",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(51) The classification of an AI system as high-risk pursuant to this Regulation should not necessarily mean that the product whose safety component is the AI system, or the AI system itself as a product, is considered to be high-risk under the criteria established in the relevant Union harmonisation legislation that applies to the product. This is, in particular, the case for Regulations (EU) 2017/745 and (EU) 2017/746, where a third-party conformity assessment is provided for medium-risk and high-risk products.",
      "original_content": "(51) Die Einstufung eines KI-Systems als hochriskant gemäß dieser Verordnung sollte nicht zwangsläufig bedeuten, dass von dem Produkt, dessen Sicherheitsbauteil das KI-System ist, oder von dem KI-System als eigenständigem Produkt nach den Kriterien der einschlägigen Harmonisierungsrechtsvorschriften der Union für das betreffende Produkt ein hohes Risiko ausgeht. Dies gilt insbesondere für die Verordnungen (EU) 2017/745 und (EU) 2017/746, wo für Produkte mit mittlerem und hohem Risiko eine Konformitätsbewertung durch Dritte vorgesehen ist."
    },
    {
      "chunk_idx": 51,
      "id": "8a27e104-58f9-4f9d-8bbb-aff055c16634",
      "title": "Recital 52",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463",
        "aef2917e-0009-4636-8202-ef41bb6b33ff"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(52) As regards stand-alone AI systems, namely high-risk AI systems other than those that are safety components of products, or that are themselves products, it is appropriate to classify them as high-risk if, in light of their intended purpose, they pose a high risk of harm to the health and safety or the fundamental rights of persons, taking into account both the severity of the possible harm and its probability of occurrence and they are used in a number of specifically pre-defined areas specified in this Regulation. The identification of those systems is based on the same methodology and criteria envisaged also for any future amendments of the list of high-risk AI systems that the Commission should be empowered to adopt, via delegated acts, to take into account the rapid pace of technological development, as well as the potential changes in the use of AI systems.",
      "original_content": "(52) Bei eigenständigen KI-Systemen, d. h. Hochrisiko-KI-Systemen, bei denen es sich um andere Systeme als Sicherheitsbauteile von Produkten handelt oder die selbst Produkte sind, ist es angezeigt, sie als hochriskant einzustufen, wenn sie aufgrund ihrer Zweckbestimmung ein hohes Risiko bergen, die Gesundheit und Sicherheit oder die Grundrechte von Personen zu schädigen, wobei sowohl die Schwere des möglichen Schadens als auch die Wahrscheinlichkeit seines Auftretens zu berücksichtigen sind, und sofern sie in einer Reihe von Bereichen verwendet werden, die in dieser Verordnung ausdrücklich festgelegt sind. Die Bestimmung dieser Systeme erfolgt nach derselben Methodik und denselben Kriterien, die auch für künftige Änderungen der Liste der Hochrisiko-KI-Systeme vorgesehen sind, zu deren Annahme die Kommission im Wege delegierter Rechtsakte ermächtigt werden sollte, um dem rasanten Tempo der technologischen Entwicklung sowie den möglichen Änderungen bei der Verwendung von KI-Systemen Rechnung zu tragen."
    },
    {
      "chunk_idx": 52,
      "id": "c810069a-3ec4-433a-8bff-0faacb8796dd",
      "title": "Recital 53",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(53) It is also important to clarify that there may be specific cases in which AI systems referred to in pre-defined areas specified in this Regulation do not lead to a significant risk of harm to the legal interests protected under those areas because they do not materially influence the decision-making or do not harm those interests substantially. For the purposes of this Regulation, an AI system that does not materially influence the outcome of decision-making should be understood to be an AI system that does not have an impact on the substance, and thereby the outcome, of decision-making, whether human or automated. An AI system that does not materially influence the outcome of decision-making could include situations in which one or more of the following conditions are fulfilled. The first such condition should be that the AI system is intended to perform a narrow procedural task, such as an AI system that transforms unstructured data into structured data, an AI system that classifies incoming documents into categories or an AI system that is used to detect duplicates among a large number of applications. Those tasks are of such narrow and limited nature that they pose only limited risks which are not increased through the use of an AI system in a context that is listed as a high-risk use in an annex to this Regulation. The second condition should be that the task performed by the AI system is intended to improve the result of a previously completed human activity that may be relevant for the purposes of the high-risk uses listed in an annex to this Regulation. Considering those characteristics, the AI system provides only an additional layer to a human activity with consequently lowered risk. That condition would, for example, apply to AI systems that are intended to improve the language used in previously drafted documents, for example in relation to professional tone, academic style of language or by aligning text to a certain brand messaging. The third condition should be that the AI system is intended to detect decision-making patterns or deviations from prior decision-making patterns. The risk would be lowered because the use of the AI system follows a previously completed human assessment which it is not meant to replace or influence, without proper human review. Such AI systems include for instance those that, given a certain grading pattern of a teacher, can be used to check ex post whether the teacher may have deviated from the grading pattern so as to flag potential inconsistencies or anomalies. The fourth condition should be that the AI system is intended to perform a task that is only preparatory to an assessment relevant for the purposes of the AI systems listed in an annex to this Regulation, thus making the possible impact of the output of the system very low in terms of representing a risk for the assessment to follow. That condition covers, inter alia, smart solutions for file handling, which include various functions from indexing, searching, text and speech processing or linking data to other data sources, or AI systems used for translation of initial documents. In any case, AI systems used in high-risk use-cases listed in an annex to this Regulation should be considered to pose significant risks of harm to the health, safety or fundamental rights if the AI system implies profiling within the meaning of Article 4, point (4) of Regulation (EU) 2016/679 or Article 3, point (4) of Directive (EU) 2016/680 or Article 3, point (5) of Regulation (EU) 2018/1725. To ensure traceability and transparency, a provider who considers that an AI system is not high-risk on the basis of the conditions referred to above should draw up documentation of the assessment before that system is placed on the market or put into service and should provide that documentation to national competent authorities upon request. Such a provider should be obliged to register the AI system in the EU database established under this Regulation. With a view to providing further guidance for the practical implementation of the conditions under which the AI systems listed in an annex to this Regulation are, on an exceptional basis, non-high-risk, the Commission should, after consulting the Board, provide guidelines specifying that practical implementation, completed by a comprehensive list of practical examples of use cases of AI systems that are high-risk and use cases that are not.",
      "original_content": "(53) Ferner muss klargestellt werden, dass es bestimmte Fälle geben kann, in denen KI-Systeme für vordefinierte in dieser Verordnung festgelegte Bereiche nicht zu einem bedeutenden Risiko der Beeinträchtigung der in diesen Bereichen geschützten rechtlichen Interessen führen, da sie die Entscheidungsfindung nicht wesentlich beeinflussen oder diesen Interessen nicht erheblich schaden. Für die Zwecke dieser Verordnung sollte ein KI-System, das das Ergebnis der Entscheidungsfindung nicht wesentlich beeinflusst, als ein KI-System verstanden werden, das keine Auswirkungen auf den Inhalt und damit das Ergebnis der Entscheidungsfindung hat, unabhängig davon, ob es sich um menschliche oder automatisierte Entscheidungen handelt. Ein KI-System, das das Ergebnis der Entscheidungsfindung nicht wesentlich beeinflusst, könnte Situationen einschließen, in denen eine oder mehrere der folgenden Bedingungen erfüllt sind. Die erste dieser Bedingungen ist, dass das KI-System dazu bestimmt ist, in einem Verfahren eine eng gefasste Aufgabe zu erfüllen, wie etwa ein KI-System, das unstrukturierte Daten in strukturierte Daten umwandelt, ein KI-System, das eingehende Dokumente in Kategorien einordnet, oder ein KI-System, das zur Erkennung von Duplikaten unter einer großen Zahl von Anwendungen eingesetzt wird. Diese Aufgaben sind so eng gefasst und begrenzt, dass sie nur beschränkte Risiken darstellen, die sich durch die Verwendung eines KI-Systems in einem Kontext, der in einem Anhang dieser Verordnung als Verwendung mit hohem Risiko aufgeführt ist, nicht erhöhen. Die zweite Bedingung sollte darin bestehen, dass die von einem KI-System ausgeführte Aufgabe das Ergebnis einer zuvor abgeschlossenen menschlichen Tätigkeit verbessert, die für die Zwecke der in einem Anhang dieser Verordnung aufgeführten Verwendungen mit hohem Risiko relevant sein kann. Unter Berücksichtigung dieser Merkmale wird eine menschliche Tätigkeit durch das KI-System lediglich durch eine zusätzliche Ebene ergänzt und stellt daher ein geringeres Risiko dar. Diese Bedingung würde beispielsweise für KI-Systeme gelten, deren Ziel es ist, die in zuvor verfassten Dokumenten verwendete Sprache zu verbessern, etwa den professionellen Ton, den wissenschaftlichen Sprachstil oder um den Text an einen bestimmten mit einer Marke verbundenen Stil anzupassen. Dritte Bedingung sollte sein, dass mit dem KI-System Entscheidungsmuster oder Abweichungen von früheren Entscheidungsmustern erkannt werden sollen. Das Risiko wäre geringer, da die Verwendung des KI-Systems einer zuvor abgeschlossenen menschlichen Bewertung folgt, die das KI-System ohne angemessene menschliche Überprüfung nicht ersetzen oder beeinflussen soll. Zu solchen KI-Systemen gehören beispielsweise solche, die in Bezug auf ein bestimmtes Benotungsmuster eines Lehrers dazu verwendet werden können, nachträglich zu prüfen, ob der Lehrer möglicherweise von dem Benotungsmuster abgewichen ist, um so auf mögliche Unstimmigkeiten oder Unregelmäßigkeiten aufmerksam zu machen. Die vierte Bedingung sollte darin bestehen, dass das KI-System dazu bestimmt ist, eine Aufgabe auszuführen, die eine Bewertung, die für die Zwecke der in einem Anhang dieser Verordnung aufgeführten KI-Systeme relevant ist, lediglich vorbereitet, wodurch die mögliche Wirkung der Ausgaben des Systems im Hinblick auf das Risiko für die folgende Bewertung sehr gering bleibt. Diese Bedingung umfasst u. a. intelligente Lösungen für die Bearbeitung von Dossiers, wozu verschiedene Funktionen wie Indexierung, Suche, Text- und Sprachverarbeitung oder Verknüpfung von Daten mit anderen Datenquellen gehören, oder KI-Systeme, die für die Übersetzung von Erstdokumenten verwendet werden. In jedem Fall sollten KI-Systeme, die in den in einem Anhang dieser Verordnung aufgeführten Anwendungsfälle mit hohem Risiko verwendet werden, als erhebliche Risiken für die Gesundheit, Sicherheit oder Grundrechte gelten, wenn das KI-System Profiling im Sinne von Artikel 4 Nummer 4 der Verordnung (EU) 2016/679 oder Artikel 3 Nummer 4 der Richtlinie (EU) 2016/680 oder Artikel 3 Nummer 5 der Verordnung (EU) 2018/1725 beinhaltet. Um die Nachvollziehbarkeit und Transparenz zu gewährleisten, sollte ein Anbieter, der der Auffassung ist, dass ein KI-System auf der Grundlage der oben genannten Bedingungen kein hohes Risiko darstellt, eine Dokumentation der Bewertung erstellen, bevor dieses System in Verkehr gebracht oder in Betrieb genommen wird, und die genannte Dokumentation den zuständigen nationalen Behörden auf Anfrage zur Verfügung stellen. Ein solcher Anbieter sollte verpflichtet sein, das KI-System in der gemäß dieser Verordnung eingerichteten EU-Datenbank zu registrieren. Um eine weitere Anleitung für die praktische Umsetzung der Bedingungen zu geben, unter denen die in einem Anhang dieser Verordnung aufgeführten Systeme ausnahmsweise kein hohes Risiko darstellen, sollte die Kommission nach Konsultation des KI-Gremiums Leitlinien bereitstellen, in denen diese praktische Umsetzung detailliert aufgeführt ist und durch eine umfassende Liste praktischer Beispiele für Anwendungsfälle von KI-Systemen, die ein hohes Risiko und Anwendungsfälle, die kein hohes Risiko darstellen, ergänzt wird."
    },
    {
      "chunk_idx": 53,
      "id": "09d31d87-cf07-441e-a4c2-84ae865db022",
      "title": "Recital 54",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(54) As biometric data constitutes a special category of personal data, it is appropriate to classify as high-risk several critical-use cases of biometric systems, insofar as their use is permitted under relevant Union and national law. Technical inaccuracies of AI systems intended for the remote biometric identification of natural persons can lead to biased results and entail discriminatory effects. The risk of such biased results and discriminatory effects is particularly relevant with regard to age, ethnicity, race, sex or disabilities. Remote biometric identification systems should therefore be classified as high-risk in view of the risks that they pose. Such a classification excludes AI systems intended to be used for biometric verification, including authentication, the sole purpose of which is to confirm that a specific natural person is who that person claims to be and to confirm the identity of a natural person for the sole purpose of having access to a service, unlocking a device or having secure access to premises. In addition, AI systems intended to be used for biometric categorisation according to sensitive attributes or characteristics protected under Article 9(1) of Regulation (EU) 2016/679 on the basis of biometric data, in so far as these are not prohibited under this Regulation, and emotion recognition systems that are not prohibited under this Regulation, should be classified as high-risk. Biometric systems which are intended to be used solely for the purpose of enabling cybersecurity and personal data protection measures should not be considered to be high-risk AI systems.",
      "original_content": "(54) Da biometrische Daten eine besondere Kategorie personenbezogener Daten darstellen, sollten einige kritische Anwendungsfälle biometrischer Systeme als hochriskant eingestuft werden, sofern ihre Verwendung nach den einschlägigen Rechtsvorschriften der Union und den nationalen Rechtsvorschriften zulässig ist. Technische Ungenauigkeiten von KI-Systemen, die für die biometrische Fernidentifizierung natürlicher Personen bestimmt sind, können zu verzerrten Ergebnissen führen und eine diskriminierende Wirkung haben. Das Risiko solcher verzerrter Ergebnisse und solcher diskriminierender Wirkungen ist von besonderer Bedeutung, wenn es um das Alter, die ethnische Herkunft, die Rasse, das Geschlecht oder Behinderungen geht. Biometrische Fernidentifizierungssysteme sollten daher angesichts der von ihnen ausgehenden Risiken als hochriskant eingestuft werden. Diese Einstufung umfasst keine KI-Systeme, die bestimmungsgemäß für die biometrische Verifizierung, wozu die Authentifizierung gehört, verwendet werden sollen, deren einziger Zweck darin besteht, zu bestätigen, dass eine bestimmte natürliche Person die Person ist, für die sie sich ausgibt, sowie zur Bestätigung der Identität einer natürlichen Person zu dem alleinigen Zweck Zugang zu einem Dienst zu erhalten, ein Gerät zu entriegeln oder sicheren Zugang zu Räumlichkeiten zu erhalten. Darüber hinaus sollten KI-Systeme, die bestimmungsgemäß für die biometrische Kategorisierung nach sensiblen Attributen oder Merkmalen, die gemäß Artikel 9 Absatz 1 der Verordnung (EU) 2016/679 auf der Grundlage biometrischer Daten geschützt sind, und sofern sie nicht nach der vorliegenden Verordnung verboten sind, sowie Emotionserkennungssysteme, die nach dieser Verordnung nicht verboten sind, als hochriskant eingestuft werden. Biometrische Systeme, die ausschließlich dazu bestimmt sind, um Maßnahmen zur Cybersicherheit und zum Schutz personenbezogener Daten durchführen zu können, sollten nicht als Hochrisiko-KI-Systeme gelten."
    },
    {
      "chunk_idx": 54,
      "id": "d89b42c0-fb0b-40c3-a99d-40aae0d9dea9",
      "title": "Recital 55",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(55) As regards the management and operation of critical infrastructure, it is appropriate to classify as high-risk the AI systems intended to be used as safety components in the management and operation of critical digital infrastructure as listed in point (8) of the Annex to Directive (EU) 2022/2557, road traffic and the supply of water, gas, heating and electricity, since their failure or malfunctioning may put at risk the life and health of persons at large scale and lead to appreciable disruptions in the ordinary conduct of social and economic activities. Safety components of critical infrastructure, including critical digital infrastructure, are systems used to directly protect the physical integrity of critical infrastructure or the health and safety of persons and property but which are not necessary in order for the system to function. The failure or malfunctioning of such components might directly lead to risks to the physical integrity of critical infrastructure and thus to risks to health and safety of persons and property. Components intended to be used solely for cybersecurity purposes should not qualify as safety components. Examples of safety components of such critical infrastructure may include systems for monitoring water pressure or fire alarm controlling systems in cloud computing centres.",
      "original_content": "(55) Was die Verwaltung und den Betrieb kritischer Infrastruktur anbelangt, so ist es angezeigt, KI-Systeme, die als Sicherheitsbauteile für die Verwaltung und den Betrieb kritischer digitaler Infrastruktur gemäß Nummer 8 des Anhangs der Richtlinie (EU) 2022/2557, des Straßenverkehrs sowie für die Wasser-, Gas-, Wärme- und Stromversorgung verwendet werden sollen, als hochriskant einzustufen, da ihr Ausfall oder ihre Störung in großem Umfang ein Risiko für das Leben und die Gesundheit von Personen darstellen und zu erheblichen Störungen bei der normalen Durchführung sozialer und wirtschaftlicher Tätigkeiten führen kann. Sicherheitsbauteile kritischer Infrastruktur, einschließlich kritischer digitaler Infrastruktur, sind Systeme, die verwendet werden, um die physische Integrität kritischer Infrastruktur oder die Gesundheit und Sicherheit von Personen und Eigentum zu schützen, die aber nicht notwendig sind, damit das System funktioniert. Ausfälle oder Störungen solcher Komponenten können direkt zu Risiken für die physische Integrität kritischer Infrastruktur und somit zu Risiken für die Gesundheit und Sicherheit von Personen und Eigentum führen. Komponenten, die für die ausschließliche Verwendung zu Zwecken der Cybersicherheit vorgesehen sind, sollten nicht als Sicherheitsbauteile gelten. Zu Beispielen von Sicherheitsbauteilen solcher kritischen Infrastruktur zählen etwa Systeme für die Überwachung des Wasserdrucks oder Feuermelder-Kontrollsysteme in Cloud-Computing-Zentren."
    },
    {
      "chunk_idx": 55,
      "id": "332aaf1e-5867-464e-b22e-94979182c753",
      "title": "Recital 56",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(56) The deployment of AI systems in education is important to promote high-quality digital education and training and to allow all learners and teachers to acquire and share the necessary digital skills and competences, including media literacy, and critical thinking, to take an active part in the economy, society, and in democratic processes. However, AI systems used in education or vocational training, in particular for determining access or admission, for assigning persons to educational and vocational training institutions or programmes at all levels, for evaluating learning outcomes of persons, for assessing the appropriate level of education for an individual and materially influencing the level of education and training that individuals will receive or will be able to access or for monitoring and detecting prohibited behaviour of students during tests should be classified as high-risk AI systems, since they may determine the educational and professional course of a person’s life and therefore may affect that person’s ability to secure a livelihood. When improperly designed and used, such systems may be particularly intrusive and may violate the right to education and training as well as the right not to be discriminated against and perpetuate historical patterns of discrimination, for example against women, certain age groups, persons with disabilities, or persons of certain racial or ethnic origins or sexual orientation.",
      "original_content": "(56) Der Einsatz von KI-Systemen in der Bildung ist wichtig, um eine hochwertige digitale allgemeine und berufliche Bildung zu fördern und es allen Lernenden und Lehrkräften zu ermöglichen, die erforderlichen digitalen Fähigkeiten und Kompetenzen, einschließlich Medienkompetenz und kritischem Denken, zu erwerben und auszutauschen, damit sie sich aktiv an Wirtschaft, Gesellschaft und demokratischen Prozessen beteiligen können. Allerdings sollten KI-Systeme, die in der allgemeinen oder beruflichen Bildung eingesetzt werden, um insbesondere den Zugang oder die Zulassung zum Zweck der Zuordnung von Personen zu Bildungs- und Berufsbildungseinrichtungen oder -programmen auf allen Ebenen zu bestimmen, die Lernergebnisse von Personen zu beurteilen, das angemessene Bildungsniveau einer Person zu bewerten und das Niveau der Bildung und Ausbildung, das die Person erhält oder zu dem sie Zugang erhält, wesentlich zu beeinflussen und verbotenes Verhalten von Schülern während Prüfungen zu überwachen und zu erkennen als hochriskante KI-Systeme eingestuft werden, da sie über den Verlauf der Bildung und des Berufslebens einer Person entscheiden und daher ihre Fähigkeit beeinträchtigen können, ihren Lebensunterhalt zu sichern. Bei unsachgemäßer Konzeption und Verwendung können solche Systeme sehr intrusiv sein und das Recht auf allgemeine und berufliche Bildung sowie das Recht auf Nichtdiskriminierung verletzen und historische Diskriminierungsmuster fortschreiben, beispielsweise gegenüber Frauen, bestimmten Altersgruppen und Menschen mit Behinderungen oder Personen mit einer bestimmten rassischen oder ethnischen Herkunft oder sexuellen Ausrichtung."
    },
    {
      "chunk_idx": 56,
      "id": "396c4d54-1dac-4555-bd49-3204cc61a7f3",
      "title": "Recital 57",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(57) AI systems used in employment, workers management and access to self-employment, in particular for the recruitment and selection of persons, for making decisions affecting terms of the work-related relationship, promotion and termination of work-related contractual relationships, for allocating tasks on the basis of individual behaviour, personal traits or characteristics and for monitoring or evaluation of persons in work-related contractual relationships, should also be classified as high-risk, since those systems may have an appreciable impact on future career prospects, livelihoods of those persons and workers’ rights. Relevant work-related contractual relationships should, in a meaningful manner, involve employees and persons providing services through platforms as referred to in the Commission Work Programme 2021. Throughout the recruitment process and in the evaluation, promotion, or retention of persons in work-related contractual relationships, such systems may perpetuate historical patterns of discrimination, for example against women, certain age groups, persons with disabilities, or persons of certain racial or ethnic origins or sexual orientation. AI systems used to monitor the performance and behaviour of such persons may also undermine their fundamental rights to data protection and privacy.",
      "original_content": "(57) KI-Systeme, die in den Bereichen Beschäftigung, Personalmanagement und Zugang zur Selbstständigkeit eingesetzt werden, insbesondere für die Einstellung und Auswahl von Personen, für Entscheidungen über die Bedingungen des Arbeitsverhältnisses sowie die Beförderung und die Beendigung von Arbeitsvertragsverhältnissen, für die Zuweisung von Arbeitsaufgaben auf der Grundlage von individuellem Verhalten, persönlichen Eigenschaften oder Merkmalen sowie für die Überwachung oder Bewertung von Personen in Arbeitsvertragsverhältnissen sollten ebenfalls als hochriskant eingestuft werden, da diese Systeme die künftigen Karriereaussichten und die Lebensgrundlagen dieser Personen und die Arbeitnehmerrechte spürbar beeinflussen können. Einschlägige Arbeitsvertragsverhältnisse sollten in sinnvoller Weise Beschäftigte und Personen erfassen, die Dienstleistungen über Plattformen erbringen, auf die im Arbeitsprogramm der Kommission für 2021 Bezug genommen wird. Solche Systeme können während des gesamten Einstellungsverfahrens und bei der Bewertung, Beförderung oder Weiterbeschäftigung von Personen in Arbeitsvertragsverhältnissen historische Diskriminierungsmuster fortschreiben, beispielsweise gegenüber Frauen, bestimmten Altersgruppen und Menschen mit Behinderungen oder Personen mit einer bestimmten rassischen oder ethnischen Herkunft oder sexuellen Ausrichtung. KI-Systeme zur Überwachung der Leistung und des Verhaltens solcher Personen können auch deren Grundrechte auf Datenschutz und Privatsphäre untergraben."
    },
    {
      "chunk_idx": 57,
      "id": "8321c080-a535-45b2-a8d8-2201f5fb36c4",
      "title": "Recital 58",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(58) Another area in which the use of AI systems deserves special consideration is the access to and enjoyment of certain essential private and public services and benefits necessary for people to fully participate in society or to improve one’s standard of living. In particular, natural persons applying for or receiving essential public assistance benefits and services from public authorities namely healthcare services, social security benefits, social services providing protection in cases such as maternity, illness, industrial accidents, dependency or old age and loss of employment and social and housing assistance, are typically dependent on those benefits and services and in a vulnerable position in relation to the responsible authorities. If AI systems are used for determining whether such benefits and services should be granted, denied, reduced, revoked or reclaimed by authorities, including whether beneficiaries are legitimately entitled to such benefits or services, those systems may have a significant impact on persons’ livelihood and may infringe their fundamental rights, such as the right to social protection, non-discrimination, human dignity or an effective remedy and should therefore be classified as high-risk. Nonetheless, this Regulation should not hamper the development and use of innovative approaches in the public administration, which would stand to benefit from a wider use of compliant and safe AI systems, provided that those systems do not entail a high risk to legal and natural persons. In addition, AI systems used to evaluate the credit score or creditworthiness of natural persons should be classified as high-risk AI systems, since they determine those persons’ access to financial resources or essential services such as housing, electricity, and telecommunication services. AI systems used for those purposes may lead to discrimination between persons or groups and may perpetuate historical patterns of discrimination, such as that based on racial or ethnic origins, gender, disabilities, age or sexual orientation, or may create new forms of discriminatory impacts. However, AI systems provided for by Union law for the purpose of detecting fraud in the offering of financial services and for prudential purposes to calculate credit institutions’ and insurance undertakings’ capital requirements should not be considered to be high-risk under this Regulation. Moreover, AI systems intended to be used for risk assessment and pricing in relation to natural persons for health and life insurance can also have a significant impact on persons’ livelihood and if not duly designed, developed and used, can infringe their fundamental rights and can lead to serious consequences for people’s life and health, including financial exclusion and discrimination. Finally, AI systems used to evaluate and classify emergency calls by natural persons or to dispatch or establish priority in the dispatching of emergency first response services, including by police, firefighters and medical aid, as well as of emergency healthcare patient triage systems, should also be classified as high-risk since they make decisions in very critical situations for the life and health of persons and their property.",
      "original_content": "(58) Ein weiterer Bereich, in dem der Einsatz von KI-Systemen besondere Aufmerksamkeit verdient, ist der Zugang zu und die Nutzung von bestimmten grundlegenden privaten und öffentlichen Diensten und Leistungen, die erforderlich sind, damit Menschen uneingeschränkt an der Gesellschaft teilhaben oder ihren Lebensstandard verbessern können. Insbesondere natürliche Personen, die grundlegende staatliche Unterstützungsleistungen und -dienste von Behörden beantragen oder erhalten, wie etwa Gesundheitsdienste, Leistungen der sozialen Sicherheit, soziale Dienste, die Schutz in Fällen wie Mutterschaft, Krankheit, Arbeitsunfall, Pflegebedürftigkeit oder Alter und Arbeitsplatzverlust sowie Sozialhilfe und Wohngeld bieten, sind in der Regel von diesen Leistungen und Diensten abhängig und befinden sich gegenüber den zuständigen Behörden in einer prekären Lage. Wenn KI-Systeme eingesetzt werden, um zu bestimmen, ob solche Leistungen und Dienste von den Behörden gewährt, verweigert, gekürzt, widerrufen oder zurückgefordert werden sollten, einschließlich der Frage, ob Begünstigte rechtmäßig Anspruch auf solche Leistungen oder Dienste haben, können diese Systeme erhebliche Auswirkungen auf die Lebensgrundlage von Personen haben und ihre Grundrechte wie etwa das Recht auf sozialen Schutz, Nichtdiskriminierung, Menschenwürde oder einen wirksamen Rechtsbehelf verletzen und sollten daher als hochriskant eingestuft werden. Dennoch sollte diese Verordnung die Entwicklung und Verwendung innovativer Ansätze in der öffentlichen Verwaltung nicht behindern, die von einer breiteren Verwendung konformer und sicherer KI-Systeme profitieren würde, sofern diese Systeme kein hohes Risiko für juristische und natürliche Personen bergen. Darüber hinaus sollten KI-Systeme, die zur Bewertung der Bonität oder Kreditwürdigkeit natürlicher Personen verwendet werden, als Hochrisiko-KI-Systeme eingestuft werden, da sie den Zugang dieser Personen zu Finanzmitteln oder wesentlichen Dienstleistungen wie etwa Wohnraum, Elektrizität und Telekommunikationsdienstleistungen bestimmen. KI-Systeme, die für diese Zwecke eingesetzt werden, können zur Diskriminierung von Personen oder Gruppen führen und historische Diskriminierungsmuster, wie etwa aufgrund der rassischen oder ethnischen Herkunft, des Geschlechts, einer Behinderung, des Alters oder der sexuellen Ausrichtung, fortschreiben oder neue Formen von Diskriminierung mit sich bringen. Allerdings sollten KI-Systeme, die nach Unionsrecht zur Aufdeckung von Betrug beim Angebot von Finanzdienstleistungen oder für Aufsichtszwecke zur Berechnung der Eigenkapitalanforderungen von Kreditinstituten und Versicherungsunternehmen vorgesehen sind, nicht als Hochrisiko-Systeme gemäß dieser Verordnung angesehen werden. Darüber hinaus können KI-Systeme, die für die Risikobewertung und Preisbildung in Bezug auf natürliche Personen im Fall von Kranken- und Lebensversicherungen eingesetzt werden, auch erhebliche Auswirkungen auf die Existenzgrundlage der Menschen haben und bei nicht ordnungsgemäßer Konzeption, Entwicklung und Verwendung schwerwiegende Konsequenzen für das Leben und die Gesundheit von Menschen nach sich ziehen, einschließlich finanzieller Ausgrenzung und Diskriminierung. Schließlich sollten KI-Systeme, die bei der Bewertung und Einstufung von Notrufen durch natürliche Personen oder der Entsendung oder der Priorisierung der Entsendung von Not- und Rettungsdiensten wie Polizei, Feuerwehr und medizinischer Nothilfe sowie für die Triage von Patienten bei der Notfallversorgung eingesetzt werden, ebenfalls als hochriskant eingestuft werden, da sie in für das Leben und die Gesundheit von Personen und für ihr Eigentum sehr kritischen Situationen Entscheidungen treffen."
    },
    {
      "chunk_idx": 58,
      "id": "d47b3160-2283-4955-bec9-6ae45acbef8b",
      "title": "Recital 59",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(59) Given their role and responsibility, actions by law enforcement authorities involving certain uses of AI systems are characterised by a significant degree of power imbalance and may lead to surveillance, arrest or deprivation of a natural person’s liberty as well as other adverse impacts on fundamental rights guaranteed in the Charter. In particular, if the AI system is not trained with high-quality data, does not meet adequate requirements in terms of its performance, its accuracy or robustness, or is not properly designed and tested before being put on the market or otherwise put into service, it may single out people in a discriminatory or otherwise incorrect or unjust manner. Furthermore, the exercise of important procedural fundamental rights, such as the right to an effective remedy and to a fair trial as well as the right of defence and the presumption of innocence, could be hampered, in particular, where such AI systems are not sufficiently transparent, explainable and documented. It is therefore appropriate to classify as high-risk, insofar as their use is permitted under relevant Union and national law, a number of AI systems intended to be used in the law enforcement context where accuracy, reliability and transparency is particularly important to avoid adverse impacts, retain public trust and ensure accountability and effective redress. In view of the nature of the activities and the risks relating thereto, those high-risk AI systems should include in particular AI systems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies, offices, or agencies in support of law enforcement authorities for assessing the risk of a natural person to become a victim of criminal offences, as polygraphs and similar tools, for the evaluation of the reliability of evidence in in the course of investigation or prosecution of criminal offences, and, insofar as not prohibited under this Regulation, for assessing the risk of a natural person offending or reoffending not solely on the basis of the profiling of natural persons or the assessment of personality traits and characteristics or the past criminal behaviour of natural persons or groups, for profiling in the course of detection, investigation or prosecution of criminal offences. AI systems specifically intended to be used for administrative proceedings by tax and customs authorities as well as by financial intelligence units carrying out administrative tasks analysing information pursuant to Union anti-money laundering law should not be classified as high-risk AI systems used by law enforcement authorities for the purpose of prevention, detection, investigation and prosecution of criminal offences. The use of AI tools by law enforcement and other relevant authorities should not become a factor of inequality, or exclusion. The impact of the use of AI tools on the defence rights of suspects should not be ignored, in particular the difficulty in obtaining meaningful information on the functioning of those systems and the resulting difficulty in challenging their results in court, in particular by natural persons under investigation.",
      "original_content": "(59) In Anbetracht der Rolle und Zuständigkeit von Strafverfolgungsbehörden sind deren Maßnahmen im Zusammenhang mit bestimmten Verwendungen von KI-Systemen durch ein erhebliches Machtungleichgewicht gekennzeichnet und können zur Überwachung, zur Festnahme oder zum Entzug der Freiheit einer natürlichen Person sowie zu anderen nachteiligen Auswirkungen auf die in der Charta verankerten Grundrechte führen. Insbesondere wenn das KI-System nicht mit hochwertigen Daten trainiert wird, die Anforderungen an seine Leistung, Genauigkeit oder Robustheit nicht erfüllt werden oder das System nicht ordnungsgemäß konzipiert und getestet wird, bevor es in Verkehr gebracht oder in anderer Weise in Betrieb genommen wird, kann es Personen in diskriminierender oder anderweitig falscher oder ungerechter Weise ausgrenzen. Darüber hinaus könnte die Ausübung wichtiger verfahrensrechtlicher Grundrechte wie etwa des Rechts auf einen wirksamen Rechtsbehelf und ein unparteiisches Gericht sowie das Verteidigungsrecht und die Unschuldsvermutung behindert werden, insbesondere wenn solche KI-Systeme nicht hinreichend transparent, erklärbar und dokumentiert sind. Daher ist es angezeigt, eine Reihe von KI-Systemen — sofern deren Einsatz nach einschlägigem Unions- oder nationalem Recht zugelassen ist —, die im Rahmen der Strafverfolgung eingesetzt werden sollen und bei denen Genauigkeit, Zuverlässigkeit und Transparenz besonders wichtig sind, als hochriskant einzustufen, um nachteilige Auswirkungen zu vermeiden, das Vertrauen der Öffentlichkeit zu erhalten und die Rechenschaftspflicht und einen wirksamen Rechtsschutz zu gewährleisten. Angesichts der Art der Tätigkeiten und der damit verbundenen Risiken sollten diese Hochrisiko-KI-Systeme insbesondere KI-Systeme umfassen, die von Strafverfolgungsbehörden oder in ihrem Auftrag oder von Organen, Einrichtungen und sonstigen Stellen der Union zur Unterstützung von Strafverfolgungsbehörden für folgende Zwecke eingesetzt werden: zur Bewertung des Risikos, dass eine natürliche Person Opfer von Straftaten wird, wie Lügendetektoren und ähnliche Instrumente, zur Bewertung der Zuverlässigkeit von Beweismitteln im Rahmen der Ermittlung oder Verfolgung von Straftaten und — soweit nach dieser Verordnung nicht untersagt — zur Bewertung des Risikos, dass eine natürliche Person eine Straftat begeht oder erneut begeht, nicht nur auf der Grundlage der Erstellung von Profilen natürlicher Personen oder zur Bewertung von Persönlichkeitsmerkmalen und Eigenschaften oder vergangenem kriminellen Verhalten von natürlichen Personen oder Gruppen, zur Erstellung von Profilen während der Aufdeckung, Untersuchung oder strafrechtlichen Verfolgung einer Straftat. KI-Systeme, die speziell für Verwaltungsverfahren in Steuer- und Zollbehörden sowie in Zentralstellen für Geldwäsche-Verdachtsanzeigen, die Verwaltungsaufgaben zur Analyse von Informationen gemäß dem Unionsrecht zur Bekämpfung der Geldwäsche durchführen, bestimmt sind, sollten nicht als Hochrisiko-KI-Systeme eingestuft werden, die von Strafverfolgungsbehörden zum Zweck der Verhütung, Aufdeckung, Untersuchung und strafrechtlichen Verfolgung von Straftaten eingesetzt werden. Der Einsatz von KI-Instrumenten durch Strafverfolgungsbehörden und anderen relevanten Behörden sollte nicht zu einem Faktor der Ungleichheit oder Ausgrenzung werden. Die Auswirkungen des Einsatzes von KI-Instrumenten auf die Verteidigungsrechte von Verdächtigen sollten nicht außer Acht gelassen werden, insbesondere nicht die Schwierigkeit, aussagekräftige Informationen über die Funktionsweise solcher Systeme zu erhalten, und die daraus resultierende Schwierigkeit einer gerichtlichen Anfechtung ihrer Ergebnisse, insbesondere durch natürliche Personen, gegen die ermittelt wird."
    },
    {
      "chunk_idx": 59,
      "id": "7729fd74-193d-4df9-b6a1-c3c6d261090f",
      "title": "Recital 60",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(60) AI systems used in migration, asylum and border control management affect persons who are often in particularly vulnerable position and who are dependent on the outcome of the actions of the competent public authorities. The accuracy, non-discriminatory nature and transparency of the AI systems used in those contexts are therefore particularly important to guarantee respect for the fundamental rights of the affected persons, in particular their rights to free movement, non-discrimination, protection of private life and personal data, international protection and good administration. It is therefore appropriate to classify as high-risk, insofar as their use is permitted under relevant Union and national law, AI systems intended to be used by or on behalf of competent public authorities or by Union institutions, bodies, offices or agencies charged with tasks in the fields of migration, asylum and border control management as polygraphs and similar tools, for assessing certain risks posed by natural persons entering the territory of a Member State or applying for visa or asylum, for assisting competent public authorities for the examination, including related assessment of the reliability of evidence, of applications for asylum, visa and residence permits and associated complaints with regard to the objective to establish the eligibility of the natural persons applying for a status, for the purpose of detecting, recognising or identifying natural persons in the context of migration, asylum and border control management, with the exception of verification of travel documents. AI systems in the area of migration, asylum and border control management covered by this Regulation should comply with the relevant procedural requirements set by the Regulation (EC) No 810/2009 of the European Parliament and of the Council (32), the Directive 2013/32/EU of the European Parliament and of the Council (33), and other relevant Union law. The use of AI systems in migration, asylum and border control management should, in no circumstances, be used by Member States or Union institutions, bodies, offices or agencies as a means to circumvent their international obligations under the UN Convention relating to the Status of Refugees done at Geneva on 28 July 1951 as amended by the Protocol of 31 January 1967. Nor should they be used to in any way infringe on the principle of non-refoulement, or to deny safe and effective legal avenues into the territory of the Union, including the right to international protection.",
      "original_content": "(60) KI-Systeme, die in den Bereichen Migration, Asyl und Grenzkontrolle eingesetzt werden, betreffen Personen, die sich häufig in einer besonders prekären Lage befinden und vom Ergebnis der Maßnahmen der zuständigen Behörden abhängig sind. Die Genauigkeit, der nichtdiskriminierende Charakter und die Transparenz der KI-Systeme, die in solchen Zusammenhängen eingesetzt werden, sind daher besonders wichtig, um die Achtung der Grundrechte der betroffenen Personen, insbesondere ihrer Rechte auf Freizügigkeit, Nichtdiskriminierung, Schutz des Privatlebens und personenbezogener Daten, internationalen Schutz und gute Verwaltung, zu gewährleisten. Daher ist es angezeigt, KI-Systeme — sofern deren Einsatz nach einschlägigem Unions- oder nationalem Recht zugelassen ist — als hochriskant einzustufen, die von den zuständigen mit Aufgaben in den Bereichen Migration, Asyl und Grenzkontrolle betrauten Behörden oder in deren Auftrag oder von Organen, Einrichtungen oder sonstigen Stellen der Union für Folgendes eingesetzt werden: als Lügendetektoren und ähnliche Instrumente; zur Bewertung bestimmter Risiken, die von natürlichen Personen ausgehen, die in das Hoheitsgebiet eines Mitgliedstaats einreisen oder ein Visum oder Asyl beantragen; zur Unterstützung der zuständigen Behörden bei der Prüfung — einschließlich der damit zusammenhängenden Bewertung der Zuverlässigkeit von Beweismitteln — von Asyl- und Visumanträgen sowie Aufenthaltstiteln und damit verbundenen Beschwerden im Hinblick darauf, die Berechtigung der den Antrag stellenden natürlichen Personen festzustellen; zum Zweck der Aufdeckung, Anerkennung oder Identifizierung natürlicher Personen im Zusammenhang mit Migration, Asyl und Grenzkontrolle, mit Ausnahme der Überprüfung von Reisedokumenten. KI-Systeme im Bereich Migration, Asyl und Grenzkontrolle, die unter diese Verordnung fallen, sollten den einschlägigen Verfahrensvorschriften der Verordnung (EG) Nr. 810/2009 des Europäischen Parlaments und des Rates (Fußnote 32), der Richtlinie 2013/32/EU des Europäischen Parlaments und des Rates (Fußnote 33) und anderem einschlägigen Unionsrecht entsprechen. Der Einsatz von KI-Systemen in den Bereichen Migration, Asyl und Grenzkontrolle sollte unter keinen Umständen von den Mitgliedstaaten oder Organen, Einrichtungen oder sonstigen Stellen der Union als Mittel zur Umgehung ihrer internationalen Verpflichtungen aus dem am 28. Juli 1951 in Genf unterzeichneten Abkommen der Vereinten Nationen über die Rechtsstellung der Flüchtlinge in der durch das Protokoll vom 31. Januar 1967 geänderten Fassung genutzt werden. Die Systeme sollten auch nicht dazu genutzt werden, in irgendeiner Weise gegen den Grundsatz der Nichtzurückweisung zu verstoßen oder sichere und wirksame legale Wege in das Gebiet der Union, einschließlich des Rechts auf internationalen Schutz, zu verweigern. Fußnote 32: Verordnung (EG) Nr. 810/2009 des Europäischen Parlaments und des Rates vom 13. Juli 2009 über einen Visakodex der Gemeinschaft (Visakodex) (ABl. L 243 vom 15.9.2009, S. 1)., Fußnote 33: Richtlinie 2013/32/EU des Europäischen Parlaments und des Rates vom 26. Juni 2013 zu gemeinsamen Verfahren für die Zuerkennung und Aberkennung des internationalen Schutzes (ABl. L 180 vom 29.6.2013, S. 60)."
    },
    {
      "chunk_idx": 60,
      "id": "8a98c63a-448a-4311-9ab0-7e6c56c2a6f8",
      "title": "Recital 61",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(61) Certain AI systems intended for the administration of justice and democratic processes should be classified as high-risk, considering their potentially significant impact on democracy, the rule of law, individual freedoms as well as the right to an effective remedy and to a fair trial. In particular, to address the risks of potential biases, errors and opacity, it is appropriate to qualify as high-risk AI systems intended to be used by a judicial authority or on its behalf to assist judicial authorities in researching and interpreting facts and the law and in applying the law to a concrete set of facts. AI systems intended to be used by alternative dispute resolution bodies for those purposes should also be considered to be high-risk when the outcomes of the alternative dispute resolution proceedings produce legal effects for the parties. The use of AI tools can support the decision-making power of judges or judicial independence, but should not replace it: the final decision-making must remain a human-driven activity. The classification of AI systems as high-risk should not, however, extend to AI systems intended for purely ancillary administrative activities that do not affect the actual administration of justice in individual cases, such as anonymisation or pseudonymisation of judicial decisions, documents or data, communication between personnel, administrative tasks.",
      "original_content": "(61) Bestimmte KI-Systeme, die für die Rechtspflege und demokratische Prozesse bestimmt sind, sollten angesichts ihrer möglichen erheblichen Auswirkungen auf die Demokratie, die Rechtsstaatlichkeit, die individuellen Freiheiten sowie das Recht auf einen wirksamen Rechtsbehelf und ein unparteiisches Gericht als hochriskant eingestuft werden. Um insbesondere den Risiken möglicher Verzerrungen, Fehler und Undurchsichtigkeiten zu begegnen, sollten KI-Systeme, die von einer Justizbehörde oder in ihrem Auftrag dazu genutzt werden sollen, Justizbehörden bei der Ermittlung und Auslegung von Sachverhalten und Rechtsvorschriften und bei der Anwendung des Rechts auf konkrete Sachverhalte zu unterstützen, als hochriskant eingestuft werden. KI-Systeme, die von Stellen für die alternative Streitbeilegung für diese Zwecke genutzt werden sollen, sollten ebenfalls als hochriskant gelten, wenn die Ergebnisse der alternativen Streitbeilegung Rechtswirkung für die Parteien entfalten. Der Einsatz von KI-Instrumenten kann die Entscheidungsgewalt von Richtern oder die Unabhängigkeit der Justiz unterstützen, sollte sie aber nicht ersetzen; die endgültige Entscheidungsfindung muss eine von Menschen gesteuerte Tätigkeit bleiben. Die Einstufung von KI-Systemen als hochriskant sollte sich jedoch nicht auf KI-Systeme erstrecken, die für rein begleitende Verwaltungstätigkeiten bestimmt sind, die die tatsächliche Rechtspflege in Einzelfällen nicht beeinträchtigen, wie etwa die Anonymisierung oder Pseudonymisierung gerichtlicher Urteile, Dokumente oder Daten, die Kommunikation zwischen dem Personal oder Verwaltungsaufgaben."
    },
    {
      "chunk_idx": 61,
      "id": "20cafabb-1545-4c9a-b9fd-5adeb4509205",
      "title": "Recital 62",
      "relevantChunksIds": [
        "6b073c5f-3422-4b34-b139-b8645e89b463"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(62) Without prejudice to the rules provided for in Regulation (EU) 2024/900 of the European Parliament and of the Council (34), and in order to address the risks of undue external interference with the right to vote enshrined in Article 39 of the Charter, and of adverse effects on democracy and the rule of law, AI systems intended to be used to influence the outcome of an election or referendum or the voting behaviour of natural persons in the exercise of their vote in elections or referenda should be classified as high-risk AI systems with the exception of AI systems whose output natural persons are not directly exposed to, such as tools used to organise, optimise and structure political campaigns from an administrative and logistical point of view.",
      "original_content": "(62) Unbeschadet der Vorschriften der Verordnung (EU) 2024/900 des Europäischen Parlaments und des Rates (Fußnote 34) und um den Risiken eines unzulässigen externen Eingriffs in das in Artikel 39 der Charta verankerte Wahlrecht und nachteiligen Auswirkungen auf die Demokratie und die Rechtsstaatlichkeit zu begegnen, sollten KI-Systeme, die verwendet werden sollen, um das Ergebnis einer Wahl oder eines Referendums oder das Wahlverhalten natürlicher Personen bei der Ausübung ihres Wahlrechts in einer Wahl oder in Referenden zu beeinflussen, als Hochrisiko-KI-Systeme eingestuft werden, mit Ausnahme von KI-Systemen, deren Ausgaben natürliche Personen nicht direkt ausgesetzt sind, wie Instrumente zur Organisation, Optimierung und Strukturierung politischer Kampagnen in administrativer und logistischer Hinsicht. Fußnote 34: Verordnung (EU) 2024/900 des Europäischen Parlaments und des Rates vom 13. März 2024 über die Transparenz und das Targeting politischer Werbung (ABl. L, 2024/900, 20.3.2024, ELI: http://data.europa.eu/eli/reg/2024/900/oj)."
    },
    {
      "chunk_idx": 62,
      "id": "3be859ed-68c0-4253-864e-73cecf5b5500",
      "title": "Recital 63",
      "relevantChunksIds": [
        "181144f1-acf1-42ef-a1a2-aca3c1a52b7d"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(63) The fact that an AI system is classified as a high-risk AI system under this Regulation should not be interpreted as indicating that the use of the system is lawful under other acts of Union law or under national law compatible with Union law, such as on the protection of personal data, on the use of polygraphs and similar tools or other systems to detect the emotional state of natural persons. Any such use should continue to occur solely in accordance with the applicable requirements resulting from the Charter and from the applicable acts of secondary Union law and national law. This Regulation should not be understood as providing for the legal ground for processing of personal data, including special categories of personal data, where relevant, unless it is specifically otherwise provided for in this Regulation.",
      "original_content": "(63) Die Tatsache, dass ein KI-System gemäß dieser Verordnung als ein Hochrisiko-KI-System eingestuft wird, sollte nicht dahin gehend ausgelegt werden, dass die Verwendung des Systems nach anderen Rechtsakten der Union oder nach nationalen Rechtsvorschriften, die mit dem Unionsrecht vereinbar sind, rechtmäßig ist, beispielsweise in Bezug auf den Schutz personenbezogener Daten, die Verwendung von Lügendetektoren und ähnlichen Instrumenten oder anderen Systemen zur Ermittlung des emotionalen Zustands natürlicher Personen. Eine solche Verwendung sollte weiterhin ausschließlich gemäß den geltenden Anforderungen erfolgen, die sich aus der Charta, dem anwendbaren Sekundärrecht der Union und nationalen Recht ergeben. Diese Verordnung sollte nicht so verstanden werden, dass sie eine Rechtsgrundlage für die Verarbeitung personenbezogener Daten, gegebenenfalls einschließlich besonderer Kategorien personenbezogener Daten, bildet, es sei denn, in dieser Verordnung ist ausdrücklich etwas anderes vorgesehen."
    },
    {
      "chunk_idx": 63,
      "id": "3ebbd833-32ab-4a37-87be-8bae31dbef92",
      "title": "Recital 64",
      "relevantChunksIds": [
        "181144f1-acf1-42ef-a1a2-aca3c1a52b7d"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(64) To mitigate the risks from high-risk AI systems placed on the market or put into service and to ensure a high level of trustworthiness, certain mandatory requirements should apply to high-risk AI systems, taking into account the intended purpose and the context of use of the AI system and according to the risk-management system to be established by the provider. The measures adopted by the providers to comply with the mandatory requirements of this Regulation should take into account the generally acknowledged state of the art on AI, be proportionate and effective to meet the objectives of this Regulation. Based on the New Legislative Framework, as clarified in Commission notice ‘The “Blue Guide” on the implementation of EU product rules 2022’, the general rule is that more than one legal act of Union harmonisation legislation may be applicable to one product, since the making available or putting into service can take place only when the product complies with all applicable Union harmonisation legislation. The hazards of AI systems covered by the requirements of this Regulation concern different aspects than the existing Union harmonisation legislation and therefore the requirements of this Regulation would complement the existing body of the Union harmonisation legislation. For example, machinery or medical devices products incorporating an AI system might present risks not addressed by the essential health and safety requirements set out in the relevant Union harmonised legislation, as that sectoral law does not deal with risks specific to AI systems. This calls for a simultaneous and complementary application of the various legislative acts. To ensure consistency and to avoid an unnecessary administrative burden and unnecessary costs, providers of a product that contains one or more high-risk AI system, to which the requirements of this Regulation and of the Union harmonisation legislation based on the New Legislative Framework and listed in an annex to this Regulation apply, should have flexibility with regard to operational decisions on how to ensure compliance of a product that contains one or more AI systems with all the applicable requirements of that Union harmonised legislation in an optimal manner. That flexibility could mean, for example a decision by the provider to integrate a part of the necessary testing and reporting processes, information and documentation required under this Regulation into already existing documentation and procedures required under existing Union harmonisation legislation based on the New Legislative Framework and listed in an annex to this Regulation. This should not, in any way, undermine the obligation of the provider to comply with all the applicable requirements.",
      "original_content": "(64) Um die von in Verkehr gebrachten oder in Betrieb genommenen Hochrisiko-KI-Systemen ausgehenden Risiken zu mindern und ein hohes Maß an Vertrauenswürdigkeit zu gewährleisten, sollten für Hochrisiko-KI-Systeme bestimmte verbindliche Anforderungen gelten, wobei der Zweckbestimmung und dem Nutzungskontext des KI-Systems sowie dem vom Anbieter einzurichtenden Risikomanagementsystem Rechnung zu tragen ist. Die von den Anbietern zur Erfüllung der verbindlichen Anforderungen dieser Verordnung ergriffenen Maßnahmen sollten dem allgemein anerkannten Stand der KI Rechnung tragen, verhältnismäßig und wirksam sein, um die Ziele dieser Verordnung zu erreichen. Auf der Grundlage des neuen Rechtsrahmens, wie in der Bekanntmachung der Kommission „Leitfaden für die Umsetzung der Produktvorschriften der EU 2022 (Blue Guide)“ dargelegt, gilt als allgemeine Regel, dass mehr als ein Rechtsakt der Harmonisierungsrechtsvorschriften der Union auf ein Produkt anwendbar sein können, da die Bereitstellung oder Inbetriebnahme nur erfolgen kann, wenn das Produkt allen geltenden Harmonisierungsrechtsvorschriften der Union entspricht. Die Gefahren von KI-Systemen, die unter die Anforderungen dieser Verordnung fallen, decken andere Aspekte ab als die bestehenden Harmonisierungsrechtsvorschriften der Union, weshalb die Anforderungen dieser Verordnung das bestehende Regelwerk der Harmonisierungsrechtsvorschriften der Union ergänzen würden. So bergen etwa Maschinen oder Medizinprodukte mit einer KI-Komponente möglicherweise Risiken, die von den grundlegenden Gesundheits- und Sicherheitsanforderungen der einschlägigen harmonisierten Rechtsvorschriften der Union nicht erfasst werden, da diese sektoralen Rechtsvorschriften keine spezifischen KI-Risiken behandeln. Dies erfordert die gleichzeitige und ergänzende Anwendung mehrerer Rechtsakte. Um Kohärenz zu gewährleisten und unnötigen Verwaltungsaufwand sowie unnötige Kosten zu vermeiden, sollten die Anbieter eines Produkts, das ein oder mehrere Hochrisiko-KI-Systeme enthält, für die die Anforderungen dieser Verordnung und der in einem Anhang dieser Verordnung aufgeführten und auf dem neuen Rechtsrahmen beruhenden Harmonisierungsvorschriften der Union gelten, in Bezug auf betriebliche Entscheidungen darüber flexibel sein, wie die Konformität eines Produkts, das ein oder mehrere Hochrisiko-KI-Systeme enthält, bestmöglich mit allen geltenden Anforderungen dieser harmonisierten Rechtsvorschriften der Union sichergestellt werden kann. Diese Flexibilität könnte beispielsweise bedeuten, dass der Anbieter beschließt, einen Teil der gemäß dieser Verordnung erforderlichen Test- und Berichterstattungsverfahren, Informationen und Unterlagen in bereits bestehende Dokumentationen und Verfahren zu integrieren, die nach den auf dem neuen Rechtsrahmen beruhenden und in einem Anhang dieser Verordnung aufgeführten geltenden Harmonisierungsrechtsvorschriften der Union erforderlich sind. Dies sollte in keiner Weise die Verpflichtung des Anbieters untergraben, alle geltenden Anforderungen zu erfüllen."
    },
    {
      "chunk_idx": 64,
      "id": "1d3e3007-9327-4f7b-ba9b-44bd93c2f4ae",
      "title": "Recital 65",
      "relevantChunksIds": [
        "6d7d2b9c-9224-4f54-ad18-671918597d0c"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(65) The risk-management system should consist of a continuous, iterative process that is planned and run throughout the entire lifecycle of a high-risk AI system. That process should be aimed at identifying and mitigating the relevant risks of AI systems on health, safety and fundamental rights. The risk-management system should be regularly reviewed and updated to ensure its continuing effectiveness, as well as justification and documentation of any significant decisions and actions taken subject to this Regulation. This process should ensure that the provider identifies risks or adverse impacts and implements mitigation measures for the known and reasonably foreseeable risks of AI systems to the health, safety and fundamental rights in light of their intended purpose and reasonably foreseeable misuse, including the possible risks arising from the interaction between the AI system and the environment within which it operates. The risk-management system should adopt the most appropriate risk-management measures in light of the state of the art in AI. When identifying the most appropriate risk-management measures, the provider should document and explain the choices made and, when relevant, involve experts and external stakeholders. In identifying the reasonably foreseeable misuse of high-risk AI systems, the provider should cover uses of AI systems which, while not directly covered by the intended purpose and provided for in the instruction for use may nevertheless be reasonably expected to result from readily predictable human behaviour in the context of the specific characteristics and use of a particular AI system. Any known or foreseeable circumstances related to the use of the high-risk AI system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety or fundamental rights should be included in the instructions for use that are provided by the provider. This is to ensure that the deployer is aware and takes them into account when using the high-risk AI system. Identifying and implementing risk mitigation measures for foreseeable misuse under this Regulation should not require specific additional training for the high-risk AI system by the provider to address foreseeable misuse. The providers however are encouraged to consider such additional training measures to mitigate reasonable foreseeable misuses as necessary and appropriate.",
      "original_content": "(65) Das Risikomanagementsystem sollte in einem kontinuierlichen iterativen Prozess bestehen, der während des gesamten Lebenszyklus eines Hochrisiko-KI-Systems geplant und durchgeführt wird. Ziel dieses Prozesses sollte es ein, die einschlägigen Risiken von KI-Systemen für Gesundheit, Sicherheit und Grundrechte zu ermitteln und zu mindern. Das Risikomanagementsystem sollte regelmäßig überprüft und aktualisiert werden, um seine dauerhafte Wirksamkeit sowie die Begründetheit und Dokumentierung aller gemäß dieser Verordnung getroffenen wesentlichen Entscheidungen und Maßnahmen zu gewährleisten. Mit diesem Prozess sollte sichergestellt werden, dass der Anbieter Risiken oder negative Auswirkungen ermittelt und Minderungsmaßnahmen ergreift in Bezug auf die bekannten und vernünftigerweise vorhersehbaren Risiken von KI-Systemen für die Gesundheit, die Sicherheit und die Grundrechte angesichts ihrer Zweckbestimmung und vernünftigerweise vorhersehbaren Fehlanwendung, einschließlich der möglichen Risiken, die sich aus der Interaktion zwischen dem KI-System und der Umgebung, in der es betrieben wird, ergeben könnten. Im Rahmen des Risikomanagementsystems sollten die vor dem Hintergrund des Stands der KI am besten geeigneten Risikomanagementmaßnahmen ergriffen werden. Bei der Ermittlung der am besten geeigneten Risikomanagementmaßnahmen sollte der Anbieter die getroffenen Entscheidungen dokumentieren und erläutern und gegebenenfalls Sachverständige und externe Interessenträger hinzuziehen. Bei der Ermittlung der vernünftigerweise vorhersehbaren Fehlanwendung von Hochrisiko-KI-Systemen sollte der Anbieter die Verwendungen von KI-Systemen erfassen, die zwar nicht unmittelbar der Zweckbestimmung entsprechen und in der Betriebsanleitung vorgesehen sind, jedoch nach vernünftigem Ermessen davon auszugehen ist, dass sie sich aus einem leicht absehbaren menschlichen Verhalten im Zusammenhang mit den spezifischen Merkmalen und der Verwendung eines bestimmten KI-Systems ergeben. Alle bekannten oder vorhersehbaren Umstände bezüglich der Verwendung des Hochrisiko-KI-Systems im Einklang mit seiner Zweckbestimmung oder einer vernünftigerweise vorhersehbaren Fehlanwendung, die zu Risiken für die Gesundheit und Sicherheit oder die Grundrechte führen können, sollten vom Anbieter in der Betriebsanleitung aufgeführt werden. Damit soll sichergestellt werden, dass der Betreiber diese Umstände bei der Nutzung des Hochrisiko-KI-Systems kennt und berücksichtigt. Die Ermittlung und Umsetzung von Risikominderungsmaßnahmen in Bezug auf vorhersehbare Fehlanwendungen im Rahmen dieser Verordnung sollte keine spezifische zusätzliche Schulung für das Hochrisiko-KI-System durch den Anbieter erfordern, um gegen vorhersehbare Fehlanwendungen vorzugehen. Die Anbieter sind jedoch gehalten, solche zusätzlichen Schulungsmaßnahmen in Erwägung zu ziehen, um einer vernünftigerweise vorhersehbare Fehlanwendung entgegenzuwirken, soweit dies erforderlich und angemessen ist."
    },
    {
      "chunk_idx": 65,
      "id": "25407f28-0e07-4101-a714-b193ca14783e",
      "title": "Recital 66",
      "relevantChunksIds": [
        "6d7d2b9c-9224-4f54-ad18-671918597d0c"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(66) Requirements should apply to high-risk AI systems as regards risk management, the quality and relevance of data sets used, technical documentation and record-keeping, transparency and the provision of information to deployers, human oversight, and robustness, accuracy and cybersecurity. Those requirements are necessary to effectively mitigate the risks for health, safety and fundamental rights. As no other less trade restrictive measures are reasonably available those requirements are not unjustified restrictions to trade.",
      "original_content": "(66) Die Anforderungen sollten für Hochrisiko-KI-Systeme im Hinblick auf das Risikomanagement, die Qualität und Relevanz der verwendeten Datensätze, die technische Dokumentation und die Aufzeichnungspflichten, die Transparenz und die Bereitstellung von Informationen für die Betreiber, die menschliche Aufsicht sowie die Robustheit, Genauigkeit und Sicherheit gelten. Diese Anforderungen sind erforderlich, um die Risiken für Gesundheit, Sicherheit und Grundrechte wirksam zu mindern. Nachdem nach vernünftigem Ermessen keine anderen weniger handelsbeschränkenden Maßnahmen zur Verfügung stehen, stellen sie keine ungerechtfertigten Handelsbeschränkungen dar."
    },
    {
      "chunk_idx": 66,
      "id": "2aedd016-7002-471a-af6e-131b4f9f1f54",
      "title": "Recital 67",
      "relevantChunksIds": [
        "69b883ca-3e2d-46ed-b797-e2c62832a376",
        "a95f9b76-59d0-4655-9859-ce20a0190849"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(67) High-quality data and access to high-quality data plays a vital role in providing structure and in ensuring the performance of many AI systems, especially when techniques involving the training of models are used, with a view to ensure that the high-risk AI system performs as intended and safely and it does not become a source of discrimination prohibited by Union law. High-quality data sets for training, validation and testing require the implementation of appropriate data governance and management practices. Data sets for training, validation and testing, including the labels, should be relevant, sufficiently representative, and to the best extent possible free of errors and complete in view of the intended purpose of the system. In order to facilitate compliance with Union data protection law, such as Regulation (EU) 2016/679, data governance and management practices should include, in the case of personal data, transparency about the original purpose of the data collection. The data sets should also have the appropriate statistical properties, including as regards the persons or groups of persons in relation to whom the high-risk AI system is intended to be used, with specific attention to the mitigation of possible biases in the data sets, that are likely to affect the health and safety of persons, have a negative impact on fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence inputs for future operations (feedback loops). Biases can for example be inherent in underlying data sets, especially when historical data is being used, or generated when the systems are implemented in real world settings. Results provided by AI systems could be influenced by such inherent biases that are inclined to gradually increase and thereby perpetuate and amplify existing discrimination, in particular for persons belonging to certain vulnerable groups, including racial or ethnic groups. The requirement for the data sets to be to the best extent possible complete and free of errors should not affect the use of privacy-preserving techniques in the context of the development and testing of AI systems. In particular, data sets should take into account, to the extent required by their intended purpose, the features, characteristics or elements that are particular to the specific geographical, contextual, behavioural or functional setting which the AI system is intended to be used. The requirements related to data governance can be complied with by having recourse to third parties that offer certified compliance services including verification of data governance, data set integrity, and data training, validation and testing practices, as far as compliance with the data requirements of this Regulation are ensured.",
      "original_content": "(67) Hochwertige Daten und der Zugang dazu spielen eine zentrale Rolle bei der Bereitstellung von Strukturen und für die Sicherstellung der Leistung vieler KI-Systeme, insbesondere wenn Techniken eingesetzt werden, bei denen Modelle mit Daten trainiert werden, um sicherzustellen, dass das Hochrisiko-KI-System bestimmungsgemäß und sicher funktioniert und nicht zur Ursache für Diskriminierung wird, die nach dem Unionsrecht verboten ist. Hochwertige Trainings-, Validierungs- und Testdatensätze erfordern geeignete Daten-Governance- und Datenverwaltungsverfahren. Die Trainings-, Validierungs- und Testdatensätze, einschließlich der Kennzeichnungen, sollten im Hinblick auf die Zweckbestimmung des Systems relevant, hinreichend repräsentativ und so weit wie möglich fehlerfrei und vollständig sein. Um die Einhaltung des Datenschutzrechts der Union, wie der Verordnung (EU) 2016/679, zu erleichtern, sollten Daten-Governance- und Datenverwaltungsverfahren bei personenbezogenen Daten Transparenz in Bezug auf den ursprünglichen Zweck der Datenerhebung umfassen. Die Datensätze sollten auch die geeigneten statistischen Merkmale haben, auch bezüglich der Personen oder Personengruppen, auf die das Hochrisiko-KI-System bestimmungsgemäß angewandt werden soll, unter besonderer Berücksichtigung der Minderung möglicher Verzerrungen in den Datensätzen, die die Gesundheit und Sicherheit von Personen beeinträchtigen, sich negativ auf die Grundrechte auswirken oder zu einer nach dem Unionsrecht verbotenen Diskriminierung führen könnten, insbesondere wenn die Datenausgaben die Eingaben für künftige Operationen beeinflussen (Rückkopplungsschleifen). Verzerrungen können zum Beispiel — insbesondere bei Verwendung historischer Daten — den zugrunde liegenden Datensätzen innewohnen oder bei der Implementierung der Systeme in der realen Welt generiert werden. Die von einem KI-System ausgegebenen Ergebnisse könnten durch solche inhärenten Verzerrungen beeinflusst werden, die tendenziell allmählich zunehmen und dadurch bestehende Diskriminierungen fortschreiben und verstärken, insbesondere in Bezug auf Personen, die bestimmten schutzbedürftigen Gruppen wie aufgrund von Rassismus benachteiligten oder ethnischen Gruppen angehören. Die Anforderung, dass die Datensätze so weit wie möglich vollständig und fehlerfrei sein müssen, sollte sich nicht auf den Einsatz von Techniken zur Wahrung der Privatsphäre im Zusammenhang mit der Entwicklung und dem Testen von KI-Systemen auswirken. Insbesondere sollten die Datensätze, soweit dies für die Zweckbestimmung erforderlich ist, den Eigenschaften, Merkmalen oder Elementen entsprechen, die für die besonderen geografischen, kontextuellen, verhaltensbezogenen oder funktionalen Rahmenbedingungen, unter denen das Hochrisiko-KI-System bestimmungsgemäß verwendet werden soll, typisch sind. Die Anforderungen an die Daten-Governance können durch die Inanspruchnahme Dritter erfüllt werden, die zertifizierte Compliance-Dienste anbieten, einschließlich der Überprüfung der Daten-Governance, der Datensatzintegrität und der Datenschulungs-, Validierungs- und Testverfahren, sofern die Einhaltung der Datenanforderungen dieser Verordnung gewährleistet ist."
    },
    {
      "chunk_idx": 67,
      "id": "316fe982-49dc-4467-a230-7b526feb2812",
      "title": "Recital 68",
      "relevantChunksIds": [
        "69b883ca-3e2d-46ed-b797-e2c62832a376"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(68) For the development and assessment of high-risk AI systems, certain actors, such as providers, notified bodies and other relevant entities, such as European Digital Innovation Hubs, testing experimentation facilities and researchers, should be able to access and use high-quality data sets within the fields of activities of those actors which are related to this Regulation. European common data spaces established by the Commission and the facilitation of data sharing between businesses and with government in the public interest will be instrumental to provide trustful, accountable and non-discriminatory access to high-quality data for the training, validation and testing of AI systems. For example, in health, the European health data space will facilitate non-discriminatory access to health data and the training of AI algorithms on those data sets, in a privacy-preserving, secure, timely, transparent and trustworthy manner, and with an appropriate institutional governance. Relevant competent authorities, including sectoral ones, providing or supporting the access to data may also support the provision of high-quality data for the training, validation and testing of AI systems.",
      "original_content": "(68) Für die Entwicklung und Bewertung von Hochrisiko-KI-Systemen sollten bestimmte Akteure wie etwa Anbieter, notifizierte Stellen und andere einschlägige Einrichtungen wie etwa Europäische Digitale Innovationszentren, Test- und Versuchseinrichtungen und Forscher in der Lage sein, in den Tätigkeitsbereichen, in denen diese Akteure tätig sind und die mit dieser Verordnung in Zusammenhang stehen, auf hochwertige Datensätze zuzugreifen und diese zu nutzen. Die von der Kommission eingerichteten gemeinsamen europäischen Datenräume und die Erleichterung des Datenaustauschs im öffentlichen Interesse zwischen Unternehmen und mit Behörden werden entscheidend dazu beitragen, einen vertrauensvollen, rechenschaftspflichtigen und diskriminierungsfreien Zugang zu hochwertigen Daten für das Training, die Validierung und das Testen von KI-Systemen zu gewährleisten. Im Gesundheitsbereich beispielsweise wird der europäische Raum für Gesundheitsdaten den diskriminierungsfreien Zugang zu Gesundheitsdaten und das Training von KI-Algorithmen mithilfe dieser Datensätze erleichtern, und zwar unter Wahrung der Privatsphäre, auf sichere, zeitnahe, transparente und vertrauenswürdige Weise und unter angemessener institutioneller Leitung. Die einschlägigen zuständigen Behörden, einschließlich sektoraler Behörden, die den Zugang zu Daten bereitstellen oder unterstützen, können auch die Bereitstellung hochwertiger Daten für das Training, die Validierung und das Testen von KI-Systemen unterstützen."
    },
    {
      "chunk_idx": 68,
      "id": "03a48ad3-88c0-41d1-be59-9032cdfa8840",
      "title": "Recital 69",
      "relevantChunksIds": [
        "181144f1-acf1-42ef-a1a2-aca3c1a52b7d"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(69) The right to privacy and to protection of personal data must be guaranteed throughout the entire lifecycle of the AI system. In this regard, the principles of data minimisation and data protection by design and by default, as set out in Union data protection law, are applicable when personal data are processed. Measures taken by providers to ensure compliance with those principles may include not only anonymisation and encryption, but also the use of technology that permits algorithms to be brought to the data and allows training of AI systems without the transmission between parties or copying of the raw or structured data themselves, without prejudice to the requirements on data governance provided for in this Regulation.",
      "original_content": "(69) Das Recht auf Privatsphäre und den Schutz personenbezogener Daten muss während des gesamten Lebenszyklus des KI-Systems sichergestellt sein. In dieser Hinsicht gelten die Grundsätze der Datenminimierung und des Datenschutzes durch Technikgestaltung und Voreinstellungen, wie sie im Datenschutzrecht der Union festgelegt sind, wenn personenbezogene Daten verarbeitet werden. Unbeschadet der in dieser Verordnung festgelegten Anforderungen an die Daten-Governance können zu den Maßnahmen, mit denen die Anbieter die Einhaltung dieser Grundsätze sicherstellen, nicht nur Anonymisierung und Verschlüsselung gehören, sondern auch der Einsatz von Technik, die es ermöglicht, Algorithmen direkt am Ort der Datenerzeugung einzusetzen und KI-Systeme zu trainieren, ohne dass Daten zwischen Parteien übertragen oder die Rohdaten oder strukturierten Daten selbst kopiert werden."
    },
    {
      "chunk_idx": 69,
      "id": "53ee5174-71e9-4bc8-81f2-251f7a1bc278",
      "title": "Recital 70",
      "relevantChunksIds": [
        "69b883ca-3e2d-46ed-b797-e2c62832a376"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(70) In order to protect the right of others from the discrimination that might result from the bias in AI systems, the providers should, exceptionally, to the extent that it is strictly necessary for the purpose of ensuring bias detection and correction in relation to the high-risk AI systems, subject to appropriate safeguards for the fundamental rights and freedoms of natural persons and following the application of all applicable conditions laid down under this Regulation in addition to the conditions laid down in Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, be able to process also special categories of personal data, as a matter of substantial public interest within the meaning of Article 9(2), point (g) of Regulation (EU) 2016/679 and Article 10(2), point (g) of Regulation (EU) 2018/1725.",
      "original_content": "(70) Um das Recht anderer auf Schutz vor Diskriminierung, die sich aus Verzerrungen in KI-Systemen ergeben könnte, zu wahren, sollten die Anbieter ausnahmsweise und in dem unbedingt erforderlichen Ausmaß, um die Erkennung und Korrektur von Verzerrungen im Zusammenhang mit Hochrisiko-KI-Systemen sicherzustellen, vorbehaltlich angemessener Vorkehrungen für den Schutz der Grundrechte und Grundfreiheiten natürlicher Personen und nach Anwendung aller in dieser Verordnung festgelegten geltenden Bedingungen zusätzlich zu den in den Verordnungen (EU) 2016/679 und (EU) 2018/1725 sowie der Richtlinie (EU) 2016/680 festgelegten Bedingungen besondere Kategorien personenbezogener Daten als Angelegenheit von erheblichem öffentlichen Interesse im Sinne des Artikels 9 Absatz 2 Buchstabe g der Verordnung (EU) 2016/679 und des Artikels 10 Absatz 2 Buchstabe g der Verordnung (EU) 2018/1725 verarbeiten können."
    },
    {
      "chunk_idx": 70,
      "id": "817c9521-a9db-4149-99df-86ce3f02b215",
      "title": "Recital 71",
      "relevantChunksIds": [
        "355be535-f654-47e6-9608-95369e856b37",
        "40622ac2-1630-4d8b-a8a6-0aadd9c20248",
        "b4faa5d7-2c2b-43c9-b793-47d6579e2fc8"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(71) Having comprehensible information on how high-risk AI systems have been developed and how they perform throughout their lifetime is essential to enable traceability of those systems, verify compliance with the requirements under this Regulation, as well as monitoring of their operations and post market monitoring. This requires keeping records and the availability of technical documentation, containing information which is necessary to assess the compliance of the AI system with the relevant requirements and facilitate post market monitoring. Such information should include the general characteristics, capabilities and limitations of the system, algorithms, data, training, testing and validation processes used as well as documentation on the relevant risk-management system and drawn in a clear and comprehensive form. The technical documentation should be kept up to date, appropriately throughout the lifetime of the AI system. Furthermore, high-risk AI systems should technically allow for the automatic recording of events, by means of logs, over the duration of the lifetime of the system.",
      "original_content": "(71) Umfassende Informationen darüber, wie Hochrisiko-KI-Systeme entwickelt wurden und wie sie während ihrer gesamten Lebensdauer funktionieren, sind unerlässlich, um die Nachvollziehbarkeit dieser Systeme, die Überprüfung der Einhaltung der Anforderungen dieser Verordnung sowie die Beobachtung ihres Betriebs und ihre Beobachtung nach dem Inverkehrbringen zu ermöglichen. Dies erfordert die Führung von Aufzeichnungen und die Verfügbarkeit einer technischen Dokumentation, die alle erforderlichen Informationen enthält, um die Einhaltung der einschlägigen Anforderungen durch das KI-System zu beurteilen und die Beobachtung nach dem Inverkehrbringen zu erleichtern. Diese Informationen sollten die allgemeinen Merkmale, Fähigkeiten und Grenzen des Systems, die verwendeten Algorithmen, Daten und Trainings-, Test- und Validierungsverfahren sowie die Dokumentation des einschlägigen Risikomanagementsystems umfassen und in klarer und umfassender Form abgefasst sein. Die technische Dokumentation sollte während der gesamten Lebensdauer des KI-Systems angemessen auf dem neuesten Stand gehalten werden. Darüber hinaus sollten die Hochrisiko-KI-Systeme technisch die automatische Aufzeichnung von Ereignissen mittels Protokollierung während der Lebensdauer des Systems ermöglichen."
    },
    {
      "chunk_idx": 71,
      "id": "9032a3a6-c85d-43bc-b8da-fb8d7a40e48c",
      "title": "Recital 72",
      "relevantChunksIds": [
        "e36b6aab-d7e1-44ce-8d20-d6bee82bff7d"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(72) To address concerns related to opacity and complexity of certain AI systems and help deployers to fulfil their obligations under this Regulation, transparency should be required for high-risk AI systems before they are placed on the market or put it into service. High-risk AI systems should be designed in a manner to enable deployers to understand how the AI system works, evaluate its functionality, and comprehend its strengths and limitations. High-risk AI systems should be accompanied by appropriate information in the form of instructions of use. Such information should include the characteristics, capabilities and limitations of performance of the AI system. Those would cover information on possible known and foreseeable circumstances related to the use of the high-risk AI system, including deployer action that may influence system behaviour and performance, under which the AI system can lead to risks to health, safety, and fundamental rights, on the changes that have been pre-determined and assessed for conformity by the provider and on the relevant human oversight measures, including the measures to facilitate the interpretation of the outputs of the AI system by the deployers. Transparency, including the accompanying instructions for use, should assist deployers in the use of the system and support informed decision making by them. Deployers should, inter alia, be in a better position to make the correct choice of the system that they intend to use in light of the obligations applicable to them, be educated about the intended and precluded uses, and use the AI system correctly and as appropriate. In order to enhance legibility and accessibility of the information included in the instructions of use, where appropriate, illustrative examples, for instance on the limitations and on the intended and precluded uses of the AI system, should be included. Providers should ensure that all documentation, including the instructions for use, contains meaningful, comprehensive, accessible and understandable information, taking into account the needs and foreseeable knowledge of the target deployers. Instructions for use should be made available in a language which can be easily understood by target deployers, as determined by the Member State concerned.",
      "original_content": "(72) Um Bedenken hinsichtlich der Undurchsichtigkeit und Komplexität bestimmter KI-Systeme auszuräumen und die Betreiber bei der Erfüllung ihrer Pflichten gemäß dieser Verordnung zu unterstützen, sollte für Hochrisiko-KI-Systeme Transparenz vorgeschrieben werden, bevor sie in Verkehr gebracht oder in Betrieb genommen werden. Hochrisiko-KI-Systeme sollten so gestaltet sein, dass die Betreiber in der Lage sind, zu verstehen, wie das KI-System funktioniert, seine Funktionalität zu bewerten und seine Stärken und Grenzen zu erfassen. Hochrisiko-KI-Systemen sollten angemessene Informationen in Form von Betriebsanleitungen beigefügt sein. Zu diesen Informationen sollten die Merkmale, Fähigkeiten und Leistungsbeschränkungen des KI-Systems gehören. Diese würden Informationen über mögliche bekannte und vorhersehbare Umstände im Zusammenhang mit der Nutzung des Hochrisiko-KI-Systems, einschließlich Handlungen des Betreibers, die das Verhalten und die Leistung des Systems beeinflussen können, unter denen das KI-System zu Risiken in Bezug auf die Gesundheit, die Sicherheit und die Grundrechte führen kann, über die Änderungen, die vom Anbieter vorab festgelegt und auf Konformität geprüft wurden, und über die einschlägigen Maßnahmen der menschlichen Aufsicht, einschließlich der Maßnahmen, um den Betreibern die Interpretation der Ausgaben von KI-Systemen zu erleichtern, umfassen. Transparenz, einschließlich der begleitenden Betriebsanleitungen, sollte den Betreibern bei der Nutzung des Systems helfen und ihre fundierte Entscheidungsfindung unterstützen. Unter anderem sollten Betreiber besser in der Lage sein, das richtige System auszuwählen, das sie angesichts der für sie geltenden Pflichten verwenden wollen, über die beabsichtigten und ausgeschlossenen Verwendungszwecke informiert sein und das KI-System korrekt und angemessen verwenden. Um die Lesbarkeit und Zugänglichkeit der in der Betriebsanleitung enthaltenen Informationen zu verbessern, sollten diese gegebenenfalls anschauliche Beispiele enthalten, zum Beispiel zu den Beschränkungen sowie zu den beabsichtigten und ausgeschlossenen Verwendungen des KI-Systems. Die Anbieter sollten dafür sorgen, dass in der gesamten Dokumentation, einschließlich der Betriebsanleitungen, aussagekräftige, umfassende, zugängliche und verständliche Informationen enthalten sind, wobei die Bedürfnisse und vorhersehbaren Kenntnisse der Zielbetreiber zu berücksichtigen sind. Die Betriebsanleitungen sollten in einer vom betreffenden Mitgliedstaat festgelegten Sprache zur Verfügung gestellt werden, die von den Zielbetreibern leicht verstanden werden kann."
    },
    {
      "chunk_idx": 72,
      "id": "550c995f-0a8e-4714-b73e-9418fb982eed",
      "title": "Recital 73",
      "relevantChunksIds": [
        "09b711a2-571c-4a78-9b84-60675ea83b5e",
        "67156c3d-d006-41f0-87e3-725cd933dbdb"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(73) High-risk AI systems should be designed and developed in such a way that natural persons can oversee their functioning, ensure that they are used as intended and that their impacts are addressed over the system’s lifecycle. To that end, appropriate human oversight measures should be identified by the provider of the system before its placing on the market or putting into service. In particular, where appropriate, such measures should guarantee that the system is subject to in-built operational constraints that cannot be overridden by the system itself and is responsive to the human operator, and that the natural persons to whom human oversight has been assigned have the necessary competence, training and authority to carry out that role. It is also essential, as appropriate, to ensure that high-risk AI systems include mechanisms to guide and inform a natural person to whom human oversight has been assigned to make informed decisions if, when and how to intervene in order to avoid negative consequences or risks, or stop the system if it does not perform as intended. Considering the significant consequences for persons in the case of an incorrect match by certain biometric identification systems, it is appropriate to provide for an enhanced human oversight requirement for those systems so that no action or decision may be taken by the deployer on the basis of the identification resulting from the system unless this has been separately verified and confirmed by at least two natural persons. Those persons could be from one or more entities and include the person operating or using the system. This requirement should not pose unnecessary burden or delays and it could be sufficient that the separate verifications by the different persons are automatically recorded in the logs generated by the system. Given the specificities of the areas of law enforcement, migration, border control and asylum, this requirement should not apply where Union or national law considers the application of that requirement to be disproportionate.",
      "original_content": "(73) Hochrisiko-KI-Systeme sollten so gestaltet und entwickelt werden, dass natürliche Personen ihre Funktionsweise überwachen und sicherstellen können, dass sie bestimmungsgemäß verwendet werden und dass ihre Auswirkungen während des Lebenszyklus des Systems berücksichtigt werden. Zu diesem Zweck sollte der Anbieter des Systems vor dem Inverkehrbringen oder der Inbetriebnahme geeignete Maßnahmen zur Gewährleistung der menschlichen Aufsicht festlegen. Insbesondere sollten solche Maßnahmen gegebenenfalls gewährleisten, dass das System integrierten Betriebseinschränkungen unterliegt, über die sich das System selbst nicht hinwegsetzen kann, dass es auf den menschlichen Bediener reagiert und dass die natürlichen Personen, denen die menschliche Aufsicht übertragen wurde, über die erforderliche Kompetenz, Ausbildung und Befugnis verfügen, um diese Aufgabe wahrzunehmen. Es ist außerdem unerlässlich, gegebenenfalls dafür zu sorgen, dass in Hochrisiko-KI-Systemen Mechanismen enthalten sind, um eine natürliche Person, der die menschliche Aufsicht übertragen wurde, zu beraten und zu informieren, damit sie fundierte Entscheidungen darüber trifft, ob, wann und wie einzugreifen ist, um negative Folgen oder Risiken zu vermeiden, oder das System anzuhalten, wenn es nicht wie beabsichtigt funktioniert. Angesichts der bedeutenden Konsequenzen für Personen im Falle eines falschen Treffers durch bestimmte biometrische Identifizierungssysteme ist es angezeigt, für diese Systeme eine verstärkte Anforderung im Hinblick auf die menschliche Aufsicht vorzusehen, sodass der Betreiber keine Maßnahmen oder Entscheidungen aufgrund des vom System hervorgebrachten Identifizierungsergebnisses treffen kann, solange dies nicht von mindestens zwei natürlichen Personen getrennt überprüft und bestätigt wurde. Diese Personen könnten von einer oder mehreren Einrichtungen stammen und die Person umfassen, die das System bedient oder verwendet. Diese Anforderung sollte keine unnötigen Belastungen oder Verzögerungen mit sich bringen, und es könnte ausreichen, dass die getrennten Überprüfungen durch die verschiedenen Personen automatisch in die vom System erzeugten Protokolle aufgenommen werden. Angesichts der Besonderheiten der Bereiche Strafverfolgung, Migration, Grenzkontrolle und Asyl sollte diese Anforderung nicht gelten, wenn die Geltung dieser Anforderung nach Unionsrecht oder nationalem Recht unverhältnismäßig ist."
    },
    {
      "chunk_idx": 73,
      "id": "3b55e0a7-53c5-443d-a628-f905ea635201",
      "title": "Recital 74",
      "relevantChunksIds": [
        "91bb9b2c-b571-4042-8143-f399e538ffe2"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(74) High-risk AI systems should perform consistently throughout their lifecycle and meet an appropriate level of accuracy, robustness and cybersecurity, in light of their intended purpose and in accordance with the generally acknowledged state of the art. The Commission and relevant organisations and stakeholders are encouraged to take due consideration of the mitigation of risks and the negative impacts of the AI system. The expected level of performance metrics should be declared in the accompanying instructions of use. Providers are urged to communicate that information to deployers in a clear and easily understandable way, free of misunderstandings or misleading statements. Union law on legal metrology, including Directives 2014/31/EU (35) and 2014/32/EU (36) of the European Parliament and of the Council, aims to ensure the accuracy of measurements and to help the transparency and fairness of commercial transactions. In that context, in cooperation with relevant stakeholders and organisation, such as metrology and benchmarking authorities, the Commission should encourage, as appropriate, the development of benchmarks and measurement methodologies for AI systems. In doing so, the Commission should take note and collaborate with international partners working on metrology and relevant measurement indicators relating to AI.",
      "original_content": "(74) Hochrisiko-KI-Systeme sollten während ihres gesamten Lebenszyklus beständig funktionieren und ein angemessenes Maß an Genauigkeit, Robustheit und Cybersicherheit angesichts ihrer Zweckbestimmung und entsprechend dem allgemein anerkannten Stand der Technik aufweisen. Die Kommission sowie einschlägige Interessenträger und Organisationen sind aufgefordert, der Minderung der Risiken und negativen Auswirkungen des KI-Systems gebührend Rechnung zu tragen. Das erwartete Leistungskennzahlenniveau sollte in der beigefügten Betriebsanleitung angegeben werden. Die Anbieter werden nachdrücklich aufgefordert, diese Informationen den Betreibern in klarer und leicht verständlicher Weise ohne Missverständnisse oder irreführende Aussagen zu übermitteln. Die Rechtsvorschriften der Union zum gesetzlichen Messwesen, einschließlich der Richtlinien 2014/31/EU (Fußnote 35) und 2014/32/EU des Europäischen Parlaments und des Rates (Fußnote 36), zielt darauf ab, die Genauigkeit von Messungen sicherzustellen und die Transparenz und Fairness im Geschäftsverkehr zu fördern. In diesem Zusammenhang sollte die Kommission in Zusammenarbeit mit einschlägigen Interessenträgern und Organisationen, wie Metrologie- und Benchmarking-Behörden, gegebenenfalls die Entwicklung von Benchmarks und Messmethoden für KI-Systeme fördern. Dabei sollte die Kommission internationale Partner, die an Metrologie und einschlägigen Messindikatoren für KI arbeiten, beachten und mit ihnen zusammenarbeiten. Fußnote 35: Richtlinie 2014/31/EU des Europäischen Parlaments und des Rates vom 26. Februar 2014 zur Angleichung der Rechtsvorschriften der Mitgliedstaaten betreffend die Bereitstellung nichtselbsttätiger Waagen auf dem Markt (ABl. L 96 vom 29.3.2014, S. 107)., Fußnote 36: Richtlinie 2014/32/EU des Europäischen Parlaments und des Rates vom 26. Februar 2014 zur Harmonisierung der Rechtsvorschriften der Mitgliedstaaten über die Bereitstellung von Messgeräten auf dem Markt (ABl. L 96 vom 29.3.2014, S. 149)."
    },
    {
      "chunk_idx": 74,
      "id": "c9ddca61-d229-4d40-b57a-f7c249377ecb",
      "title": "Recital 75",
      "relevantChunksIds": [
        "91bb9b2c-b571-4042-8143-f399e538ffe2"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(75) Technical robustness is a key requirement for high-risk AI systems. They should be resilient in relation to harmful or otherwise undesirable behaviour that may result from limitations within the systems or the environment in which the systems operate (e.g. errors, faults, inconsistencies, unexpected situations). Therefore, technical and organisational measures should be taken to ensure robustness of high-risk AI systems, for example by designing and developing appropriate technical solutions to prevent or minimise harmful or otherwise undesirable behaviour. Those technical solution may include for instance mechanisms enabling the system to safely interrupt its operation (fail-safe plans) in the presence of certain anomalies or when operation takes place outside certain predetermined boundaries. Failure to protect against these risks could lead to safety impacts or negatively affect the fundamental rights, for example due to erroneous decisions or wrong or biased outputs generated by the AI system.",
      "original_content": "(75) Die technische Robustheit ist eine wesentliche Voraussetzung für Hochrisiko-KI-Systeme. Sie sollten widerstandsfähig in Bezug auf schädliches oder anderweitig unerwünschtes Verhalten sein, das sich aus Einschränkungen innerhalb der Systeme oder der Umgebung, in der die Systeme betrieben werden, ergeben kann (z. B. Fehler, Störungen, Unstimmigkeiten, unerwartete Situationen). Daher sollten technische und organisatorische Maßnahmen ergriffen werden, um die Robustheit von Hochrisiko-KI-Systemen sicherzustellen, indem beispielsweise geeignete technische Lösungen konzipiert und entwickelt werden, um schädliches oder anderweitig unerwünschtes Verhalten zu verhindern oder zu minimieren. Zu diesen technischen Lösungen können beispielsweise Mechanismen gehören, die es dem System ermöglichen, seinen Betrieb bei bestimmten Anomalien oder beim Betrieb außerhalb bestimmter vorab festgelegter Grenzen sicher zu unterbrechen (Störungssicherheitspläne). Ein fehlender Schutz vor diesen Risiken könnte die Sicherheit beeinträchtigen oder sich negativ auf die Grundrechte auswirken, wenn das KI-System beispielsweise falsche Entscheidungen trifft oder falsche oder verzerrte Ausgaben hervorbringt."
    },
    {
      "chunk_idx": 75,
      "id": "f13362fc-1e4e-4ac2-80ec-5def8ff64f2a",
      "title": "Recital 76",
      "relevantChunksIds": [
        "91bb9b2c-b571-4042-8143-f399e538ffe2"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(76) Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behaviour, performance or compromise their security properties by malicious third parties exploiting the system’s vulnerabilities. Cyberattacks against AI systems can leverage AI specific assets, such as training data sets (e.g. data poisoning) or trained models (e.g. adversarial attacks or membership inference), or exploit vulnerabilities in the AI system’s digital assets or the underlying ICT infrastructure. To ensure a level of cybersecurity appropriate to the risks, suitable measures, such as security controls, should therefore be taken by the providers of high-risk AI systems, also taking into account as appropriate the underlying ICT infrastructure.",
      "original_content": "(76) Die Cybersicherheit spielt eine entscheidende Rolle, wenn es darum geht, sicherzustellen, dass KI-Systeme widerstandsfähig gegenüber Versuchen böswilliger Dritter sind, unter Ausnutzung der Schwachstellen der Systeme deren Verwendung, Verhalten, Leistung zu verändern oder ihre Sicherheitsmerkmale zu beeinträchtigen. Cyberangriffe auf KI-Systeme können KI-spezifische Ressourcen wie Trainingsdatensätze (z. B. Datenvergiftung) oder trainierte Modelle (z. B. feindliche Angriffe oder Inferenzangriffe auf Mitgliederdaten) nutzen oder Schwachstellen in den digitalen Ressourcen des KI-Systems oder der zugrunde liegenden IKT-Infrastruktur ausnutzen. Um ein den Risiken angemessenes Cybersicherheitsniveau zu gewährleisten, sollten die Anbieter von Hochrisiko-KI-Systemen daher geeignete Maßnahmen, etwa Sicherheitskontrollen, ergreifen, wobei gegebenenfalls auch die zugrunde liegende IKT-Infrastruktur zu berücksichtigen ist."
    },
    {
      "chunk_idx": 76,
      "id": "3c1ccff0-1d4e-4217-9828-90e5bb65d611",
      "title": "Recital 77",
      "relevantChunksIds": [
        "91bb9b2c-b571-4042-8143-f399e538ffe2"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(77) Without prejudice to the requirements related to robustness and accuracy set out in this Regulation, high-risk AI systems which fall within the scope of a regulation of the European Parliament and of the Council on horizontal cybersecurity requirements for products with digital elements, in accordance with that regulation may demonstrate compliance with the cybersecurity requirements of this Regulation by fulfilling the essential cybersecurity requirements set out in that regulation. When high-risk AI systems fulfil the essential requirements of a regulation of the European Parliament and of the Council on horizontal cybersecurity requirements for products with digital elements, they should be deemed compliant with the cybersecurity requirements set out in this Regulation in so far as the achievement of those requirements is demonstrated in the EU declaration of conformity or parts thereof issued under that regulation. To that end, the assessment of the cybersecurity risks, associated to a product with digital elements classified as high-risk AI system according to this Regulation, carried out under a regulation of the European Parliament and of the Council on horizontal cybersecurity requirements for products with digital elements, should consider risks to the cyber resilience of an AI system as regards attempts by unauthorised third parties to alter its use, behaviour or performance, including AI specific vulnerabilities such as data poisoning or adversarial attacks, as well as, as relevant, risks to fundamental rights as required by this Regulation.",
      "original_content": "(77) Unbeschadet der in dieser Verordnung festgelegten Anforderungen an Robustheit und Genauigkeit können Hochrisiko-AI-Systeme, die in den Geltungsbereich einer Verordnung des Europäischen Parlaments und des Rates über horizontale Cybersicherheitsanforderungen für Produkte mit digitalen Elementen gemäß der genannten Verordnung fallen, die Erfüllung der Cybersicherheitsanforderungen der vorliegenden Verordnung nachweisen, indem sie die in der genannten Verordnung festgelegten grundlegenden Cybersicherheitsanforderungen erfüllen. Wenn Hochrisiko-KI-Systeme die grundlegenden Anforderungen einer Verordnung des Europäischen Parlaments und des Rates über horizontale Cybersicherheitsanforderungen für Produkte mit digitalen Elementen erfüllen, sollten sie als die in der vorliegenden Verordnung festgelegten Cybersicherheitsanforderungen erfüllend gelten, soweit die Erfüllung der genannten Anforderungen in der gemäß der genannten Verordnung ausgestellten EU-Konformitätserklärung oder in Teilen davon nachgewiesen wird. Zu diesem Zweck sollten bei der im Rahmen einer Verordnung des Europäischen Parlaments und des Rates über horizontale Cybersicherheitsanforderungen für Produkte mit digitalen Elementen durchgeführten Bewertung der Cybersicherheitsrisiken, die mit einem gemäß der vorliegenden Verordnung als Hochrisiko-KI-System eingestuften Produkt mit digitalen Elementen verbunden sind, Risiken für die Cyberabwehrfähigkeit eines KI-Systems in Bezug auf Versuche unbefugter Dritter, seine Verwendung, sein Verhalten oder seine Leistung zu verändern, einschließlich KI-spezifischer Schwachstellen wie Datenvergiftung oder feindlicher Angriffe, sowie gegebenenfalls Risiken für die Grundrechte gemäß der vorliegenden Verordnung berücksichtigt werden."
    },
    {
      "chunk_idx": 77,
      "id": "63c8bc3d-ba5f-4134-97e6-4294cf1e2697",
      "title": "Recital 78",
      "relevantChunksIds": [
        "91bb9b2c-b571-4042-8143-f399e538ffe2"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(78) The conformity assessment procedure provided by this Regulation should apply in relation to the essential cybersecurity requirements of a product with digital elements covered by a regulation of the European Parliament and of the Council on horizontal cybersecurity requirements for products with digital elements and classified as a high-risk AI system under this Regulation. However, this rule should not result in reducing the necessary level of assurance for critical products with digital elements covered by a regulation of the European Parliament and of the Council on horizontal cybersecurity requirements for products with digital elements. Therefore, by way of derogation from this rule, high-risk AI systems that fall within the scope of this Regulation and are also qualified as important and critical products with digital elements pursuant to a regulation of the European Parliament and of the Council on horizontal cybersecurity requirements for products with digital elements and to which the conformity assessment procedure based on internal control set out in an annex to this Regulation applies, are subject to the conformity assessment provisions of a regulation of the European Parliament and of the Council on horizontal cybersecurity requirements for products with digital elements insofar as the essential cybersecurity requirements of that regulation are concerned. In this case, for all the other aspects covered by this Regulation the respective provisions on conformity assessment based on internal control set out in an annex to this Regulation should apply. Building on the knowledge and expertise of ENISA on the cybersecurity policy and tasks assigned to ENISA under the Regulation (EU) 2019/881 of the European Parliament and of the Council (37), the Commission should cooperate with ENISA on issues related to cybersecurity of AI systems.",
      "original_content": "(78) Das in dieser Verordnung vorgesehene Konformitätsbewertungsverfahren sollte in Bezug auf die grundlegenden Cybersicherheitsanforderungen an ein Produkt mit digitalen Elementen, das unter eine Verordnung des Europäischen Parlaments und des Rates über horizontale Cybersicherheitsanforderungen für Produkte mit digitalen Elementen fällt und gemäß der vorliegenden Verordnung als Hochrisiko-KI-System eingestuft ist, gelten. Diese Regel sollte jedoch nicht dazu führen, dass die erforderliche Vertrauenswürdigkeit für unter eine Verordnung des Europäischen Parlaments und des Rates über horizontale Cybersicherheitsanforderungen für Produkte mit digitalen Elementen fallende kritische Produkte mit digitalen Elementen verringert wird. Daher unterliegen abweichend von dieser Regel Hochrisiko-KI-Systeme, die in den Anwendungsbereich der vorliegenden Verordnung fallen und gemäß einer Verordnung des Europäischen Parlaments und des Rates über horizontale Cybersicherheitsanforderungen für Produkte mit digitalen Elementen als wichtige und kritische Produkte mit digitalen Elementen eingestuft werden und für die das Konformitätsbewertungsverfahren auf der Grundlage der internen Kontrolle gemäß einem Anhang der vorliegenden Verordnung gilt, den Konformitätsbewertungsbestimmungen einer Verordnung des Europäischen Parlaments und des Rates über horizontale Cybersicherheitsanforderungen für Produkte mit digitalen Elementen, soweit die wesentlichen Cybersicherheitsanforderungen der genannten Verordnung betroffen sind. In diesem Fall sollten für alle anderen Aspekte, die unter die vorliegende Verordnung fallen, die entsprechenden Bestimmungen über die Konformitätsbewertung auf der Grundlage der internen Kontrolle gelten, die in einem Anhang der vorliegenden Verordnung festgelegt sind. Aufbauend auf den Kenntnissen und dem Fachwissen der ENISA in Bezug auf die Cybersicherheitspolitik und die der ENISA gemäß der Verordnung (EU) 2019/881 des Europäischen Parlaments und des Rates (Fußnote 37) übertragenen Aufgaben sollte die Kommission in Fragen im Zusammenhang mit der Cybersicherheit von KI-Systemen mit der ENISA zusammenarbeiten. Fußnote 37: Verordnung (EU) 2019/881 des Europäischen Parlaments und des Rates vom 17. April 2019 über die ENISA (Agentur der Europäischen Union für Cybersicherheit) und über die Zertifizierung der Cybersicherheit von Informations- und Kommunikationstechnik und zur Aufhebung der Verordnung (EU) Nr. 526/2013 (Rechtsakt zur Cybersicherheit) (ABl. L 151 vom 7.6.2019, S. 15)."
    },
    {
      "chunk_idx": 78,
      "id": "d5118a3b-1529-4362-9b1b-c99a8dd748ec",
      "title": "Recital 79",
      "relevantChunksIds": [
        "86806ab4-b869-4e14-8e9f-39249fac5a50"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(79) It is appropriate that a specific natural or legal person, defined as the provider, takes responsibility for the placing on the market or the putting into service of a high-risk AI system, regardless of whether that natural or legal person is the person who designed or developed the system.",
      "original_content": "(79) Es ist angezeigt, dass eine bestimmte als Anbieter definierte natürliche oder juristische Person die Verantwortung für das Inverkehrbringen oder die Inbetriebnahme eines Hochrisiko-KI-Systems übernimmt, unabhängig davon, ob es sich bei dieser natürlichen oder juristischen Person um die Person handelt, die das System konzipiert oder entwickelt hat."
    },
    {
      "chunk_idx": 79,
      "id": "0fa0541d-b4e5-48ce-b134-aff5e22c8eec",
      "title": "Recital 80",
      "relevantChunksIds": [
        "86806ab4-b869-4e14-8e9f-39249fac5a50"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(80) As signatories to the United Nations Convention on the Rights of Persons with Disabilities, the Union and the Member States are legally obliged to protect persons with disabilities from discrimination and promote their equality, to ensure that persons with disabilities have access, on an equal basis with others, to information and communications technologies and systems, and to ensure respect for privacy for persons with disabilities. Given the growing importance and use of AI systems, the application of universal design principles to all new technologies and services should ensure full and equal access for everyone potentially affected by or using AI technologies, including persons with disabilities, in a way that takes full account of their inherent dignity and diversity. It is therefore essential that providers ensure full compliance with accessibility requirements, including Directive (EU) 2016/2102 of the European Parliament and of the Council (38) and Directive (EU) 2019/882. Providers should ensure compliance with these requirements by design. Therefore, the necessary measures should be integrated as much as possible into the design of the high-risk AI system.",
      "original_content": "(80) Als Unterzeichner des Übereinkommens über die Rechte von Menschen mit Behinderungen der Vereinten Nationen sind die Union und alle Mitgliedstaaten rechtlich verpflichtet, Menschen mit Behinderungen vor Diskriminierung zu schützen und ihre Gleichstellung zu fördern, sicherzustellen, dass Menschen mit Behinderungen gleichberechtigt Zugang zu Informations- und Kommunikationstechnologien und -systemen haben, und die Achtung der Privatsphäre von Menschen mit Behinderungen sicherzustellen. Angesichts der zunehmenden Bedeutung und Nutzung von KI-Systemen sollte die strikte Anwendung der Grundsätze des universellen Designs auf alle neuen Technologien und Dienste einen vollständigen und gleichberechtigten Zugang für alle Menschen sicherstellen, die potenziell von KI-Technologien betroffen sind oder diese nutzen, einschließlich Menschen mit Behinderungen, und zwar in einer Weise, die ihrer Würde und Vielfalt in vollem Umfang Rechnung trägt. Es ist daher von wesentlicher Bedeutung, dass die Anbieter die uneingeschränkte Einhaltung der Barrierefreiheitsanforderungen sicherstellen, einschließlich der in der Richtlinie (EU) 2016/2102 des Europäischen Parlaments und des Rates (Fußnote 38) und in der Richtlinie (EU) 2019/882 festgelegten Anforderungen. Die Anbieter sollten die Einhaltung dieser Anforderungen durch Voreinstellungen sicherstellen. Die erforderlichen Maßnahmen sollten daher so weit wie möglich in die Konzeption von Hochrisiko-KI-Systemen integriert werden. Fußnote 38: Richtlinie (EU) 2016/2102 des Europäischen Parlaments und des Rates vom 26. Oktober 2016 über den barrierefreien Zugang zu den Websites und mobilen Anwendungen öffentlicher Stellen (ABl. L 327 vom 2.12.2016, S. 1)."
    },
    {
      "chunk_idx": 80,
      "id": "da5fac2f-a0db-40a4-8e4c-03c6125fa585",
      "title": "Recital 81",
      "relevantChunksIds": [
        "9cd5b042-8095-49be-8ad3-51f2fb94e473",
        "b4faa5d7-2c2b-43c9-b793-47d6579e2fc8"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(81) The provider should establish a sound quality management system, ensure the accomplishment of the required conformity assessment procedure, draw up the relevant documentation and establish a robust post-market monitoring system. Providers of high-risk AI systems that are subject to obligations regarding quality management systems under relevant sectoral Union law should have the possibility to include the elements of the quality management system provided for in this Regulation as part of the existing quality management system provided for in that other sectoral Union law. The complementarity between this Regulation and existing sectoral Union law should also be taken into account in future standardisation activities or guidance adopted by the Commission. Public authorities which put into service high-risk AI systems for their own use may adopt and implement the rules for the quality management system as part of the quality management system adopted at a national or regional level, as appropriate, taking into account the specificities of the sector and the competences and organisation of the public authority concerned.",
      "original_content": "(81) Der Anbieter sollte ein solides Qualitätsmanagementsystem einrichten, die Durchführung des vorgeschriebenen Konformitätsbewertungsverfahrens sicherstellen, die einschlägige Dokumentation erstellen und ein robustes System zur Beobachtung nach dem Inverkehrbringen einrichten. Anbieter von Hochrisiko-KI-Systemen, die Pflichten in Bezug auf Qualitätsmanagementsysteme gemäß den einschlägigen sektorspezifischen Rechtsvorschriften der Union unterliegen, sollten die Möglichkeit haben, die Elemente des in dieser Verordnung vorgesehenen Qualitätsmanagementsystems als Teil des bestehenden, in diesen anderen sektoralen Rechtsvorschriften der Union vorgesehen Qualitätsmanagementsystems aufzunehmen. Auch bei künftigen Normungstätigkeiten oder Leitlinien, die von der Kommission angenommen werden, sollte der Komplementarität zwischen dieser Verordnung und den bestehenden sektorspezifischen Rechtsvorschriften der Union Rechnung getragen werden. Behörden, die Hochrisiko-KI-Systeme für den Eigengebrauch in Betrieb nehmen, können unter Berücksichtigung der Besonderheiten des Bereichs sowie der Zuständigkeiten und der Organisation der besagten Behörde die Vorschriften für das Qualitätsmanagementsystem als Teil des auf nationaler oder regionaler Ebene eingesetzten Qualitätsmanagementsystems annehmen und umsetzen."
    },
    {
      "chunk_idx": 81,
      "id": "86e3f5bd-eef6-4109-ab46-aedd548f4a71",
      "title": "Recital 82",
      "relevantChunksIds": [
        "19b32d95-81d7-4097-8ca6-ca2c6c0358df",
        "49e88b0c-2141-4832-bc84-ee8b5fef7d4f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(82) To enable enforcement of this Regulation and create a level playing field for operators, and, taking into account the different forms of making available of digital products, it is important to ensure that, under all circumstances, a person established in the Union can provide authorities with all the necessary information on the compliance of an AI system. Therefore, prior to making their AI systems available in the Union, providers established in third countries should, by written mandate, appoint an authorised representative established in the Union. This authorised representative plays a pivotal role in ensuring the compliance of the high-risk AI systems placed on the market or put into service in the Union by those providers who are not established in the Union and in serving as their contact person established in the Union.",
      "original_content": "(82) Um die Durchsetzung dieser Verordnung zu ermöglichen und gleiche Wettbewerbsbedingungen für die Akteure zu schaffen, muss unter Berücksichtigung der verschiedenen Formen der Bereitstellung digitaler Produkte sichergestellt sein, dass unter allen Umständen eine in der Union niedergelassene Person den Behörden alle erforderlichen Informationen über die Konformität eines KI-Systems zur Verfügung stellen kann. Daher sollten Anbieter, die in Drittländern niedergelassen sind, vor der Bereitstellung ihrer KI-Systeme in der Union schriftlich einen in der Union niedergelassenen Bevollmächtigten benennen. Dieser Bevollmächtigte spielt eine zentrale Rolle bei der Gewährleistung der Konformität der von den betreffenden Anbietern, die nicht in der Union niedergelassen sind, in der Union in Verkehr gebrachten oder in Betrieb genommenen Hochrisiko-KI-Systeme und indem er als ihr in der Union niedergelassener Ansprechpartner dient."
    },
    {
      "chunk_idx": 82,
      "id": "0a83841c-2931-4e56-a212-b73e3ab410f1",
      "title": "Recital 83",
      "relevantChunksIds": [
        "9d898b6d-782d-4626-8838-1296950b5b7c",
        "984b9ce1-d58b-474f-8062-48d47f385b5d",
        "63f69c82-a393-43d5-bf47-0ea7b61acc5f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(83) In light of the nature and complexity of the value chain for AI systems and in line with the New Legislative Framework, it is essential to ensure legal certainty and facilitate the compliance with this Regulation. Therefore, it is necessary to clarify the role and the specific obligations of relevant operators along that value chain, such as importers and distributors who may contribute to the development of AI systems. In certain situations those operators could act in more than one role at the same time and should therefore fulfil cumulatively all relevant obligations associated with those roles. For example, an operator could act as a distributor and an importer at the same time.",
      "original_content": "(83) Angesichts des Wesens und der Komplexität der Wertschöpfungskette für KI-Systeme und im Einklang mit dem neuen Rechtsrahmen ist es von wesentlicher Bedeutung, Rechtssicherheit zu gewährleisten und die Einhaltung dieser Verordnung zu erleichtern. Daher müssen die Rolle und die spezifischen Pflichten der relevanten Akteure entlang dieser Wertschöpfungskette, wie Einführer und Händler, die zur Entwicklung von KI-Systemen beitragen können, präzisiert werden. In bestimmten Situationen könnten diese Akteure mehr als eine Rolle gleichzeitig wahrnehmen und sollten daher alle einschlägigen Pflichten, die mit diesen Rollen verbunden sind, kumulativ erfüllen. So könnte ein Akteur beispielsweise gleichzeitig als Händler und als Einführer auftreten."
    },
    {
      "chunk_idx": 83,
      "id": "89b37334-a989-4eca-bb8e-55165382038d",
      "title": "Recital 84",
      "relevantChunksIds": [
        "63f69c82-a393-43d5-bf47-0ea7b61acc5f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(84) To ensure legal certainty, it is necessary to clarify that, under certain specific conditions, any distributor, importer, deployer or other third-party should be considered to be a provider of a high-risk AI system and therefore assume all the relevant obligations. This would be the case if that party puts its name or trademark on a high-risk AI system already placed on the market or put into service, without prejudice to contractual arrangements stipulating that the obligations are allocated otherwise. This would also be the case if that party makes a substantial modification to a high-risk AI system that has already been placed on the market or has already been put into service in a way that it remains a high-risk AI system in accordance with this Regulation, or if it modifies the intended purpose of an AI system, including a general-purpose AI system, which has not been classified as high-risk and has already been placed on the market or put into service, in a way that the AI system becomes a high-risk AI system in accordance with this Regulation. Those provisions should apply without prejudice to more specific provisions established in certain Union harmonisation legislation based on the New Legislative Framework, together with which this Regulation should apply. For example, Article 16(2) of Regulation (EU) 2017/745, establishing that certain changes should not be considered to be modifications of a device that could affect its compliance with the applicable requirements, should continue to apply to high-risk AI systems that are medical devices within the meaning of that Regulation.",
      "original_content": "(84) Um Rechtssicherheit zu gewährleisten, muss präzisiert werden, dass unter bestimmten spezifischen Bedingungen jeder Händler, Einführer, Betreiber oder andere Dritte als Anbieter eines Hochrisiko-KI-Systems betrachtet werden und daher alle einschlägigen Pflichten erfüllen sollte. Dies wäre auch der Fall, wenn diese Partei ein bereits in Verkehr gebrachtes oder in Betrieb genommenes Hochrisiko-KI-System mit ihrem Namen oder ihrer Handelsmarke versieht, unbeschadet vertraglicher Vereinbarungen, die eine andere Aufteilung der Pflichten vorsehen, oder wenn sie eine wesentliche Veränderung des Hochrisiko-KI-Systems, das bereits in Verkehr gebracht oder bereits in Betrieb genommen wurde, so vornimmt, dass es ein Hochrisiko-KI-System im Sinne dieser Verordnung bleibt, oder wenn sie die Zweckbestimmung eines KI-Systems, einschließlich eines KI-Systems mit allgemeinem Verwendungszweck, das nicht als Hochrisiko-KI-System eingestuft wurde und bereits in Verkehr gebracht oder in Betrieb genommen wurde, so verändert, dass das KI-System zu einem Hochrisiko-KI-System im Sinne dieser Verordnung wird. Diese Bestimmungen sollten unbeschadet spezifischerer Bestimmungen in bestimmten Harmonisierungsrechtsvorschriften der Union auf Grundlage des neuen Rechtsrahmens gelten, mit denen diese Verordnung zusammen gelten sollte. So sollte beispielsweise Artikel 16 Absatz 2 der Verordnung (EU) 2017/745, wonach bestimmte Änderungen nicht als eine Änderung des Produkts, die Auswirkungen auf seine Konformität mit den geltenden Anforderungen haben könnte, gelten sollten, weiterhin auf Hochrisiko-KI-Systeme angewandt werden, bei denen es sich um Medizinprodukte im Sinne der genannten Verordnung handelt."
    },
    {
      "chunk_idx": 84,
      "id": "3ae98876-3fe7-4515-8012-b96f73db643a",
      "title": "Recital 85",
      "relevantChunksIds": [
        "63f69c82-a393-43d5-bf47-0ea7b61acc5f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(85) General-purpose AI systems may be used as high-risk AI systems by themselves or be components of other high-risk AI systems. Therefore, due to their particular nature and in order to ensure a fair sharing of responsibilities along the AI value chain, the providers of such systems should, irrespective of whether they may be used as high-risk AI systems as such by other providers or as components of high-risk AI systems and unless provided otherwise under this Regulation, closely cooperate with the providers of the relevant high-risk AI systems to enable their compliance with the relevant obligations under this Regulation and with the competent authorities established under this Regulation.",
      "original_content": "(85) KI-Systeme mit allgemeinem Verwendungszweck können als eigenständige Hochrisiko-KI-Systeme eingesetzt werden oder Komponenten anderer Hochrisiko-KI-Systemen sein. Daher sollten, aufgrund der besonderen Merkmale dieser KI-Systeme und um für eine gerechte Verteilung der Verantwortlichkeiten entlang der KI-Wertschöpfungskette zu sorgen, Anbieter solcher Systeme, unabhängig davon, ob sie von anderen Anbietern als eigenständige Hochrisiko-KI-Systeme oder als Komponenten von Hochrisiko-KI-Systemen verwendet werden können, und sofern in dieser Verordnung nichts anderes bestimmt ist, eng mit den Anbietern der relevanten Hochrisiko-KI-Systeme, um ihnen die Einhaltung der entsprechenden Pflichten aus dieser Verordnung zu ermöglichen, und mit den gemäß dieser Verordnung eingerichteten zuständigen Behörden zusammenarbeiten."
    },
    {
      "chunk_idx": 85,
      "id": "116d04ba-85cc-42e0-9e8e-ac7f1460a22d",
      "title": "Recital 86",
      "relevantChunksIds": [
        "63f69c82-a393-43d5-bf47-0ea7b61acc5f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(86) Where, under the conditions laid down in this Regulation, the provider that initially placed the AI system on the market or put it into service should no longer be considered to be the provider for the purposes of this Regulation, and when that provider has not expressly excluded the change of the AI system into a high-risk AI system, the former provider should nonetheless closely cooperate and make available the necessary information and provide the reasonably expected technical access and other assistance that are required for the fulfilment of the obligations set out in this Regulation, in particular regarding the compliance with the conformity assessment of high-risk AI systems.",
      "original_content": "(86) Sollte der Anbieter, der das KI-System ursprünglich in Verkehr gebracht oder in Betrieb genommen hat, unter den in dieser Verordnung festgelegten Bedingungen nicht mehr als Anbieter im Sinne dieser Verordnung gelten und hat jener Anbieter die Änderung des KI-Systems in ein Hochrisiko-KI-System nicht ausdrücklich ausgeschlossen, so sollte der erstgenannte Anbieter dennoch eng zusammenarbeiten, die erforderlichen Informationen zur Verfügung stellen und den vernünftigerweise erwarteten technischen Zugang und sonstige Unterstützung leisten, die für die Erfüllung der in dieser Verordnung festgelegten Pflichten, insbesondere in Bezug auf die Konformitätsbewertung von Hochrisiko-KI-Systemen, erforderlich sind."
    },
    {
      "chunk_idx": 86,
      "id": "4123b06a-0100-490b-9631-66e3f5f66ba2",
      "title": "Recital 87",
      "relevantChunksIds": [
        "63f69c82-a393-43d5-bf47-0ea7b61acc5f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(87) In addition, where a high-risk AI system that is a safety component of a product which falls within the scope of Union harmonisation legislation based on the New Legislative Framework is not placed on the market or put into service independently from the product, the product manufacturer defined in that legislation should comply with the obligations of the provider established in this Regulation and should, in particular, ensure that the AI system embedded in the final product complies with the requirements of this Regulation.",
      "original_content": "(87) Zusätzlich sollte, wenn ein Hochrisiko-KI-System, bei dem es sich um ein Sicherheitsbauteil eines Produkts handelt, das in den Geltungsbereich von Harmonisierungsrechtsvorschriften der Union auf Grundlage des neuen Rechtsrahmens fällt, nicht unabhängig von dem Produkt in Verkehr gebracht oder in Betrieb genommen wird, der Produkthersteller im Sinne der genannten Rechtsvorschriften die in der vorliegenden Verordnung festgelegten Anbieterpflichten erfüllen und sollte insbesondere sicherstellen, dass das in das Endprodukt eingebettete KI-System den Anforderungen dieser Verordnung entspricht."
    },
    {
      "chunk_idx": 87,
      "id": "d5cb7544-49fd-4991-8ab4-23d8dab0ff08",
      "title": "Recital 88",
      "relevantChunksIds": [
        "63f69c82-a393-43d5-bf47-0ea7b61acc5f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(88) Along the AI value chain multiple parties often supply AI systems, tools and services but also components or processes that are incorporated by the provider into the AI system with various objectives, including the model training, model retraining, model testing and evaluation, integration into software, or other aspects of model development. Those parties have an important role to play in the value chain towards the provider of the high-risk AI system into which their AI systems, tools, services, components or processes are integrated, and should provide by written agreement this provider with the necessary information, capabilities, technical access and other assistance based on the generally acknowledged state of the art, in order to enable the provider to fully comply with the obligations set out in this Regulation, without compromising their own intellectual property rights or trade secrets.",
      "original_content": "(88) Entlang der KI-Wertschöpfungskette liefern häufig mehrere Parteien KI-Systeme, Instrumente und Dienstleistungen, aber auch Komponenten oder Prozesse, die vom Anbieter zu diversen Zwecken in das KI-System integriert werden; dazu gehören das Trainieren, Neutrainieren, Testen und Bewerten von Modellen, die Integration in Software oder andere Aspekte der Modellentwicklung. Diese Parteien haben eine wichtige Rolle in der Wertschöpfungskette gegenüber dem Anbieter des Hochrisiko-KI-Systems, in das ihre KI-Systeme, Instrumente, Dienste, Komponenten oder Verfahren integriert werden, und sollten in einer schriftlichen Vereinbarung die Informationen, die Fähigkeiten, den technischen Zugang und die sonstige Unterstützung nach dem allgemein anerkannten Stand der Technik bereitstellen, die erforderlich sind, damit der Anbieter die in dieser Verordnung festgelegten Pflichten vollständig erfüllen kann, ohne seine eigenen Rechte des geistigen Eigentums oder Geschäftsgeheimnisse zu gefährden."
    },
    {
      "chunk_idx": 88,
      "id": "dfe0e9c1-22e5-4ba8-ad77-a65c24cee14d",
      "title": "Recital 89",
      "relevantChunksIds": [
        "63f69c82-a393-43d5-bf47-0ea7b61acc5f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(89) Third parties making accessible to the public tools, services, processes, or AI components other than general-purpose AI models, should not be mandated to comply with requirements targeting the responsibilities along the AI value chain, in particular towards the provider that has used or integrated them, when those tools, services, processes, or AI components are made accessible under a free and open-source licence. Developers of free and open-source tools, services, processes, or AI components other than general-purpose AI models should be encouraged to implement widely adopted documentation practices, such as model cards and data sheets, as a way to accelerate information sharing along the AI value chain, allowing the promotion of trustworthy AI systems in the Union.",
      "original_content": "(89) Dritte, die Instrumente, Dienste, Verfahren oder Komponenten, bei denen es sich nicht um KI-Modelle mit allgemeinem Verwendungszweck handelt, öffentlich zugänglich machen, sollten nicht dazu verpflichtet werden, Anforderungen zu erfüllen, die auf die Verantwortlichkeiten entlang der KI-Wertschöpfungskette ausgerichtet sind, insbesondere gegenüber dem Anbieter, der sie genutzt oder integriert hat, wenn diese Instrumente, Dienste, Verfahren oder KI-Komponenten im Rahmen einer freien und quelloffenen Lizenz zugänglich gemacht werden. Die Entwickler von freien und quelloffenen Instrumenten, Diensten, Verfahren oder KI-Komponenten, bei denen es sich nicht um KI-Modelle mit allgemeinem Verwendungszweck handelt, sollten dazu ermutigt werden, weit verbreitete Dokumentationsverfahren, wie z. B. Modellkarten und Datenblätter, als Mittel dazu einzusetzen, den Informationsaustausch entlang der KI-Wertschöpfungskette zu beschleunigen, sodass vertrauenswürdige KI-Systeme in der Union gefördert werden können."
    },
    {
      "chunk_idx": 89,
      "id": "a513a6fc-8abc-4dd4-b7b9-1b2c4520a19c",
      "title": "Recital 90",
      "relevantChunksIds": [
        "63f69c82-a393-43d5-bf47-0ea7b61acc5f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(90) The Commission could develop and recommend voluntary model contractual terms between providers of high-risk AI systems and third parties that supply tools, services, components or processes that are used or integrated in high-risk AI systems, to facilitate the cooperation along the value chain. When developing voluntary model contractual terms, the Commission should also take into account possible contractual requirements applicable in specific sectors or business cases.",
      "original_content": "(90) Die Kommission könnte freiwillige Mustervertragsbedingungen für Verträge zwischen Anbietern von Hochrisiko-KI-Systemen und Dritten ausarbeiten und empfehlen, in deren Rahmen Instrumente, Dienste, Komponenten oder Verfahren bereitgestellt werden, die für Hochrisiko-KI-Systeme verwendet oder in diese integriert werden, um die Zusammenarbeit entlang der Wertschöpfungskette zu erleichtern. Bei der Ausarbeitung dieser freiwilligen Mustervertragsbedingungen sollte die Kommission auch mögliche vertragliche Anforderungen berücksichtigen, die in bestimmten Sektoren oder Geschäftsfällen gelten."
    },
    {
      "chunk_idx": 90,
      "id": "c7976729-6c7e-451a-8143-a8a52257644d",
      "title": "Recital 91",
      "relevantChunksIds": [
        "54d92cd3-752d-4303-b8e9-f8c7efac0e64"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(91) Given the nature of AI systems and the risks to safety and fundamental rights possibly associated with their use, including as regards the need to ensure proper monitoring of the performance of an AI system in a real-life setting, it is appropriate to set specific responsibilities for deployers. Deployers should in particular take appropriate technical and organisational measures to ensure they use high-risk AI systems in accordance with the instructions of use and certain other obligations should be provided for with regard to monitoring of the functioning of the AI systems and with regard to record-keeping, as appropriate. Furthermore, deployers should ensure that the persons assigned to implement the instructions for use and human oversight as set out in this Regulation have the necessary competence, in particular an adequate level of AI literacy, training and authority to properly fulfil those tasks. Those obligations should be without prejudice to other deployer obligations in relation to high-risk AI systems under Union or national law.",
      "original_content": "(91) Angesichts des Charakters von KI-Systemen und der Risiken für die Sicherheit und die Grundrechte, die mit ihrer Verwendung verbunden sein können, ist es angezeigt, besondere Zuständigkeiten für die Betreiber festzulegen, auch im Hinblick darauf, dass eine angemessene Beobachtung der Leistung eines KI-Systems unter Realbedingungen sichergestellt werden muss. Die Betreiber sollten insbesondere geeignete technische und organisatorische Maßnahmen treffen, um sicherzustellen, dass sie Hochrisiko-KI-Systeme gemäß den Betriebsanleitungen verwenden, und es sollten bestimmte andere Pflichten in Bezug auf die Überwachung der Funktionsweise der KI-Systeme und gegebenenfalls auch Aufzeichnungspflichten festgelegt werden. Darüber hinaus sollten die Betreiber sicherstellen, dass die Personen, denen die Umsetzung der Betriebsanleitungen und die menschliche Aufsicht gemäß dieser Verordnung übertragen wurde, über die erforderliche Kompetenz verfügen, insbesondere über ein angemessenes Niveau an KI-Kompetenz, Schulung und Befugnis, um diese Aufgaben ordnungsgemäß zu erfüllen. Diese Pflichten sollten sonstige Pflichten des Betreibers in Bezug auf Hochrisiko-KI-Systeme nach Unionsrecht oder nationalem Recht unberührt lassen."
    },
    {
      "chunk_idx": 91,
      "id": "b43808c6-abf1-4af0-8243-6e7e62e8e30b",
      "title": "Recital 92",
      "relevantChunksIds": [
        "54d92cd3-752d-4303-b8e9-f8c7efac0e64"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(92) This Regulation is without prejudice to obligations for employers to inform or to inform and consult workers or their representatives under Union or national law and practice, including Directive 2002/14/EC of the European Parliament and of the Council (39), on decisions to put into service or use AI systems. It remains necessary to ensure information of workers and their representatives on the planned deployment of high-risk AI systems at the workplace where the conditions for those information or information and consultation obligations in other legal instruments are not fulfilled. Moreover, such information right is ancillary and necessary to the objective of protecting fundamental rights that underlies this Regulation. Therefore, an information requirement to that effect should be laid down in this Regulation, without affecting any existing rights of workers.",
      "original_content": "(92) Diese Verordnung lässt Pflichten der Arbeitgeber unberührt, Arbeitnehmer oder ihre Vertreter nach dem Unionsrecht oder nationalem Recht und nationaler Praxis, einschließlich der Richtlinie 2002/14/EG des Europäischen Parlaments und des Rates (Fußnote 39) über Entscheidungen zur Inbetriebnahme oder Nutzung von KI-Systemen zu unterrichten oder zu unterrichten und anzuhören. Es muss nach wie vor sichergestellt werden, dass Arbeitnehmer und ihre Vertreter über die geplante Einführung von Hochrisiko-KI-Systemen am Arbeitsplatz unterrichtet werden, wenn die Bedingungen für diese Pflichten zur Unterrichtung oder zur Unterrichtung und Anhörung gemäß anderen Rechtsinstrumenten nicht erfüllt sind. Darüber hinaus ist dieses Recht, unterrichtet zu werden, ein Nebenrecht und für das Ziel des Schutzes der Grundrechte, das dieser Verordnung zugrunde liegt, erforderlich. Daher sollte in dieser Verordnung eine entsprechende Unterrichtungsanforderung festgelegt werden, ohne bestehende Arbeitnehmerrechte zu beeinträchtigen. Fußnote 39: Richtlinie 2002/14/EG des Europäischen Parlaments und des Rates vom 11. März 2002 zur Festlegung eines allgemeinen Rahmens für die Unterrichtung und Anhörung der Arbeitnehmer in der Europäischen Gemeinschaft (ABl. L 80 vom 23.3.2002, S. 29)."
    },
    {
      "chunk_idx": 92,
      "id": "7188729e-4dec-4ecf-a3b9-8494a9fc716e",
      "title": "Recital 93",
      "relevantChunksIds": [
        "54d92cd3-752d-4303-b8e9-f8c7efac0e64"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(93) Whilst risks related to AI systems can result from the way such systems are designed, risks can as well stem from how such AI systems are used. Deployers of high-risk AI system therefore play a critical role in ensuring that fundamental rights are protected, complementing the obligations of the provider when developing the AI system. Deployers are best placed to understand how the high-risk AI system will be used concretely and can therefore identify potential significant risks that were not foreseen in the development phase, due to a more precise knowledge of the context of use, the persons or groups of persons likely to be affected, including vulnerable groups. Deployers of high-risk AI systems listed in an annex to this Regulation also play a critical role in informing natural persons and should, when they make decisions or assist in making decisions related to natural persons, where applicable, inform the natural persons that they are subject to the use of the high-risk AI system. This information should include the intended purpose and the type of decisions it makes. The deployer should also inform the natural persons about their right to an explanation provided under this Regulation. With regard to high-risk AI systems used for law enforcement purposes, that obligation should be implemented in accordance with Article 13 of Directive (EU) 2016/680.",
      "original_content": "(93) Während Risiken im Zusammenhang mit KI-Systemen einerseits aus der Art und Weise entstehen können, in der solche Systeme konzipiert sind, können sie sich andererseits auch aus der Art und Weise ergeben, in der diese Systeme verwendet werden. Betreiber von Hochrisiko-KI-Systemen spielen daher eine entscheidende Rolle bei der Gewährleistung des Schutzes der Grundrechte in Ergänzung der Pflichten der Anbieter bei der Entwicklung der KI-Systeme. Betreiber können am besten verstehen, wie das Hochrisiko-KI-System konkret eingesetzt wird, und können somit dank einer genaueren Kenntnis des Verwendungskontextes sowie der wahrscheinlich betroffenen Personen oder Personengruppen, einschließlich schutzbedürftiger Gruppen, erhebliche potenzielle Risiken erkennen, die in der Entwicklungsphase nicht vorausgesehen wurden. Betreiber der in einem Anhang dieser Verordnung aufgeführten Hochrisiko-KI-Systeme spielen ebenfalls eine entscheidende Rolle bei der Unterrichtung natürlicher Personen und sollten, wenn sie natürliche Personen betreffende Entscheidungen treffen oder bei solchen Entscheidungen Unterstützung leisten, gegebenenfalls die natürlichen Personen darüber unterrichten, dass sie Gegenstand des Einsatzes des Hochrisiko-KI-Systems sind. Diese Unterrichtung sollte die Zweckbestimmung und die Art der getroffenen Entscheidungen umfassen. Der Betreiber sollte die natürlichen Personen auch über ihr Recht auf eine Erklärung gemäß dieser Verordnung unterrichten. Bei Hochrisiko-KI-Systemen, die zu Strafverfolgungszwecken eingesetzt werden, sollte diese Pflicht im Einklang mit Artikel 13 der Richtlinie (EU) 2016/680 umgesetzt werden."
    },
    {
      "chunk_idx": 93,
      "id": "90669d84-c0e3-4ecb-9c0f-f8bbef4679aa",
      "title": "Recital 94",
      "relevantChunksIds": [
        "54d92cd3-752d-4303-b8e9-f8c7efac0e64"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(94) Any processing of biometric data involved in the use of AI systems for biometric identification for the purpose of law enforcement needs to comply with Article 10 of Directive (EU) 2016/680, that allows such processing only where strictly necessary, subject to appropriate safeguards for the rights and freedoms of the data subject, and where authorised by Union or Member State law. Such use, when authorised, also needs to respect the principles laid down in Article 4 (1) of Directive (EU) 2016/680 including lawfulness, fairness and transparency, purpose limitation, accuracy and storage limitation.",
      "original_content": "(94) Jede Verarbeitung biometrischer Daten im Zusammenhang mit der Verwendung von KI-Systemen für die biometrische Identifizierung zu Strafverfolgungszwecken muss im Einklang mit Artikel 10 der Richtlinie (EU) 2016/680, demzufolge eine solche Verarbeitung nur dann erlaubt ist, wenn sie unbedingt erforderlich ist, vorbehaltlich angemessener Vorkehrungen für den Schutz der Rechte und Freiheiten der betroffenen Person, und sofern sie nach dem Unionsrecht oder dem Recht der Mitgliedstaaten zulässig ist, erfolgen. Bei einer solchen Nutzung, sofern sie zulässig ist, müssen auch die in Artikel 4 Absatz 1 der Richtlinie (EU) 2016/680 festgelegten Grundsätze geachtet werden, einschließlich Rechtmäßigkeit, Fairness und Transparenz, Zweckbindung, sachliche Richtigkeit und Speicherbegrenzung."
    },
    {
      "chunk_idx": 94,
      "id": "4441c4d9-747f-4f05-8fc8-b67033b9273f",
      "title": "Recital 95",
      "relevantChunksIds": [
        "54d92cd3-752d-4303-b8e9-f8c7efac0e64"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(95) Without prejudice to applicable Union law, in particular Regulation (EU) 2016/679 and Directive (EU) 2016/680, considering the intrusive nature of post-remote biometric identification systems, the use of post-remote biometric identification systems should be subject to safeguards. Post-remote biometric identification systems should always be used in a way that is proportionate, legitimate and strictly necessary, and thus targeted, in terms of the individuals to be identified, the location, temporal scope and based on a closed data set of legally acquired video footage. In any case, post-remote biometric identification systems should not be used in the framework of law enforcement to lead to indiscriminate surveillance. The conditions for post-remote biometric identification should in any case not provide a basis to circumvent the conditions of the prohibition and strict exceptions for real time remote biometric identification.",
      "original_content": "(95) Unbeschadet des geltenden Unionsrechts, insbesondere der Verordnung (EU) 2016/679 und der Richtlinie (EU) 2016/680, sollte die Verwendung von Systemen zur nachträglichen biometrischen Fernidentifizierung in Anbetracht des intrusiven Charakters von Systemen zur nachträglichen biometrischen Fernidentifizierung Schutzvorkehrungen unterliegen. Systeme zur nachträglichen biometrischen Fernidentifizierung sollten stets auf verhältnismäßige, legitime und unbedingt erforderliche Weise eingesetzt werden und somit zielgerichtet sein, was die zu identifizierenden Personen, den Ort und den zeitlichen Anwendungsbereich betrifft, und auf einem geschlossenen Datensatz rechtmäßig erworbener Videoaufnahmen basieren. In jedem Fall sollten Systeme zur nachträglichen biometrischen Fernidentifizierung im Rahmen der Strafverfolgung nicht so verwendet werden, dass sie zu willkürlicher Überwachung führen. Die Bedingungen für die nachträgliche biometrische Fernidentifizierung sollten keinesfalls eine Grundlage dafür bieten, die Bedingungen des Verbots und der strengen Ausnahmen für biometrische Echtzeit-Fernidentifizierung zu umgehen."
    },
    {
      "chunk_idx": 95,
      "id": "90ac7181-45af-41f1-817b-5a51223d7825",
      "title": "Recital 96",
      "relevantChunksIds": [
        "1a8cb301-fedf-4e50-a05e-2d3ad00b5264",
        "069d1fdf-6329-4927-865f-862b02fbc7c1"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(96) In order to efficiently ensure that fundamental rights are protected, deployers of high-risk AI systems that are bodies governed by public law, or private entities providing public services and deployers of certain high-risk AI systems listed in an annex to this Regulation, such as banking or insurance entities, should carry out a fundamental rights impact assessment prior to putting it into use. Services important for individuals that are of public nature may also be provided by private entities. Private entities providing such public services are linked to tasks in the public interest such as in the areas of education, healthcare, social services, housing, administration of justice. The aim of the fundamental rights impact assessment is for the deployer to identify the specific risks to the rights of individuals or groups of individuals likely to be affected, identify measures to be taken in the case of a materialisation of those risks. The impact assessment should be performed prior to deploying the high-risk AI system, and should be updated when the deployer considers that any of the relevant factors have changed. The impact assessment should identify the deployer’s relevant processes in which the high-risk AI system will be used in line with its intended purpose, and should include a description of the period of time and frequency in which the system is intended to be used as well as of specific categories of natural persons and groups who are likely to be affected in the specific context of use. The assessment should also include the identification of specific risks of harm likely to have an impact on the fundamental rights of those persons or groups. While performing this assessment, the deployer should take into account information relevant to a proper assessment of the impact, including but not limited to the information given by the provider of the high-risk AI system in the instructions for use. In light of the risks identified, deployers should determine measures to be taken in the case of a materialisation of those risks, including for example governance arrangements in that specific context of use, such as arrangements for human oversight according to the instructions of use or, complaint handling and redress procedures, as they could be instrumental in mitigating risks to fundamental rights in concrete use-cases. After performing that impact assessment, the deployer should notify the relevant market surveillance authority. Where appropriate, to collect relevant information necessary to perform the impact assessment, deployers of high-risk AI system, in particular when AI systems are used in the public sector, could involve relevant stakeholders, including the representatives of groups of persons likely to be affected by the AI system, independent experts, and civil society organisations in conducting such impact assessments and designing measures to be taken in the case of materialisation of the risks. The European Artificial Intelligence Office (AI Office) should develop a template for a questionnaire in order to facilitate compliance and reduce the administrative burden for deployers.",
      "original_content": "(96) Um wirksam sicherzustellen, dass die Grundrechte geschützt werden, sollten Betreiber von Hochrisiko-KI-Systemen, bei denen es sich um Einrichtungen des öffentlichen Rechts oder private Einrichtungen, die öffentliche Dienste erbringen, handelt, und Betreiber, die bestimmte Hochrisiko-KI-Systemen gemäß einem Anhang dieser Verordnung betreiben, wie Bank- oder Versicherungsunternehmen, vor der Inbetriebnahme eine Grundrechte-Folgenabschätzung durchführen. Für Einzelpersonen wichtige Dienstleistungen öffentlicher Art können auch von privaten Einrichtungen erbracht werden. Private Einrichtungen, die solche öffentliche Dienstleistungen erbringen, sind mit Aufgaben im öffentlichen Interesse verknüpft, etwa in den Bereichen Bildung, Gesundheitsversorgung, Sozialdienste, Wohnungswesen und Justizverwaltung. Ziel der Grundrechte-Folgenabschätzung ist es, dass der Betreiber die spezifischen Risiken für die Rechte von Einzelpersonen oder Gruppen von Einzelpersonen, die wahrscheinlich betroffen sein werden, ermittelt und Maßnahmen ermittelt, die im Falle eines Eintretens dieser Risiken zu ergreifen sind. Die Folgenabschätzung sollte vor dem erstmaligen Einsatz des Hochrisiko-KI-Systems durchgeführt werden, und sie sollte aktualisiert werden, wenn der Betreiber der Auffassung ist, dass sich einer der relevanten Faktoren geändert hat. In der Folgenabschätzung sollten die einschlägigen Verfahren des Betreibers, bei denen das Hochrisiko-KI-System im Einklang mit seiner Zweckbestimmung verwendet wird, genannt werden, und sie sollte eine Beschreibung des Zeitraums und der Häufigkeit, innerhalb dessen bzw. mit der das Hochrisiko-KI-System verwendet werden soll, sowie der Kategorien der natürlichen Personen und Gruppen, die im spezifischen Verwendungskontext betroffen sein könnten, enthalten. Die Abschätzung sollte außerdem die spezifischen Schadensrisiken enthalten, die sich auf die Grundrechte dieser Personen oder Gruppen auswirken können. Bei der Durchführung dieser Bewertung sollte der Betreiber Informationen Rechnung tragen, die für eine ordnungsgemäße Abschätzung der Folgen relevant sind, unter anderem die vom Anbieter des Hochrisiko-KI-Systems in der Betriebsanleitung angegebenen Informationen. Angesichts der ermittelten Risiken sollten die Betreiber Maßnahmen festlegen, die im Falle eines Eintretens dieser Risiken zu ergreifen sind, einschließlich beispielsweise Unternehmensführungsregelungen in diesem spezifischen Verwendungskontext, etwa Regelungen für die menschliche Aufsicht gemäß den Betriebsanleitungen oder Verfahren für die Bearbeitung von Beschwerden und Rechtsbehelfsverfahren, da sie dazu beitragen könnten, Risiken für die Grundrechte in konkreten Anwendungsfällen zu mindern. Nach Durchführung dieser Folgenabschätzung sollte der Betreiber die zuständige Marktüberwachungsbehörde unterrichten. Um einschlägige Informationen einzuholen, die für die Durchführung der Folgenabschätzung erforderlich sind, könnten die Betreiber von Hochrisiko-KI-Systemen, insbesondere wenn KI-Systeme im öffentlichen Sektor verwendet werden, relevante Interessenträger, unter anderem Vertreter von Personengruppen, die von dem KI-System betroffen sein könnten, unabhängige Sachverständige und Organisationen der Zivilgesellschaft, in die Durchführung solcher Folgenabschätzungen und die Gestaltung von Maßnahmen, die im Falle des Eintretens der Risiken zu ergreifen sind, einbeziehen. Das Europäische Büro für Künstliche Intelligenz (im Folgenden „Büro für Künstliche Intelligenz“) sollte ein Muster für einen Fragebogen ausarbeiten, um den Betreibern die Einhaltung der Vorschriften zu erleichtern und den Verwaltungsaufwand für sie zu verringern."
    },
    {
      "chunk_idx": 96,
      "id": "89e675f5-d397-4279-80b8-8d3635280832",
      "title": "Recital 97",
      "relevantChunksIds": [
        "16cd04eb-0068-4a58-9062-a5e44f5f15a0"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(97) The notion of general-purpose AI models should be clearly defined and set apart from the notion of AI systems to enable legal certainty. The definition should be based on the key functional characteristics of a general-purpose AI model, in particular the generality and the capability to competently perform a wide range of distinct tasks. These models are typically trained on large amounts of data, through various methods, such as self-supervised, unsupervised or reinforcement learning. General-purpose AI models may be placed on the market in various ways, including through libraries, application programming interfaces (APIs), as direct download, or as physical copy. These models may be further modified or fine-tuned into new models. Although AI models are essential components of AI systems, they do not constitute AI systems on their own. AI models require the addition of further components, such as for example a user interface, to become AI systems. AI models are typically integrated into and form part of AI systems. This Regulation provides specific rules for general-purpose AI models and for general-purpose AI models that pose systemic risks, which should apply also when these models are integrated or form part of an AI system. It should be understood that the obligations for the providers of general-purpose AI models should apply once the general-purpose AI models are placed on the market. When the provider of a general-purpose AI model integrates an own model into its own AI system that is made available on the market or put into service, that model should be considered to be placed on the market and, therefore, the obligations in this Regulation for models should continue to apply in addition to those for AI systems. The obligations laid down for models should in any case not apply when an own model is used for purely internal processes that are not essential for providing a product or a service to third parties and the rights of natural persons are not affected. Considering their potential significantly negative effects, the general-purpose AI models with systemic risk should always be subject to the relevant obligations under this Regulation. The definition should not cover AI models used before their placing on the market for the sole purpose of research, development and prototyping activities. This is without prejudice to the obligation to comply with this Regulation when, following such activities, a model is placed on the market.",
      "original_content": "(97) Der Begriff „KI-Modelle mit allgemeinem Verwendungszweck“ sollte klar bestimmt und vom Begriff der KI-Systeme abgegrenzt werden, um Rechtssicherheit zu schaffen. Die Begriffsbestimmung sollte auf den wesentlichen funktionalen Merkmalen eines KI-Modells mit allgemeinem Verwendungszweck beruhen, insbesondere auf der allgemeinen Verwendbarkeit und der Fähigkeit, ein breites Spektrum unterschiedlicher Aufgaben kompetent zu erfüllen. Diese Modelle werden in der Regel mit großen Datenmengen durch verschiedene Methoden, etwa überwachtes, unüberwachtes und bestärkendes Lernen, trainiert. KI-Modelle mit allgemeinem Verwendungszweck können auf verschiedene Weise in Verkehr gebracht werden, unter anderem über Bibliotheken, Anwendungsprogrammierschnittstellen (API), durch direktes Herunterladen oder als physische Kopie. Diese Modelle können weiter geändert oder zu neuen Modellen verfeinert werden. Obwohl KI-Modelle wesentliche Komponenten von KI-Systemen sind, stellen sie für sich genommen keine KI-Systeme dar. Damit KI-Modelle zu KI-Systemen werden, ist die Hinzufügung weiterer Komponenten, zum Beispiel einer Nutzerschnittstelle, erforderlich. KI-Modelle sind in der Regel in KI-Systeme integriert und Teil davon. Diese Verordnung enthält spezifische Vorschriften für KI-Modelle mit allgemeinem Verwendungszweck und für KI-Modelle mit allgemeinem Verwendungszweck, die systemische Risiken bergen; diese sollten auch gelten, wenn diese Modelle in ein KI-System integriert oder Teil davon sind. Es sollte klar sein, dass die Pflichten für die Anbieter von KI-Modellen mit allgemeinem Verwendungszweck gelten sollten, sobald die KI-Modelle mit allgemeinem Verwendungszweck in Verkehr gebracht werden. Wenn der Anbieter eines KI-Modells mit allgemeinem Verwendungszweck ein eigenes Modell in sein eigenes KI-System integriert, das auf dem Markt bereitgestellt oder in Betrieb genommen wird, sollte jenes Modell als in Verkehr gebracht gelten und sollten daher die Pflichten aus dieser Verordnung für Modelle weiterhin zusätzlich zu den Pflichten für KI-Systeme gelten. Die für Modelle festgelegten Pflichten sollten in jedem Fall nicht gelten, wenn ein eigenes Modell für rein interne Verfahren verwendet wird, die für die Bereitstellung eines Produkts oder einer Dienstleistung an Dritte nicht wesentlich sind, und die Rechte natürlicher Personen nicht beeinträchtigt werden. Angesichts ihrer potenziellen in erheblichem Ausmaße negativen Auswirkungen sollten KI-Modelle mit allgemeinem Verwendungszweck mit systemischem Risiko stets den einschlägigen Pflichten gemäß dieser Verordnung unterliegen. Die Begriffsbestimmung sollte nicht für KI-Modelle gelten, die vor ihrem Inverkehrbringen ausschließlich für Forschungs- und Entwicklungstätigkeiten oder die Konzipierung von Prototypen verwendet werden. Dies gilt unbeschadet der Pflicht, dieser Verordnung nachzukommen, wenn ein Modell nach solchen Tätigkeiten in Verkehr gebracht wird."
    },
    {
      "chunk_idx": 97,
      "id": "20fb5c2c-4faf-4c7c-9ae1-4d97ec37e7c0",
      "title": "Recital 98",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(98) Whereas the generality of a model could, inter alia, also be determined by a number of parameters, models with at least a billion of parameters and trained with a large amount of data using self-supervision at scale should be considered to display significant generality and to competently perform a wide range of distinctive tasks.",
      "original_content": "(98) Die allgemeine Verwendbarkeit eines Modells könnte zwar unter anderem auch durch eine bestimmte Anzahl von Parametern bestimmt werden, doch sollten Modelle mit mindestens einer Milliarde Parametern, die mit einer großen Datenmenge unter umfassender Selbstüberwachung trainiert werden, als Modelle gelten, die eine erhebliche allgemeine Verwendbarkeit aufweisen und ein breites Spektrum unterschiedlicher Aufgaben kompetent erfüllen."
    },
    {
      "chunk_idx": 98,
      "id": "c9a0bc19-9064-435f-a8d1-73a20a754192",
      "title": "Recital 99",
      "relevantChunksIds": [
        "18f063a4-31d6-40a9-b45a-da3f602f920f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(99) Large generative AI models are a typical example for a general-purpose AI model, given that they allow for flexible generation of content, such as in the form of text, audio, images or video, that can readily accommodate a wide range of distinctive tasks.",
      "original_content": "(99) Große generative KI-Modelle sind ein typisches Beispiel für ein KI-Modell mit allgemeinem Verwendungszweck, da sie eine flexible Erzeugung von Inhalten ermöglichen, etwa in Form von Text- Audio-, Bild- oder Videoinhalten, die leicht ein breites Spektrum unterschiedlicher Aufgaben umfassen können."
    },
    {
      "chunk_idx": 99,
      "id": "8543b14a-dfa5-4fd3-9673-e854faf06935",
      "title": "Recital 100",
      "relevantChunksIds": [
        "18f063a4-31d6-40a9-b45a-da3f602f920f",
        "55614e42-478f-480a-a5c3-2ecb6c402441"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(100) When a general-purpose AI model is integrated into or forms part of an AI system, this system should be considered to be general-purpose AI system when, due to this integration, this system has the capability to serve a variety of purposes. A general-purpose AI system can be used directly, or it may be integrated into other AI systems.",
      "original_content": "(100) Wenn ein KI-Modell mit allgemeinem Verwendungszweck in ein KI-System integriert oder Teil davon ist, sollte dieses System als KI-System mit allgemeinem Verwendungszweck gelten, wenn dieses System aufgrund dieser Integration in der Lage ist, einer Vielzahl von Zwecken zu dienen. Ein KI-System mit allgemeinem Verwendungszweck kann direkt eingesetzt oder in andere KI-Systeme integriert werden."
    },
    {
      "chunk_idx": 100,
      "id": "e4baece6-7335-43c7-bc93-770f04b64211",
      "title": "Recital 101",
      "relevantChunksIds": [
        "7bed8e5e-3585-48d6-889d-68089e0581a4"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(101) Providers of general-purpose AI models have a particular role and responsibility along the AI value chain, as the models they provide may form the basis for a range of downstream systems, often provided by downstream providers that necessitate a good understanding of the models and their capabilities, both to enable the integration of such models into their products, and to fulfil their obligations under this or other regulations. Therefore, proportionate transparency measures should be laid down, including the drawing up and keeping up to date of documentation, and the provision of information on the general-purpose AI model for its usage by the downstream providers. Technical documentation should be prepared and kept up to date by the general-purpose AI model provider for the purpose of making it available, upon request, to the AI Office and the national competent authorities. The minimal set of elements to be included in such documentation should be set out in specific annexes to this Regulation. The Commission should be empowered to amend those annexes by means of delegated acts in light of evolving technological developments.",
      "original_content": "(101) Anbieter von KI-Modellen mit allgemeinem Verwendungszweck nehmen entlang der KI-Wertschöpfungskette eine besondere Rolle und Verantwortung wahr, da die von ihnen bereitgestellten Modelle die Grundlage für eine Reihe nachgelagerter Systeme bilden können, die häufig von nachgelagerten Anbietern bereitgestellt werden und ein gutes Verständnis der Modelle und ihrer Fähigkeiten erfordern, sowohl um die Integration solcher Modelle in ihre Produkte zu ermöglichen als auch ihre Pflichten im Rahmen dieser oder anderer Verordnungen zu erfüllen. Daher sollten verhältnismäßige Transparenzmaßnahmen festgelegt werden, einschließlich der Erstellung und Aktualisierung von Dokumentation und der Bereitstellung von Informationen über das KI-Modell mit allgemeinem Verwendungszweck für dessen Nutzung durch die nachgelagerten Anbieter. Der Anbieter von KI-Modellen mit allgemeinem Verwendungszweck sollte technische Dokumentation erarbeiten und aktualisieren, damit sie dem Büro für Künstliche Intelligenz und den zuständigen nationalen Behörden auf Anfrage zur Verfügung gestellt werden kann. Welche Elemente mindestens in eine solche Dokumentation aufzunehmen sind, sollte in bestimmten Anhängen dieser Verordnung festgelegt werden. Der Kommission sollte die Befugnis übertragen werden, diese Anhänge im Wege delegierter Rechtsakte vor dem Hintergrund sich wandelnder technologischer Entwicklungen zu ändern."
    },
    {
      "chunk_idx": 101,
      "id": "b908b1fb-c1a0-41d0-85cd-96d3d195d360",
      "title": "Recital 102",
      "relevantChunksIds": [
        "7bed8e5e-3585-48d6-889d-68089e0581a4"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(102) Software and data, including models, released under a free and open-source licence that allows them to be openly shared and where users can freely access, use, modify and redistribute them or modified versions thereof, can contribute to research and innovation in the market and can provide significant growth opportunities for the Union economy. General-purpose AI models released under free and open-source licences should be considered to ensure high levels of transparency and openness if their parameters, including the weights, the information on the model architecture, and the information on model usage are made publicly available. The licence should be considered to be free and open-source also when it allows users to run, copy, distribute, study, change and improve software and data, including models under the condition that the original provider of the model is credited, the identical or comparable terms of distribution are respected.",
      "original_content": "(102) Software und Daten, einschließlich Modellen, die im Rahmen einer freien und quelloffenen Lizenz freigegeben werden, die ihre offene Weitergabe erlaubt und die Nutzer kostenlos abrufen, nutzen, verändern und weiter verteilen können, auch in veränderter Form, können zu Forschung und Innovation auf dem Markt beitragen und der Wirtschaft der Union erhebliche Wachstumschancen eröffnen. KI-Modelle mit allgemeinem Verwendungszweck, die im Rahmen freier und quelloffener Lizenzen freigegeben werden, sollten als ein hohes Maß an Transparenz und Offenheit sicherstellend gelten, wenn ihre Parameter, einschließlich Gewichte, Informationen über die Modellarchitektur und Informationen über die Modellnutzung, öffentlich zugänglich gemacht werden. Die Lizenz sollte auch als freie quelloffene Lizenz gelten, wenn sie es den Nutzern ermöglicht, Software und Daten zu betreiben, zu kopieren, zu verbreiten, zu untersuchen, zu ändern und zu verbessern, einschließlich Modelle, sofern der ursprüngliche Anbieter des Modells genannt und identische oder vergleichbare Vertriebsbedingungen eingehalten werden."
    },
    {
      "chunk_idx": 102,
      "id": "aa588b7b-71e5-42c1-9adc-5aaa8688320a",
      "title": "Recital 103",
      "relevantChunksIds": [
        "18f063a4-31d6-40a9-b45a-da3f602f920f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(103) Free and open-source AI components covers the software and data, including models and general-purpose AI models, tools, services or processes of an AI system. Free and open-source AI components can be provided through different channels, including their development on open repositories. For the purposes of this Regulation, AI components that are provided against a price or otherwise monetised, including through the provision of technical support or other services, including through a software platform, related to the AI component, or the use of personal data for reasons other than exclusively for improving the security, compatibility or interoperability of the software, with the exception of transactions between microenterprises, should not benefit from the exceptions provided to free and open-source AI components. The fact of making AI components available through open repositories should not, in itself, constitute a monetisation.",
      "original_content": "(103) Zu freien und quelloffenen KI-Komponenten zählen Software und Daten, einschließlich Modelle und KI-Modelle mit allgemeinem Verwendungszweck, Instrumente, Dienste oder Verfahren eines KI-Systems. Freie und quelloffene KI-Komponenten können über verschiedene Kanäle bereitgestellt werden, einschließlich ihrer Entwicklung auf offenen Speichern. Für die Zwecke dieser Verordnung sollten KI-Komponenten, die gegen einen Preis bereitgestellt oder anderweitig monetarisiert werden, einschließlich durch die Bereitstellung technischer Unterstützung oder anderer Dienste — einschließlich über eine Softwareplattform — im Zusammenhang mit der KI-Komponente oder durch die Verwendung personenbezogener Daten aus anderen Gründen als der alleinigen Verbesserung der Sicherheit, Kompatibilität oder Interoperabilität der Software, mit Ausnahme von Transaktionen zwischen Kleinstunternehmen, nicht unter die Ausnahmen für freie und quelloffene KI-Komponenten fallen. Die Bereitstellung von KI-Komponenten über offene Speicher sollte für sich genommen keine Monetarisierung darstellen."
    },
    {
      "chunk_idx": 103,
      "id": "0a6d98df-01bf-4a5c-b316-f87ecb2b8f17",
      "title": "Recital 104",
      "relevantChunksIds": [
        "7bed8e5e-3585-48d6-889d-68089e0581a4"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(104) The providers of general-purpose AI models that are released under a free and open-source licence, and whose parameters, including the weights, the information on the model architecture, and the information on model usage, are made publicly available should be subject to exceptions as regards the transparency-related requirements imposed on general-purpose AI models, unless they can be considered to present a systemic risk, in which case the circumstance that the model is transparent and accompanied by an open-source license should not be considered to be a sufficient reason to exclude compliance with the obligations under this Regulation. In any case, given that the release of general-purpose AI models under free and open-source licence does not necessarily reveal substantial information on the data set used for the training or fine-tuning of the model and on how compliance of copyright law was thereby ensured, the exception provided for general-purpose AI models from compliance with the transparency-related requirements should not concern the obligation to produce a summary about the content used for model training and the obligation to put in place a policy to comply with Union copyright law, in particular to identify and comply with the reservation of rights pursuant to Article 4(3) of Directive (EU) 2019/790 of the European Parliament and of the Council (40).",
      "original_content": "(104) Für die Anbieter von KI-Modellen mit allgemeinem Verwendungszweck, die im Rahmen einer freien und quelloffenen Lizenz freigegeben werden und deren Parameter, einschließlich Gewichte, Informationen über die Modellarchitektur und Informationen über die Modellnutzung, öffentlich zugänglich gemacht werden, sollten Ausnahmen in Bezug auf die Transparenzanforderungen für KI-Modelle mit allgemeinem Verwendungszweck gelten, es sei denn, sie können als Modelle gelten, die ein systemisches Risiko bergen; in diesem Fall sollte der Umstand, dass das Modell transparent ist und mit einer quelloffenen Lizenz einhergeht, nicht als ausreichender Grund gelten, um sie von der Einhaltung der Pflichten aus dieser Verordnung auszunehmen. Da die Freigabe von KI-Modellen mit allgemeinem Verwendungszweck im Rahmen einer freien und quelloffenen Lizenz nicht unbedingt wesentliche Informationen über den für das Trainieren oder die Feinabstimmung des Modells verwendeten Datensatz und die Art und Weise, wie damit die Einhaltung des Urheberrechts sichergestellt wurde, offenbart, sollte die für KI-Modelle mit allgemeinem Verwendungszweck vorgesehene Ausnahme von der Einhaltung der Transparenzanforderungen in jedem Fall nicht die Pflicht zur Erstellung einer Zusammenfassung der für das Training des Modells verwendeten Inhalte und die Pflicht, eine Strategie zur Einhaltung des Urheberrechts der Union, insbesondere zur Ermittlung und Einhaltung der gemäß Artikel 4 Absatz 3 der Richtlinie (EU) 2019/790 des Europäischen Parlaments und des Rates (Fußnote 40) geltend gemachten Rechtsvorbehalte, auf den Weg zu bringen, betreffen. Fußnote 40: Richtlinie (EU) 2019/790 des Europäischen Parlaments und des Rates vom 17. April 2019 über das Urheberrecht und die verwandten Schutzrechte im digitalen Binnenmarkt und zur Änderung der Richtlinien 96/9/EG und 2001/29/EG (ABl. L 130 vom 17.5.2019, S. 92)."
    },
    {
      "chunk_idx": 104,
      "id": "3b3065a2-cb7e-4271-8016-ccad88444a89",
      "title": "Recital 105",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(105) General-purpose AI models, in particular large generative AI models, capable of generating text, images, and other content, present unique innovation opportunities but also challenges to artists, authors, and other creators and the way their creative content is created, distributed, used and consumed. The development and training of such models require access to vast amounts of text, images, videos and other data. Text and data mining techniques may be used extensively in this context for the retrieval and analysis of such content, which may be protected by copyright and related rights. Any use of copyright protected content requires the authorisation of the rightsholder concerned unless relevant copyright exceptions and limitations apply. Directive (EU) 2019/790 introduced exceptions and limitations allowing reproductions and extractions of works or other subject matter, for the purpose of text and data mining, under certain conditions. Under these rules, rightsholders may choose to reserve their rights over their works or other subject matter to prevent text and data mining, unless this is done for the purposes of scientific research. Where the rights to opt out has been expressly reserved in an appropriate manner, providers of general-purpose AI models need to obtain an authorisation from rightsholders if they want to carry out text and data mining over such works.",
      "original_content": "(105) KI-Modelle mit allgemeinem Verwendungszweck, insbesondere große generative KI-Modelle, die Text, Bilder und andere Inhalte erzeugen können, bedeuten einzigartige Innovationsmöglichkeiten, aber auch Herausforderungen für Künstler, Autoren und andere Kreative sowie die Art und Weise, wie ihre kreativen Inhalte geschaffen, verbreitet, genutzt und konsumiert werden. Für die Entwicklung und das Training solcher Modelle ist der Zugang zu riesigen Mengen an Text, Bildern, Videos und anderen Daten erforderlich. In diesem Zusammenhang können Text-und-Data-Mining-Techniken in großem Umfang für das Abrufen und die Analyse solcher Inhalte, die urheberrechtlich und durch verwandte Schutzrechte geschützt sein können, eingesetzt werden. Für jede Nutzung urheberrechtlich geschützter Inhalte ist die Zustimmung des betreffenden Rechteinhabers erforderlich, es sei denn, es gelten einschlägige Ausnahmen und Beschränkungen des Urheberrechts. Mit der Richtlinie (EU) 2019/790 wurden Ausnahmen und Beschränkungen eingeführt, um unter bestimmten Bedingungen Vervielfältigungen und Entnahmen von Werken oder sonstigen Schutzgegenständen für die Zwecke des Text und Data Mining zu erlauben. Nach diesen Vorschriften können Rechteinhaber beschließen, ihre Rechte an ihren Werken oder sonstigen Schutzgegenständen vorzubehalten, um Text und Data Mining zu verhindern, es sei denn, es erfolgt zum Zwecke der wissenschaftlichen Forschung. Wenn die Vorbehaltsrechte ausdrücklich und in geeigneter Weise vorbehalten wurden, müssen Anbieter von KI-Modellen mit allgemeinem Verwendungszweck eine Genehmigung von den Rechteinhabern einholen, wenn sie Text und Data Mining bei solchen Werken durchführen wollen."
    },
    {
      "chunk_idx": 105,
      "id": "e232c65a-adde-46cf-a5ae-7c6dc025ac7b",
      "title": "Recital 106",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(106) Providers that place general-purpose AI models on the Union market should ensure compliance with the relevant obligations in this Regulation. To that end, providers of general-purpose AI models should put in place a policy to comply with Union law on copyright and related rights, in particular to identify and comply with the reservation of rights expressed by rightsholders pursuant to Article 4(3) of Directive (EU) 2019/790. Any provider placing a general-purpose AI model on the Union market should comply with this obligation, regardless of the jurisdiction in which the copyright-relevant acts underpinning the training of those general-purpose AI models take place. This is necessary to ensure a level playing field among providers of general-purpose AI models where no provider should be able to gain a competitive advantage in the Union market by applying lower copyright standards than those provided in the Union.",
      "original_content": "(106) Anbieter, die KI-Modelle mit allgemeinem Verwendungszweck in der Union in Verkehr bringen, sollten die Erfüllung der einschlägigen Pflichten aus dieser Verordnung gewährleisten. Zu diesem Zweck sollten Anbieter von KI-Modellen mit allgemeinem Verwendungszweck eine Strategie zur Einhaltung des Urheberrechts der Union und der verwandten Schutzrechte einführen, insbesondere zur Ermittlung und Einhaltung des gemäß Artikel 4 Absatz 3 der Richtlinie (EU) 2019/790 durch die Rechteinhaber geltend gemachten Rechtsvorbehalts. Jeder Anbieter, der ein KI-Modell mit allgemeinem Verwendungszweck in der Union in Verkehr bringt, sollte diese Pflicht erfüllen, unabhängig davon, in welchem Hoheitsgebiet die urheberrechtlich relevanten Handlungen, die dem Training dieser KI-Modelle mit allgemeinem Verwendungszweck zugrunde liegen, stattfinden. Dies ist erforderlich, um gleiche Wettbewerbsbedingungen für Anbieter von KI-Modellen mit allgemeinem Verwendungszweck sicherzustellen, unter denen kein Anbieter in der Lage sein sollte, durch die Anwendung niedrigerer Urheberrechtsstandards als in der Union einen Wettbewerbsvorteil auf dem Unionsmarkt zu erlangen."
    },
    {
      "chunk_idx": 106,
      "id": "b5f38203-47bc-4aba-a5fb-0270a939ad11",
      "title": "Recital 107",
      "relevantChunksIds": [
        "7bed8e5e-3585-48d6-889d-68089e0581a4"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(107) In order to increase transparency on the data that is used in the pre-training and training of general-purpose AI models, including text and data protected by copyright law, it is adequate that providers of such models draw up and make publicly available a sufficiently detailed summary of the content used for training the general-purpose AI model. While taking into due account the need to protect trade secrets and confidential business information, this summary should be generally comprehensive in its scope instead of technically detailed to facilitate parties with legitimate interests, including copyright holders, to exercise and enforce their rights under Union law, for example by listing the main data collections or sets that went into training the model, such as large private or public databases or data archives, and by providing a narrative explanation about other data sources used. It is appropriate for the AI Office to provide a template for the summary, which should be simple, effective, and allow the provider to provide the required summary in narrative form.",
      "original_content": "(107) Um die Transparenz in Bezug auf die beim Vortraining und Training von KI-Modellen mit allgemeinem Verwendungszweck verwendeten Daten, einschließlich urheberrechtlich geschützter Texte und Daten, zu erhöhen, ist es angemessen, dass die Anbieter solcher Modelle eine hinreichend detaillierte Zusammenfassung der für das Training des KI-Modells mit allgemeinem Verwendungszweck verwendeten Inhalte erstellen und veröffentlichen. Unter gebührender Berücksichtigung der Notwendigkeit, Geschäftsgeheimnisse und vertrauliche Geschäftsinformationen zu schützen, sollte der Umfang dieser Zusammenfassung allgemein weitreichend und nicht technisch detailliert sein, um Parteien mit berechtigtem Interesse, einschließlich der Inhaber von Urheberrechten, die Ausübung und Durchsetzung ihrer Rechte nach dem Unionsrecht zu erleichtern, beispielsweise indem die wichtigsten Datenerhebungen oder Datensätze aufgeführt werden, die beim Training des Modells verwendet wurden, etwa große private oder öffentliche Datenbanken oder Datenarchive, und indem eine beschreibende Erläuterung anderer verwendeter Datenquellen bereitgestellt wird. Es ist angebracht, dass das Büro für Künstliche Intelligenz eine Vorlage für die Zusammenfassung bereitstellt, die einfach und wirksam sein sollte und es dem Anbieter ermöglichen sollte, die erforderliche Zusammenfassung in beschreibender Form bereitzustellen."
    },
    {
      "chunk_idx": 107,
      "id": "867f6662-88e1-490c-9191-9ba91afbd52d",
      "title": "Recital 108",
      "relevantChunksIds": [
        "7bed8e5e-3585-48d6-889d-68089e0581a4"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(108) With regard to the obligations imposed on providers of general-purpose AI models to put in place a policy to comply with Union copyright law and make publicly available a summary of the content used for the training, the AI Office should monitor whether the provider has fulfilled those obligations without verifying or proceeding to a work-by-work assessment of the training data in terms of copyright compliance. This Regulation does not affect the enforcement of copyright rules as provided for under Union law.",
      "original_content": "(108) In Bezug auf die den Anbietern von KI-Modellen mit allgemeinem Verwendungszweck auferlegten Pflichten, eine Strategie zur Einhaltung des Urheberrechts der Union einzuführen und eine Zusammenfassung der für das Training verwendeten Inhalte zu veröffentlichen, sollte das Büro für Künstliche Intelligenz überwachen, ob der Anbieter diese Pflichten erfüllt hat, ohne dies zu überprüfen oder die Trainingsdaten im Hinblick auf die Einhaltung des Urheberrechts Werk für Werk zu bewerten. Diese Verordnung berührt nicht die Durchsetzung der Urheberrechtsvorschriften des Unionsrechts."
    },
    {
      "chunk_idx": 108,
      "id": "674c9e28-6fa6-43ed-b1df-ddc7c8028941",
      "title": "Recital 109",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(109) Compliance with the obligations applicable to the providers of general-purpose AI models should be commensurate and proportionate to the type of model provider, excluding the need for compliance for persons who develop or use models for non-professional or scientific research purposes, who should nevertheless be encouraged to voluntarily comply with these requirements. Without prejudice to Union copyright law, compliance with those obligations should take due account of the size of the provider and allow simplified ways of compliance for SMEs, including start-ups, that should not represent an excessive cost and not discourage the use of such models. In the case of a modification or fine-tuning of a model, the obligations for providers of general-purpose AI models should be limited to that modification or fine-tuning, for example by complementing the already existing technical documentation with information on the modifications, including new training data sources, as a means to comply with the value chain obligations provided in this Regulation.",
      "original_content": "(109) Die Einhaltung der für die Anbieter von KI-Modellen mit allgemeinem Verwendungszweck geltenden Pflichten sollte der Art des Anbieters von Modellen angemessen und verhältnismäßig sein, wobei Personen, die Modelle für nicht berufliche oder wissenschaftliche Forschungszwecke entwickeln oder verwenden, ausgenommen sind, jedoch ermutigt werden sollten, diese Anforderungen freiwillig zu erfüllen. Unbeschadet des Urheberrechts der Union sollte bei der Einhaltung dieser Pflichten der Größe des Anbieters gebührend Rechnung getragen und für KMU, einschließlich Start-up-Unternehmen, vereinfachte Verfahren zur Einhaltung ermöglicht werden, die keine übermäßigen Kosten verursachen und nicht von der Verwendung solcher Modelle abhalten sollten. Im Falle einer Änderung oder Feinabstimmung eines Modells sollten die Pflichten der Anbieter von KI-Modellen mit allgemeinem Verwendungszweck auf diese Änderung oder Feinabstimmung beschränkt sein, indem beispielsweise die bereits vorhandene technische Dokumentation um Informationen über die Änderungen, einschließlich neuer Trainingsdatenquellen, ergänzt wird, um die in dieser Verordnung festgelegten Pflichten in der Wertschöpfungskette zu erfüllen."
    },
    {
      "chunk_idx": 109,
      "id": "2c82d63c-2447-4597-a563-4cc4a371cd25",
      "title": "Recital 110",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(110) General-purpose AI models could pose systemic risks which include, but are not limited to, any actual or reasonably foreseeable negative effects in relation to major accidents, disruptions of critical sectors and serious consequences to public health and safety; any actual or reasonably foreseeable negative effects on democratic processes, public and economic security; the dissemination of illegal, false, or discriminatory content. Systemic risks should be understood to increase with model capabilities and model reach, can arise along the entire lifecycle of the model, and are influenced by conditions of misuse, model reliability, model fairness and model security, the level of autonomy of the model, its access to tools, novel or combined modalities, release and distribution strategies, the potential to remove guardrails and other factors. In particular, international approaches have so far identified the need to pay attention to risks from potential intentional misuse or unintended issues of control relating to alignment with human intent; chemical, biological, radiological, and nuclear risks, such as the ways in which barriers to entry can be lowered, including for weapons development, design acquisition, or use; offensive cyber capabilities, such as the ways in vulnerability discovery, exploitation, or operational use can be enabled; the effects of interaction and tool use, including for example the capacity to control physical systems and interfere with critical infrastructure; risks from models of making copies of themselves or ‘self-replicating’ or training other models; the ways in which models can give rise to harmful bias and discrimination with risks to individuals, communities or societies; the facilitation of disinformation or harming privacy with threats to democratic values and human rights; risk that a particular event could lead to a chain reaction with considerable negative effects that could affect up to an entire city, an entire domain activity or an entire community.",
      "original_content": "(110) KI-Modelle mit allgemeinem Verwendungszweck könnten systemische Risiken bergen, unter anderem tatsächliche oder vernünftigerweise vorhersehbare negative Auswirkungen im Zusammenhang mit schweren Unfällen, Störungen kritischer Sektoren und schwerwiegende Folgen für die öffentliche Gesundheit und Sicherheit; alle tatsächlichen oder vernünftigerweise vorhersehbaren negativen Auswirkungen auf die demokratischen Prozesse und die öffentliche und wirtschaftliche Sicherheit; die Verbreitung illegaler, falscher oder diskriminierender Inhalte. Bei systemischen Risiken sollte davon ausgegangen werden, dass sie mit den Fähigkeiten und der Reichweite des Modells zunehmen, während des gesamten Lebenszyklus des Modells auftreten können und von Bedingungen einer Fehlanwendung, der Zuverlässigkeit des Modells, der Modellgerechtigkeit und der Modellsicherheit, dem Grad der Autonomie des Modells, seinem Zugang zu Instrumenten, neuartigen oder kombinierten Modalitäten, Freigabe- und Vertriebsstrategien, dem Potenzial zur Beseitigung von Leitplanken und anderen Faktoren beeinflusst werden. Insbesondere bei internationalen Ansätzen wurde bisher festgestellt, dass folgenden Risiken Rechnung getragen werden muss: den Risiken einer möglichen vorsätzlichen Fehlanwendung oder unbeabsichtigter Kontrollprobleme im Zusammenhang mit der Ausrichtung auf menschliche Absicht; chemischen, biologischen, radiologischen und nuklearen Risiken, zum Beispiel Möglichkeiten zur Verringerung der Zutrittsschranken, einschließlich für Entwicklung, Gestaltung, Erwerb oder Nutzung von Waffen; offensiven Cyberfähigkeiten, zum Beispiel die Art und Weise, wie Entdeckung, Ausbeutung oder operative Nutzung von Schwachstellen ermöglicht werden können; den Auswirkungen der Interaktion und des Einsatzes von Instrumenten, einschließlich zum Beispiel der Fähigkeit, physische Systeme zu steuern und in kritische Infrastrukturen einzugreifen; Risiken, dass Modelle sich selbst vervielfältigen, oder der „Selbstreplikation“ oder des Trainings anderer Modelle; der Art und Weise, wie Modelle zu schädlichen Verzerrungen und Diskriminierung mit Risiken für Einzelpersonen, Gemeinschaften oder Gesellschaften führen können; der Erleichterung von Desinformation oder der Verletzung der Privatsphäre mit Gefahren für demokratische Werte und Menschenrechte; dem Risiko, dass ein bestimmtes Ereignis zu einer Kettenreaktion mit erheblichen negativen Auswirkungen führen könnte, die sich auf eine ganze Stadt, eine ganze Tätigkeit in einem Bereich oder eine ganze Gemeinschaft auswirken könnten."
    },
    {
      "chunk_idx": 110,
      "id": "9bd99d7c-7727-4df7-abb0-c3f4754eeb7f",
      "title": "Recital 111",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(111) It is appropriate to establish a methodology for the classification of general-purpose AI models as general-purpose AI model with systemic risks. Since systemic risks result from particularly high capabilities, a general-purpose AI model should be considered to present systemic risks if it has high-impact capabilities, evaluated on the basis of appropriate technical tools and methodologies, or significant impact on the internal market due to its reach. High-impact capabilities in general-purpose AI models means capabilities that match or exceed the capabilities recorded in the most advanced general-purpose AI models. The full range of capabilities in a model could be better understood after its placing on the market or when deployers interact with the model. According to the state of the art at the time of entry into force of this Regulation, the cumulative amount of computation used for the training of the general-purpose AI model measured in floating point operations is one of the relevant approximations for model capabilities. The cumulative amount of computation used for training includes the computation used across the activities and methods that are intended to enhance the capabilities of the model prior to deployment, such as pre-training, synthetic data generation and fine-tuning. Therefore, an initial threshold of floating point operations should be set, which, if met by a general-purpose AI model, leads to a presumption that the model is a general-purpose AI model with systemic risks. This threshold should be adjusted over time to reflect technological and industrial changes, such as algorithmic improvements or increased hardware efficiency, and should be supplemented with benchmarks and indicators for model capability. To inform this, the AI Office should engage with the scientific community, industry, civil society and other experts. Thresholds, as well as tools and benchmarks for the assessment of high-impact capabilities, should be strong predictors of generality, its capabilities and associated systemic risk of general-purpose AI models, and could take into account the way the model will be placed on the market or the number of users it may affect. To complement this system, there should be a possibility for the Commission to take individual decisions designating a general-purpose AI model as a general-purpose AI model with systemic risk if it is found that such model has capabilities or an impact equivalent to those captured by the set threshold. That decision should be taken on the basis of an overall assessment of the criteria for the designation of a general-purpose AI model with systemic risk set out in an annex to this Regulation, such as quality or size of the training data set, number of business and end users, its input and output modalities, its level of autonomy and scalability, or the tools it has access to. Upon a reasoned request of a provider whose model has been designated as a general-purpose AI model with systemic risk, the Commission should take the request into account and may decide to reassess whether the general-purpose AI model can still be considered to present systemic risks.",
      "original_content": "(111) Es ist angezeigt, eine Methodik für die Einstufung von KI-Modellen mit allgemeinem Verwendungszweck als KI-Modelle mit allgemeinem Verwendungszweck mit systemischen Risiken festzulegen. Da sich systemische Risiken aus besonders hohen Fähigkeiten ergeben, sollte ein KI-Modell mit allgemeinem Verwendungszweck als Modell mit systemischen Risiken gelten, wenn es über auf der Grundlage geeigneter technischer Instrumente und Methoden bewertete Fähigkeiten mit hoher Wirkkraft verfügt oder aufgrund seiner Reichweite erhebliche Auswirkungen auf den Binnenmarkt hat. „Fähigkeiten mit hoher Wirkkraft“ bei KI-Modellen mit allgemeinem Verwendungszweck bezeichnet Fähigkeiten, die den bei den fortschrittlichsten KI-Modellen mit allgemeinem Verwendungszweck festgestellten Fähigkeiten entsprechen oder diese übersteigen. Das gesamte Spektrum der Fähigkeiten eines Modells könnte besser verstanden werden, nachdem es in Verkehr gebracht wurde oder wenn die Betreiber mit dem Modell interagieren. Nach dem Stand der Technik zum Zeitpunkt des Inkrafttretens dieser Verordnung ist die kumulierte Menge der für das Training des KI-Modells mit allgemeinem Verwendungszweck verwendeten Berechnungen, gemessen in Gleitkommaoperationen, einer der einschlägigen Näherungswerte für Modellfähigkeiten. Die kumulierte Menge der für das Training verwendeten Berechnungen umfasst die kumulierte Menge der für die Tätigkeiten und Methoden, mit denen die Fähigkeiten des Modells vor der Einführung verbessert werden sollen, wie zum Beispiel Vortraining, Generierung synthetischer Daten und Feinabstimmung, verwendeten Berechnungen. Daher sollte ein erster Schwellenwert der Gleitkommaoperationen festgelegt werden, dessen Erreichen durch ein KI-Modell mit allgemeinem Verwendungszweck zu der Annahme führt, dass es sich bei dem Modell um ein KI-Modell mit allgemeinem Verwendungszweck mit systemischen Risiken handelt. Dieser Schwellenwert sollte im Laufe der Zeit angepasst werden, um technologischen und industriellen Veränderungen, wie zum Beispiel algorithmischen Verbesserungen oder erhöhter Hardwareeffizienz, Rechnung zu tragen, und um Benchmarks und Indikatoren für die Modellfähigkeit ergänzt werden. Um die Grundlage dafür zu schaffen, sollte das Büro für Künstliche Intelligenz mit der Wissenschaftsgemeinschaft, der Industrie, der Zivilgesellschaft und anderen Sachverständigen zusammenarbeiten. Schwellenwerte sowie Instrumente und Benchmarks für die Bewertung von Fähigkeiten mit hoher Wirkkraft sollten zuverlässig die allgemeine Verwendbarkeit, die Fähigkeiten und die mit ihnen verbundenen systemischen Risikos von KI-Modellen mit allgemeinem Verwendungszweck vorhersagen können und könnten die Art und Weise, wie das Modell in Verkehr gebracht wird, oder die Zahl der Nutzer, auf die es sich auswirken könnte, berücksichtigen. Ergänzend zu diesem System sollte die Kommission Einzelentscheidungen treffen können, mit denen ein KI-Modell mit allgemeinem Verwendungszweck als KI-Modell mit allgemeinem Verwendungszweck mit systemischem Risiko eingestuft wird, wenn festgestellt wurde, dass dieses Modell Fähigkeiten oder Auswirkungen hat, die den von dem festgelegten Schwellenwert erfassten entsprechen. Die genannte Entscheidung sollte auf der Grundlage einer Gesamtbewertung der in einem Anhang dieser Verordnung festgelegten Kriterien für die Benennung von KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko getroffen werden, etwa Qualität oder Größe des Trainingsdatensatzes, Anzahl der gewerblichen Nutzer und Endnutzer, seine Ein- und Ausgabemodalitäten, sein Grad an Autonomie und Skalierbarkeit oder die Instrumente, zu denen es Zugang hat. Stellt der Anbieter, dessen Modell als KI-Modell mit allgemeinem Verwendungszweck mit systemischem Risiko benannt wurde, einen entsprechenden Antrag, sollte die Kommission den Antrag berücksichtigen, und sie kann entscheiden, erneut zu prüfen, ob beim KI-Modell mit allgemeinem Verwendungszweck immer noch davon ausgegangen werden kann, dass es systemische Risiken aufweist."
    },
    {
      "chunk_idx": 111,
      "id": "2e8446c1-8259-4a99-8353-ae8eee2bf2ab",
      "title": "Recital 112",
      "relevantChunksIds": [
        "e4d5fea4-38ce-4829-87ad-748c71e5bfd1"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(112) It is also necessary to clarify a procedure for the classification of a general-purpose AI model with systemic risks. A general-purpose AI model that meets the applicable threshold for high-impact capabilities should be presumed to be a general-purpose AI models with systemic risk. The provider should notify the AI Office at the latest two weeks after the requirements are met or it becomes known that a general-purpose AI model will meet the requirements that lead to the presumption. This is especially relevant in relation to the threshold of floating point operations because training of general-purpose AI models takes considerable planning which includes the upfront allocation of compute resources and, therefore, providers of general-purpose AI models are able to know if their model would meet the threshold before the training is completed. In the context of that notification, the provider should be able to demonstrate that, because of its specific characteristics, a general-purpose AI model exceptionally does not present systemic risks, and that it thus should not be classified as a general-purpose AI model with systemic risks. That information is valuable for the AI Office to anticipate the placing on the market of general-purpose AI models with systemic risks and the providers can start to engage with the AI Office early on. That information is especially important with regard to general-purpose AI models that are planned to be released as open-source, given that, after the open-source model release, necessary measures to ensure compliance with the obligations under this Regulation may be more difficult to implement.",
      "original_content": "(112) Außerdem muss das Verfahren für die Einstufung eines KI-Modell mit allgemeinem Verwendungszweck mit systemischem Risiko präzisiert werden. Bei einem KI-Modell mit allgemeinem Verwendungszweck, das den geltenden Schwellenwert für Fähigkeiten mit hoher Wirkkraft erreicht, sollte angenommen werden, dass es sich um ein KI-Modell mit allgemeinem Verwendungszweck mit systemischem Risiko handelt. Der Anbieter sollte spätestens zwei Wochen, nachdem die Bedingungen erfüllt sind oder bekannt wird, dass ein KI-Modell mit allgemeinem Verwendungszweck die Bedingungen, die die Annahme bewirken, erfüllen wird, dies dem Büro für Künstliche Intelligenz mitteilen. Dies ist insbesondere im Zusammenhang mit dem Schwellenwert der Gleitkommaoperationen relevant, da das Training von KI-Modellen mit allgemeinem Verwendungszweck eine erhebliche Planung erfordert, die die vorab durchgeführte Zuweisung von Rechenressourcen umfasst, sodass die Anbieter von KI-Modellen mit allgemeinem Verwendungszweck vor Abschluss des Trainings erfahren können, ob ihr Modell den Schwellenwert erreichen wird. Im Rahmen dieser Mitteilung sollte der Anbieter nachweisen können, dass ein KI-Modell mit allgemeinem Verwendungszweck aufgrund seiner besonderen Merkmale außerordentlicherweise keine systemischen Risiken birgt und daher nicht als KI-Modell mit allgemeinem Verwendungszweck mit systemischem Risiko eingestuft werden sollte. Diese Informationen sind für das Büro für Künstliche Intelligenz wertvoll, um das Inverkehrbringen von KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko zu antizipieren, und die Anbieter können frühzeitig mit der Zusammenarbeit mit dem Büro für Künstliche Intelligenz beginnen. Diese Informationen sind besonders wichtig im Hinblick auf KI-Modell mit allgemeinem Verwendungszweck, die als quelloffene Modelle bereitgestellt werden sollen, da nach der Bereitstellung von quelloffenen Modellen die erforderlichen Maßnahmen zur Gewährleistung der Einhaltung der Pflichten gemäß dieser Verordnung möglicherweise schwieriger umzusetzen sind."
    },
    {
      "chunk_idx": 112,
      "id": "f940e170-4283-4e3c-94b6-e2e60a0e4271",
      "title": "Recital 113",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(113) If the Commission becomes aware of the fact that a general-purpose AI model meets the requirements to classify as a general-purpose AI model with systemic risk, which previously had either not been known or of which the relevant provider has failed to notify the Commission, the Commission should be empowered to designate it so. A system of qualified alerts should ensure that the AI Office is made aware by the scientific panel of general-purpose AI models that should possibly be classified as general-purpose AI models with systemic risk, in addition to the monitoring activities of the AI Office.",
      "original_content": "(113) Erhält die Kommission Kenntnis davon, dass ein KI-Modell mit allgemeinem Verwendungszweck die Anforderungen für die Einstufung als KI-Modell mit allgemeinem Verwendungszweck mit systemischem Risiko erfüllt, das zuvor nicht bekannt war oder das der betreffende Anbieter nicht der Kommission gemeldet hat, sollte die Kommission befugt sein, es als solches auszuweisen. Zusätzlich zu den Überwachungstätigkeiten des Büros für Künstliche Intelligenz sollte ein System qualifizierter Warnungen sicherstellen, dass das Büro für Künstliche Intelligenz von dem wissenschaftlichen Gremium von KI-Modelle mit allgemeinem Verwendungszweck in Kenntnis gesetzt wird, die möglicherweise als KI-Modelle mit allgemeinem Verwendungszweck mit systemischem Risiko eingestuft werden sollten, was zu den hinzukommt."
    },
    {
      "chunk_idx": 113,
      "id": "0a7a1261-b5ed-470c-b949-42e393849b95",
      "title": "Recital 114",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(114) The providers of general-purpose AI models presenting systemic risks should be subject, in addition to the obligations provided for providers of general-purpose AI models, to obligations aimed at identifying and mitigating those risks and ensuring an adequate level of cybersecurity protection, regardless of whether it is provided as a standalone model or embedded in an AI system or a product. To achieve those objectives, this Regulation should require providers to perform the necessary model evaluations, in particular prior to its first placing on the market, including conducting and documenting adversarial testing of models, also, as appropriate, through internal or independent external testing. In addition, providers of general-purpose AI models with systemic risks should continuously assess and mitigate systemic risks, including for example by putting in place risk-management policies, such as accountability and governance processes, implementing post-market monitoring, taking appropriate measures along the entire model’s lifecycle and cooperating with relevant actors along the AI value chain.",
      "original_content": "(114) Anbieter von KI-Modellen mit allgemeinem Verwendungszweck, die systemische Risiken bergen, sollten zusätzlich zu den Pflichten für Anbieter von KI-Modellen mit allgemeinem Verwendungszweck Pflichten unterliegen, die darauf abzielen, diese Risiken zu ermitteln und zu mindern und ein angemessenes Maß an Cybersicherheit zu gewährleisten, unabhängig davon, ob es als eigenständiges Modell bereitgestellt wird oder in ein KI-System oder ein Produkt eingebettet ist. Um diese Ziele zu erreichen, sollten die Anbieter in dieser Verordnung verpflichtet werden, die erforderlichen Bewertungen des Modells — insbesondere vor seinem ersten Inverkehrbringen — durchzuführen, wozu auch die Durchführung und Dokumentation von Angriffstests bei Modellen gehören, gegebenenfalls auch im Rahmen interner oder unabhängiger externer Tests. Darüber hinaus sollten KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko fortlaufend systemische Risiken bewerten und mindern, unter anderem durch die Einführung von Risikomanagementstrategien wie Verfahren der Rechenschaftspflicht und Governance-Verfahren, die Umsetzung der Beobachtung nach dem Inverkehrbringen, die Ergreifung geeigneter Maßnahmen während des gesamten Lebenszyklus des Modells und die Zusammenarbeit mit einschlägigen Akteuren entlang der KI-Wertschöpfungskette."
    },
    {
      "chunk_idx": 114,
      "id": "7b5cb5db-20b5-4e2c-8079-565b6f264eaf",
      "title": "Recital 115",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(115) Providers of general-purpose AI models with systemic risks should assess and mitigate possible systemic risks. If, despite efforts to identify and prevent risks related to a general-purpose AI model that may present systemic risks, the development or use of the model causes a serious incident, the general-purpose AI model provider should without undue delay keep track of the incident and report any relevant information and possible corrective measures to the Commission and national competent authorities. Furthermore, providers should ensure an adequate level of cybersecurity protection for the model and its physical infrastructure, if appropriate, along the entire model lifecycle. Cybersecurity protection related to systemic risks associated with malicious use or attacks should duly consider accidental model leakage, unauthorised releases, circumvention of safety measures, and defence against cyberattacks, unauthorised access or model theft. That protection could be facilitated by securing model weights, algorithms, servers, and data sets, such as through operational security measures for information security, specific cybersecurity policies, adequate technical and established solutions, and cyber and physical access controls, appropriate to the relevant circumstances and the risks involved.",
      "original_content": "(115) Anbieter von KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko sollten mögliche systemische Risiken bewerten und mindern. Wenn trotz der Bemühungen um Ermittlung und Vermeidung von Risiken im Zusammenhang mit einem KI-Modell mit allgemeinem Verwendungszweck, das systemische Risiken bergen könnte, die Entwicklung oder Verwendung des Modells einen schwerwiegenden Vorfall verursacht, so sollte der Anbieter des KI-Modells mit allgemeinem Verwendungszweck unverzüglich dem Vorfall nachgehen und der Kommission und den zuständigen nationalen Behörden alle einschlägigen Informationen und mögliche Korrekturmaßnahmen mitteilen. Zudem sollten die Anbieter während des gesamten Lebenszyklus des Modells ein angemessenes Maß an Cybersicherheit für das Modell und seine physische Infrastruktur gewährleisten. Beim Schutz der Cybersicherheit im Zusammenhang mit systemischen Risiken, die mit böswilliger Nutzung oder böswilligen Angriffen verbunden sind, sollte der unbeabsichtigte Modelldatenverlust, die unerlaubte Bereitstellung, die Umgehung von Sicherheitsmaßnahmen und der Schutz vor Cyberangriffen, unbefugtem Zugriff oder Modelldiebstahl gebührend beachtet werden. Dieser Schutz könnte durch die Sicherung von Modellgewichten, Algorithmen, Servern und Datensätzen erleichtert werden, z. B. durch Betriebssicherheitsmaßnahmen für die Informationssicherheit, spezifische Cybersicherheitsstrategien, geeignete technische und etablierte Lösungen sowie Kontrollen des physischen Zugangs und des Cyberzugangs, die den jeweiligen Umständen und den damit verbundenen Risiken angemessen sind."
    },
    {
      "chunk_idx": 115,
      "id": "5a1b156a-cc8f-4a42-aa77-4853f6aff72f",
      "title": "Recital 116",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(116) The AI Office should encourage and facilitate the drawing up, review and adaptation of codes of practice, taking into account international approaches. All providers of general-purpose AI models could be invited to participate. To ensure that the codes of practice reflect the state of the art and duly take into account a diverse set of perspectives, the AI Office should collaborate with relevant national competent authorities, and could, where appropriate, consult with civil society organisations and other relevant stakeholders and experts, including the Scientific Panel, for the drawing up of such codes. Codes of practice should cover obligations for providers of general-purpose AI models and of general-purpose AI models presenting systemic risks. In addition, as regards systemic risks, codes of practice should help to establish a risk taxonomy of the type and nature of the systemic risks at Union level, including their sources. Codes of practice should also be focused on specific risk assessment and mitigation measures.",
      "original_content": "(116) Das Büro für Künstliche Intelligenz sollte die Ausarbeitung, Überprüfung und Anpassung von Praxisleitfäden unter Berücksichtigung internationaler Ansätze fördern und erleichtern. Alle Anbieter von KI-Modellen mit allgemeinem Verwendungszweck könnten ersucht werden, sich daran zu beteiligen. Um sicherzustellen, dass die Praxisleitfäden dem Stand der Technik entsprechen und unterschiedlichen Perspektiven gebührend Rechnung tragen, sollte das Büro für Künstliche Intelligenz bei der Ausarbeitung solcher Leitfäden mit den einschlägigen zuständigen nationalen Behörden zusammenarbeiten und könnte dabei gegebenenfalls Organisationen der Zivilgesellschaft und andere einschlägige Interessenträger und Sachverständige, einschließlich des wissenschaftlichen Gremiums, konsultieren. Die Praxisleitfäden sollten die Pflichten für Anbieter von KI-Modellen mit allgemeinem Verwendungszweck und von KI-Modellen mit allgemeinem Verwendungszweck, die systemische Risiken bergen, abdecken. Ferner sollten Praxisleitfäden im Zusammenhang mit systemischen Risiken dazu beitragen, dass eine Risikotaxonomie für Art und Wesen der systemischen Risiken auf Unionsebene, einschließlich ihrer Ursachen, festgelegt wird. Bei den Praxisleitfäden sollten auch auf spezifische Maßnahmen zur Risikobewertung und -minderung im Mittelpunkt stehen."
    },
    {
      "chunk_idx": 116,
      "id": "29a9575c-1751-41a9-868d-6f767a78ee56",
      "title": "Recital 117",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(117) The codes of practice should represent a central tool for the proper compliance with the obligations provided for under this Regulation for providers of general-purpose AI models. Providers should be able to rely on codes of practice to demonstrate compliance with the obligations. By means of implementing acts, the Commission may decide to approve a code of practice and give it a general validity within the Union, or, alternatively, to provide common rules for the implementation of the relevant obligations, if, by the time this Regulation becomes applicable, a code of practice cannot be finalised or is not deemed adequate by the AI Office. Once a harmonised standard is published and assessed as suitable to cover the relevant obligations by the AI Office, compliance with a European harmonised standard should grant providers the presumption of conformity. Providers of general-purpose AI models should furthermore be able to demonstrate compliance using alternative adequate means, if codes of practice or harmonised standards are not available, or they choose not to rely on those.",
      "original_content": "(117) Die Verhaltenskodizes sollten ein zentrales Instrument für die ordnungsgemäße Einhaltung der in dieser Verordnung für Anbieter von KI-Modellen mit allgemeinem Verwendungszweck vorgesehenen Pflichten darstellen. Die Anbieter sollten sich auf Verhaltenskodizes stützen können, um die Einhaltung der Pflichten nachzuweisen. Die Kommission kann im Wege von Durchführungsrechtsakten beschließen, einen Praxisleitfaden zu genehmigen und ihm eine allgemeine Gültigkeit in der Union zu verleihen oder alternativ gemeinsame Vorschriften für die Umsetzung der einschlägigen Pflichten festzulegen, wenn ein Verhaltenskodex bis zum Zeitpunkt der Anwendbarkeit dieser Verordnung nicht fertiggestellt werden kann oder dies vom Büro für Künstliche Intelligenz für nicht angemessen erachtet wird. Sobald eine harmonisierte Norm veröffentlicht und als geeignet bewertet wurde, um die einschlägigen Pflichten des Büros für Künstliche Intelligenz abzudecken, sollte die Einhaltung einer harmonisierten europäischen Norm den Anbietern die Konformitätsvermutung begründen. Anbieter von KI-Modellen mit allgemeinem Verwendungszweck sollten darüber hinaus in der Lage sein, die Konformität mit angemessenen alternativen Mitteln nachzuweisen, wenn Praxisleitfäden oder harmonisierte Normen nicht verfügbar sind oder sie sich dafür entscheiden, sich nicht auf diese zu stützen."
    },
    {
      "chunk_idx": 117,
      "id": "525946cd-e7be-4cdc-9f49-6d8d48ea6199",
      "title": "Recital 118",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(118) This Regulation regulates AI systems and AI models by imposing certain requirements and obligations for relevant market actors that are placing them on the market, putting into service or use in the Union, thereby complementing obligations for providers of intermediary services that embed such systems or models into their services regulated by Regulation (EU) 2022/2065. To the extent that such systems or models are embedded into designated very large online platforms or very large online search engines, they are subject to the risk-management framework provided for in Regulation (EU) 2022/2065. Consequently, the corresponding obligations of this Regulation should be presumed to be fulfilled, unless significant systemic risks not covered by Regulation (EU) 2022/2065 emerge and are identified in such models. Within this framework, providers of very large online platforms and very large online search engines are obliged to assess potential systemic risks stemming from the design, functioning and use of their services, including how the design of algorithmic systems used in the service may contribute to such risks, as well as systemic risks stemming from potential misuses. Those providers are also obliged to take appropriate mitigating measures in observance of fundamental rights.",
      "original_content": "(118) Mit dieser Verordnung werden KI-Systeme und KI-Modelle reguliert, indem einschlägigen Marktteilnehmern, die sie in der Union in Verkehr bringen, in Betrieb nehmen oder verwenden, bestimmte Anforderungen und Pflichten auferlegt werden, wodurch die Pflichten für Anbieter von Vermittlungsdiensten ergänzt werden, die solche Systeme oder Modelle in ihre unter die Verordnung (EU) 2022/2065 fallenden Dienste integrieren. Soweit solche Systeme oder Modelle in als sehr groß eingestufte Online-Plattformen oder als sehr groß eingestufte Online-Suchmaschinen eingebettet sind, unterliegen sie dem in der Verordnung (EU) 2022/2065 vorgesehenen Rahmen für das Risikomanagement. Folglich sollte angenommen werden, dass die entsprechenden Verpflichtungen dieser Verordnung erfüllt sind, es sei denn, in solchen Modellen treten erhebliche systemische Risiken auf, die nicht unter die Verordnung (EU) 2022/2065 fallen, und werden dort ermittelt. Im vorliegenden Rahmen sind Anbieter sehr großer Online-Plattformen und sehr großer Online-Suchmaschinen verpflichtet, potenzielle systemische Risiken, die sich aus dem Entwurf, dem Funktionieren und der Nutzung ihrer Dienste ergeben, einschließlich der Frage, wie der Entwurf der in dem Dienst verwendeten algorithmischen Systeme zu solchen Risiken beitragen kann, sowie systemische Risiken, die sich aus potenziellen Fehlanwendungen ergeben, zu bewerten. Diese Anbieter sind zudem verpflichtet, unter Wahrung der Grundrechte geeignete Risikominderungsmaßnahmen zu ergreifen."
    },
    {
      "chunk_idx": 118,
      "id": "ffdebb2b-b187-494e-a8b1-c85372d41a5a",
      "title": "Recital 119",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(119) Considering the quick pace of innovation and the technological evolution of digital services in scope of different instruments of Union law in particular having in mind the usage and the perception of their recipients, the AI systems subject to this Regulation may be provided as intermediary services or parts thereof within the meaning of Regulation (EU) 2022/2065, which should be interpreted in a technology-neutral manner. For example, AI systems may be used to provide online search engines, in particular, to the extent that an AI system such as an online chatbot performs searches of, in principle, all websites, then incorporates the results into its existing knowledge and uses the updated knowledge to generate a single output that combines different sources of information.",
      "original_content": "(119) Angesichts des raschen Innovationstempos und der technologischen Entwicklung digitaler Dienste, die in den Anwendungsbereich verschiedener Instrumente des Unionsrechts fallen, können insbesondere unter Berücksichtigung der Verwendung durch ihre Nutzer und deren Wahrnehmung die dieser Verordnung unterliegenden KI-Systeme als Vermittlungsdienste oder Teile davon im Sinne der Verordnung (EU) 2022/2065 bereitgestellt werden, was technologieneutral ausgelegt werden sollte. Beispielsweise können KI-Systeme als Online-Suchmaschinen verwendet werden, insbesondere wenn ein KI-System wie ein Online-Chatbot grundsätzlich alle Websites durchsucht, die Ergebnisse anschließend in sein vorhandenes Wissen integriert und das aktualisierte Wissen nutzt, um eine einzige Ausgabe zu generieren, bei der verschiedene Informationsquellen zusammengeführt wurden."
    },
    {
      "chunk_idx": 119,
      "id": "8f179a57-c1c8-4502-b569-01a45d4dbd4b",
      "title": "Recital 120",
      "relevantChunksIds": [
        "b47202cc-5133-4ed0-aca4-2ec68b90632f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(120) Furthermore, obligations placed on providers and deployers of certain AI systems in this Regulation to enable the detection and disclosure that the outputs of those systems are artificially generated or manipulated are particularly relevant to facilitate the effective implementation of Regulation (EU) 2022/2065. This applies in particular as regards the obligations of providers of very large online platforms or very large online search engines to identify and mitigate systemic risks that may arise from the dissemination of content that has been artificially generated or manipulated, in particular risk of the actual or foreseeable negative effects on democratic processes, civic discourse and electoral processes, including through disinformation.",
      "original_content": "(120) Zudem sind die Pflichten, die Anbietern und Betreibern bestimmter KI-Systeme mit dieser Verordnung auferlegt werden, um die Feststellung und Offenlegung zu ermöglichen, dass die Ausgaben dieser Systeme künstlich erzeugt oder manipuliert werden, von besonderer Bedeutung für die Erleichterung der wirksamen Umsetzung der Verordnung (EU) 2022/2065. Dies gilt insbesondere für die Pflichten der Anbieter sehr großer Online-Plattformen oder sehr großer Online-Suchmaschinen, systemische Risiken zu ermitteln und zu mindern, die aus der Verbreitung von künstlich erzeugten oder manipulierten Inhalten entstehen können, insbesondere das Risiko tatsächlicher oder vorhersehbarer negativer Auswirkungen auf demokratische Prozesse, den gesellschaftlichen Diskurs und Wahlprozesse, unter anderem durch Desinformation."
    },
    {
      "chunk_idx": 120,
      "id": "4f2899cb-4889-4299-8ac6-1c62a78e48ba",
      "title": "Recital 121",
      "relevantChunksIds": [
        "2c00d4ce-c731-4696-9693-eda24b9eaf27",
        "e7a7f354-d2ea-480e-80eb-6f29beca4569"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(121) Standardisation should play a key role to provide technical solutions to providers to ensure compliance with this Regulation, in line with the state of the art, to promote innovation as well as competitiveness and growth in the single market. Compliance with harmonised standards as defined in Article 2, point (1)(c), of Regulation (EU) No 1025/2012 of the European Parliament and of the Council (41), which are normally expected to reflect the state of the art, should be a means for providers to demonstrate conformity with the requirements of this Regulation. A balanced representation of interests involving all relevant stakeholders in the development of standards, in particular SMEs, consumer organisations and environmental and social stakeholders in accordance with Articles 5 and 6 of Regulation (EU) No 1025/2012 should therefore be encouraged. In order to facilitate compliance, the standardisation requests should be issued by the Commission without undue delay. When preparing the standardisation request, the Commission should consult the advisory forum and the Board in order to collect relevant expertise. However, in the absence of relevant references to harmonised standards, the Commission should be able to establish, via implementing acts, and after consultation of the advisory forum, common specifications for certain requirements under this Regulation. The common specification should be an exceptional fall back solution to facilitate the provider’s obligation to comply with the requirements of this Regulation, when the standardisation request has not been accepted by any of the European standardisation organisations, or when the relevant harmonised standards insufficiently address fundamental rights concerns, or when the harmonised standards do not comply with the request, or when there are delays in the adoption of an appropriate harmonised standard. Where such a delay in the adoption of a harmonised standard is due to the technical complexity of that standard, this should be considered by the Commission before contemplating the establishment of common specifications. When developing common specifications, the Commission is encouraged to cooperate with international partners and international standardisation bodies.",
      "original_content": "(121) Die Normung sollte eine Schlüsselrolle dabei spielen, den Anbietern technische Lösungen zur Verfügung zu stellen, um im Einklang mit dem Stand der Technik die Einhaltung dieser Verordnung zu gewährleisten und Innovation sowie Wettbewerbsfähigkeit und Wachstum im Binnenmarkt zu fördern. Die Einhaltung harmonisierter Normen im Sinne von Artikel 2 Nummer 1 Buchstabe c der Verordnung (EU) Nr. 1025/2012 des Europäischen Parlaments und des Rates (Fußnote 41), die normalerweise den Stand der Technik widerspiegeln sollten, sollte den Anbietern den Nachweis der Konformität mit den Anforderungen der vorliegenden Verordnung ermöglichen. Daher sollte eine ausgewogene Interessenvertretung unter Einbeziehung aller relevanten Interessenträger, insbesondere KMU, Verbraucherorganisationen sowie ökologischer und sozialer Interessenträger, bei der Entwicklung von Normen gemäß den Artikeln 5 und 6 der Verordnung (EU) Nr. 1025/2012, gefördert werden. Um die Einhaltung der Vorschriften zu erleichtern, sollten die Normungsaufträge von der Kommission unverzüglich erteilt werden. Bei der Ausarbeitung des Normungsauftrags sollte die Kommission das Beratungsforum und das KI-Gremium konsultieren, um einschlägiges Fachwissen einzuholen. In Ermangelung einschlägiger Fundstellen zu harmonisierten Normen sollte die Kommission jedoch im Wege von Durchführungsrechtsakten und nach Konsultation des Beratungsforums gemeinsame Spezifikationen für bestimmte Anforderungen im Rahmen dieser Verordnung festlegen können. Die gemeinsame Spezifikation sollte eine außergewöhnliche Ausweichlösung sein, um die Pflicht des Anbieters zur Einhaltung der Anforderungen dieser Verordnung zu erleichtern, wenn der Normungsauftrag von keiner der europäischen Normungsorganisationen angenommen wurde oder die einschlägigen harmonisierten Normen den Bedenken im Bereich der Grundrechte nicht ausreichend Rechnung tragen oder die harmonisierten Normen dem Auftrag nicht entsprechen oder es Verzögerungen bei der Annahme einer geeigneten harmonisierten Norm gibt. Ist eine solche Verzögerung bei der Annahme einer harmonisierten Norm auf die technische Komplexität dieser Norm zurückzuführen, so sollte die Kommission dies prüfen, bevor sie die Festlegung gemeinsamer Spezifikationen in Erwägung zieht. Die Kommission wird ermutigt, bei der Entwicklung gemeinsamer Spezifikationen mit internationalen Partnern und internationalen Normungsgremien zusammenzuarbeiten. Fußnote 41: Verordnung (EU) Nr. 1025/2012 des Europäischen Parlaments und des Rates vom 25. Oktober 2012 zur europäischen Normung, zur Änderung der Richtlinien 89/686/EWG und 93/15/EWG des Rates sowie der Richtlinien 94/9/EG, 94/25/EG, 95/16/EG, 97/23/EG, 98/34/EG, 2004/22/EG, 2007/23/EG, 2009/23/EG und 2009/105/EG des Europäischen Parlaments und des Rates und zur Aufhebung des Beschlusses 87/95/EWG des Rates und des Beschlusses Nr. 1673/2006/EG des Europäischen Parlaments und des Rates (ABl. L 316 vom 14.11.2012, S. 12)."
    },
    {
      "chunk_idx": 121,
      "id": "b7a14263-88d3-44cc-9341-5e2d1a7ddc73",
      "title": "Recital 122",
      "relevantChunksIds": [
        "7461fda7-1c33-468c-9be3-4f3ad8d29452",
        "aeae4d53-6d81-4c49-843d-b93704615c05"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(122) It is appropriate that, without prejudice to the use of harmonised standards and common specifications, providers of a high-risk AI system that has been trained and tested on data reflecting the specific geographical, behavioural, contextual or functional setting within which the AI system is intended to be used, should be presumed to comply with the relevant measure provided for under the requirement on data governance set out in this Regulation. Without prejudice to the requirements related to robustness and accuracy set out in this Regulation, in accordance with Article 54(3) of Regulation (EU) 2019/881, high-risk AI systems that have been certified or for which a statement of conformity has been issued under a cybersecurity scheme pursuant to that Regulation and the references of which have been published in the Official Journal of the European Union should be presumed to comply with the cybersecurity requirement of this Regulation in so far as the cybersecurity certificate or statement of conformity or parts thereof cover the cybersecurity requirement of this Regulation. This remains without prejudice to the voluntary nature of that cybersecurity scheme.",
      "original_content": "(122) Unbeschadet der Anwendung harmonisierter Normen und gemeinsamer Spezifikationen ist es angezeigt, dass für Anbieter von Hochrisiko-KI-Systemen, die mit Daten, in denen sich die besonderen geografischen, verhaltensbezogenen, kontextuellen oder funktionalen Rahmenbedingungen niederschlagen, unter denen sie verwendet werden sollen, trainiert und getestet wurden, die Vermutung der Konformität mit der einschlägigen Maßnahme gilt, die im Rahmen der in dieser Verordnung festgelegten Anforderungen an die Daten-Governance vorgesehen ist. Unbeschadet der in dieser Verordnung festgelegten Anforderungen an Robustheit und Genauigkeit sollte gemäß Artikel 54 Absatz 3 der Verordnung (EU) 2019/881 bei Hochrisiko-KI-Systemen, die im Rahmen eines Schemas für die Cybersicherheit gemäß der genannten Verordnung zertifiziert wurden oder für die eine Konformitätserklärung ausgestellt wurde und deren Fundstellen im Amtsblatt der Europäischen Union veröffentlicht wurden, vermutet werden, dass eine Übereinstimmung mit den Cybersicherheitsanforderungen der vorliegenden Verordnung gegeben ist, sofern das Cybersicherheitszertifikat oder die Konformitätserklärung oder Teile davon die Cybersicherheitsanforderungen dieser Verordnung abdecken. Dies gilt unbeschadet des freiwilligen Charakters dieses Schemas für die Cybersicherheit."
    },
    {
      "chunk_idx": 122,
      "id": "dffe5a54-c8df-4391-9a4c-7180f3dbda14",
      "title": "Recital 123",
      "relevantChunksIds": [
        "aeae4d53-6d81-4c49-843d-b93704615c05"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(123) In order to ensure a high level of trustworthiness of high-risk AI systems, those systems should be subject to a conformity assessment prior to their placing on the market or putting into service.",
      "original_content": "(123) Um ein hohes Maß an Vertrauenswürdigkeit von Hochrisiko-KI-Systemen zu gewährleisten, sollten diese Systeme einer Konformitätsbewertung unterzogen werden, bevor sie in Verkehr gebracht oder in Betrieb genommen werden."
    },
    {
      "chunk_idx": 123,
      "id": "33677500-8ade-4c9b-96ec-eff3f9ffb701",
      "title": "Recital 124",
      "relevantChunksIds": [
        "aeae4d53-6d81-4c49-843d-b93704615c05"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(124) It is appropriate that, in order to minimise the burden on operators and avoid any possible duplication, for high-risk AI systems related to products which are covered by existing Union harmonisation legislation based on the New Legislative Framework, the compliance of those AI systems with the requirements of this Regulation should be assessed as part of the conformity assessment already provided for in that law. The applicability of the requirements of this Regulation should thus not affect the specific logic, methodology or general structure of conformity assessment under the relevant Union harmonisation legislation.",
      "original_content": "(124) Damit für Akteure möglichst wenig Aufwand entsteht und etwaige Doppelarbeit vermieden wird, ist es angezeigt, dass bei Hochrisiko-KI-Systemen im Zusammenhang mit Produkten, die auf der Grundlage des neuen Rechtsrahmens unter bestehende Harmonisierungsrechtsvorschriften der Union fallen, im Rahmen der bereits in den genannten Rechtsvorschriften vorgesehenen Konformitätsbewertung bewertet wird, ob diese KI-Systeme den Anforderungen dieser Verordnung genügen. Die Anwendbarkeit der Anforderungen dieser Verordnung sollte daher die besondere Logik, Methodik oder allgemeine Struktur der Konformitätsbewertung gemäß den einschlägigen Harmonisierungsrechtsvorschriften der Union unberührt lassen."
    },
    {
      "chunk_idx": 124,
      "id": "c7c4a6e2-92a4-40d5-9243-1d698685d1a0",
      "title": "Recital 125",
      "relevantChunksIds": [
        "897ec415-d93f-4928-9ce9-712efa9fd275",
        "49563d87-4fce-4a1e-adc5-839e980a9965",
        "8eb6153c-6d1f-4d6b-b0ec-1b046b06ee6b",
        "57f3c023-6709-4194-b70c-d00ff5c769b5",
        "aeae4d53-6d81-4c49-843d-b93704615c05"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(125) Given the complexity of high-risk AI systems and the risks that are associated with them, it is important to develop an adequate conformity assessment procedure for high-risk AI systems involving notified bodies, so-called third party conformity assessment. However, given the current experience of professional pre-market certifiers in the field of product safety and the different nature of risks involved, it is appropriate to limit, at least in an initial phase of application of this Regulation, the scope of application of third-party conformity assessment for high-risk AI systems other than those related to products. Therefore, the conformity assessment of such systems should be carried out as a general rule by the provider under its own responsibility, with the only exception of AI systems intended to be used for biometrics.",
      "original_content": "(125) Angesichts der Komplexität von Hochrisiko-KI-Systemen und der damit verbundenen Risiken ist es wichtig, ein angemessenes Konformitätsbewertungsverfahren für Hochrisiko-KI-Systeme, an denen notifizierte Stellen beteiligt sind, — die sogenannte Konformitätsbewertung durch Dritte — zu entwickeln. In Anbetracht der derzeitigen Erfahrung professioneller dem Inverkehrbringen vorgeschalteter Zertifizierer im Bereich der Produktsicherheit und der unterschiedlichen Art der damit verbundenen Risiken empfiehlt es sich jedoch, zumindest während der anfänglichen Anwendung dieser Verordnung für Hochrisiko-KI-Systeme, die nicht mit Produkten in Verbindung stehen, den Anwendungsbereich der Konformitätsbewertung durch Dritte einzuschränken. Daher sollte die Konformitätsbewertung solcher Systeme in der Regel vom Anbieter in eigener Verantwortung durchgeführt werden, mit Ausnahme von KI-Systemen, die für die Biometrie verwendet werden sollen."
    },
    {
      "chunk_idx": 125,
      "id": "ab48c7d3-9758-42ad-80bd-2aef56bf20b4",
      "title": "Recital 126",
      "relevantChunksIds": [
        "a95f9b76-59d0-4655-9859-ce20a0190849",
        "49563d87-4fce-4a1e-adc5-839e980a9965",
        "8eb6153c-6d1f-4d6b-b0ec-1b046b06ee6b",
        "57f3c023-6709-4194-b70c-d00ff5c769b5",
        "aeae4d53-6d81-4c49-843d-b93704615c05",
        "6132fe3a-5c8b-4fbe-8bd9-0fd34eea0e03"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(126) In order to carry out third-party conformity assessments when so required, notified bodies should be notified under this Regulation by the national competent authorities, provided that they comply with a set of requirements, in particular on independence, competence, absence of conflicts of interests and suitable cybersecurity requirements. Notification of those bodies should be sent by national competent authorities to the Commission and the other Member States by means of the electronic notification tool developed and managed by the Commission pursuant to Article R23 of Annex I to Decision No 768/2008/EC.",
      "original_content": "(126) Damit KI-Systeme, falls vorgeschrieben, Konformitätsbewertungen durch Dritte unterzogen werden können, sollten die notifizierten Stellen gemäß dieser Verordnung von den zuständigen nationalen Behörden notifiziert werden, sofern sie eine Reihe von Anforderungen erfüllen, insbesondere in Bezug auf Unabhängigkeit, Kompetenz, Nichtvorliegen von Interessenkonflikten und geeignete Anforderungen an die Cybersicherheit. Die Notifizierung dieser Stellen sollte von den zuständigen nationalen Behörden der Kommission und den anderen Mitgliedstaaten mittels des von der Kommission entwickelten und verwalteten elektronischen Notifizierungsinstruments gemäß Anhang I Artikel R23 des Beschlusses Nr. 768/2008/EG übermittelt werden."
    },
    {
      "chunk_idx": 126,
      "id": "a709f623-ae8c-4f40-97fc-ce028af806d3",
      "title": "Recital 127",
      "relevantChunksIds": [
        "aeae4d53-6d81-4c49-843d-b93704615c05"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(127) In line with Union commitments under the World Trade Organization Agreement on Technical Barriers to Trade, it is adequate to facilitate the mutual recognition of conformity assessment results produced by competent conformity assessment bodies, independent of the territory in which they are established, provided that those conformity assessment bodies established under the law of a third country meet the applicable requirements of this Regulation and the Union has concluded an agreement to that extent. In this context, the Commission should actively explore possible international instruments for that purpose and in particular pursue the conclusion of mutual recognition agreements with third countries.",
      "original_content": "(127) Im Einklang mit den Verpflichtungen der Union im Rahmen des Übereinkommens der Welthandelsorganisation über technische Handelshemmnisse ist es angemessen, die gegenseitige Anerkennung von Konformitätsbewertungsergebnissen zu erleichtern, die von den zuständigen Konformitätsbewertungsstellen unabhängig von dem Gebiet, in dem sie niedergelassen sind, generiert wurden, sofern diese nach dem Recht eines Drittlandes errichteten Konformitätsbewertungsstellen die geltenden Anforderungen dieser Verordnung erfüllen und die Union ein entsprechendes Abkommen geschlossen hat. In diesem Zusammenhang sollte die Kommission aktiv mögliche internationale Instrumente zu diesem Zweck prüfen und insbesondere den Abschluss von Abkommen über die gegenseitige Anerkennung mit Drittländern anstreben."
    },
    {
      "chunk_idx": 127,
      "id": "16e58dc6-88e2-4ab6-8bb3-6f208fb0c25d",
      "title": "Recital 128",
      "relevantChunksIds": [
        "aeae4d53-6d81-4c49-843d-b93704615c05"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(128) In line with the commonly established notion of substantial modification for products regulated by Union harmonisation legislation, it is appropriate that whenever a change occurs which may affect the compliance of a high-risk AI system with this Regulation (e.g. change of operating system or software architecture), or when the intended purpose of the system changes, that AI system should be considered to be a new AI system which should undergo a new conformity assessment. However, changes occurring to the algorithm and the performance of AI systems which continue to ‘learn’ after being placed on the market or put into service, namely automatically adapting how functions are carried out, should not constitute a substantial modification, provided that those changes have been pre-determined by the provider and assessed at the moment of the conformity assessment.",
      "original_content": "(128) Im Einklang mit dem allgemein anerkannten Begriff der wesentlichen Änderung von Produkten, für die Harmonisierungsvorschriften der Union gelten, ist es angezeigt, dass das KI-System bei jeder Änderung, die die Einhaltung dieser Verordnung durch das Hochrisiko-KI-System beeinträchtigen könnte (z. B. Änderung des Betriebssystems oder der Softwarearchitektur), oder wenn sich die Zweckbestimmung des Systems ändert, als neues KI-System betrachtet werden sollte, das einer neuen Konformitätsbewertung unterzogen werden sollte. Änderungen, die den Algorithmus und die Leistung von KI-Systemen betreffen, die nach dem Inverkehrbringen oder der Inbetriebnahme weiterhin dazulernen — d. h., sie passen automatisch an, wie die Funktionen ausgeführt werden —, sollten jedoch keine wesentliche Veränderung darstellen, sofern diese Änderungen vom Anbieter vorab festgelegt und zum Zeitpunkt der Konformitätsbewertung bewertet wurden."
    },
    {
      "chunk_idx": 128,
      "id": "5668652e-ee66-42a3-9cd2-d8f70595e52e",
      "title": "Recital 129",
      "relevantChunksIds": [
        "8eb6153c-6d1f-4d6b-b0ec-1b046b06ee6b",
        "9819f1c8-9e45-4ff4-8edf-575415272e9f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(129) High-risk AI systems should bear the CE marking to indicate their conformity with this Regulation so that they can move freely within the internal market. For high-risk AI systems embedded in a product, a physical CE marking should be affixed, and may be complemented by a digital CE marking. For high-risk AI systems only provided digitally, a digital CE marking should be used. Member States should not create unjustified obstacles to the placing on the market or the putting into service of high-risk AI systems that comply with the requirements laid down in this Regulation and bear the CE marking.",
      "original_content": "(129) Hochrisiko-KI-Systeme sollten grundsätzlich mit der CE-Kennzeichnung versehen sein, aus der ihre Konformität mit dieser Verordnung hervorgeht, sodass sie frei im Binnenmarkt verkehren können. Bei in ein Produkt integrierten Hochrisiko-KI-Systemen sollte eine physische CE-Kennzeichnung angebracht werden, die durch eine digitale CE-Kennzeichnung ergänzt werden kann. Bei Hochrisiko-KI-Systemen, die nur digital bereitgestellt werden, sollte eine digitale CE-Kennzeichnung verwendet werden. Die Mitgliedstaaten sollten keine ungerechtfertigten Hindernisse für das Inverkehrbringen oder die Inbetriebnahme von Hochrisiko-KI-Systemen schaffen, die die in dieser Verordnung festgelegten Anforderungen erfüllen und mit der CE-Kennzeichnung versehen sind."
    },
    {
      "chunk_idx": 129,
      "id": "2484d9b1-5411-470b-8e36-171561caba05",
      "title": "Recital 130",
      "relevantChunksIds": [
        "2a95bf85-4c68-478b-b8d0-0d8874b0a5c5"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(130) Under certain conditions, rapid availability of innovative technologies may be crucial for health and safety of persons, the protection of the environment and climate change and for society as a whole. It is thus appropriate that under exceptional reasons of public security or protection of life and health of natural persons, environmental protection and the protection of key industrial and infrastructural assets, market surveillance authorities could authorise the placing on the market or the putting into service of AI systems which have not undergone a conformity assessment. In duly justified situations, as provided for in this Regulation, law enforcement authorities or civil protection authorities may put a specific high-risk AI system into service without the authorisation of the market surveillance authority, provided that such authorisation is requested during or after the use without undue delay.",
      "original_content": "(130) Unter bestimmten Bedingungen kann die rasche Verfügbarkeit innovativer Technik für die Gesundheit und Sicherheit von Menschen, den Schutz der Umwelt und vor dem Klimawandel und die Gesellschaft insgesamt von entscheidender Bedeutung sein. Es ist daher angezeigt, dass die Aufsichtsbehörden aus außergewöhnlichen Gründen der öffentlichen Sicherheit, des Schutzes des Lebens und der Gesundheit natürlicher Personen, des Umweltschutzes und des Schutzes wichtiger Industrie- und Infrastrukturanlagen das Inverkehrbringen oder die Inbetriebnahme von KI-Systemen, die keiner Konformitätsbewertung unterzogen wurden, genehmigen könnten. In hinreichend begründeten Fällen gemäß dieser Verordnung können Strafverfolgungs- oder Katastrophenschutzbehörden ein bestimmtes Hochrisiko-KI-System ohne Genehmigung der Marktüberwachungsbehörde in Betrieb nehmen, sofern diese Genehmigung während der Verwendung oder im Anschluss daran unverzüglich beantragt wird."
    },
    {
      "chunk_idx": 130,
      "id": "90893d61-25fd-4ae1-8232-5e4cdb5675e9",
      "title": "Recital 131",
      "relevantChunksIds": [
        "80666904-0b31-4c56-8bd9-2d238d969a2f",
        "53305eb0-be55-40af-8070-94d5214e877c",
        "bea96906-be50-4eb8-9cbe-cf07816645c5",
        "e85b8add-b4ef-4422-bafa-b843f3f02662"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(131) In order to facilitate the work of the Commission and the Member States in the AI field as well as to increase the transparency towards the public, providers of high-risk AI systems other than those related to products falling within the scope of relevant existing Union harmonisation legislation, as well as providers who consider that an AI system listed in the high-risk use cases in an annex to this Regulation is not high-risk on the basis of a derogation, should be required to register themselves and information about their AI system in an EU database, to be established and managed by the Commission. Before using an AI system listed in the high-risk use cases in an annex to this Regulation, deployers of high-risk AI systems that are public authorities, agencies or bodies, should register themselves in such database and select the system that they envisage to use. Other deployers should be entitled to do so voluntarily. This section of the EU database should be publicly accessible, free of charge, the information should be easily navigable, understandable and machine-readable. The EU database should also be user-friendly, for example by providing search functionalities, including through keywords, allowing the general public to find relevant information to be submitted upon the registration of high-risk AI systems and on the use case of high-risk AI systems, set out in an annex to this Regulation, to which the high-risk AI systems correspond. Any substantial modification of high-risk AI systems should also be registered in the EU database. For high-risk AI systems in the area of law enforcement, migration, asylum and border control management, the registration obligations should be fulfilled in a secure non-public section of the EU database. Access to the secure non-public section should be strictly limited to the Commission as well as to market surveillance authorities with regard to their national section of that database. High-risk AI systems in the area of critical infrastructure should only be registered at national level. The Commission should be the controller of the EU database, in accordance with Regulation (EU) 2018/1725. In order to ensure the full functionality of the EU database, when deployed, the procedure for setting the database should include the development of functional specifications by the Commission and an independent audit report. The Commission should take into account cybersecurity risks when carrying out its tasks as data controller on the EU database. In order to maximise the availability and use of the EU database by the public, the EU database, including the information made available through it, should comply with requirements under the Directive (EU) 2019/882.",
      "original_content": "(131) Um die Arbeit der Kommission und der Mitgliedstaaten im KI-Bereich zu erleichtern und die Transparenz gegenüber der Öffentlichkeit zu erhöhen, sollten Anbieter von Hochrisiko-KI-Systemen, die nicht im Zusammenhang mit Produkten stehen, welche in den Anwendungsbereich einschlägiger Harmonisierungsrechtsvorschriften der Union fallen, und Anbieter, die der Auffassung sind, dass ein KI-System, das in den in einem Anhang dieser Verordnung aufgeführten Anwendungsfällen mit hohem Risiko aufgeführt ist, auf der Grundlage einer Ausnahme nicht hochriskant ist, dazu verpflichtet werden, sich und Informationen über ihr KI-System in einer von der Kommission einzurichtenden und zu verwaltenden EU-Datenbank zu registrieren. Vor der Verwendung eines KI-Systems, das in den in einem Anhang dieser Verordnung aufgeführten Anwendungsfällen mit hohem Risiko aufgeführt ist, sollten sich Betreiber von Hochrisiko-KI-Systemen, die Behörden, Einrichtungen oder sonstige Stellen sind, in dieser Datenbank registrieren und das System auswählen, dessen Verwendung sie planen. Andere Betreiber sollten berechtigt sein, dies freiwillig zu tun. Dieser Teil der EU-Datenbank sollte öffentlich und kostenlos zugänglich sein, und die Informationen sollten leicht zu navigieren, verständlich und maschinenlesbar sein. Die EU-Datenbank sollte außerdem benutzerfreundlich sein und beispielsweise die Suche, auch mit Stichwörtern, vorsehen, damit die breite Öffentlichkeit die einschlägigen Informationen finden kann, die bei der Registrierung von Hochrisiko-KI-Systemen einzureichen sind und die sich auf einen in einem Anhang dieser Verordnung aufgeführten Anwendungsfall der Hochrisiko-KI-Systeme, denen die betreffenden Hochrisiko-KI-Systeme entsprechen, beziehen. Jede wesentliche Veränderung von Hochrisiko-KI-Systemen sollte ebenfalls in der EU-Datenbank registriert werden. Bei Hochrisiko-KI-Systemen, die in den Bereichen Strafverfolgung, Migration, Asyl und Grenzkontrolle eingesetzt werden, sollten die Registrierungspflichten in einem sicheren nicht öffentlichen Teil der EU-Datenbank erfüllt werden. Der Zugang zu dem gesicherten nicht öffentlichen Teil sollte sich strikt auf die Kommission sowie auf die Marktüberwachungsbehörden und bei diesen auf ihren nationalen Teil dieser Datenbank beschränken. Hochrisiko-KI-Systeme im Bereich kritischer Infrastrukturen sollten nur auf nationaler Ebene registriert werden. Die Kommission sollte gemäß der Verordnung (EU) 2018/1725 als für die EU-Datenbank Verantwortlicher gelten. Um die volle Funktionsfähigkeit der EU-Datenbank zu gewährleisten, sollte das Verfahren für die Einrichtung der Datenbank auch die Entwicklung von funktionalen Spezifikationen durch die Kommission und einen unabhängigen Prüfbericht umfassen. Die Kommission sollte bei der Wahrnehmung ihrer Aufgaben als Verantwortliche für die EU-Datenbank die Risiken im Zusammenhang mit Cybersicherheit berücksichtigen. Um für ein Höchstmaß an Verfügbarkeit und Nutzung der EU-Datenbank durch die Öffentlichkeit zu sorgen, sollte die EU-Datenbank, einschließlich der über sie zur Verfügung gestellten Informationen, den Anforderungen der Richtlinie (EU) 2019/882 entsprechen."
    },
    {
      "chunk_idx": 131,
      "id": "42b80e11-733d-441e-98e1-d79837892537",
      "title": "Recital 132",
      "relevantChunksIds": [
        "986b540c-851f-4387-b2d1-e9b5d9bed869"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(132) Certain AI systems intended to interact with natural persons or to generate content may pose specific risks of impersonation or deception irrespective of whether they qualify as high-risk or not. In certain circumstances, the use of these systems should therefore be subject to specific transparency obligations without prejudice to the requirements and obligations for high-risk AI systems and subject to targeted exceptions to take into account the special need of law enforcement. In particular, natural persons should be notified that they are interacting with an AI system, unless this is obvious from the point of view of a natural person who is reasonably well-informed, observant and circumspect taking into account the circumstances and the context of use. When implementing that obligation, the characteristics of natural persons belonging to vulnerable groups due to their age or disability should be taken into account to the extent the AI system is intended to interact with those groups as well. Moreover, natural persons should be notified when they are exposed to AI systems that, by processing their biometric data, can identify or infer the emotions or intentions of those persons or assign them to specific categories. Such specific categories can relate to aspects such as sex, age, hair colour, eye colour, tattoos, personal traits, ethnic origin, personal preferences and interests. Such information and notifications should be provided in accessible formats for persons with disabilities.",
      "original_content": "(132) Bestimmte KI-Systeme, die mit natürlichen Personen interagieren oder Inhalte erzeugen sollen, können unabhängig davon, ob sie als hochriskant eingestuft werden, ein besonderes Risiko in Bezug auf Identitätsbetrug oder Täuschung bergen. Unter bestimmten Umständen sollte die Verwendung solcher Systeme daher — unbeschadet der Anforderungen an und Pflichten für Hochrisiko-KI-Systeme und vorbehaltlich punktueller Ausnahmen, um den besonderen Erfordernissen der Strafverfolgung Rechnung zu tragen — besonderen Transparenzpflichten unterliegen. Insbesondere sollte natürlichen Personen mitgeteilt werden, dass sie es mit einem KI-System zu tun haben, es sei denn, dies ist aus Sicht einer angemessen informierten, aufmerksamen und verständigen natürlichen Person aufgrund der Umstände und des Kontexts der Nutzung offensichtlich. Bei der Umsetzung dieser Pflicht sollten die Merkmale von natürlichen Personen, die aufgrund ihres Alters oder einer Behinderung schutzbedürftigen Gruppen angehören, berücksichtigt werden, soweit das KI-System auch mit diesen Gruppen interagieren soll. Darüber hinaus sollte natürlichen Personen mitgeteilt werden, wenn sie KI-Systemen ausgesetzt sind, die durch die Verarbeitung ihrer biometrischen Daten die Gefühle oder Absichten dieser Personen identifizieren oder ableiten oder sie bestimmten Kategorien zuordnen können. Solche spezifischen Kategorien können Aspekte wie etwa Geschlecht, Alter, Haarfarbe, Augenfarbe, Tätowierungen, persönliche Merkmale, ethnische Herkunft sowie persönliche Vorlieben und Interessen betreffen. Diese Informationen und Mitteilungen sollten für Menschen mit Behinderungen in entsprechend barrierefrei zugänglicher Form bereitgestellt werden."
    },
    {
      "chunk_idx": 132,
      "id": "aedc6b30-3ec3-4277-a8aa-bcc97fad2474",
      "title": "Recital 133",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(133) A variety of AI systems can generate large quantities of synthetic content that becomes increasingly hard for humans to distinguish from human-generated and authentic content. The wide availability and increasing capabilities of those systems have a significant impact on the integrity and trust in the information ecosystem, raising new risks of misinformation and manipulation at scale, fraud, impersonation and consumer deception. In light of those impacts, the fast technological pace and the need for new methods and techniques to trace origin of information, it is appropriate to require providers of those systems to embed technical solutions that enable marking in a machine readable format and detection that the output has been generated or manipulated by an AI system and not a human. Such techniques and methods should be sufficiently reliable, interoperable, effective and robust as far as this is technically feasible, taking into account available techniques or a combination of such techniques, such as watermarks, metadata identifications, cryptographic methods for proving provenance and authenticity of content, logging methods, fingerprints or other techniques, as may be appropriate. When implementing this obligation, providers should also take into account the specificities and the limitations of the different types of content and the relevant technological and market developments in the field, as reflected in the generally acknowledged state of the art. Such techniques and methods can be implemented at the level of the AI system or at the level of the AI model, including general-purpose AI models generating content, thereby facilitating fulfilment of this obligation by the downstream provider of the AI system. To remain proportionate, it is appropriate to envisage that this marking obligation should not cover AI systems performing primarily an assistive function for standard editing or AI systems not substantially altering the input data provided by the deployer or the semantics thereof.",
      "original_content": "(133) Eine Vielzahl von KI-Systemen kann große Mengen synthetischer Inhalte erzeugen, bei denen es für Menschen immer schwieriger wird, sie vom Menschen erzeugten und authentischen Inhalten zu unterscheiden. Die breite Verfügbarkeit und die zunehmenden Fähigkeiten dieser Systeme wirken sich erheblich auf die Integrität des Informationsökosystems und das ihm entgegengebrachte Vertrauen aus, weil neue Risiken in Bezug auf Fehlinformation und Manipulation in großem Maßstab, Betrug, Identitätsbetrug und Täuschung der Verbraucher entstehen. Angesichts dieser Auswirkungen, des raschen Tempos im Technologiebereich und der Notwendigkeit neuer Methoden und Techniken zur Rückverfolgung der Herkunft von Informationen sollten die Anbieter dieser Systeme verpflichtet werden, technische Lösungen zu integrieren, die die Kennzeichnung in einem maschinenlesbaren Format und die Feststellung ermöglichen, dass die Ausgabe von einem KI-System und nicht von einem Menschen erzeugt oder manipuliert wurde. Diese Techniken und Methoden sollten — soweit technisch möglich — hinreichend zuverlässig, interoperabel, wirksam und belastbar sein, wobei verfügbare Techniken, wie Wasserzeichen, Metadatenidentifizierungen, kryptografische Methoden zum Nachweis der Herkunft und Authentizität des Inhalts, Protokollierungsmethoden, Fingerabdrücke oder andere Techniken, oder eine Kombination solcher Techniken je nach Sachlage zu berücksichtigen sind. Bei der Umsetzung dieser Pflicht sollten die Anbieter auch die Besonderheiten und Einschränkungen der verschiedenen Arten von Inhalten und die einschlägigen technologischen Entwicklungen und Marktentwicklungen in diesem Bereich, die dem allgemein anerkannten Stand der Technik entsprechen, berücksichtigen. Solche Techniken und Methoden können auf der Ebene des KI-Systems oder der Ebene des KI-Modells, darunter KI-Modelle mit allgemeinem Verwendungszweck zur Erzeugung von Inhalten, angewandt werden, wodurch dem nachgelagerten Anbieter des KI-Systems die Erfüllung dieser Pflicht erleichtert wird. Um die Verhältnismäßigkeit zu wahren, sollte vorgesehen werden, dass diese Kennzeichnungspflicht weder für KI-Systeme, die in erster Linie eine unterstützende Funktion für die Standardbearbeitung ausführen, noch für KI-Systeme, die die vom Betreiber bereitgestellten Eingabedaten oder deren Semantik nicht wesentlich verändern, gilt."
    },
    {
      "chunk_idx": 133,
      "id": "b3367208-8bb2-457e-b691-a0408c96d242",
      "title": "Recital 134",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(134) Further to the technical solutions employed by the providers of the AI system, deployers who use an AI system to generate or manipulate image, audio or video content that appreciably resembles existing persons, objects, places, entities or events and would falsely appear to a person to be authentic or truthful (deep fakes), should also clearly and distinguishably disclose that the content has been artificially created or manipulated by labelling the AI output accordingly and disclosing its artificial origin. Compliance with this transparency obligation should not be interpreted as indicating that the use of the AI system or its output impedes the right to freedom of expression and the right to freedom of the arts and sciences guaranteed in the Charter, in particular where the content is part of an evidently creative, satirical, artistic, fictional or analogous work or programme, subject to appropriate safeguards for the rights and freedoms of third parties. In those cases, the transparency obligation for deep fakes set out in this Regulation is limited to disclosure of the existence of such generated or manipulated content in an appropriate manner that does not hamper the display or enjoyment of the work, including its normal exploitation and use, while maintaining the utility and quality of the work. In addition, it is also appropriate to envisage a similar disclosure obligation in relation to AI-generated or manipulated text to the extent it is published with the purpose of informing the public on matters of public interest unless the AI-generated content has undergone a process of human review or editorial control and a natural or legal person holds editorial responsibility for the publication of the content.",
      "original_content": "(134) Neben den technischen Lösungen, die von den Anbietern von KI-Systemen eingesetzt werden, sollten Betreiber, die ein KI-System zum Erzeugen oder Manipulieren von Bild-, Audio- oder Videoinhalte verwenden, die wirklichen Personen, Gegenständen, Orten, Einrichtungen oder Ereignissen merklich ähneln und einer Person fälschlicherweise echt oder wahr erscheinen würden (Deepfakes), auch klar und deutlich offenlegen, dass die Inhalte künstlich erzeugt oder manipuliert wurden, indem sie die Ausgaben von KI entsprechend kennzeichnen und auf ihren künstlichen Ursprung hinweisen. Die Einhaltung dieser Transparenzpflicht sollte nicht so ausgelegt werden, dass sie darauf hindeutet, dass die Verwendung des KI-Systems oder seiner Ausgabe das Recht auf freie Meinungsäußerung und das Recht auf Freiheit der Kunst und Wissenschaft, die in der Charta garantiert sind, behindern, insbesondere wenn der Inhalt Teil eines offensichtlich kreativen, satirischen, künstlerischen, fiktionalen oder analogen Werks oder Programms ist und geeignete Schutzvorkehrungen für die Rechte und Freiheiten Dritter bestehen. In diesen Fällen beschränkt sich die in dieser Verordnung festgelegte Transparenzpflicht für Deepfakes darauf, das Vorhandenseins solcher erzeugten oder manipulierten Inhalte in geeigneter Weise offenzulegen, die die Darstellung oder den Genuss des Werks, einschließlich seiner normalen Nutzung und Verwendung, nicht beeinträchtigt und gleichzeitig den Nutzen und die Qualität des Werks aufrechterhält. Darüber hinaus ist es angezeigt, eine ähnliche Offenlegungspflicht in Bezug auf durch KI erzeugte oder manipulierte Texte anzustreben, soweit diese veröffentlicht werden, um die Öffentlichkeit über Angelegenheiten von öffentlichem Interesse zu informieren, es sei denn, die durch KI erzeugten Inhalte wurden einem Verfahren der menschlichen Überprüfung oder redaktionellen Kontrolle unterzogen und eine natürliche oder juristische Person trägt die redaktionelle Verantwortung für die Veröffentlichung der Inhalte."
    },
    {
      "chunk_idx": 134,
      "id": "469d4ab0-c495-4611-95c6-edda07e5451d",
      "title": "Recital 135",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(135) Without prejudice to the mandatory nature and full applicability of the transparency obligations, the Commission may also encourage and facilitate the drawing up of codes of practice at Union level to facilitate the effective implementation of the obligations regarding the detection and labelling of artificially generated or manipulated content, including to support practical arrangements for making, as appropriate, the detection mechanisms accessible and facilitating cooperation with other actors along the value chain, disseminating content or checking its authenticity and provenance to enable the public to effectively distinguish AI-generated content.",
      "original_content": "(135) Unbeschadet des verbindlichen Charakters und der uneingeschränkten Anwendbarkeit der Transparenzpflichten kann die Kommission zudem die Ausarbeitung von Praxisleitfäden auf Unionsebene im Hinblick auf die Ermöglichung der wirksamen Umsetzung der Pflichten in Bezug auf die Feststellung und Kennzeichnung künstlich erzeugter oder manipulierter Inhalte erleichtern und fördern, auch um praktische Vorkehrungen zu unterstützen, mit denen gegebenenfalls die Feststellungsmechanismen zugänglich gemacht werden, die Zusammenarbeit mit anderen Akteuren entlang der Wertschöpfungskette erleichtert wird und Inhalte verbreitet oder ihre Echtheit und Herkunft überprüft werden, damit die Öffentlichkeit durch KI erzeugte Inhalte wirksam erkennen kann."
    },
    {
      "chunk_idx": 135,
      "id": "6761a274-7b37-48ce-a5ff-f63011fa3e4e",
      "title": "Recital 136",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(136) The obligations placed on providers and deployers of certain AI systems in this Regulation to enable the detection and disclosure that the outputs of those systems are artificially generated or manipulated are particularly relevant to facilitate the effective implementation of Regulation (EU) 2022/2065. This applies in particular as regards the obligations of providers of very large online platforms or very large online search engines to identify and mitigate systemic risks that may arise from the dissemination of content that has been artificially generated or manipulated, in particular the risk of the actual or foreseeable negative effects on democratic processes, civic discourse and electoral processes, including through disinformation. The requirement to label content generated by AI systems under this Regulation is without prejudice to the obligation in Article 16(6) of Regulation (EU) 2022/2065 for providers of hosting services to process notices on illegal content received pursuant to Article 16(1) of that Regulation and should not influence the assessment and the decision on the illegality of the specific content. That assessment should be performed solely with reference to the rules governing the legality of the content.",
      "original_content": "(136) Die Pflichten, die Anbietern und Betreibern bestimmter KI-Systeme mit dieser Verordnung auferlegt werden, die Feststellung und Offenlegung zu ermöglichen, dass die Ausgaben dieser Systeme künstlich erzeugt oder manipuliert werden, sind von besonderer Bedeutung für die Erleichterung der wirksamen Umsetzung der Verordnung (EU) 2022/2065. Dies gilt insbesondere für die Pflicht der Anbieter sehr großer Online-Plattformen oder sehr großer Online-Suchmaschinen, systemische Risiken zu ermitteln und zu mindern, die aus der Verbreitung von künstlich erzeugten oder manipulierten Inhalten entstehen können, insbesondere das Risiko tatsächlicher oder vorhersehbarer negativer Auswirkungen auf demokratische Prozesse, den gesellschaftlichen Diskurs und Wahlprozesse, unter anderem durch Desinformation. Die Anforderung gemäß dieser Verordnung, durch KI-Systeme erzeugte Inhalte zu kennzeichnen, berührt nicht die Pflicht in Artikel 16 Absatz 6 der Verordnung (EU) 2022/2065 für Anbieter von Hostingdiensten, gemäß Artikel 16 Absatz 1 der genannten Verordnung eingegangene Meldungen über illegale Inhalte zu bearbeiten, und sollte nicht die Beurteilung der Rechtswidrigkeit der betreffenden Inhalte und die Entscheidung darüber beeinflussen. Diese Beurteilung sollte ausschließlich anhand der Vorschriften für die Rechtmäßigkeit der Inhalte vorgenommen werden."
    },
    {
      "chunk_idx": 136,
      "id": "fc13bc03-3af1-43d8-92da-69554717c194",
      "title": "Recital 137",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(137) Compliance with the transparency obligations for the AI systems covered by this Regulation should not be interpreted as indicating that the use of the AI system or its output is lawful under this Regulation or other Union and Member State law and should be without prejudice to other transparency obligations for deployers of AI systems laid down in Union or national law.",
      "original_content": "(137) Die Einhaltung der Transparenzpflichten für die von dieser Verordnung erfassten KI-Systeme sollte nicht als Hinweis darauf ausgelegt werden, dass die Verwendung des KI-Systems oder seiner Ausgabe nach dieser Verordnung oder anderen Rechtsvorschriften der Union und der Mitgliedstaaten rechtmäßig ist, und sollte andere Transparenzpflichten für Betreiber von KI-Systemen, die im Unionsrecht oder im nationalen Recht festgelegt sind, unberührt lassen."
    },
    {
      "chunk_idx": 137,
      "id": "0af8ee0f-22ec-4daa-92dd-589a6b0f94f1",
      "title": "Recital 138",
      "relevantChunksIds": [
        "8c75b44d-51ac-48e9-b7d7-45c0362eaefb"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(138) AI is a rapidly developing family of technologies that requires regulatory oversight and a safe and controlled space for experimentation, while ensuring responsible innovation and integration of appropriate safeguards and risk mitigation measures. To ensure a legal framework that promotes innovation, is future-proof and resilient to disruption, Member States should ensure that their national competent authorities establish at least one AI regulatory sandbox at national level to facilitate the development and testing of innovative AI systems under strict regulatory oversight before these systems are placed on the market or otherwise put into service. Member States could also fulfil this obligation through participating in already existing regulatory sandboxes or establishing jointly a sandbox with one or more Member States’ competent authorities, insofar as this participation provides equivalent level of national coverage for the participating Member States. AI regulatory sandboxes could be established in physical, digital or hybrid form and may accommodate physical as well as digital products. Establishing authorities should also ensure that the AI regulatory sandboxes have the adequate resources for their functioning, including financial and human resources.",
      "original_content": "(138) KI bezeichnet eine Reihe sich rasch entwickelnder Technologien, die eine Regulierungsaufsicht und einen sicheren und kontrollierten Raum für die Erprobung erfordern, wobei gleichzeitig eine verantwortungsvolle Innovation und die Integration geeigneter Schutzvorkehrungen und Risikominderungsmaßnahmen gewährleistet werden müssen. Um einen innovationsfördernden, zukunftssicheren und gegenüber Störungen widerstandsfähigen Rechtsrahmen sicherzustellen, sollten die Mitgliedstaaten sicherstellen, dass ihre zuständigen nationalen Behörden mindestens ein KI-Reallabor auf nationaler Ebene einrichten, um die Entwicklung und die Erprobung innovativer KI-Systeme vor deren Inverkehrbringen oder anderweitiger Inbetriebnahme unter strenger Regulierungsaufsicht zu erleichtern. Die Mitgliedstaaten könnten diese Pflicht auch erfüllen, indem sie sich an bereits bestehenden Reallaboren beteiligen oder ein Reallabor mit den zuständigen Behörden eines oder mehrerer Mitgliedstaaten gemeinsam einrichten, insoweit diese Beteiligung eine gleichwertige nationale Abdeckung für die teilnehmenden Mitgliedstaaten bietet. KI-Reallabore könnten in physischer, digitaler oder Hybrid-Form eingerichtet werden, und sie können physische sowie digitale Produkte umfassen. Die einrichtenden Behörden sollten ferner sicherstellen, dass die KI-Reallabore über angemessene Ressourcen für ihre Aufgaben, einschließlich finanzieller und personeller Ressourcen, verfügen."
    },
    {
      "chunk_idx": 138,
      "id": "ca5614f2-4cf4-460e-addd-b5609143a0cd",
      "title": "Recital 139",
      "relevantChunksIds": [
        "8c75b44d-51ac-48e9-b7d7-45c0362eaefb",
        "8c8b84b1-d7b3-4ed3-a2ae-fb679465dbe9"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(139) The objectives of the AI regulatory sandboxes should be to foster AI innovation by establishing a controlled experimentation and testing environment in the development and pre-marketing phase with a view to ensuring compliance of the innovative AI systems with this Regulation and other relevant Union and national law. Moreover, the AI regulatory sandboxes should aim to enhance legal certainty for innovators and the competent authorities’ oversight and understanding of the opportunities, emerging risks and the impacts of AI use, to facilitate regulatory learning for authorities and undertakings, including with a view to future adaptions of the legal framework, to support cooperation and the sharing of best practices with the authorities involved in the AI regulatory sandbox, and to accelerate access to markets, including by removing barriers for SMEs, including start-ups. AI regulatory sandboxes should be widely available throughout the Union, and particular attention should be given to their accessibility for SMEs, including start-ups. The participation in the AI regulatory sandbox should focus on issues that raise legal uncertainty for providers and prospective providers to innovate, experiment with AI in the Union and contribute to evidence-based regulatory learning. The supervision of the AI systems in the AI regulatory sandbox should therefore cover their development, training, testing and validation before the systems are placed on the market or put into service, as well as the notion and occurrence of substantial modification that may require a new conformity assessment procedure. Any significant risks identified during the development and testing of such AI systems should result in adequate mitigation and, failing that, in the suspension of the development and testing process. Where appropriate, national competent authorities establishing AI regulatory sandboxes should cooperate with other relevant authorities, including those supervising the protection of fundamental rights, and could allow for the involvement of other actors within the AI ecosystem such as national or European standardisation organisations, notified bodies, testing and experimentation facilities, research and experimentation labs, European Digital Innovation Hubs and relevant stakeholder and civil society organisations. To ensure uniform implementation across the Union and economies of scale, it is appropriate to establish common rules for the AI regulatory sandboxes’ implementation and a framework for cooperation between the relevant authorities involved in the supervision of the sandboxes. AI regulatory sandboxes established under this Regulation should be without prejudice to other law allowing for the establishment of other sandboxes aiming to ensure compliance with law other than this Regulation. Where appropriate, relevant competent authorities in charge of those other regulatory sandboxes should consider the benefits of using those sandboxes also for the purpose of ensuring compliance of AI systems with this Regulation. Upon agreement between the national competent authorities and the participants in the AI regulatory sandbox, testing in real world conditions may also be operated and supervised in the framework of the AI regulatory sandbox.",
      "original_content": "(139) Die Ziele der KI-Reallabore sollten in Folgendem bestehen: Innovationen im Bereich KI zu fördern, indem eine kontrollierte Versuchs- und Testumgebung für die Entwicklungsphase und die dem Inverkehrbringen vorgelagerte Phase geschaffen wird, um sicherzustellen, dass die innovativen KI-Systeme mit dieser Verordnung und anderem einschlägigen Unionsrecht und dem nationalen Recht in Einklang stehen. Darüber hinaus sollten die KI-Reallabore darauf abzielen, die Rechtssicherheit für Innovatoren sowie die Aufsicht und das Verständnis der zuständigen Behörden in Bezug auf die Möglichkeiten, neu auftretenden Risiken und Auswirkungen der KI-Nutzung zu verbessern, das regulatorische Lernen für Behörden und Unternehmen zu erleichtern, unter anderem im Hinblick auf künftige Anpassungen des Rechtsrahmens, die Zusammenarbeit und den Austausch bewährter Praktiken mit den an dem KI-Reallabor beteiligten Behörden zu unterstützen und den Marktzugang zu beschleunigen, unter anderem indem Hindernisse für KMU, einschließlich Start-up-Unternehmen, abgebaut werden. KI-Reallabore sollten in der gesamten Union weithin verfügbar sein, und ein besonderes Augenmerk sollte auf ihre Zugänglichkeit für KMU, einschließlich Start-up-Unternehmen, gelegt werden. Die Beteiligung am KI-Reallabor sollte sich auf Fragen konzentrieren, die zu Rechtsunsicherheit für Anbieter und zukünftige Anbieter führen, damit sie Innovationen vornehmen, mit KI in der Union experimentieren und zu evidenzbasiertem regulatorischen Lernen beitragen. Die Beaufsichtigung der KI-Systeme im KI-Reallabor sollte sich daher auf deren Entwicklung, Training, Testen und Validierung vor dem Inverkehrbringen oder der Inbetriebnahme der Systeme sowie auf das Konzept und das Auftreten wesentlicher Änderungen erstrecken, die möglicherweise ein neues Konformitätsbewertungsverfahren erfordern. Alle erheblichen Risiken, die bei der Entwicklung und Erprobung solcher KI-Systeme festgestellt werden, sollten eine angemessene Risikominderung und, in Ermangelung dessen, die Aussetzung des Entwicklungs- und Erprobungsprozesses nach sich ziehen. Gegebenenfalls sollten die zuständigen nationalen Behörden, die KI-Reallabore einrichten, mit anderen einschlägigen Behörden zusammenarbeiten, einschließlich derjenigen, die den Schutz der Grundrechte überwachen, und könnten die Einbeziehung anderer Akteure innerhalb des KI-Ökosystems gestatten, wie etwa nationaler oder europäischer Normungsorganisationen, notifizierter Stellen, Test- und Versuchseinrichtungen, Forschungs- und Versuchslabore, Europäischer Digitaler Innovationszentren und einschlägiger Interessenträger und Organisationen der Zivilgesellschaft. Im Interesse einer unionsweit einheitlichen Umsetzung und der Erzielung von Größenvorteilen ist es angezeigt, dass gemeinsame Vorschriften für die Umsetzung von KI-Reallaboren und ein Rahmen für die Zusammenarbeit zwischen den an der Beaufsichtigung der Reallabore beteiligten Behörden festgelegt werden. KI-Reallabore, die im Rahmen dieser Verordnung eingerichtet werden, sollten anderes Recht, das die Einrichtung anderer Reallabore ermöglicht, unberührt lassen, um die Einhaltung anderen Rechts als dieser Verordnung sicherzustellen. Gegebenenfalls sollten die für diese anderen Reallabore zuständigen Behörden die Vorteile der Nutzung dieser Reallabore auch zum Zweck der Gewährleistung der Konformität der KI-Systeme mit dieser Verordnung berücksichtigen. Im Einvernehmen zwischen den zuständigen nationalen Behörden und den am KI-Reallabor Beteiligten können Tests unter Realbedingungen auch im Rahmen des KI-Reallabors durchgeführt und beaufsichtigt werden."
    },
    {
      "chunk_idx": 139,
      "id": "b0f84d4d-7c65-4840-a9aa-aa32b68acc74",
      "title": "Recital 140",
      "relevantChunksIds": [
        "8c75b44d-51ac-48e9-b7d7-45c0362eaefb",
        "4095ee13-aa11-4440-bbb9-b3bd17715a47"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(140) This Regulation should provide the legal basis for the providers and prospective providers in the AI regulatory sandbox to use personal data collected for other purposes for developing certain AI systems in the public interest within the AI regulatory sandbox, only under specified conditions, in accordance with Article 6(4) and Article 9(2), point (g), of Regulation (EU) 2016/679, and Articles 5, 6 and 10 of Regulation (EU) 2018/1725, and without prejudice to Article 4(2) and Article 10 of Directive (EU) 2016/680. All other obligations of data controllers and rights of data subjects under Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680 remain applicable. In particular, this Regulation should not provide a legal basis in the meaning of Article 22(2), point (b) of Regulation (EU) 2016/679 and Article 24(2), point (b) of Regulation (EU) 2018/1725. Providers and prospective providers in the AI regulatory sandbox should ensure appropriate safeguards and cooperate with the competent authorities, including by following their guidance and acting expeditiously and in good faith to adequately mitigate any identified significant risks to safety, health, and fundamental rights that may arise during the development, testing and experimentation in that sandbox.",
      "original_content": "(140) Die vorliegende Verordnung sollte im Einklang mit Artikel 6 Absatz 4 und Artikel 9 Absatz 2 Buchstabe g der Verordnung (EU) 2016/679 und den Artikeln 5, 6 und 10 der Verordnung (EU) 2018/1725 sowie unbeschadet des Artikels 4 Absatz 2 und des Artikels 10 der Richtlinie (EU) 2016/680 die Rechtsgrundlage für die Verwendung — ausschließlich unter bestimmten Bedingungen — personenbezogener Daten, die für andere Zwecke erhoben wurden, zur Entwicklung bestimmter KI-Systeme im öffentlichen Interesse innerhalb des KI-Reallabors durch die Anbieter und zukünftigen Anbieter im KI-Reallabor bilden. Alle anderen Pflichten von Verantwortlichen und Rechte betroffener Personen im Rahmen der Verordnungen (EU) 2016/679 und (EU) 2018/1725 und der Richtlinie (EU) 2016/680 gelten weiterhin. Insbesondere sollte diese Verordnung keine Rechtsgrundlage im Sinne des Artikels 22 Absatz 2 Buchstabe b der Verordnung (EU) 2016/679 und des Artikels 24 Absatz 2 Buchstabe b der Verordnung (EU) 2018/1725 bilden. Anbieter und zukünftige Anbieter im KI-Reallabor sollten angemessene Schutzvorkehrungen treffen und mit den zuständigen Behörden zusammenarbeiten, unter anderem indem sie deren Anleitung folgen und zügig und nach Treu und Glauben handeln, um etwaige erhebliche Risiken für die Sicherheit, die Gesundheit und die Grundrechte, die bei der Entwicklung, bei der Erprobung und bei Versuchen in diesem Reallabor auftreten können, zu mindern."
    },
    {
      "chunk_idx": 140,
      "id": "d9af965b-cc32-4306-b4f8-f4caeb8bfa48",
      "title": "Recital 141",
      "relevantChunksIds": [
        "cbfbb51e-9675-4e61-b006-d153a588f1e1",
        "9c639c94-6184-4e06-bc69-abb1f3fffbfe",
        "8c8b84b1-d7b3-4ed3-a2ae-fb679465dbe9"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(141) In order to accelerate the process of development and the placing on the market of the high-risk AI systems listed in an annex to this Regulation, it is important that providers or prospective providers of such systems may also benefit from a specific regime for testing those systems in real world conditions, without participating in an AI regulatory sandbox. However, in such cases, taking into account the possible consequences of such testing on individuals, it should be ensured that appropriate and sufficient guarantees and conditions are introduced by this Regulation for providers or prospective providers. Such guarantees should include, inter alia, requesting informed consent of natural persons to participate in testing in real world conditions, with the exception of law enforcement where the seeking of informed consent would prevent the AI system from being tested. Consent of subjects to participate in such testing under this Regulation is distinct from, and without prejudice to, consent of data subjects for the processing of their personal data under the relevant data protection law. It is also important to minimise the risks and enable oversight by competent authorities and therefore require prospective providers to have a real-world testing plan submitted to competent market surveillance authority, register the testing in dedicated sections in the EU database subject to some limited exceptions, set limitations on the period for which the testing can be done and require additional safeguards for persons belonging to certain vulnerable groups, as well as a written agreement defining the roles and responsibilities of prospective providers and deployers and effective oversight by competent personnel involved in the real world testing. Furthermore, it is appropriate to envisage additional safeguards to ensure that the predictions, recommendations or decisions of the AI system can be effectively reversed and disregarded and that personal data is protected and is deleted when the subjects have withdrawn their consent to participate in the testing without prejudice to their rights as data subjects under the Union data protection law. As regards transfer of data, it is also appropriate to envisage that data collected and processed for the purpose of testing in real-world conditions should be transferred to third countries only where appropriate and applicable safeguards under Union law are implemented, in particular in accordance with bases for transfer of personal data under Union law on data protection, while for non-personal data appropriate safeguards are put in place in accordance with Union law, such as Regulations (EU) 2022/868 (42) and (EU) 2023/2854 (43) of the European Parliament and of the Council.",
      "original_content": "(141) Um den Prozess der Entwicklung und des Inverkehrbringens der in einem Anhang dieser Verordnung aufgeführten Hochrisiko-KI-Systeme zu beschleunigen, ist es wichtig, dass Anbieter oder zukünftige Anbieter solcher Systeme auch von einer spezifischen Regelung für das Testen dieser Systeme unter Realbedingungen profitieren können, ohne sich an einem KI-Reallabor zu beteiligen. In solchen Fällen, unter Berücksichtigung der möglichen Folgen solcher Tests für Einzelpersonen, sollte jedoch sichergestellt werden, dass mit dieser Verordnung angemessene und ausreichende Garantien und Bedingungen für Anbieter oder zukünftige Anbieter eingeführt werden. Diese Garantien sollten unter anderem die Einholung der informierten Einwilligung natürlicher Personen in die Beteiligung an Tests unter Realbedingungen umfassen, mit Ausnahme der Strafverfolgung, wenn die Einholung der informierten Einwilligung verhindern würde, dass das KI-System getestet wird. Die Einwilligung der Testteilnehmer zur Teilnahme an solchen Tests im Rahmen dieser Verordnung unterscheidet sich von der Einwilligung betroffener Personen in die Verarbeitung ihrer personenbezogenen Daten nach den einschlägigen Datenschutzvorschriften und greift dieser nicht vor. Ferner ist es wichtig, die Risiken zu minimieren und die Aufsicht durch die zuständigen Behörden zu ermöglichen und daher von zukünftigen Anbietern zu verlangen, dass sie der zuständigen Marktüberwachungsbehörde einen Plan für einen Test unter Realbedingungen vorgelegt haben, die Tests — vorbehaltlich einiger begrenzter Ausnahmen — in den dafür vorgesehenen Abschnitten der EU-Datenbank zu registrieren, den Zeitraum zu begrenzen, in dem die Tests durchgeführt werden können, und zusätzliche Schutzmaßnahmen für Personen, die schutzbedürftigen Gruppen angehören, sowie eine schriftliche Einwilligung mit der Festlegung der Aufgaben und Zuständigkeiten der zukünftigen Anbieter und der Betreiber und eine wirksame Aufsicht durch zuständiges Personal, das an den Tests unter Realbedingungen beteiligt ist, zu verlangen. Darüber hinaus ist es angezeigt, zusätzliche Schutzmaßnahmen vorzusehen, um sicherzustellen, dass die Vorhersagen, Empfehlungen oder Entscheidungen des KI-Systems effektiv rückgängig gemacht und missachtet werden können, und dass personenbezogene Daten geschützt sind und gelöscht werden, wenn die Testteilnehmer ihre Einwilligung zur Teilnahme an den Tests widerrufen haben, und zwar unbeschadet ihrer Rechte als betroffene Personen nach dem Datenschutzrecht der Union. Was die Datenübermittlung betrifft, so ist es angezeigt vorzusehen, dass Daten, die zum Zweck von Tests unter Realbedingungen erhoben und verarbeitet wurden, nur dann an Drittstaaten übermittelt werden sollten, wenn angemessene und anwendbare Schutzmaßnahmen nach dem Unionsrecht umgesetzt wurden, insbesondere im Einklang mit den Grundlagen für die Übermittlung personenbezogener Daten nach dem Datenschutzrecht der Union, während für nicht personenbezogene Daten angemessene Schutzmaßnahmen im Einklang mit dem Unionsrecht, z. B. den Verordnungen (EU) 2022/868 (Fußnote 42) und (EU) 2023/2854 (Fußnote 43) des Europäischen Parlaments und des Rates eingerichtet wurden. Fußnote 42: Verordnung (EU) 2022/868 des Europäischen Parlaments und des Rates vom 30. Mai 2022 über europäische Daten-Governance und zur Änderung der Verordnung (EU) 2018/1724 (Daten-Governance-Rechtsakt) (ABl. L 152 vom 3.6.2022, S. 1)., Fußnote 43: Verordnung (EU) 2023/2854 des Rates und des Europäischen Parlaments vom 13. Dezember 2023 über harmonisierte Vorschriften für einen fairen Datenzugang und eine faire Datennutzung sowie zur Änderung der Verordnung (EU) 2017/2394 und der Richtlinie (EU) 2020/1828 (Datenverordnung) (ABl. L, 2023/2854, 22.12.2023, ELI: http://data.europa.eu/eli/reg/2023/2854/oj)."
    },
    {
      "chunk_idx": 141,
      "id": "d5fd389b-f8c7-499f-a3a9-2283e1b0befd",
      "title": "Recital 142",
      "relevantChunksIds": [
        "6ad1abec-e75e-499f-9eb0-9839e8ff38cc"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(142) To ensure that AI leads to socially and environmentally beneficial outcomes, Member States are encouraged to support and promote research and development of AI solutions in support of socially and environmentally beneficial outcomes, such as AI-based solutions to increase accessibility for persons with disabilities, tackle socio-economic inequalities, or meet environmental targets, by allocating sufficient resources, including public and Union funding, and, where appropriate and provided that the eligibility and selection criteria are fulfilled, considering in particular projects which pursue such objectives. Such projects should be based on the principle of interdisciplinary cooperation between AI developers, experts on inequality and non-discrimination, accessibility, consumer, environmental, and digital rights, as well as academics.",
      "original_content": "(142) Um sicherzustellen, dass KI zu sozial und ökologisch vorteilhaften Ergebnissen führt, werden die Mitgliedstaaten ermutigt, Forschung und Entwicklung zu KI-Lösungen, die zu sozial und ökologisch vorteilhaften Ergebnissen beitragen, zu unterstützen und zu fördern, wie KI-gestützte Lösungen für mehr Barrierefreiheit für Personen mit Behinderungen, zur Bekämpfung sozioökonomischer Ungleichheiten oder zur Erreichung von Umweltzielen, indem ausreichend Ressourcen — einschließlich öffentlicher Mittel und Unionsmittel — bereitgestellt werden, und — soweit angebracht und sofern die Voraussetzungen und Zulassungskriterien erfüllt sind — insbesondere unter Berücksichtigung von Projekten, mit denen diese Ziele verfolgt werden. Diese Projekte sollten auf dem Grundsatz der interdisziplinären Zusammenarbeit zwischen KI-Entwicklern, Sachverständigen in den Bereichen Gleichstellung und Nichtdiskriminierung, Barrierefreiheit und Verbraucher-, Umwelt- und digitale Rechte sowie Wissenschaftlern beruhen."
    },
    {
      "chunk_idx": 142,
      "id": "984548c3-63d4-4d21-99dc-c4f542c22ab3",
      "title": "Recital 143",
      "relevantChunksIds": [
        "576c7db3-5572-4780-85cc-23bdb6f5f13b",
        "61300139-468e-4f24-871e-61a39036b15f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(143) In order to promote and protect innovation, it is important that the interests of SMEs, including start-ups, that are providers or deployers of AI systems are taken into particular account. To that end, Member States should develop initiatives, which are targeted at those operators, including on awareness raising and information communication. Member States should provide SMEs, including start-ups, that have a registered office or a branch in the Union, with priority access to the AI regulatory sandboxes provided that they fulfil the eligibility conditions and selection criteria and without precluding other providers and prospective providers to access the sandboxes provided the same conditions and criteria are fulfilled. Member States should utilise existing channels and where appropriate, establish new dedicated channels for communication with SMEs, including start-ups, deployers, other innovators and, as appropriate, local public authorities, to support SMEs throughout their development path by providing guidance and responding to queries about the implementation of this Regulation. Where appropriate, these channels should work together to create synergies and ensure homogeneity in their guidance to SMEs, including start-ups, and deployers. Additionally, Member States should facilitate the participation of SMEs and other relevant stakeholders in the standardisation development processes. Moreover, the specific interests and needs of providers that are SMEs, including start-ups, should be taken into account when notified bodies set conformity assessment fees. The Commission should regularly assess the certification and compliance costs for SMEs, including start-ups, through transparent consultations and should work with Member States to lower such costs. For example, translation costs related to mandatory documentation and communication with authorities may constitute a significant cost for providers and other operators, in particular those of a smaller scale. Member States should possibly ensure that one of the languages determined and accepted by them for relevant providers’ documentation and for communication with operators is one which is broadly understood by the largest possible number of cross-border deployers. In order to address the specific needs of SMEs, including start-ups, the Commission should provide standardised templates for the areas covered by this Regulation, upon request of the Board. Additionally, the Commission should complement Member States’ efforts by providing a single information platform with easy-to-use information with regards to this Regulation for all providers and deployers, by organising appropriate communication campaigns to raise awareness about the obligations arising from this Regulation, and by evaluating and promoting the convergence of best practices in public procurement procedures in relation to AI systems. Medium-sized enterprises which until recently qualified as small enterprises within the meaning of the Annex to Commission Recommendation 2003/361/EC (44) should have access to those support measures, as those new medium-sized enterprises may sometimes lack the legal resources and training necessary to ensure proper understanding of, and compliance with, this Regulation.",
      "original_content": "(143) Um Innovationen zu fördern und zu schützen, ist es wichtig, die Interessen von KMU, einschließlich Start-up-Unternehmen, die Anbieter und Betreiber von KI-Systemen sind, besonders zu berücksichtigen. Zu diesem Zweck sollten die Mitgliedstaaten Initiativen ergreifen, die sich an diese Akteure richten, darunter auch Sensibilisierungs- und Informationsmaßnahmen. Die Mitgliedstaaten sollten KMU, einschließlich Start-up-Unternehmen, die ihren Sitz oder eine Zweigniederlassung in der Union haben, vorrangigen Zugang zu den KI-Reallaboren gewähren, soweit sie die Voraussetzungen und Zulassungskriterien erfüllen und ohne andere Anbieter und zukünftige Anbieter am Zugang zu den Reallaboren zu hindern, sofern die gleichen Voraussetzungen und Kriterien erfüllt sind. Die Mitgliedstaaten sollten bestehende Kanäle nutzen und gegebenenfalls neue Kanäle für die Kommunikation mit KMU, einschließlich Start-up-Unternehmen, Betreibern, anderen Innovatoren und gegebenenfalls Behörden einrichten, um KMU auf ihrem gesamten Entwicklungsweg zu unterstützen, indem sie ihnen Orientierungshilfe bieten und Fragen zur Durchführung dieser Verordnung beantworten. Diese Kanäle sollten gegebenenfalls zusammenarbeiten, um Synergien zu schaffen und eine Homogenität ihrer Leitlinien für KMU, einschließlich Start-up-Unternehmen, und Betreiber sicherzustellen. Darüber hinaus sollten die Mitgliedstaaten die Beteiligung von KMU und anderen einschlägigen Interessenträgern an der Entwicklung von Normen fördern. Außerdem sollten die besonderen Interessen und Bedürfnisse von Anbietern, die KMU, einschließlich Start-up-Unternehmen, sind, bei der Festlegung der Gebühren für die Konformitätsbewertung durch die notifizierten Stellen berücksichtigt werden. Die Kommission sollte regelmäßig die Zertifizierungs- und Befolgungskosten für KMU, einschließlich Start-up-Unternehmen, durch transparente Konsultationen bewerten, und sie sollte mit den Mitgliedstaaten zusammenarbeiten, um diese Kosten zu senken. So können beispielsweise Übersetzungen im Zusammenhang mit der verpflichtenden Dokumentation und Kommunikation mit Behörden für Anbieter und andere Akteure, insbesondere die kleineren unter ihnen, erhebliche Kosten verursachen. Die Mitgliedstaaten sollten möglichst dafür sorgen, dass eine der Sprachen, die sie für die einschlägige Dokumentation der Anbieter und für die Kommunikation mit den Akteuren bestimmen und akzeptieren, eine Sprache ist, die von der größtmöglichen Zahl grenzüberschreitender Betreiber weitgehend verstanden wird. Um den besonderen Bedürfnissen von KMU, einschließlich Start-up-Unternehmen, gerecht zu werden, sollte die Kommission auf Ersuchen des KI-Gremiums standardisierte Vorlagen für die unter diese Verordnung fallenden Bereiche bereitstellen. Ferner sollte die Kommission die Bemühungen der Mitgliedstaaten ergänzen, indem sie eine zentrale Informationsplattform mit leicht nutzbaren Informationen über diese Verordnung für alle Anbieter und Betreiber bereitstellt, indem sie angemessene Informationskampagnen durchführt, um für die aus dieser Verordnung erwachsenden Pflichten zu sensibilisieren, und indem sie die Konvergenz bewährter Praktiken bei Vergabeverfahren im Zusammenhang mit KI-Systemen bewertet und fördert. Mittlere Unternehmen, die bis vor kurzem als kleine Unternehmen im Sinne des Anhangs der Empfehlung 2003/361/EG der Kommission (Fußnote 44) galten, sollten Zugang zu diesen Unterstützungsmaßnahmen haben, da diese neuen mittleren Unternehmen mitunter nicht über die erforderlichen rechtlichen Ressourcen und Ausbildung verfügen, um ein ordnungsgemäßes Verständnis und eine entsprechende Einhaltung dieser Verordnung zu gewährleisten. Fußnote 44: Empfehlung der Kommission vom 6. Mai 2003 betreffend die Definition der Kleinstunternehmen sowie der kleinen und mittleren Unternehmen (ABl. L 124 vom 20.5.2003, S. 36)."
    },
    {
      "chunk_idx": 143,
      "id": "a59cee1a-f590-4641-ad87-8b4559272e1c",
      "title": "Recital 144",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(144) In order to promote and protect innovation, the AI-on-demand platform, all relevant Union funding programmes and projects, such as Digital Europe Programme, Horizon Europe, implemented by the Commission and the Member States at Union or national level should, as appropriate, contribute to the achievement of the objectives of this Regulation.",
      "original_content": "(144) Um Innovationen zu fördern und zu schützen, sollten die Plattform für KI auf Abruf, alle einschlägigen Finanzierungsprogramme und -projekte der Union, wie etwa das Programm „Digitales Europa“ und Horizont Europa, die von der Kommission und den Mitgliedstaaten auf Unionsebene bzw. auf nationaler Ebene durchgeführt werden, zur Verwirklichung der Ziele dieser Verordnung beitragen."
    },
    {
      "chunk_idx": 144,
      "id": "c87d7613-0407-4eb6-8bb8-5dad9dd7e48d",
      "title": "Recital 145",
      "relevantChunksIds": [
        "0d4481cd-fcdd-40f0-9386-6c5ccf4eeb97",
        "576c7db3-5572-4780-85cc-23bdb6f5f13b"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(145) In order to minimise the risks to implementation resulting from lack of knowledge and expertise in the market as well as to facilitate compliance of providers, in particular SMEs, including start-ups, and notified bodies with their obligations under this Regulation, the AI-on-demand platform, the European Digital Innovation Hubs and the testing and experimentation facilities established by the Commission and the Member States at Union or national level should contribute to the implementation of this Regulation. Within their respective mission and fields of competence, the AI-on-demand platform, the European Digital Innovation Hubs and the testing and experimentation Facilities are able to provide in particular technical and scientific support to providers and notified bodies.",
      "original_content": "(145) Um die Risiken bei der Umsetzung, die sich aus mangelndem Wissen und fehlenden Fachkenntnissen auf dem Markt ergeben, zu minimieren und den Anbietern, insbesondere KMU, einschließlich Start-up-Unternehmen, und notifizierten Stellen die Einhaltung ihrer Pflichten aus dieser Verordnung zu erleichtern, sollten insbesondere die Plattform für KI auf Abruf, die europäischen Zentren für digitale Innovation und die Test- und Versuchseinrichtungen, die von der Kommission und den Mitgliedstaaten auf Unionsebene bzw. auf nationaler Ebene eingerichtet werden, zur Durchführung dieser Verordnung beitragen. Die Plattform für KI auf Abruf, die europäischen Zentren für digitale Innovation und die Test- und Versuchseinrichtungen können Anbieter und notifizierte Stellen im Rahmen ihres jeweiligen Auftrags und ihrer jeweiligen Kompetenzbereiche insbesondere technisch und wissenschaftlich unterstützen."
    },
    {
      "chunk_idx": 145,
      "id": "c9e5148b-7510-4514-9f53-91eb5588c91e",
      "title": "Recital 146",
      "relevantChunksIds": [
        "0c488757-5b64-4a4f-ac00-98ce42a3968a"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(146) Moreover, in light of the very small size of some operators and in order to ensure proportionality regarding costs of innovation, it is appropriate to allow microenterprises to fulfil one of the most costly obligations, namely to establish a quality management system, in a simplified manner which would reduce the administrative burden and the costs for those enterprises without affecting the level of protection and the need for compliance with the requirements for high-risk AI systems. The Commission should develop guidelines to specify the elements of the quality management system to be fulfilled in this simplified manner by microenterprises.",
      "original_content": "(146) Angesichts der sehr geringen Größe einiger Akteure und um die Verhältnismäßigkeit in Bezug auf die Innovationskosten sicherzustellen, ist es darüber hinaus angezeigt, Kleinstunternehmen zu erlauben, eine der kostspieligsten Pflichten, nämlich die Einführung eines Qualitätsmanagementsystems, in vereinfachter Weise zu erfüllen, was den Verwaltungsaufwand und die Kosten für diese Unternehmen verringern würde, ohne das Schutzniveau und die Notwendigkeit der Einhaltung der Anforderungen für Hochrisiko-KI-Systeme zu beeinträchtigen. Die Kommission sollte Leitlinien ausarbeiten, um die Elemente des Qualitätsmanagementsystems zu bestimmen, die von Kleinstunternehmen auf diese vereinfachte Weise zu erfüllen sind."
    },
    {
      "chunk_idx": 146,
      "id": "2ffebd22-e662-4762-80e2-63f3edced074",
      "title": "Recital 147",
      "relevantChunksIds": [
        "aeae4d53-6d81-4c49-843d-b93704615c05"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(147) It is appropriate that the Commission facilitates, to the extent possible, access to testing and experimentation facilities to bodies, groups or laboratories established or accredited pursuant to any relevant Union harmonisation legislation and which fulfil tasks in the context of conformity assessment of products or devices covered by that Union harmonisation legislation. This is, in particular, the case as regards expert panels, expert laboratories and reference laboratories in the field of medical devices pursuant to Regulations (EU) 2017/745 and (EU) 2017/746.",
      "original_content": "(147) Es ist angezeigt, dass die Kommission den Stellen, Gruppen oder Laboratorien, die gemäß den einschlägigen Harmonisierungsrechtsvorschriften der Union eingerichtet oder akkreditiert sind und Aufgaben im Zusammenhang mit der Konformitätsbewertung von Produkten oder Geräten wahrnehmen, die unter diese Harmonisierungsrechtsvorschriften der Union fallen, so weit wie möglich den Zugang zu Test- und Versuchseinrichtungen erleichtert. Dies gilt insbesondere für Expertengremien, Fachlaboratorien und Referenzlaboratorien im Bereich Medizinprodukte gemäß den Verordnungen (EU) 2017/745 und (EU) 2017/746."
    },
    {
      "chunk_idx": 147,
      "id": "e0492e6c-e987-41c3-b664-775286020859",
      "title": "Recital 148",
      "relevantChunksIds": [
        "63ae5b47-b44b-4b4b-82ac-5fa868177399"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(148) This Regulation should establish a governance framework that both allows to coordinate and support the application of this Regulation at national level, as well as build capabilities at Union level and integrate stakeholders in the field of AI. The effective implementation and enforcement of this Regulation require a governance framework that allows to coordinate and build up central expertise at Union level. The AI Office was established by Commission Decision (45) and has as its mission to develop Union expertise and capabilities in the field of AI and to contribute to the implementation of Union law on AI. Member States should facilitate the tasks of the AI Office with a view to support the development of Union expertise and capabilities at Union level and to strengthen the functioning of the digital single market. Furthermore, a Board composed of representatives of the Member States, a scientific panel to integrate the scientific community and an advisory forum to contribute stakeholder input to the implementation of this Regulation, at Union and national level, should be established. The development of Union expertise and capabilities should also include making use of existing resources and expertise, in particular through synergies with structures built up in the context of the Union level enforcement of other law and synergies with related initiatives at Union level, such as the EuroHPC Joint Undertaking and the AI testing and experimentation facilities under the Digital Europe Programme.",
      "original_content": "(148) Mit dieser Verordnung sollte ein Governance-Rahmen geschaffen werden, der sowohl die Koordinierung und Unterstützung der Anwendung dieser Verordnung auf nationaler Ebene als auch den Aufbau von Kapazitäten auf Unionsebene und die Integration von Interessenträgern im Bereich der KI ermöglicht. Für die wirksame Umsetzung und Durchsetzung dieser Verordnung ist ein Governance-Rahmen erforderlich, der es ermöglicht, zentrales Fachwissen auf Unionsebene zu koordinieren und aufzubauen. Per Kommissionbeschluss (Fußnote 45) wurde das Büro für Künstliche Intelligenz errichtet, dessen Aufgabe es ist, Fachwissen und Kapazitäten der Union im Bereich der KI zu entwickeln und zur Umsetzung des Unionsrechts im KI-Bereich beizutragen. Die Mitgliedstaaten sollten die Aufgaben des Büros für Künstliche Intelligenz erleichtern, um die Entwicklung von Fachwissen und Kapazitäten auf Unionsebene zu unterstützen und die Funktionsweise des digitalen Binnenmarkts zu stärken. Darüber hinaus sollten ein aus Vertretern der Mitgliedstaaten zusammengesetztes KI-Gremium, ein wissenschaftliches Gremium zur Integration der Wissenschaftsgemeinschaft und ein Beratungsforum für Beiträge von Interessenträgern zur Durchführung dieser Verordnung auf Unionsebene und auf nationaler Ebene eingerichtet werden. Die Entwicklung von Fachwissen und Kapazitäten der Union sollte auch die Nutzung bestehender Ressourcen und Fachkenntnisse umfassen, insbesondere durch Synergien mit Strukturen, die im Rahmen der Durchsetzung anderen Rechts auf Unionsebene aufgebaut wurden, und Synergien mit einschlägigen Initiativen auf Unionsebene, wie dem Gemeinsamen Unternehmen EuroHPC und den KI-Test- und Versuchseinrichtungen im Rahmen des Programms „Digitales Europa“. Fußnote 45: Beschluss C(2024) 390 der Kommission vom 24.1.2024 zur Errichtung des Europäischen Amts für künstliche Intelligenz."
    },
    {
      "chunk_idx": 148,
      "id": "bf998658-e660-43ca-8849-a1cc9c46930c",
      "title": "Recital 149",
      "relevantChunksIds": [
        "e684a05a-dce1-41e7-9743-fd5b1b778be8",
        "4035dbfb-54eb-4161-9b2d-4974bd7440ba"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(149) In order to facilitate a smooth, effective and harmonised implementation of this Regulation a Board should be established. The Board should reflect the various interests of the AI eco-system and be composed of representatives of the Member States. The Board should be responsible for a number of advisory tasks, including issuing opinions, recommendations, advice or contributing to guidance on matters related to the implementation of this Regulation, including on enforcement matters, technical specifications or existing standards regarding the requirements established in this Regulation and providing advice to the Commission and the Member States and their national competent authorities on specific questions related to AI. In order to give some flexibility to Member States in the designation of their representatives in the Board, such representatives may be any persons belonging to public entities who should have the relevant competences and powers to facilitate coordination at national level and contribute to the achievement of the Board’s tasks. The Board should establish two standing sub-groups to provide a platform for cooperation and exchange among market surveillance authorities and notifying authorities on issues related, respectively, to market surveillance and notified bodies. The standing subgroup for market surveillance should act as the administrative cooperation group (ADCO) for this Regulation within the meaning of Article 30 of Regulation (EU) 2019/1020. In accordance with Article 33 of that Regulation, the Commission should support the activities of the standing subgroup for market surveillance by undertaking market evaluations or studies, in particular with a view to identifying aspects of this Regulation requiring specific and urgent coordination among market surveillance authorities. The Board may establish other standing or temporary sub-groups as appropriate for the purpose of examining specific issues. The Board should also cooperate, as appropriate, with relevant Union bodies, experts groups and networks active in the context of relevant Union law, including in particular those active under relevant Union law on data, digital products and services.",
      "original_content": "(149) Um eine reibungslose, wirksame und harmonisierte Durchführung dieser Verordnung zu erleichtern, sollte ein KI-Gremium eingerichtet werden. Das KI-Gremium sollte die verschiedenen Interessen des KI-Ökosystems widerspiegeln und sich aus Vertretern der Mitgliedstaaten zusammensetzen. Das KI-Gremium sollte für eine Reihe von Beratungsaufgaben zuständig sein, einschließlich der Abgabe von Stellungnahmen, Empfehlungen, Ratschlägen oder Beiträgen zu Leitlinien zu Fragen im Zusammenhang mit der Durchführung dieser Verordnung — darunter zu Durchsetzungsfragen, technischen Spezifikationen oder bestehenden Normen in Bezug auf die in dieser Verordnung festgelegten Anforderungen — sowie der Beratung der Kommission und der Mitgliedstaaten und ihrer zuständigen nationalen Behörden in spezifischen Fragen im Zusammenhang mit KI. Um den Mitgliedstaaten eine gewisse Flexibilität bei der Benennung ihrer Vertreter im KI-Gremium zu geben, können diese Vertreter alle Personen sein, die öffentlichen Einrichtungen angehören, die über einschlägige Zuständigkeiten und Befugnisse verfügen sollten, um die Koordinierung auf nationaler Ebene zu erleichtern und zur Erfüllung der Aufgaben des KI-Gremiums beizutragen. Das KI-Gremium sollte zwei ständige Untergruppen einrichten, um Marktüberwachungsbehörden und notifizierenden Behörden für die Zusammenarbeit und den Austausch in Fragen, die die Marktüberwachung bzw. notifizierende Stellen betreffen, eine Plattform zu bieten. Die ständige Untergruppe für Marktüberwachung sollte für diese Verordnung als Gruppe für die Verwaltungszusammenarbeit (ADCO-Gruppe) im Sinne des Artikels 30 der Verordnung (EU) 2019/1020 fungieren. Im Einklang mit Artikel 33 der genannten Verordnung sollte die Kommission die Tätigkeiten der ständigen Untergruppe für Marktüberwachung durch die Durchführung von Marktbewertungen oder -untersuchungen unterstützen, insbesondere im Hinblick auf die Ermittlung von Aspekten dieser Verordnung, die eine spezifische und dringende Koordinierung zwischen den Marktüberwachungsbehörden erfordern. Das KI-Gremium kann weitere ständige oder nichtständige Untergruppen einrichten, falls das für die Prüfung bestimmter Fragen zweckmäßig sein sollte. Das KI-Gremium sollte gegebenenfalls auch mit einschlägigen Einrichtungen, Sachverständigengruppen und Netzwerken der Union zusammenarbeiten, die im Zusammenhang mit dem einschlägigen Unionsrecht tätig sind, einschließlich insbesondere derjenigen, die im Rahmen des einschlägigen Unionsrechts über Daten, digitale Produkte und Dienstleistungen tätig sind."
    },
    {
      "chunk_idx": 149,
      "id": "199c6e64-698e-4e3f-a806-9ff9d364cc85",
      "title": "Recital 150",
      "relevantChunksIds": [
        "957b417e-f553-4553-aa90-daea77a9a036"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(150) With a view to ensuring the involvement of stakeholders in the implementation and application of this Regulation, an advisory forum should be established to advise and provide technical expertise to the Board and the Commission. To ensure a varied and balanced stakeholder representation between commercial and non-commercial interest and, within the category of commercial interests, with regards to SMEs and other undertakings, the advisory forum should comprise inter alia industry, start-ups, SMEs, academia, civil society, including the social partners, as well as the Fundamental Rights Agency, ENISA, the European Committee for Standardization (CEN), the European Committee for Electrotechnical Standardization (CENELEC) and the European Telecommunications Standards Institute (ETSI).",
      "original_content": "(150) Im Hinblick auf die Einbeziehung von Interessenträgern in die Umsetzung und Anwendung dieser Verordnung sollte ein Beratungsforum eingerichtet werden, um das KI-Gremium und die Kommission zu beraten und ihnen technisches Fachwissen bereitzustellen. Um eine vielfältige und ausgewogene Vertretung der Interessenträger mit gewerblichen und nicht gewerblichen Interessen und — innerhalb der Kategorie mit gewerblichen Interessen — in Bezug auf KMU und andere Unternehmen zu gewährleisten, sollten in dem Beratungsforum unter anderem die Industrie, Start-up-Unternehmen, KMU, die Wissenschaft, die Zivilgesellschaft, einschließlich der Sozialpartner, sowie die Agentur für Grundrechte, die ENISA, das Europäische Komitee für Normung (CEN), das Europäische Komitee für elektrotechnische Normung (CENELEC) und das Europäische Institut für Telekommunikationsnormen (ETSI) vertreten sein."
    },
    {
      "chunk_idx": 150,
      "id": "47710b63-0230-41a5-80af-2581299ac9d6",
      "title": "Recital 151",
      "relevantChunksIds": [
        "6efc5cc4-f5b5-4581-bc5d-10d2aeca4486",
        "3810e28c-9fab-460b-ac8f-f4dd846665f9",
        "2d6ab2fa-54df-417d-8d80-b1cad8c8e2db"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(151) To support the implementation and enforcement of this Regulation, in particular the monitoring activities of the AI Office as regards general-purpose AI models, a scientific panel of independent experts should be established. The independent experts constituting the scientific panel should be selected on the basis of up-to-date scientific or technical expertise in the field of AI and should perform their tasks with impartiality, objectivity and ensure the confidentiality of information and data obtained in carrying out their tasks and activities. To allow the reinforcement of national capacities necessary for the effective enforcement of this Regulation, Member States should be able to request support from the pool of experts constituting the scientific panel for their enforcement activities.",
      "original_content": "(151) Zur Unterstützung der Umsetzung und Durchsetzung dieser Verordnung, insbesondere der Beobachtungstätigkeiten des Büros für Künstliche Intelligenz in Bezug auf KI-Modelle mit allgemeinem Verwendungszweck, sollte ein wissenschaftliches Gremium mit unabhängigen Sachverständigen eingerichtet werden. Die unabhängigen Sachverständigen, aus denen sich das wissenschaftliche Gremium zusammensetzt, sollten auf der Grundlage des aktuellen wissenschaftlichen oder technischen Fachwissens im KI-Bereich ausgewählt werden und ihre Aufgaben unparteiisch, objektiv und unter Achtung der Vertraulichkeit der bei der Durchführung ihrer Aufgaben und Tätigkeiten erhaltenen Informationen und Daten ausüben. Um eine Aufstockung der nationalen Kapazitäten, die für die wirksame Durchsetzung dieser Verordnung erforderlich sind, zu ermöglichen, sollten die Mitgliedstaaten für ihre Durchsetzungstätigkeiten Unterstützung aus dem Pool von Sachverständigen anfordern können, der das wissenschaftliche Gremium bildet."
    },
    {
      "chunk_idx": 151,
      "id": "c3ef1a35-c082-4ce7-b4c5-a4c3d24e98ad",
      "title": "Recital 152",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(152) In order to support adequate enforcement as regards AI systems and reinforce the capacities of the Member States, Union AI testing support structures should be established and made available to the Member States.",
      "original_content": "(152) Um eine angemessene Durchsetzung in Bezug auf KI-Systeme zu unterstützen und die Kapazitäten der Mitgliedstaaten zu stärken, sollten Unionsstrukturen zur Unterstützung der Prüfung von KI eingerichtet und den Mitgliedstaaten zur Verfügung gestellt werden."
    },
    {
      "chunk_idx": 152,
      "id": "577d5a3c-36b6-48b6-a3fd-27b111efe5fd",
      "title": "Recital 153",
      "relevantChunksIds": [
        "d2fac56c-1843-421c-baa8-2d3e0178bec6",
        "a95f9b76-59d0-4655-9859-ce20a0190849",
        "2d1207bb-4da9-4b6c-abee-bdd5de6d80e7",
        "069d1fdf-6329-4927-865f-862b02fbc7c1",
        "88bfe2ee-ca8c-4bc7-a273-1cbab6e941ed",
        "d4f161e6-4fb1-4772-9a7c-d518161ea674"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(153) Member States hold a key role in the application and enforcement of this Regulation. In that respect, each Member State should designate at least one notifying authority and at least one market surveillance authority as national competent authorities for the purpose of supervising the application and implementation of this Regulation. Member States may decide to appoint any kind of public entity to perform the tasks of the national competent authorities within the meaning of this Regulation, in accordance with their specific national organisational characteristics and needs. In order to increase organisation efficiency on the side of Member States and to set a single point of contact vis-à-vis the public and other counterparts at Member State and Union levels, each Member State should designate a market surveillance authority to act as a single point of contact.",
      "original_content": "(153) Den Mitgliedstaaten kommt bei der Anwendung und Durchsetzung dieser Verordnung eine Schlüsselrolle zu. Dazu sollte jeder Mitgliedstaat mindestens eine notifizierende Behörde und mindestens eine Marktüberwachungsbehörde als zuständige nationale Behörden benennen, die die Anwendung und Durchführung dieser Verordnung beaufsichtigen. Die Mitgliedstaaten können beschließen, öffentliche Einrichtungen jeder Art zu benennen, die die Aufgaben der zuständigen nationalen Behörden im Sinne dieser Verordnung gemäß ihren spezifischen nationalen organisatorischen Merkmalen und Bedürfnissen wahrnehmen. Um die Effizienz der Organisation aufseiten der Mitgliedstaaten zu steigern und eine zentrale Anlaufstelle gegenüber der Öffentlichkeit und anderen Ansprechpartnern auf Ebene der Mitgliedstaaten und der Union einzurichten, sollte jeder Mitgliedstaat eine Marktüberwachungsbehörde als zentrale Anlaufstelle benennen."
    },
    {
      "chunk_idx": 153,
      "id": "0d1ee002-929c-4cd0-99ba-0e8f530307b5",
      "title": "Recital 154",
      "relevantChunksIds": [
        "2d1207bb-4da9-4b6c-abee-bdd5de6d80e7",
        "b4031ce5-9908-41f5-b9a4-6bc0efc0d784"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(154) The national competent authorities should exercise their powers independently, impartially and without bias, so as to safeguard the principles of objectivity of their activities and tasks and to ensure the application and implementation of this Regulation. The members of these authorities should refrain from any action incompatible with their duties and should be subject to confidentiality rules under this Regulation.",
      "original_content": "(154) Die zuständigen nationalen Behörden sollten ihre Befugnisse unabhängig, unparteiisch und unvoreingenommen ausüben, um die Grundsätze der Objektivität ihrer Tätigkeiten und Aufgaben zu wahren und die Anwendung und Durchführung dieser Verordnung sicherzustellen. Die Mitglieder dieser Behörden sollten sich jeder Handlung enthalten, die mit ihren Aufgaben unvereinbar wäre, und sie sollten den Vertraulichkeitsvorschriften gemäß dieser Verordnung unterliegen."
    },
    {
      "chunk_idx": 154,
      "id": "3ccde775-96a9-422b-87da-43e4cd707e7f",
      "title": "Recital 155",
      "relevantChunksIds": [
        "5b7ccaab-a361-4b2e-bf6c-67a90fd684eb"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(155) In order to ensure that providers of high-risk AI systems can take into account the experience on the use of high-risk AI systems for improving their systems and the design and development process or can take any possible corrective action in a timely manner, all providers should have a post-market monitoring system in place. Where relevant, post-market monitoring should include an analysis of the interaction with other AI systems including other devices and software. Post-market monitoring should not cover sensitive operational data of deployers which are law enforcement authorities. This system is also key to ensure that the possible risks emerging from AI systems which continue to ‘learn’ after being placed on the market or put into service can be more efficiently and timely addressed. In this context, providers should also be required to have a system in place to report to the relevant authorities any serious incidents resulting from the use of their AI systems, meaning incident or malfunctioning leading to death or serious damage to health, serious and irreversible disruption of the management and operation of critical infrastructure, infringements of obligations under Union law intended to protect fundamental rights or serious damage to property or the environment.",
      "original_content": "(155) Damit Anbieter von Hochrisiko-KI-Systemen die Erfahrungen mit der Verwendung von Hochrisiko-KI-Systemen bei der Verbesserung ihrer Systeme und im Konzeptions- und Entwicklungsprozess berücksichtigen oder rechtzeitig etwaige Korrekturmaßnahmen ergreifen können, sollten alle Anbieter über ein System zur Beobachtung nach dem Inverkehrbringen verfügen. Gegebenenfalls sollte die Beobachtung nach dem Inverkehrbringen eine Analyse der Interaktion mit anderen KI-Systemen, einschließlich anderer Geräte und Software, umfassen. Die Beobachtung nach dem Inverkehrbringen sollte nicht für sensible operative Daten von Betreibern, die Strafverfolgungsbehörden sind, gelten. Dieses System ist auch wichtig, damit den möglichen Risiken, die von KI-Systemen ausgehen, die nach dem Inverkehrbringen oder der Inbetriebnahme dazulernen, effizienter und zeitnah begegnet werden kann. In diesem Zusammenhang sollten die Anbieter auch verpflichtet sein, ein System einzurichten, um den zuständigen Behörden schwerwiegende Vorfälle zu melden, die sich aus der Verwendung ihrer KI-Systeme ergeben; damit sind Vorfälle oder Fehlfunktionen gemeint, die zum Tod oder zu schweren Gesundheitsschäden führen, schwerwiegende und irreversible Störungen der Verwaltung und des Betriebs kritischer Infrastrukturen, Verstöße gegen Verpflichtungen aus dem Unionsrecht, mit denen die Grundrechte geschützt werden sollen, oder schwere Sach- oder Umweltschäden."
    },
    {
      "chunk_idx": 155,
      "id": "491d84d6-b754-4408-8bbd-21c789451f0f",
      "title": "Recital 156",
      "relevantChunksIds": [
        "d2fac56c-1843-421c-baa8-2d3e0178bec6",
        "a95f9b76-59d0-4655-9859-ce20a0190849",
        "59668afb-a6c5-4909-9fbd-3c58436a069a",
        "2d1207bb-4da9-4b6c-abee-bdd5de6d80e7",
        "a7a05ae9-8c7c-44c1-8221-328bc0dcc4a5",
        "069d1fdf-6329-4927-865f-862b02fbc7c1",
        "88bfe2ee-ca8c-4bc7-a273-1cbab6e941ed",
        "d4f161e6-4fb1-4772-9a7c-d518161ea674"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(156) In order to ensure an appropriate and effective enforcement of the requirements and obligations set out by this Regulation, which is Union harmonisation legislation, the system of market surveillance and compliance of products established by Regulation (EU) 2019/1020 should apply in its entirety. Market surveillance authorities designated pursuant to this Regulation should have all enforcement powers laid down in this Regulation and in Regulation (EU) 2019/1020 and should exercise their powers and carry out their duties independently, impartially and without bias. Although the majority of AI systems are not subject to specific requirements and obligations under this Regulation, market surveillance authorities may take measures in relation to all AI systems when they present a risk in accordance with this Regulation. Due to the specific nature of Union institutions, agencies and bodies falling within the scope of this Regulation, it is appropriate to designate the European Data Protection Supervisor as a competent market surveillance authority for them. This should be without prejudice to the designation of national competent authorities by the Member States. Market surveillance activities should not affect the ability of the supervised entities to carry out their tasks independently, when such independence is required by Union law.",
      "original_content": "(156) Zur Gewährleistung einer angemessenen und wirksamen Durchsetzung der Anforderungen und Pflichten gemäß dieser Verordnung, bei der es sich um eine Harmonisierungsrechtsvorschrift der Union handelt, sollte das mit der Verordnung (EU) 2019/1020 eingeführte System der Marktüberwachung und der Konformität von Produkten in vollem Umfang gelten. Die gemäß dieser Verordnung benannten Marktüberwachungsbehörden sollten über alle in der vorliegenden Verordnung und der Verordnung (EU) 2019/1020 festgelegten Durchsetzungsbefugnisse verfügen und ihre Befugnisse und Aufgaben unabhängig, unparteiisch und unvoreingenommen wahrnehmen. Obwohl die meisten KI-Systeme keinen spezifischen Anforderungen und Pflichten gemäß der vorliegenden Verordnung unterliegen, können die Marktüberwachungsbehörden Maßnahmen in Bezug auf alle KI-Systeme ergreifen, wenn sie ein Risiko gemäß dieser Verordnung darstellen. Aufgrund des spezifischen Charakters der Organe, Einrichtungen und sonstigen Stellen der Union, die in den Anwendungsbereich dieser Verordnung fallen, ist es angezeigt, dass der Europäische Datenschutzbeauftragte als eine zuständige Marktüberwachungsbehörde für sie benannt wird. Die Benennung zuständiger nationaler Behörden durch die Mitgliedstaaten sollte davon unberührt bleiben. Die Marktüberwachungstätigkeiten sollten die Fähigkeit der beaufsichtigten Einrichtungen, ihre Aufgaben unabhängig wahrzunehmen, nicht beeinträchtigen, wenn eine solche Unabhängigkeit nach dem Unionsrecht erforderlich ist."
    },
    {
      "chunk_idx": 156,
      "id": "ecc6bef2-71ac-41e5-a335-c79864647312",
      "title": "Recital 157",
      "relevantChunksIds": [
        "2d1207bb-4da9-4b6c-abee-bdd5de6d80e7",
        "a7a05ae9-8c7c-44c1-8221-328bc0dcc4a5",
        "fbe4d324-9497-4865-b73a-72e761e651b7",
        "b4031ce5-9908-41f5-b9a4-6bc0efc0d784",
        "069d1fdf-6329-4927-865f-862b02fbc7c1",
        "88bfe2ee-ca8c-4bc7-a273-1cbab6e941ed",
        "d4f161e6-4fb1-4772-9a7c-d518161ea674"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(157) This Regulation is without prejudice to the competences, tasks, powers and independence of relevant national public authorities or bodies which supervise the application of Union law protecting fundamental rights, including equality bodies and data protection authorities. Where necessary for their mandate, those national public authorities or bodies should also have access to any documentation created under this Regulation. A specific safeguard procedure should be set for ensuring adequate and timely enforcement against AI systems presenting a risk to health, safety and fundamental rights. The procedure for such AI systems presenting a risk should be applied to high-risk AI systems presenting a risk, prohibited systems which have been placed on the market, put into service or used in violation of the prohibited practices laid down in this Regulation and AI systems which have been made available in violation of the transparency requirements laid down in this Regulation and present a risk.",
      "original_content": "(157) Diese Verordnung berührt nicht die Zuständigkeiten, Aufgaben, Befugnisse und Unabhängigkeit der einschlägigen nationalen Behörden oder Stellen, die die Anwendung des Unionsrechts zum Schutz der Grundrechte überwachen, einschließlich Gleichbehandlungsstellen und Datenschutzbehörden. Sofern dies für die Erfüllung ihres Auftrags erforderlich ist, sollten auch diese nationalen Behörden oder Stellen Zugang zu der gesamten im Rahmen dieser Verordnung erstellten Dokumentation haben. Es sollte ein spezifisches Schutzklauselverfahren festgelegt werden, um eine angemessene und zeitnahe Durchsetzung gegenüber KI-Systemen, die ein Risiko für Gesundheit, Sicherheit und Grundrechte bergen, sicherzustellen. Das Verfahren für solche KI-Systeme, die ein Risiko bergen, sollte auf Hochrisiko-KI-Systeme, von denen ein Risiko ausgeht, auf verbotene Systeme, die unter Verstoß gegen die in dieser Verordnung festgelegten verbotenen Praktiken in Verkehr gebracht, in Betrieb genommen oder verwendet wurden, sowie auf KI-Systeme, die unter Verstoß der Transparenzanforderungen dieser Verordnung bereitgestellt wurden und ein Risiko bergen, angewandt werden."
    },
    {
      "chunk_idx": 157,
      "id": "d685b8fd-f03f-4563-8a9a-41aaf6ee30ad",
      "title": "Recital 158",
      "relevantChunksIds": [
        "5b7ccaab-a361-4b2e-bf6c-67a90fd684eb",
        "a7a05ae9-8c7c-44c1-8221-328bc0dcc4a5"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(158) Union financial services law includes internal governance and risk-management rules and requirements which are applicable to regulated financial institutions in the course of provision of those services, including when they make use of AI systems. In order to ensure coherent application and enforcement of the obligations under this Regulation and relevant rules and requirements of the Union financial services legal acts, the competent authorities for the supervision and enforcement of those legal acts, in particular competent authorities as defined in Regulation (EU) No 575/2013 of the European Parliament and of the Council (46) and Directives 2008/48/EC (47), 2009/138/EC (48), 2013/36/EU (49), 2014/17/EU (50) and (EU) 2016/97 (51) of the European Parliament and of the Council, should be designated, within their respective competences, as competent authorities for the purpose of supervising the implementation of this Regulation, including for market surveillance activities, as regards AI systems provided or used by regulated and supervised financial institutions unless Member States decide to designate another authority to fulfil these market surveillance tasks. Those competent authorities should have all powers under this Regulation and Regulation (EU) 2019/1020 to enforce the requirements and obligations of this Regulation, including powers to carry our ex post market surveillance activities that can be integrated, as appropriate, into their existing supervisory mechanisms and procedures under the relevant Union financial services law. It is appropriate to envisage that, when acting as market surveillance authorities under this Regulation, the national authorities responsible for the supervision of credit institutions regulated under Directive 2013/36/EU, which are participating in the Single Supervisory Mechanism established by Council Regulation (EU) No 1024/2013 (52), should report, without delay, to the European Central Bank any information identified in the course of their market surveillance activities that may be of potential interest for the European Central Bank’s prudential supervisory tasks as specified in that Regulation. To further enhance the consistency between this Regulation and the rules applicable to credit institutions regulated under Directive 2013/36/EU, it is also appropriate to integrate some of the providers’ procedural obligations in relation to risk management, post marketing monitoring and documentation into the existing obligations and procedures under Directive 2013/36/EU. In order to avoid overlaps, limited derogations should also be envisaged in relation to the quality management system of providers and the monitoring obligation placed on deployers of high-risk AI systems to the extent that these apply to credit institutions regulated by Directive 2013/36/EU. The same regime should apply to insurance and re-insurance undertakings and insurance holding companies under Directive 2009/138/EC and the insurance intermediaries under Directive (EU) 2016/97 and other types of financial institutions subject to requirements regarding internal governance, arrangements or processes established pursuant to the relevant Union financial services law to ensure consistency and equal treatment in the financial sector.",
      "original_content": "(158) Die Rechtsvorschriften der Union über Finanzdienstleistungen enthalten Vorschriften und Anforderungen für die interne Unternehmensführung und das Risikomanagement, die für regulierte Finanzinstitute bei der Erbringung solcher Dienstleistungen gelten, auch wenn sie KI-Systeme verwenden. Um eine kohärente Anwendung und Durchsetzung der Pflichten aus dieser Verordnung sowie der einschlägigen Vorschriften und Anforderungen der Rechtsvorschriften der Union über Finanzdienstleistungen zu gewährleisten, sollten die für die Beaufsichtigung und Durchsetzung jener Rechtsvorschriften zuständigen Behörden, insbesondere die zuständigen Behörden im Sinne der Verordnung (EU) Nr. 575/2013 des Europäischen Parlaments und des Rates (Fußnote 46) und der Richtlinien 2008/48/EG (Fußnote 47), 2009/138/EG (Fußnote 48), 2013/36/EU (Fußnote 49), 2014/17/EU (Fußnote 50) und (EU) 2016/97 (Fußnote 51) des Europäischen Parlaments und des Rates, im Rahmen ihrer jeweiligen Zuständigkeiten auch als zuständige Behörden für die Beaufsichtigung der Durchführung dieser Verordnung, einschließlich der Marktüberwachungstätigkeiten, in Bezug auf von regulierten und beaufsichtigten Finanzinstituten bereitgestellte oder verwendete KI-Systeme benannt werden, es sei denn, die Mitgliedstaaten beschließen, eine andere Behörde zu benennen, um diese Marktüberwachungsaufgaben wahrzunehmen. Diese zuständigen Behörden sollten alle Befugnisse gemäß dieser Verordnung und der Verordnung (EU) 2019/1020 haben, um die Anforderungen und Pflichten der vorliegenden Verordnung durchzusetzen, einschließlich Befugnisse zur Durchführung von Ex-post-Marktüberwachungstätigkeiten, die gegebenenfalls in ihre bestehenden Aufsichtsmechanismen und -verfahren im Rahmen des einschlägigen Unionsrechts über Finanzdienstleistungen integriert werden können. Es ist angezeigt, vorzusehen, dass die nationalen Behörden, die für die Aufsicht über unter die Richtlinie 2013/36/EU fallende Kreditinstitute zuständig sind, welche an dem mit der Verordnung (EU) Nr. 1024/2013 des Rates (Fußnote 52) eingerichteten einheitlichen Aufsichtsmechanismus teilnehmen, in ihrer Funktion als Marktüberwachungsbehörden gemäß der vorliegenden Verordnung der Europäischen Zentralbank unverzüglich alle im Zuge ihrer Marktüberwachungstätigkeiten ermittelten Informationen übermitteln, die für die in der genannten Verordnung festgelegten Aufsichtsaufgaben der Europäischen Zentralbank von Belang sein könnten. Um die Kohärenz zwischen der vorliegenden Verordnung und den Vorschriften für Kreditinstitute, die unter die Richtlinie 2013/36/EU fallen, weiter zu verbessern, ist es ferner angezeigt, einige verfahrenstechnische Anbieterpflichten in Bezug auf das Risikomanagement, die Beobachtung nach dem Inverkehrbringen und die Dokumentation in die bestehenden Pflichten und Verfahren gemäß der Richtlinie 2013/36/EU aufzunehmen. Zur Vermeidung von Überschneidungen sollten auch begrenzte Ausnahmen in Bezug auf das Qualitätsmanagementsystem der Anbieter und die Beobachtungspflicht der Betreiber von Hochrisiko-KI-Systemen in Betracht gezogen werden, soweit diese Kreditinstitute betreffen, die unter die Richtlinie 2013/36/EU fallen. Die gleiche Regelung sollte für Versicherungs- und Rückversicherungsunternehmen und Versicherungsholdinggesellschaften gemäß der Richtlinie 2009/138/EG und Versicherungsvermittler gemäß der Richtlinie (EU) 2016/97 sowie für andere Arten von Finanzinstituten gelten, die Anforderungen in Bezug auf ihre Regelungen oder Verfahren der internen Unternehmensführung unterliegen, die gemäß einschlägigem Unionsrecht der Union über Finanzdienstleistungen festgelegt wurden, um Kohärenz und Gleichbehandlung im Finanzsektor sicherzustellen. Fußnote 46: Verordnung (EU) Nr. 575/2013 des Europäischen Parlaments und des Rates vom 26. Juni 2013 über Aufsichtsanforderungen an Kreditinstitute und Wertpapierfirmen und zur Änderung der Verordnung (EU) Nr. 648/2012 (ABl. L 176 vom 27.6.2013, S. 1)., Fußnote 47: Richtlinie 2008/48/EG des Europäischen Parlaments und des Rates vom 23. April 2008 über Verbraucherkreditverträge und zur Aufhebung der Richtlinie 87/102/EWG des Rates (ABl. L 133 vom 22.5.2008, S. 66)., Fußnote 48: Richtlinie 2009/138/EG des Europäischen Parlaments und des Rates vom 25. November 2009 betreffend die Aufnahme und Ausübung der Versicherungs- und der Rückversicherungstätigkeit (Solvabilität II) (ABl. L 335 vom 17.12.2009, S. 1)., Fußnote 49: Richtlinie 2013/36/EU des Europäischen Parlaments und des Rates vom 26. Juni 2013 über den Zugang zur Tätigkeit von Kreditinstituten und die Beaufsichtigung von Kreditinstituten und Wertpapierfirmen, zur Änderung der Richtlinie 2002/87/EG und zur Aufhebung der Richtlinien 2006/48/EG und 2006/49/EG (ABl. L 176 vom 27.6.2013, S. 338)., Fußnote 50: Richtlinie 2014/17/ЕU des Europäischen Parlaments und des Rates vom 4. Februar 2014 über Wohnimmobilienkreditverträge für Verbraucher und zur Änderung der Richtlinien 2008/48/EG und 2013/36/EU und der Verordnung (EU) Nr. 1093/2010 (ABl. L 60 vom 28.2.2014, S. 34)., Fußnote 51: Richtlinie (EU) 2016/97 des Europäischen Parlaments und des Rates vom 20. Januar 2016 über Versicherungsvertrieb (ABl. L 26 vom 2.2.2016, S. 19)., Fußnote 52: Verordnung (EU) Nr. 1024/2013 des Rates vom 15. Oktober 2013 zur Übertragung besonderer Aufgaben im Zusammenhang mit der Aufsicht über Kreditinstitute auf die Europäische Zentralbank (ABl. L 287 vom 29.10.2013, S. 63)."
    },
    {
      "chunk_idx": 158,
      "id": "af6b62de-8c37-48f7-b7f7-5bcdb2b0bf39",
      "title": "Recital 159",
      "relevantChunksIds": [
        "a7a05ae9-8c7c-44c1-8221-328bc0dcc4a5",
        "43ec355f-40dd-4ebb-be43-6f6618d28b69",
        "cb445557-7bf2-48a2-bb97-bcaf072934fd",
        "95b7f020-8180-4dc6-80ab-b1c0cb455293"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(159) Each market surveillance authority for high-risk AI systems in the area of biometrics, as listed in an annex to this Regulation insofar as those systems are used for the purposes of law enforcement, migration, asylum and border control management, or the administration of justice and democratic processes, should have effective investigative and corrective powers, including at least the power to obtain access to all personal data that are being processed and to all information necessary for the performance of its tasks. The market surveillance authorities should be able to exercise their powers by acting with complete independence. Any limitations of their access to sensitive operational data under this Regulation should be without prejudice to the powers conferred to them by Directive (EU) 2016/680. No exclusion on disclosing data to national data protection authorities under this Regulation should affect the current or future powers of those authorities beyond the scope of this Regulation.",
      "original_content": "(159) Jede Marktüberwachungsbehörde für Hochrisiko-KI-Systeme im Bereich der Biometrie, die in einem Anhang zu dieser Verordnung aufgeführt sind, sollte — soweit diese Systeme für die Zwecke der Strafverfolgung, von Migration, Asyl und Grenzkontrolle oder von Rechtspflege und demokratischen Prozessen eingesetzt werden — über wirksame Ermittlungs- und Korrekturbefugnisse verfügen, einschließlich mindestens der Befugnis, Zugang zu allen personenbezogenen Daten, die verarbeitet werden, und zu allen Informationen, die für die Ausübung ihrer Aufgaben erforderlich sind, zu erhalten. Die Marktüberwachungsbehörden sollten in der Lage sein, ihre Befugnisse in völliger Unabhängigkeit auszuüben. Jede Beschränkung ihres Zugangs zu sensiblen operativen Daten im Rahmen dieser Verordnung sollte die Befugnisse unberührt lassen, die ihnen mit der Richtlinie (EU) 2016/680 übertragen wurden. Kein Ausschluss der Offenlegung von Daten gegenüber nationalen Datenschutzbehörden im Rahmen dieser Verordnung sollte die derzeitigen oder künftigen Befugnisse dieser Behörden über den Geltungsbereich dieser Verordnung hinaus beeinträchtigen."
    },
    {
      "chunk_idx": 159,
      "id": "1c919b24-516b-4255-979e-f4cbc0da447e",
      "title": "Recital 160",
      "relevantChunksIds": [
        "a7a05ae9-8c7c-44c1-8221-328bc0dcc4a5",
        "069d1fdf-6329-4927-865f-862b02fbc7c1",
        "88bfe2ee-ca8c-4bc7-a273-1cbab6e941ed",
        "43ec355f-40dd-4ebb-be43-6f6618d28b69",
        "cb445557-7bf2-48a2-bb97-bcaf072934fd"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(160) The market surveillance authorities and the Commission should be able to propose joint activities, including joint investigations, to be conducted by market surveillance authorities or market surveillance authorities jointly with the Commission, that have the aim of promoting compliance, identifying non-compliance, raising awareness and providing guidance in relation to this Regulation with respect to specific categories of high-risk AI systems that are found to present a serious risk across two or more Member States. Joint activities to promote compliance should be carried out in accordance with Article 9 of Regulation (EU) 2019/1020. The AI Office should provide coordination support for joint investigations.",
      "original_content": "(160) Die Marktüberwachungsbehörden und die Kommission sollten gemeinsame Tätigkeiten, einschließlich gemeinsamer Untersuchungen, vorschlagen können, die von den Marktüberwachungsbehörden oder von den Marktüberwachungsbehörden gemeinsam mit der Kommission durchgeführt werden, um Konformität zu fördern, Nichtkonformität festzustellen, zu sensibilisieren und Orientierung zu dieser Verordnung und bestimmten Kategorien von Hochrisiko-KI-Systemen bereitzustellen, bei denen festgestellt wird, dass sie in zwei oder mehr Mitgliedstaaten ein ernstes Risiko darstellen. Gemeinsame Tätigkeiten zur Förderung der Konformität sollten im Einklang mit Artikel 9 der Verordnung (EU) 2019/1020 durchgeführt werden. Das Büro für Künstliche Intelligenz sollte die Koordinierung gemeinsamer Untersuchungen unterstützen."
    },
    {
      "chunk_idx": 160,
      "id": "e83ab1ea-4c87-4599-978e-5912e4a9bda3",
      "title": "Recital 161",
      "relevantChunksIds": [
        "41caa9c8-36d6-4917-badd-540c9137f2cf",
        "7697d697-974a-4f6e-b011-4f0b2b838f1c",
        "1bca51b9-d068-497e-a46b-69fe66329d83",
        "4f1989fc-d70f-4469-b223-686f0035ddc7",
        "63a159a9-e3dd-40b7-a414-2fb5c10817b4"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(161) It is necessary to clarify the responsibilities and competences at Union and national level as regards AI systems that are built on general-purpose AI models. To avoid overlapping competences, where an AI system is based on a general-purpose AI model and the model and system are provided by the same provider, the supervision should take place at Union level through the AI Office, which should have the powers of a market surveillance authority within the meaning of Regulation (EU) 2019/1020 for this purpose. In all other cases, national market surveillance authorities remain responsible for the supervision of AI systems. However, for general-purpose AI systems that can be used directly by deployers for at least one purpose that is classified as high-risk, market surveillance authorities should cooperate with the AI Office to carry out evaluations of compliance and inform the Board and other market surveillance authorities accordingly. Furthermore, market surveillance authorities should be able to request assistance from the AI Office where the market surveillance authority is unable to conclude an investigation on a high-risk AI system because of its inability to access certain information related to the general-purpose AI model on which the high-risk AI system is built. In such cases, the procedure regarding mutual assistance in cross-border cases in Chapter VI of Regulation (EU) 2019/1020 should apply mutatis mutandis.",
      "original_content": "(161) Die Verantwortlichkeiten und Zuständigkeiten auf Unionsebene und nationaler Ebene in Bezug auf KI-Systeme, die auf KI-Modellen mit allgemeinem Verwendungszweck aufbauen, müssen präzisiert werden. Um sich überschneidende Zuständigkeiten zu vermeiden, sollte die Aufsicht für KI-Systeme, die auf KI-Modellen mit allgemeinem Verwendungszweck beruhen und bei denen das Modell und das System vom selben Anbieter bereitgestellt werden, auf der Unionsebene durch das Büro für Künstliche Intelligenz erfolgen, das für diesen Zweck über die Befugnisse einer Marktüberwachungsbehörde im Sinne der Verordnung (EU) 2019/1020 verfügen sollte. In allen anderen Fällen sollten die nationalen Marktüberwachungsbehörden weiterhin für die Aufsicht über KI-Systeme zuständig sein. Bei KI-Systemen mit allgemeinem Verwendungszweck, die von Betreibern direkt für mindestens einen Zweck verwendet werden können, der als hochriskant eingestuft wird, sollten die Marktüberwachungsbehörden jedoch mit dem Büro für Künstliche Intelligenz zusammenarbeiten, um Konformitätsbewertungen durchzuführen, und sie sollten KI-Gremium und andere Marktüberwachungsbehörden entsprechend informieren. Darüber hinaus sollten Marktüberwachungsbehörden das Büro für Künstliche Intelligenz um Unterstützung ersuchen können, wenn die Marktüberwachungsbehörde nicht in der Lage ist, eine Untersuchung zu einem Hochrisiko-KI-System abzuschließen, weil sie keinen Zugang zu bestimmten Informationen im Zusammenhang mit dem KI-Modell mit allgemeinem Verwendungszweck, auf dem das Hochrisiko-KI-System beruht, haben. In diesen Fällen sollte das Verfahren bezüglich grenzübergreifender Amtshilfe nach Kapitel VI der Verordnung (EU) 2019/1020 entsprechend Anwendung finden."
    },
    {
      "chunk_idx": 161,
      "id": "7ce8047c-1e69-42c0-aba4-40614e1ff6b8",
      "title": "Recital 162",
      "relevantChunksIds": [
        "6f871e54-e731-4009-a4cd-39bbd19b543e",
        "41caa9c8-36d6-4917-badd-540c9137f2cf",
        "7697d697-974a-4f6e-b011-4f0b2b838f1c",
        "1bca51b9-d068-497e-a46b-69fe66329d83",
        "4f1989fc-d70f-4469-b223-686f0035ddc7",
        "63a159a9-e3dd-40b7-a414-2fb5c10817b4"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(162) To make best use of the centralised Union expertise and synergies at Union level, the powers of supervision and enforcement of the obligations on providers of general-purpose AI models should be a competence of the Commission. The AI Office should be able to carry out all necessary actions to monitor the effective implementation of this Regulation as regards general-purpose AI models. It should be able to investigate possible infringements of the rules on providers of general-purpose AI models both on its own initiative, following the results of its monitoring activities, or upon request from market surveillance authorities in line with the conditions set out in this Regulation. To support effective monitoring of the AI Office, it should provide for the possibility that downstream providers lodge complaints about possible infringements of the rules on providers of general-purpose AI models and systems.",
      "original_content": "(162) Um das zentralisierte Fachwissen der Union und Synergien auf Unionsebene bestmöglich zu nutzen, sollte die Kommission für die Aufsicht und die Durchsetzung der Pflichten der Anbieter von KI-Modellen mit allgemeinem Verwendungszweck zuständig sein. Das Büro für Künstliche Intelligenz sollte alle erforderlichen Maßnahmen durchführen können, um die wirksame Umsetzung dieser Verordnung im Hinblick auf KI-Modelle mit allgemeinem Verwendungszweck zu überwachen. Es sollte mögliche Verstöße gegen die Vorschriften für Anbieter von KI-Modellen mit allgemeinem Verwendungszweck sowohl auf eigene Initiative, auf der Grundlage der Ergebnisse seiner Überwachungstätigkeiten, als auch auf Anfrage von Marktüberwachungsbehörden gemäß den in dieser Verordnung festgelegten Bedingungen untersuchen können. Zur Unterstützung einer wirksamen Überwachung durch das Büro für Künstliche Intelligenz sollte die Möglichkeit vorgesehen werden, dass nachgelagerte Anbieter Beschwerden über mögliche Verstöße gegen die Vorschriften für Anbieter von KI-Modellen und -Systemen mit allgemeinem Verwendungszweck einreichen können."
    },
    {
      "chunk_idx": 162,
      "id": "af44a0ad-8425-4b78-8b03-21093d6fc548",
      "title": "Recital 163",
      "relevantChunksIds": [
        "6efc5cc4-f5b5-4581-bc5d-10d2aeca4486",
        "41caa9c8-36d6-4917-badd-540c9137f2cf",
        "7697d697-974a-4f6e-b011-4f0b2b838f1c",
        "2d6ab2fa-54df-417d-8d80-b1cad8c8e2db",
        "16e69959-dc99-4627-8f3c-7ce956e1358e",
        "250d8d85-cebe-415e-a198-35b61991c230"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(163) With a view to complementing the governance systems for general-purpose AI models, the scientific panel should support the monitoring activities of the AI Office and may, in certain cases, provide qualified alerts to the AI Office which trigger follow-ups, such as investigations. This should be the case where the scientific panel has reason to suspect that a general-purpose AI model poses a concrete and identifiable risk at Union level. Furthermore, this should be the case where the scientific panel has reason to suspect that a general-purpose AI model meets the criteria that would lead to a classification as general-purpose AI model with systemic risk. To equip the scientific panel with the information necessary for the performance of those tasks, there should be a mechanism whereby the scientific panel can request the Commission to require documentation or information from a provider.",
      "original_content": "(163) Um die Governance-Systeme für KI-Modelle mit allgemeinem Verwendungszweck zu ergänzen, sollte das wissenschaftliche Gremium die Überwachungstätigkeiten des Büros für Künstliche Intelligenz unterstützen; dazu kann es in bestimmten Fällen qualifizierte Warnungen an das Büro für Künstliche Intelligenz richten, die Folgemaßnahmen wie etwa Untersuchungen auslösen. Dies sollte der Fall sein, wenn das wissenschaftliche Gremium Grund zu der Annahme hat, dass ein KI-Modell mit allgemeinem Verwendungszweck ein konkretes und identifizierbares Risiko auf Unionsebene darstellt. Außerdem sollte dies der Fall sein, wenn das wissenschaftliche Gremium Grund zu der Annahme hat, dass ein KI-Modell mit allgemeinem Verwendungszweck die Kriterien erfüllt, die zu einer Einstufung als KI-Modell mit allgemeinem Verwendungszweck mit systemischem Risiko führen würde. Um dem wissenschaftlichen Gremium die Informationen zur Verfügung zu stellen, die für die Ausübung dieser Aufgaben erforderlich sind, sollte es einen Mechanismus geben, wonach das wissenschaftliche Gremium die Kommission ersuchen kann, Unterlagen oder Informationen von einem Anbieter anzufordern."
    },
    {
      "chunk_idx": 163,
      "id": "a0c2ce4c-d906-4d3e-adb7-96e4be45e9bf",
      "title": "Recital 164",
      "relevantChunksIds": [
        "2e8446c1-8259-4a99-8353-ae8eee2bf2ab",
        "c9ddca61-d229-4d40-b57a-f7c249377ecb",
        "7188729e-4dec-4ecf-a3b9-8494a9fc716e",
        "90669d84-c0e3-4ecb-9c0f-f8bbef4679aa"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(164) The AI Office should be able to take the necessary actions to monitor the effective implementation of and compliance with the obligations for providers of general-purpose AI models laid down in this Regulation. The AI Office should be able to investigate possible infringements in accordance with the powers provided for in this Regulation, including by requesting documentation and information, by conducting evaluations, as well as by requesting measures from providers of general-purpose AI models. When conducting evaluations, in order to make use of independent expertise, the AI Office should be able to involve independent experts to carry out the evaluations on its behalf. Compliance with the obligations should be enforceable, inter alia, through requests to take appropriate measures, including risk mitigation measures in the case of identified systemic risks as well as restricting the making available on the market, withdrawing or recalling the model. As a safeguard, where needed beyond the procedural rights provided for in this Regulation, providers of general-purpose AI models should have the procedural rights provided for in Article 18 of Regulation (EU) 2019/1020, which should apply mutatis mutandis, without prejudice to more specific procedural rights provided for by this Regulation.",
      "original_content": "(164) Das Büro für Künstliche Intelligenz sollte die erforderlichen Maßnahmen ergreifen können, um die wirksame Umsetzung und die Einhaltung der in dieser Verordnung festgelegten Pflichten der Anbieter von KI-Modellen mit allgemeinem Verwendungszweck zu überwachen. Das Büro für Künstliche Intelligenz sollte mögliche Verstöße im Einklang mit den in dieser Verordnung vorgesehenen Befugnissen untersuchen können, unter anderem indem es Unterlagen und Informationen anfordert, Bewertungen durchführt und Maßnahmen von Anbietern von KI-Modellen mit allgemeinem Verwendungszweck verlangt. Was die Durchführung von Bewertungen betrifft, so sollte das Büro für Künstliche Intelligenz unabhängige Sachverständige mit der Durchführung der Bewertungen in seinem Namen beauftragen können, damit unabhängiges Fachwissen genutzt werden kann. Die Einhaltung der Pflichten sollte durchsetzbar sein, unter anderem durch die Aufforderung zum Ergreifen angemessener Maßnahmen, einschließlich Risikominderungsmaßnahmen im Fall von festgestellten systemischen Risiken, sowie durch die Einschränkung der Bereitstellung des Modells auf dem Markt, die Rücknahme des Modells oder den Rückruf des Modells. Als Schutzmaßnahme, die erforderlichenfalls über die in dieser Verordnung vorgesehenen Verfahrensrechte hinausgeht, sollten die Anbieter von KI-Modellen mit allgemeinem Verwendungszweck über die in Artikel 18 der Verordnung (EU) 2019/1020 vorgesehenen Verfahrensrechte verfügen, die — unbeschadet in der vorliegenden Verordnung vorgesehener spezifischerer Verfahrensrechte — entsprechend gelten sollten."
    },
    {
      "chunk_idx": 164,
      "id": "7cf70e13-b212-45e3-bf7b-66d860a91315",
      "title": "Recital 165",
      "relevantChunksIds": [
        "b39b09ad-8deb-4ec1-bfae-a098c9425de2",
        "3d0fab99-63f1-4b12-97f4-6a481cd86228",
        "6f871e54-e731-4009-a4cd-39bbd19b543e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(165) The development of AI systems other than high-risk AI systems in accordance with the requirements of this Regulation may lead to a larger uptake of ethical and trustworthy AI in the Union. Providers of AI systems that are not high-risk should be encouraged to create codes of conduct, including related governance mechanisms, intended to foster the voluntary application of some or all of the mandatory requirements applicable to high-risk AI systems, adapted in light of the intended purpose of the systems and the lower risk involved and taking into account the available technical solutions and industry best practices such as model and data cards. Providers and, as appropriate, deployers of all AI systems, high-risk or not, and AI models should also be encouraged to apply on a voluntary basis additional requirements related, for example, to the elements of the Union’s Ethics Guidelines for Trustworthy AI, environmental sustainability, AI literacy measures, inclusive and diverse design and development of AI systems, including attention to vulnerable persons and accessibility to persons with disability, stakeholders’ participation with the involvement, as appropriate, of relevant stakeholders such as business and civil society organisations, academia, research organisations, trade unions and consumer protection organisations in the design and development of AI systems, and diversity of the development teams, including gender balance. To ensure that the voluntary codes of conduct are effective, they should be based on clear objectives and key performance indicators to measure the achievement of those objectives. They should also be developed in an inclusive way, as appropriate, with the involvement of relevant stakeholders such as business and civil society organisations, academia, research organisations, trade unions and consumer protection organisation. The Commission may develop initiatives, including of a sectoral nature, to facilitate the lowering of technical barriers hindering cross-border exchange of data for AI development, including on data access infrastructure, semantic and technical interoperability of different types of data.",
      "original_content": "(165) Die Entwicklung anderer KI-Systeme als Hochrisiko-KI-Systeme gemäß den Anforderungen dieser Verordnung kann zu einer stärkeren Verbreitung ethischer und vertrauenswürdiger KI in der Union führen. Anbieter von KI-Systemen, die kein hohes Risiko bergen, sollten angehalten werden, Verhaltenskodizes — einschließlich zugehöriger Governance-Mechanismen — zu erstellen, um eine freiwillige Anwendung einiger oder aller der für Hochrisiko-KI-Systeme geltenden Anforderungen zu fördern, die angesichts der Zweckbestimmung der Systeme und des niedrigeren Risikos angepasst werden, und unter Berücksichtigung der verfügbaren technischen Lösungen und bewährten Verfahren der Branche wie Modell- und Datenkarten. Darüber hinaus sollten die Anbieter und gegebenenfalls die Betreiber aller KI-Systeme, ob mit hohem Risiko oder nicht, und aller KI-Modelle auch ermutigt werden, freiwillig zusätzliche Anforderungen anzuwenden, z. B. in Bezug auf die Elemente der Ethikleitlinien der Union für vertrauenswürdige KI, die ökologische Nachhaltigkeit, Maßnahmen für KI-Kompetenz, die inklusive und vielfältige Gestaltung und Entwicklung von KI-Systemen, unter anderem mit Schwerpunkt auf schutzbedürftige Personen und die Barrierefreiheit für Menschen mit Behinderungen, die Beteiligung der Interessenträger, gegebenenfalls mit Einbindung einschlägiger Interessenträger wie Unternehmensverbänden und Organisationen der Zivilgesellschaft, Wissenschaft, Forschungsorganisationen, Gewerkschaften und Verbraucherschutzorganisationen an der Konzeption und Entwicklung von KI-Systemen und die Vielfalt der Entwicklungsteams, einschließlich einer ausgewogenen Vertretung der Geschlechter. Um sicherzustellen, dass die freiwilligen Verhaltenskodizes wirksam sind, sollten sie auf klaren Zielen und zentralen Leistungsindikatoren zur Messung der Verwirklichung dieser Ziele beruhen. Sie sollten außerdem in inklusiver Weise entwickelt werden, gegebenenfalls unter Einbeziehung einschlägiger Interessenträger wie Unternehmensverbände und Organisationen der Zivilgesellschaft, Wissenschaft, Forschungsorganisationen, Gewerkschaften und Verbraucherschutzorganisationen. Die Kommission kann Initiativen, auch sektoraler Art, ergreifen, um den Abbau technischer Hindernisse zu erleichtern, die den grenzüberschreitenden Datenaustausch im Zusammenhang mit der KI-Entwicklung behindern, unter anderem in Bezug auf die Infrastruktur für den Datenzugang und die semantische und technische Interoperabilität verschiedener Arten von Daten."
    },
    {
      "chunk_idx": 165,
      "id": "1b90fb32-5b4f-463a-9396-217d47295c8d",
      "title": "Recital 166",
      "relevantChunksIds": [
        "b39b09ad-8deb-4ec1-bfae-a098c9425de2",
        "3d0fab99-63f1-4b12-97f4-6a481cd86228"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(166) It is important that AI systems related to products that are not high-risk in accordance with this Regulation and thus are not required to comply with the requirements set out for high-risk AI systems are nevertheless safe when placed on the market or put into service. To contribute to this objective, Regulation (EU) 2023/988 of the European Parliament and of the Council (53) would apply as a safety net.",
      "original_content": "(166) Es ist wichtig, dass KI-Systeme im Zusammenhang mit Produkten, die gemäß dieser Verordnung kein hohes Risiko bergen und daher nicht die in dieser Verordnung festgelegten Anforderungen für Hochrisiko-KI-Systeme erfüllen müssen, dennoch sicher sind, wenn sie in Verkehr gebracht oder in Betrieb genommen werden. Um zu diesem Ziel beizutragen, würde die Verordnung (EU) 2023/988 des Europäischen Parlaments und des Rates (Fußnote 53) als Sicherheitsnetz dienen. Fußnote 53: Verordnung (EU) 2023/988 des Europäischen Parlaments und des Rates vom 10. Mai 2023 über die allgemeine Produktsicherheit, zur Änderung der Verordnung (EU) Nr. 1025/2012 des Europäischen Parlaments und des Rates und der Richtlinie (EU) 2020/1828 des Europäischen Parlaments und des Rates sowie zur Aufhebung der Richtlinie 2001/95/EG des Europäischen Parlaments und des Rates und der Richtlinie 87/357/EWG des Rates (ABl. L 135 vom 23.5.2023, S. 1)."
    },
    {
      "chunk_idx": 166,
      "id": "b1c54e3f-0856-463c-a767-339866de57a1",
      "title": "Recital 167",
      "relevantChunksIds": [
        "bd393311-1589-4e5f-8a95-ca6421fe8583",
        "31a6ae2c-6df1-4c50-b88e-8956375096cd",
        "316b96d1-61c7-4a5f-8ddd-a8337fc3ae86"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(167) In order to ensure trustful and constructive cooperation of competent authorities on Union and national level, all parties involved in the application of this Regulation should respect the confidentiality of information and data obtained in carrying out their tasks, in accordance with Union or national law. They should carry out their tasks and activities in such a manner as to protect, in particular, intellectual property rights, confidential business information and trade secrets, the effective implementation of this Regulation, public and national security interests, the integrity of criminal and administrative proceedings, and the integrity of classified information.",
      "original_content": "(167) Zur Gewährleistung einer vertrauensvollen und konstruktiven Zusammenarbeit der zuständigen Behörden auf Ebene der Union und der Mitgliedstaaten sollten alle an der Anwendung dieser Verordnung beteiligten Parteien gemäß dem Unionsrecht und dem nationalen Recht die Vertraulichkeit der im Rahmen der Wahrnehmung ihrer Aufgaben erlangten Informationen und Daten wahren. Sie sollten ihre Aufgaben und Tätigkeiten so ausüben, dass insbesondere die Rechte des geistigen Eigentums, vertrauliche Geschäftsinformationen und Geschäftsgeheimnisse, die wirksame Durchführung dieser Verordnung, die öffentlichen und nationalen Sicherheitsinteressen, die Integrität von Straf- und Verwaltungsverfahren und die Integrität von Verschlusssachen geschützt werden."
    },
    {
      "chunk_idx": 167,
      "id": "c837084f-afdb-431e-b95a-b1d3bdacff9e",
      "title": "Recital 168",
      "relevantChunksIds": [
        "61300139-468e-4f24-871e-61a39036b15f",
        "09a1468c-9fc4-44ee-a8c1-fd07084ea0d8",
        "de889fca-779f-4b70-8ae2-166b1f752102"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(168) Compliance with this Regulation should be enforceable by means of the imposition of penalties and other enforcement measures. Member States should take all necessary measures to ensure that the provisions of this Regulation are implemented, including by laying down effective, proportionate and dissuasive penalties for their infringement, and to respect the ne bis in idem principle. In order to strengthen and harmonise administrative penalties for infringement of this Regulation, the upper limits for setting the administrative fines for certain specific infringements should be laid down. When assessing the amount of the fines, Member States should, in each individual case, take into account all relevant circumstances of the specific situation, with due regard in particular to the nature, gravity and duration of the infringement and of its consequences and to the size of the provider, in particular if the provider is an SME, including a start-up. The European Data Protection Supervisor should have the power to impose fines on Union institutions, agencies and bodies falling within the scope of this Regulation.",
      "original_content": "(168) Die Einhaltung dieser Verordnung sollte durch die Verhängung von Sanktionen und anderen Durchsetzungsmaßnahmen durchsetzbar sein. Die Mitgliedstaaten sollten alle erforderlichen Maßnahmen ergreifen, um sicherzustellen, dass die Bestimmungen dieser Verordnung durchgeführt werden, und dazu unter anderem wirksame, verhältnismäßige und abschreckende Sanktionen für Verstöße festlegen und das Verbot der Doppelbestrafung befolgen. Um die verwaltungsrechtlichen Sanktionen für Verstöße gegen diese Verordnung zu verschärfen und zu harmonisieren, sollten Obergrenzen für die Festsetzung der Geldbußen bei bestimmten Verstößen festgelegt werden. Bei der Bemessung der Höhe der Geldbußen sollten die Mitgliedstaaten in jedem Einzelfall alle relevanten Umstände der jeweiligen Situation berücksichtigen, insbesondere die Art, die Schwere und die Dauer des Verstoßes und seiner Folgen sowie die Größe des Anbieters, vor allem wenn es sich bei diesem um ein KMU — einschließlich eines Start-up-Unternehmens — handelt. Der Europäische Datenschutzbeauftragte sollte befugt sein, gegen Organe, Einrichtungen und sonstige Stellen der Union, die in den Anwendungsbereich dieser Verordnung fallen, Geldbußen zu verhängen."
    },
    {
      "chunk_idx": 168,
      "id": "48edb908-8ded-493b-aa2d-f7864af2bd26",
      "title": "Recital 169",
      "relevantChunksIds": [
        "09a1468c-9fc4-44ee-a8c1-fd07084ea0d8",
        "de889fca-779f-4b70-8ae2-166b1f752102",
        "61300139-468e-4f24-871e-61a39036b15f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(169) Compliance with the obligations on providers of general-purpose AI models imposed under this Regulation should be enforceable, inter alia, by means of fines. To that end, appropriate levels of fines should also be laid down for infringement of those obligations, including the failure to comply with measures requested by the Commission in accordance with this Regulation, subject to appropriate limitation periods in accordance with the principle of proportionality. All decisions taken by the Commission under this Regulation are subject to review by the Court of Justice of the European Union in accordance with the TFEU, including the unlimited jurisdiction of the Court of Justice with regard to penalties pursuant to Article 261 TFEU.",
      "original_content": "(169) Die Einhaltung der mit dieser Verordnung auferlegten Pflichten für Anbieter von KI-Modellen mit allgemeinem Verwendungszweck sollte unter anderem durch Geldbußen durchgesetzt werden können. Zu diesem Zweck sollten Geldbußen in angemessener Höhe für Verstöße gegen diese Pflichten, einschließlich der Nichteinhaltung der von der Kommission gemäß dieser Verordnung verlangten Maßnahmen, festgesetzt werden, vorbehaltlich angemessener Verjährungsfristen im Einklang mit dem Grundsatz der Verhältnismäßigkeit. Alle Beschlüsse, die die Kommission auf der Grundlage dieser Verordnung fasst, unterliegen der Überprüfung durch den Gerichtshof der Europäischen Union im Einklang mit dem AEUV, einschließlich der Befugnis des Gerichtshofs zu unbeschränkter Ermessensnachprüfung hinsichtlich Zwangsmaßnahmen im Einklang mit Artikel 261 AEUV."
    },
    {
      "chunk_idx": 169,
      "id": "e6bb22e1-0ed7-4f6c-951f-30c4b2c4ce92",
      "title": "Recital 170",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(170) Union and national law already provide effective remedies to natural and legal persons whose rights and freedoms are adversely affected by the use of AI systems. Without prejudice to those remedies, any natural or legal person that has grounds to consider that there has been an infringement of this Regulation should be entitled to lodge a complaint to the relevant market surveillance authority.",
      "original_content": "(170) Im Unionsrecht und im nationalen Recht sind bereits wirksame Rechtsbehelfe für natürliche und juristische Personen vorgesehen, deren Rechte und Freiheiten durch die Nutzung von KI-Systemen beeinträchtigt werden. Unbeschadet dieser Rechtsbehelfe sollte jede natürliche oder juristische Person, die Grund zu der Annahme hat, dass gegen diese Verordnung verstoßen wurde, befugt sein, bei der betreffenden Marktüberwachungsbehörde eine Beschwerde einzureichen."
    },
    {
      "chunk_idx": 170,
      "id": "1c72a793-5913-426b-8620-f58c14e80a52",
      "title": "Recital 171",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(171) Affected persons should have the right to obtain an explanation where a deployer’s decision is based mainly upon the output from certain high-risk AI systems that fall within the scope of this Regulation and where that decision produces legal effects or similarly significantly affects those persons in a way that they consider to have an adverse impact on their health, safety or fundamental rights. That explanation should be clear and meaningful and should provide a basis on which the affected persons are able to exercise their rights. The right to obtain an explanation should not apply to the use of AI systems for which exceptions or restrictions follow from Union or national law and should apply only to the extent this right is not already provided for under Union law.",
      "original_content": "(171) Betroffene Personen sollten das Recht haben, eine Erklärung zu erhalten, wenn eine Entscheidung eines Betreibers überwiegend auf den Ausgaben bestimmter Hochrisiko-KI-systeme beruht, die in den Geltungsbereich dieser Verordnung fallen, und wenn diese Entscheidung Rechtswirkungen entfaltet oder diese Personen in ähnlicher Weise wesentlich beeinträchtigt, und zwar so, dass sie ihrer Ansicht nach negative Auswirkungen auf ihre Gesundheit, ihre Sicherheit oder ihre Grundrechte hat. Diese Erklärung sollte klar und aussagekräftig sein, und sie sollte eine Grundlage bieten, auf der die betroffenen Personen ihre Rechte ausüben können. Das Recht auf eine Erklärung sollte nicht für die Nutzung von KI-Systemen gelten, für die sich aus dem Unionsrecht oder dem nationalen Recht Ausnahmen oder Beschränkungen ergeben, und es sollte nur insoweit gelten, als es nicht bereits in anderem Unionsrecht vorgesehen ist."
    },
    {
      "chunk_idx": 171,
      "id": "68a7aa5c-288c-4dce-9b64-cffa47db7506",
      "title": "Recital 172",
      "relevantChunksIds": [
        "95b7f020-8180-4dc6-80ab-b1c0cb455293"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(172) Persons acting as whistleblowers on the infringements of this Regulation should be protected under the Union law. Directive (EU) 2019/1937 of the European Parliament and of the Council (54) should therefore apply to the reporting of infringements of this Regulation and the protection of persons reporting such infringements.",
      "original_content": "(172) Personen, die als Hinweisgeber in Bezug auf die in dieser Verordnung genannten Verstöße auftreten, sollten durch das Unionsrecht geschützt werden. Für die Meldung von Verstößen gegen diese Verordnung und den Schutz von Personen, die solche Verstöße melden, sollte daher die Richtlinie (EU) 2019/1937 des Europäischen Parlaments und des Rates (Fußnote 54) gelten. Fußnote 54: Richtlinie (EU) 2019/1937 des Europäischen Parlaments und des Rates vom 23. Oktober 2019 zum Schutz von Personen, die Verstöße gegen das Unionsrecht melden (ABl. L 305 vom 26.11.2019, S. 17)."
    },
    {
      "chunk_idx": 172,
      "id": "a44a56d1-e4cf-4210-a6c9-2ca8c74a72bb",
      "title": "Recital 173",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(173) In order to ensure that the regulatory framework can be adapted where necessary, the power to adopt acts in accordance with Article 290 TFEU should be delegated to the Commission to amend the conditions under which an AI system is not to be considered to be high-risk, the list of high-risk AI systems, the provisions regarding technical documentation, the content of the EU declaration of conformity the provisions regarding the conformity assessment procedures, the provisions establishing the high-risk AI systems to which the conformity assessment procedure based on assessment of the quality management system and assessment of the technical documentation should apply, the threshold, benchmarks and indicators, including by supplementing those benchmarks and indicators, in the rules for the classification of general-purpose AI models with systemic risk, the criteria for the designation of general-purpose AI models with systemic risk, the technical documentation for providers of general-purpose AI models and the transparency information for providers of general-purpose AI models. It is of particular importance that the Commission carry out appropriate consultations during its preparatory work, including at expert level, and that those consultations be conducted in accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making (55). In particular, to ensure equal participation in the preparation of delegated acts, the European Parliament and the Council receive all documents at the same time as Member States’ experts, and their experts systematically have access to meetings of Commission expert groups dealing with the preparation of delegated acts.",
      "original_content": "(173) Damit der Regelungsrahmen erforderlichenfalls angepasst werden kann, sollte der Kommission die Befugnis übertragen werden, gemäß Artikel 290 AEUV Rechtsakte zur Änderung der Bedingungen, unter denen ein KI-System nicht als Hochrisiko-System einzustufen ist, der Liste der Hochrisiko-KI-Systeme, der Bestimmungen über die technische Dokumentation, des Inhalts der EU-Konformitätserklärung, der Bestimmungen über die Konformitätsbewertungsverfahren, der Bestimmungen zur Festlegung der Hochrisiko-KI-Systeme, für die das Konformitätsbewertungsverfahren auf der Grundlage der Bewertung des Qualitätsmanagementsystems und der technischen Dokumentation gelten sollte, der Schwellenwerte, Benchmarks und Indikatoren — auch durch Ergänzung dieser Benchmarks und Indikatoren — in den Vorschriften für die Einstufung von KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko, der Kriterien für die Benennung von KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko, der technischen Dokumentation für Anbieter von KI-Modellen mit allgemeinem Verwendungszweck und der Transparenzinformationen für Anbieter von KI-Modellen mit allgemeinem Verwendungszweck zu erlassen. Es ist von besonderer Bedeutung, dass die Kommission im Zuge ihrer Vorbereitungsarbeit angemessene Konsultationen, auch auf der Ebene von Sachverständigen, durchführt, die mit den Grundsätzen in Einklang stehen, die in der Interinstitutionellen Vereinbarung vom 13. April 2016 über bessere Rechtsetzung (Fußnote 55) niedergelegt wurden. Um insbesondere für eine gleichberechtigte Beteiligung an der Vorbereitung delegierter Rechtsakte zu sorgen, erhalten das Europäische Parlament und der Rat alle Dokumente zur gleichen Zeit wie die Sachverständigen der Mitgliedstaaten, und ihre Sachverständigen haben systematisch Zugang zu den Sitzungen der Sachverständigengruppen der Kommission, die mit der Vorbereitung der delegierten Rechtsakte befasst sind. Fußnote 55: ABl. L 123 vom 12.5.2016, S. 1."
    },
    {
      "chunk_idx": 173,
      "id": "9e485960-acdc-4f5b-9227-a02780abe54f",
      "title": "Recital 174",
      "relevantChunksIds": [
        "6f871e54-e731-4009-a4cd-39bbd19b543e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(174) Given the rapid technological developments and the technical expertise required to effectively apply this Regulation, the Commission should evaluate and review this Regulation by 2 August 2029 and every four years thereafter and report to the European Parliament and the Council. In addition, taking into account the implications for the scope of this Regulation, the Commission should carry out an assessment of the need to amend the list of high-risk AI systems and the list of prohibited practices once a year. Moreover, by 2 August 2028 and every four years thereafter, the Commission should evaluate and report to the European Parliament and to the Council on the need to amend the list of high-risk areas headings in the annex to this Regulation, the AI systems within the scope of the transparency obligations, the effectiveness of the supervision and governance system and the progress on the development of standardisation deliverables on energy efficient development of general-purpose AI models, including the need for further measures or actions. Finally, by 2 August 2028 and every three years thereafter, the Commission should evaluate the impact and effectiveness of voluntary codes of conduct to foster the application of the requirements provided for high-risk AI systems in the case of AI systems other than high-risk AI systems and possibly other additional requirements for such AI systems.",
      "original_content": "(174) Angesichts der raschen technologischen Entwicklungen und des für die wirksame Anwendung dieser Verordnung erforderlichen technischen Fachwissens sollte die Kommission diese Verordnung bis zum 2. August 2029 und danach alle vier Jahre bewerten und überprüfen und dem Europäischen Parlament und dem Rat darüber Bericht erstatten. Darüber hinaus sollte die Kommission — unter Berücksichtigung der Auswirkungen auf den Geltungsbereich dieser Verordnung — einmal jährlich beurteilen, ob es notwendig ist, die Liste der Hochrisiko-KI-Systeme und die Liste der verbotenen Praktiken zu ändern. Außerdem sollte die Kommission bis zum 2. August 2028 und danach alle vier Jahre die Notwendigkeit einer Änderung der Liste der Hochrisikobereiche im Anhang dieser Verordnung, die KI-Systeme im Geltungsbereich der Transparenzpflichten, die Wirksamkeit des Aufsichts- und Governance-Systems und die Fortschritte bei der Entwicklung von Normungsdokumenten zur energieeffizienten Entwicklung von KI-Modellen mit allgemeinem Verwendungszweck, einschließlich der Notwendigkeit weiterer Maßnahmen oder Handlungen, bewerten und dem Europäischen Parlament und dem Rat darüber Bericht erstatten. Schließlich sollte die Kommission bis zum 2. August 2028 und danach alle drei Jahre eine Bewertung der Folgen und der Wirksamkeit der freiwilligen Verhaltenskodizes durchführen, mit denen die Anwendung der für Hochrisiko-KI-Systeme vorgesehenen Anforderungen bei anderen KI-Systemen als Hochrisiko-KI-Systemen und möglicherweise auch zusätzlicher Anforderungen an solche KI-Systeme gefördert werden soll."
    },
    {
      "chunk_idx": 174,
      "id": "d41dc3fc-6a3f-4db4-9c9b-6b9b1bf3454f",
      "title": "Recital 175",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(175) In order to ensure uniform conditions for the implementation of this Regulation, implementing powers should be conferred on the Commission. Those powers should be exercised in accordance with Regulation (EU) No 182/2011 of the European Parliament and of the Council (56).",
      "original_content": "(175) Zur Gewährleistung einheitlicher Bedingungen für die Durchführung dieser Verordnung sollten der Kommission Durchführungsbefugnisse übertragen werden. Diese Befugnisse sollten gemäß der Verordnung (EU) Nr. 182/2011 des Europäischen Parlaments und des Rates (Fußnote 56) ausgeübt werden. Fußnote 56: Verordnung (EU) Nr. 182/2011 des Europäischen Parlaments und des Rates vom 16. Februar 2011 zur Festlegung der allgemeinen Regeln und Grundsätze, nach denen die Mitgliedstaaten die Wahrnehmung der Durchführungsbefugnisse durch die Kommission kontrollieren (ABl. L 55 vom 28.2.2011, S. 13)."
    },
    {
      "chunk_idx": 175,
      "id": "8d281ca3-ab5a-4f6e-8b3b-e59fe81002df",
      "title": "Recital 176",
      "relevantChunksIds": [
        "8828f983-a392-468e-9e97-1217cb1ded61"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(176) Since the objective of this Regulation, namely to improve the functioning of the internal market and to promote the uptake of human centric and trustworthy AI, while ensuring a high level of protection of health, safety, fundamental rights enshrined in the Charter, including democracy, the rule of law and environmental protection against harmful effects of AI systems in the Union and supporting innovation, cannot be sufficiently achieved by the Member States and can rather, by reason of the scale or effects of the action, be better achieved at Union level, the Union may adopt measures in accordance with the principle of subsidiarity as set out in Article 5 TEU. In accordance with the principle of proportionality as set out in that Article, this Regulation does not go beyond what is necessary in order to achieve that objective.",
      "original_content": "(176) Da das Ziel dieser Verordnung, nämlich die Verbesserung der Funktionsweise des Binnenmarkts und die Förderung der Einführung einer auf den Menschen ausgerichteten und vertrauenswürdigen KI bei gleichzeitiger Gewährleistung eines hohen Maßes an Schutz der Gesundheit, der Sicherheit, der in der Charta verankerten Grundrechte, einschließlich Demokratie, Rechtsstaatlichkeit und Schutz der Umwelt vor schädlichen Auswirkungen von KI-Systemen in der Union, und der Förderung von Innovation, von den Mitgliedstaaten nicht ausreichend verwirklicht werden kann, sondern vielmehr wegen des Umfangs oder der Wirkungen der Maßnahme auf Unionsebene besser zu verwirklichen ist, kann die Union im Einklang mit dem in Artikel 5 EUV verankerten Subsidiaritätsprinzip tätig werden. Entsprechend dem in demselben Artikel genannten Grundsatz der Verhältnismäßigkeit geht diese Verordnung nicht über das für die Verwirklichung dieses Ziels erforderliche Maß hinaus."
    },
    {
      "chunk_idx": 176,
      "id": "3538822c-f1a4-4720-a75d-17a6b5e0c2e6",
      "title": "Recital 177",
      "relevantChunksIds": [
        "5a35c644-0096-47c7-88ed-c793b728774e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(177) In order to ensure legal certainty, ensure an appropriate adaptation period for operators and avoid disruption to the market, including by ensuring continuity of the use of AI systems, it is appropriate that this Regulation applies to the high-risk AI systems that have been placed on the market or put into service before the general date of application thereof, only if, from that date, those systems are subject to significant changes in their design or intended purpose. It is appropriate to clarify that, in this respect, the concept of significant change should be understood as equivalent in substance to the notion of substantial modification, which is used with regard only to high-risk AI systems pursuant to this Regulation. On an exceptional basis and in light of public accountability, operators of AI systems which are components of the large-scale IT systems established by the legal acts listed in an annex to this Regulation and operators of high-risk AI systems that are intended to be used by public authorities should, respectively, take the necessary steps to comply with the requirements of this Regulation by end of 2030 and by 2 August 2030.",
      "original_content": "(177) Um Rechtssicherheit zu gewährleisten, einen angemessenen Anpassungszeitraum für die Akteure sicherzustellen und Marktstörungen zu vermeiden, unter anderem durch Gewährleistung der Kontinuität der Verwendung von KI-Systemen, ist es angezeigt, dass diese Verordnung nur dann für die Hochrisiko-KI-Systeme, die vor dem allgemeinen Anwendungsbeginn dieser Verordnung in Verkehr gebracht oder in Betrieb genommen wurden, gilt, wenn diese Systeme ab diesem Datum erheblichen Veränderungen in Bezug auf ihre Konzeption oder Zweckbestimmung unterliegen. Es ist angezeigt, klarzustellen, dass der Begriff der erheblichen Veränderung in diesem Hinblick als gleichwertig mit dem Begriff der wesentlichen Änderung verstanden werden sollte, der nur in Bezug auf Hochrisiko-KI-Systeme im Sinne dieser Verordnung verwendet wird. Ausnahmsweise und im Lichte der öffentlichen Rechenschaftspflicht sollten Betreiber von KI-Systemen, die Komponenten der in einem Anhang zu dieser Verordnung aufgeführten durch Rechtsakte eingerichteten IT-Großsysteme sind, und Betreiber von Hochrisiko-KI-Systemen, die von Behörden genutzt werden sollen, die erforderlichen Schritte unternehmen, um den Anforderungen dieser Verordnung bis Ende 2030 bzw. bis zum 2. August 2030 nachzukommen."
    },
    {
      "chunk_idx": 177,
      "id": "6cb54c7b-96fd-468c-902d-7b11656ec6ad",
      "title": "Recital 178",
      "relevantChunksIds": [
        "5a35c644-0096-47c7-88ed-c793b728774e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(178) Providers of high-risk AI systems are encouraged to start to comply, on a voluntary basis, with the relevant obligations of this Regulation already during the transitional period.",
      "original_content": "(178) Die Anbieter von Hochrisiko-KI-Systemen werden ermutigt, auf freiwilliger Basis bereits während der Übergangsphase mit der Einhaltung der einschlägigen Pflichten aus dieser Verordnung zu beginnen."
    },
    {
      "chunk_idx": 178,
      "id": "7c5fc4fc-05c2-4781-9924-fc7ef6ca50ef",
      "title": "Recital 179",
      "relevantChunksIds": [
        "e0ff4862-edad-42af-8c72-7ac7ab5c25b7"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(179) This Regulation should apply from 2 August 2026. However, taking into account the unacceptable risk associated with the use of AI in certain ways, the prohibitions as well as the general provisions of this Regulation should already apply from 2 February 2025. While the full effect of those prohibitions follows with the establishment of the governance and enforcement of this Regulation, anticipating the application of the prohibitions is important to take account of unacceptable risks and to have an effect on other procedures, such as in civil law. Moreover, the infrastructure related to the governance and the conformity assessment system should be operational before 2 August 2026, therefore the provisions on notified bodies and governance structure should apply from 2 August 2025. Given the rapid pace of technological advancements and adoption of general-purpose AI models, obligations for providers of general-purpose AI models should apply from 2 August 2025. Codes of practice should be ready by 2 May 2025 in view of enabling providers to demonstrate compliance on time. The AI Office should ensure that classification rules and procedures are up to date in light of technological developments. In addition, Member States should lay down and notify to the Commission the rules on penalties, including administrative fines, and ensure that they are properly and effectively implemented by the date of application of this Regulation. Therefore the provisions on penalties should apply from 2 August 2025.",
      "original_content": "(179) Diese Verordnung sollte ab dem 2. August 2026 gelten. Angesichts des unannehmbaren Risikos, das mit der Nutzung von KI auf bestimmte Weise verbunden ist, sollten die Verbote sowie die allgemeinen Bestimmungen dieser Verordnung jedoch bereits ab dem 2. Februar 2025 gelten. Während die volle Wirkung dieser Verbote erst mit der Festlegung der Leitung und der Durchsetzung dieser Verordnung entsteht, ist die Vorwegnahme der Anwendung der Verbote wichtig, um unannehmbaren Risiken Rechnung zu tragen und Wirkung auf andere Verfahren, etwa im Zivilrecht, zu entfalten. Darüber hinaus sollte die Infrastruktur für die Leitung und das Konformitätsbewertungssystem vor dem 2. August 2026 einsatzbereit sein, weshalb die Bestimmungen über notifizierte Stellen und die Leitungsstruktur ab dem 2. August 2025 gelten sollten. Angesichts des raschen technologischen Fortschritts und der Einführung von KI-Modellen mit allgemeinem Verwendungszweck sollten die Pflichten der Anbieter von KI-Modellen mit allgemeinem Verwendungszweck ab dem 2. August 2025 gelten. Die Verhaltenskodizes sollten bis zum 2. Mai 2025 vorliegen, damit die Anbieter die Einhaltung fristgerecht nachweisen können. Das Büro für Künstliche Intelligenz sollte sicherstellen, dass die Vorschriften und Verfahren für die Einstufung jeweils dem Stand der technologischen Entwicklung entsprechen. Darüber hinaus sollten die Mitgliedstaaten die Vorschriften über Sanktionen, einschließlich Geldbußen, festlegen und der Kommission mitteilen sowie dafür sorgen, dass diese bis zum Geltungsbeginn dieser Verordnung ordnungsgemäß und wirksam umgesetzt werden. Daher sollten die Bestimmungen über Sanktionen ab dem 2. August 2025 gelten."
    },
    {
      "chunk_idx": 179,
      "id": "a6ffb918-84bf-4fde-b544-671ae110c24d",
      "title": "Recital 180",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "(180) The European Data Protection Supervisor and the European Data Protection Board were consulted in accordance with Article 42(1) and (2) of Regulation (EU) 2018/1725 and delivered their joint opinion on 18 June 2021,",
      "original_content": "(180) Der Europäische Datenschutzbeauftragte und der Europäische Datenschutzausschuss wurden gemäß Artikel 42 Absätze 1 und 2 der Verordnung (EU) 2018/1725 angehört und haben am 18. Juni 2021 ihre gemeinsame Stellungnahme abgegeben —"
    },
    {
      "chunk_idx": 180,
      "id": "be26e0cd-4d28-42f6-8560-20e6911c4c4f",
      "title": "Art 1",
      "relevantChunksIds": [
        "b833c1d7-ad46-4548-a2c6-63f671c1d211",
        "aa44ef37-ff65-4237-9ed5-b34174aa9c6a",
        "10c70d26-f011-44f0-89db-011879d8401c",
        "6ead0916-b1d0-4ee7-a131-b86ac144d0ac",
        "0edca1d5-9879-4157-bd56-130035f6204f",
        "27cb6f5f-8336-46c3-a0bd-07955c3bd714",
        "fde668f5-fe30-48cc-b961-be101bc4e1a7",
        "8b015b72-9483-4675-bf5a-02c2ddc3a267",
        "59e64fd2-e26e-426d-be65-48f42d265850"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "# CHAPTER I: GENERAL PROVISIONS\n### Article 1: Subject matter`\n1. The purpose of this Regulation is to improve the functioning of the internal market and promote the uptake of human-centric and trustworthy artificial intelligence (AI), while ensuring a high level of protection of health, safety, fundamental rights enshrined in the Charter, including democracy, the rule of law and environmental protection, against the harmful effects of AI systems in the Union and supporting innovation.\n2. This Regulation lays down:\n(a) harmonised rules for the placing on the market, the putting into service, and the use of AI systems in the Union;\n(b) prohibitions of certain AI practices;\n(c) specific requirements for high-risk AI systems and obligations for operators of such systems;\n(d) harmonised transparency rules for certain AI systems;\n(e) harmonised rules for the placing on the market of general-purpose AI models;\n(f) rules on market monitoring, market surveillance, governance and enforcement;\n(g) measures to support innovation, with a particular focus on SMEs, including start-ups.",
      "original_content": "# KAPITEL I: ALLGEMEINE BESTIMMUNGEN\n### Artikel 1: Gegenstand\n(1) Zweck dieser Verordnung ist es, das Funktionieren des Binnenmarkts zu verbessern und die Einführung einer auf den Menschen ausgerichteten und vertrauenswürdigen künstlichen Intelligenz (KI) zu fördern und gleichzeitig ein hohes Schutzniveau in Bezug auf Gesundheit, Sicherheit und die in der Charta verankerten Grundrechte, einschließlich Demokratie, Rechtsstaatlichkeit und Umweltschutz, vor schädlichen Auswirkungen von KI-Systemen in der Union zu gewährleisten und die Innovation zu unterstützen.\n(2) In dieser Verordnung wird Folgendes festgelegt:\na) harmonisierte Vorschriften für das Inverkehrbringen, die Inbetriebnahme und die Verwendung von KI-Systemen in der Union;\nb) Verbote bestimmter Praktiken im KI-Bereich;\nc) besondere Anforderungen an Hochrisiko-KI-Systeme und Pflichten für Akteure in Bezug auf solche Systeme;\nd) harmonisierte Transparenzvorschriften für bestimmte KI-Systeme;\ne) harmonisierte Vorschriften für das Inverkehrbringen von KI-Modellen mit allgemeinem Verwendungszweck;\nf) Vorschriften für die Marktbeobachtung sowie die Governance und Durchsetzung der Marktüberwachung;\ng) Maßnahmen zur Innovationsförderung mit besonderem Augenmerk auf KMU, einschließlich Start-up-Unternehmen."
    },
    {
      "chunk_idx": 181,
      "id": "55e8665d-afb5-46d1-b258-64f964b08d09",
      "title": "Art 2",
      "relevantChunksIds": [
        "53f9c5fc-d089-4233-9a15-16964e40e3c5",
        "c99d2066-4a8b-4bc4-85b0-b7d0c948cc56",
        "f9b21785-6f87-4438-8810-147304eb4d7e",
        "38517ea5-282c-498e-8559-1df8a9f2efdb",
        "a010e5cb-6b93-499f-ad13-0a86a4c6239a",
        "a035c99d-ea59-4c78-87db-94e1b89e1980",
        "51f69443-5b55-4fb2-b1d5-31137a66e681",
        "59e64fd2-e26e-426d-be65-48f42d265850"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 2: Scope\n1. This Regulation applies to:\n(a) providers placing on the market or putting into service AI systems or placing on the market general-purpose AI models in the Union, irrespective of whether those providers are established or located within the Union or in a third country;\n(b) deployers of AI systems that have their place of establishment or are located within the Union;\n(c) providers and deployers of AI systems that have their place of establishment or are located in a third country, where the output produced by the AI system is used in the Union;\n(d) importers and distributors of AI systems;\n(e) product manufacturers placing on the market or putting into service an AI system together with their product and under their own name or trademark;\n(f) authorised representatives of providers, which are not established in the Union;\n(g) affected persons that are located in the Union.\n2. For AI systems classified as high-risk AI systems in accordance with Article 6(1) related to products covered by the Union harmonisation legislation listed in Section B of Annex I, only Article 6(1), Articles 102 to 109 and Article 112 apply. Article 57 applies only in so far as the requirements for high-risk AI systems under this Regulation have been integrated in that Union harmonisation legislation.\n3. This Regulation does not apply to areas outside the scope of Union law, and shall not, in any event, affect the competences of the Member States concerning national security, regardless of the type of entity entrusted by the Member States with carrying out tasks in relation to those competences.\nThis Regulation does not apply to AI systems where and in so far they are placed on the market, put into service, or used with or without modification exclusively for military, defence or national security purposes, regardless of the type of entity carrying out those activities.\nThis Regulation does not apply to AI systems which are not placed on the market or put into service in the Union, where the output is used in the Union exclusively for military, defence or national security purposes, regardless of the type of entity carrying out those activities.\n4. This Regulation applies neither to public authorities in a third country nor to international organisations falling within the scope of this Regulation pursuant to paragraph 1, where those authorities or organisations use AI systems in the framework of international cooperation or agreements for law enforcement and judicial cooperation with the Union or with one or more Member States, provided that such a third country or international organisation provides adequate safeguards with respect to the protection of fundamental rights and freedoms of individuals.\n5. This Regulation shall not affect the application of the provisions on the liability of providers of intermediary services as set out in Chapter II of Regulation (EU) 2022/2065.\n6. This Regulation does not apply to AI systems or AI models, including their output, specifically developed and put into service for the sole purpose of scientific research and development.\n7. Union law on the protection of personal data, privacy and the confidentiality of communications applies to personal data processed in connection with the rights and obligations laid down in this Regulation. This Regulation shall not affect Regulation (EU) 2016/679 or (EU) 2018/1725, or Directive 2002/58/EC or (EU) 2016/680, without prejudice to Article 10(5) and Article 59 of this Regulation.\n8. This Regulation does not apply to any research, testing or development activity regarding AI systems or AI models prior to their being placed on the market or put into service. Such activities shall be conducted in accordance with applicable Union law. Testing in real world conditions shall not be covered by that exclusion.\n9. This Regulation is without prejudice to the rules laid down by other Union legal acts related to consumer protection and product safety.\n10. This Regulation does not apply to obligations of deployers who are natural persons using AI systems in the course of a purely personal non-professional activity.\n11. This Regulation does not preclude the Union or Member States from maintaining or introducing laws, regulations or administrative provisions which are more favourable to workers in terms of protecting their rights in respect of the use of AI systems by employers, or from encouraging or allowing the application of collective agreements which are more favourable to workers.\n12. This Regulation does not apply to AI systems released under free and open-source licences, unless they are placed on the market or put into service as high-risk AI systems or as an AI system that falls under Article 5 or 50.",
      "original_content": "### Artikel 2: Anwendungsbereich\n(1) Diese Verordnung gilt für\na) Anbieter, die in der Union KI-Systeme in Verkehr bringen oder in Betrieb nehmen oder KI-Modelle mit allgemeinem Verwendungszweck in Verkehr bringen, unabhängig davon, ob diese Anbieter in der Union oder in einem Drittland niedergelassen sind;\nb) Betreiber von KI-Systemen, die ihren Sitz in der Union haben oder in der Union befinden;\nc) Anbieter und Betreiber von KI-Systemen, die ihren Sitz in einem Drittland haben oder sich in einem Drittland befinden, wenn die vom KI-System hervorgebrachte Ausgabe in der Union verwendet wird;\nd) Einführer und Händler von KI-Systemen;\ne) Produkthersteller, die KI-Systeme zusammen mit ihrem Produkt unter ihrem eigenen Namen oder ihrer Handelsmarke in Verkehr bringen oder in Betrieb nehmen;\nf) Bevollmächtigte von Anbietern, die nicht in der Union niedergelassen sind;\ng) betroffene Personen, die sich in der Union befinden.\n(2) Für KI-Systeme, die als Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 1 eingestuft sind und im Zusammenhang mit Produkten stehen, die unter die in Anhang I Abschnitt B aufgeführten Harmonisierungsrechtsvorschriften der Union fallen, gelten nur Artikel 6 Absatz 1, die Artikel 102 bis 109 und Artikel 112. Artikel 57 gilt nur, soweit die Anforderungen an Hochrisiko-KI-Systeme gemäß dieser Verordnung im Rahmen der genannten Harmonisierungsrechtsvorschriften der Union eingebunden wurden.\n(3) Diese Verordnung gilt nur in den unter das Unionsrecht fallenden Bereichen und berührt keinesfalls die Zuständigkeiten der Mitgliedstaaten in Bezug auf die nationale Sicherheit, unabhängig von der Art der Einrichtung, die von den Mitgliedstaaten mit der Wahrnehmung von Aufgaben im Zusammenhang mit diesen Zuständigkeiten betraut wurde.\nDiese Verordnung gilt nicht für KI-Systeme, wenn und soweit sie ausschließlich für militärische Zwecke, Verteidigungszwecke oder Zwecke der nationalen Sicherheit in Verkehr gebracht, in Betrieb genommen oder, mit oder ohne Änderungen, verwendet werden, unabhängig von der Art der Einrichtung, die diese Tätigkeiten ausübt.\nDiese Verordnung gilt nicht für KI-Systeme, die nicht in der Union in Verkehr gebracht oder in Betrieb genommen werden, wenn die Ausgaben in der Union ausschließlich für militärische Zwecke, Verteidigungszwecke oder Zwecke der nationalen Sicherheit verwendet werden, unabhängig von der Art der Einrichtung, die diese Tätigkeiten ausübt.\n(4) Diese Verordnung gilt weder für Behörden in Drittländern noch für internationale Organisationen, die gemäß Absatz 1 in den Anwendungsbereich dieser Verordnung fallen, soweit diese Behörden oder Organisationen KI-Systeme im Rahmen der internationalen Zusammenarbeit oder internationaler Übereinkünfte im Bereich der Strafverfolgung und justiziellen Zusammenarbeit mit der Union oder mit einem oder mehreren Mitgliedstaaten verwenden und sofern ein solches Drittland oder eine solche internationale Organisation angemessene Garantien hinsichtlich des Schutz der Privatsphäre, der Grundrechte und der Grundfreiheiten von Personen bietet.\n(5) Die Anwendung der Bestimmungen über die Haftung der Anbieter von Vermittlungsdiensten in Kapitel II der Verordnung 2022/2065 bleibt von dieser Verordnung unberührt.\n(6) Diese Verordnung gilt nicht für KI-Systeme oder KI-Modelle, einschließlich ihrer Ausgabe, die eigens für den alleinigen Zweck der wissenschaftlichen Forschung und Entwicklung entwickelt und in Betrieb genommen werden.\n(7) Die Rechtsvorschriften der Union zum Schutz personenbezogener Daten, der Privatsphäre und der Vertraulichkeit der Kommunikation gelten für die Verarbeitung personenbezogener Daten im Zusammenhang mit den in dieser Verordnung festgelegten Rechten und Pflichten. Diese Verordnung berührt nicht die Verordnung (EU) 2016/679 bzw. (EU) 2018/1725 oder die Richtlinie 2002/58/EG bzw. (EU) 2016/680, unbeschadet des Artikels 10 Absatz 5 und des Artikels 59 der vorliegenden Verordnung.\n(8) Diese Verordnung gilt nicht für Forschungs-, Test- und Entwicklungstätigkeiten zu KI-Systemen oder KI-Modellen, bevor diese in Verkehr gebracht oder in Betrieb genommen werden. Solche Tätigkeiten werden im Einklang mit dem geltenden Unionsrecht durchgeführt. Tests unter Realbedingungen fallen nicht unter diesen Ausschluss.\n(9) Diese Verordnung berührt nicht die Vorschriften anderer Rechtsakte der Union zum Verbraucherschutz und zur Produktsicherheit.\n(10) Diese Verordnung gilt nicht für die Pflichten von Betreibern, die natürliche Personen sind und KI-Systeme im Rahmen einer ausschließlich persönlichen und nicht beruflichen Tätigkeit verwenden.\n(11) Diese Verordnung hindert die Union oder die Mitgliedstaaten nicht daran, Rechts- oder Verwaltungsvorschriften beizubehalten oder einzuführen, die für die Arbeitnehmer im Hinblick auf den Schutz ihrer Rechte bei der Verwendung von KI-Systemen durch die Arbeitgeber vorteilhafter sind, oder die Anwendung von Kollektivvereinbarungen zu fördern oder zuzulassen, die für die Arbeitnehmer vorteilhafter sind.\n(12) Diese Verordnung gilt nicht für KI-Systeme, die unter freien und quelloffenen Lizenzen bereitgestellt werden, es sei denn, sie werden als Hochrisiko-KI-Systeme oder als ein KI-System, das unter Artikel 5 oder 50 fällt, in Verkehr gebracht oder in Betrieb genommen."
    },
    {
      "chunk_idx": 182,
      "id": "7401fefb-6095-47f0-a00e-78aac879499e",
      "title": "Art 3: Z1 Definition KI-System",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 3: Definitions\nFor the purposes of this Regulation, the following definitions apply:\n(1) ‘AI system’ means a machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments;",
      "original_content": "### Artikel 3: Begriffsbestimmungen\nFür die Zwecke dieser Verordnung bezeichnet der Ausdruck\n1. „KI-System“ ein maschinengestütztes System, das für einen in unterschiedlichem Grade autonomen Betrieb ausgelegt ist und das nach seiner Betriebsaufnahme anpassungsfähig sein kann und das aus den erhaltenen Eingaben für explizite oder implizite Ziele ableitet, wie Ausgaben wie etwa Vorhersagen, Inhalte, Empfehlungen oder Entscheidungen erstellt werden, die physische oder virtuelle Umgebungen beeinflussen können;"
    },
    {
      "chunk_idx": 183,
      "id": "7b38ddca-c29b-49fb-aa1c-85366820f3f4",
      "title": "Art 3: Z12, Z23 Zweck und Änderung",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 3: Definitions\nFor the purposes of this Regulation, the following definitions apply:\n\n(12) ‘intended purpose’ means the use for which an AI system is intended by the provider, including the specific context and conditions of use, as specified in the information supplied by the provider in the instructions for use, promotional or sales materials and statements, as well as in the technical documentation;\n\n(23) ‘substantial modification’ means a change to an AI system after its placing on the market or putting into service which is not foreseen or planned in the initial conformity assessment carried out by the provider and as a result of which the compliance of the AI system with the requirements set out in Chapter III, Section 2 is affected or results in a modification to the intended purpose for which the AI system has been assessed;",
      "original_content": "### Artikel 3: Begriffsbestimmungen\nFür die Zwecke dieser Verordnung bezeichnet der Ausdruck\n[...]\n12. „Zweckbestimmung“ die Verwendung, für die ein KI-System laut Anbieter bestimmt ist, einschließlich der besonderen Umstände und Bedingungen für die Verwendung, entsprechend den vom Anbieter bereitgestellten Informationen in den Betriebsanleitungen, im Werbe- oder Verkaufsmaterial und in diesbezüglichen Erklärungen sowie in der technischen Dokumentation;\n[...]\n23. „wesentliche Veränderung“ eine Veränderung eines KI-Systems nach dessen Inverkehrbringen oder Inbetriebnahme, die in der vom Anbieter durchgeführten ursprünglichen Konformitätsbewertung nicht vorgesehen oder geplant war und durch die die Konformität des KI-Systems mit den Anforderungen in Kapitel III Abschnitt 2 beeinträchtigt wird oder die zu einer Änderung der Zweckbestimmung führt, für die das KI-System bewertet wurde;"
    },
    {
      "chunk_idx": 184,
      "id": "2d6c0b8d-ad3f-4548-935f-963bf8cfcf11",
      "title": "Art 3: Z13, Z15-Z18, Z20, Z24-Z25 Anbieter- und Betreiberpflichten",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 3: Definitions\nFor the purposes of this Regulation, the following definitions apply:\n\n(13) ‘reasonably foreseeable misuse’ means the use of an AI system in a way that is not in accordance with its intended purpose, but which may result from reasonably foreseeable human behaviour or interaction with other systems, including other AI systems;\n\n(15) ‘instructions for use’ means the information provided by the provider to inform the deployer of, in particular, an AI system’s intended purpose and proper use;\n(16) ‘recall of an AI system’ means any measure aiming to achieve the return to the provider or taking out of service or disabling the use of an AI system made available to deployers;\n(17) ‘withdrawal of an AI system’ means any measure aiming to prevent an AI system in the supply chain being made available on the market;\n(18) ‘performance of an AI system’ means the ability of an AI system to achieve its intended purpose;\n\n(20) ‘conformity assessment’ means the process of demonstrating whether the requirements set out in Chapter III, Section 2 relating to a high-risk AI system have been fulfilled;\n\n(24) ‘CE marking’ means a marking by which a provider indicates that an AI system is in conformity with the requirements set out in Chapter III, Section 2 and other applicable Union harmonisation legislation providing for its affixing;\n(25) ‘post-market monitoring system’ means all activities carried out by providers of AI systems to collect and review experience gained from the use of AI systems they place on the market or put into service for the purpose of identifying any need to immediately apply any necessary corrective or preventive actions;",
      "original_content": "### Artikel 3: Begriffsbestimmungen\nFür die Zwecke dieser Verordnung bezeichnet der Ausdruck\n[...]\n13. „vernünftigerweise vorhersehbare Fehlanwendung“ die Verwendung eines KI-Systems in einer Weise, die nicht seiner Zweckbestimmung entspricht, die sich aber aus einem vernünftigerweise vorhersehbaren menschlichen Verhalten oder einer vernünftigerweise vorhersehbaren Interaktion mit anderen Systemen, auch anderen KI-Systemen, ergeben kann;\n[...]\n15. „Betriebsanleitungen“ die Informationen, die der Anbieter bereitstellt, um den Betreiber insbesondere über die Zweckbestimmung und die ordnungsgemäße Verwendung eines KI-Systems zu informieren;\n16. „Rückruf eines KI-Systems“ jede Maßnahme, die auf die Rückgabe an den Anbieter oder auf die Außerbetriebsetzung oder Abschaltung eines den Betreibern bereits zur Verfügung gestellten KI-Systems abzielt;\n17. „Rücknahme eines KI-Systems“ jede Maßnahme, mit der die Bereitstellung eines in der Lieferkette befindlichen KI-Systems auf dem Markt verhindert werden soll;\n18. „Leistung eines KI-Systems“ die Fähigkeit eines KI-Systems, seine Zweckbestimmung zu erfüllen;\n[...]\n20. „Konformitätsbewertung“ ein Verfahren mit dem bewertet wird, ob die in Titel III Abschnitt 2 festgelegten Anforderungen an ein Hochrisiko-KI-System erfüllt wurden;\n[...]\n24. „CE-Kennzeichnung“ eine Kennzeichnung, durch die ein Anbieter erklärt, dass ein KI-System die Anforderungen erfüllt, die in Kapitel III Abschnitt 2 und in anderen anwendbaren Harmonisierungsrechtsvorschriften, die die Anbringung dieser Kennzeichnung vorsehen, festgelegt sind;\n25. „System zur Beobachtung nach dem Inverkehrbringen“ alle Tätigkeiten, die Anbieter von KI-Systemen zur Sammlung und Überprüfung von Erfahrungen mit der Verwendung der von ihnen in Verkehr gebrachten oder in Betrieb genommenen KI-Systeme durchführen, um festzustellen, ob unverzüglich nötige Korrektur- oder Präventivmaßnahmen zu ergreifen sind;"
    },
    {
      "chunk_idx": 185,
      "id": "ee5c5bfc-ded7-4010-bdae-d396f7798630",
      "title": "Art 3: Z14, Z62 Sicherheitsbauteil",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 3: Definitions\nFor the purposes of this Regulation, the following definitions apply:\n\n(14) ‘safety component’ means a component of a product or of an AI system which fulfils a safety function for that product or AI system, or the failure or malfunctioning of which endangers the health and safety of persons or property;\n\n(62) ‘critical infrastructure’ means critical infrastructure as defined in Article 2, point (4), of Directive (EU) 2022/2557;",
      "original_content": "### Artikel 3: Begriffsbestimmungen\nFür die Zwecke dieser Verordnung bezeichnet der Ausdruck\n[...]\n14. „Sicherheitsbauteil“ einen Bestandteil eines Produkts oder KI-Systems, der eine Sicherheitsfunktion für dieses Produkt oder KI-System erfüllt oder dessen Ausfall oder Störung die Gesundheit und Sicherheit von Personen oder Eigentum gefährdet;\n[...]\n62. „kritische Infrastrukturen“ kritische Infrastrukturen im Sinne von Artikel 2 Nummer 4 der Richtlinie (EU) 2022/2557;"
    },
    {
      "chunk_idx": 186,
      "id": "8371d991-3132-4b4f-b0b4-0cecd37c2b99",
      "title": "Art 3: Z2, Z49, Z64-Z65 Risiken",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 3: Definitions\nFor the purposes of this Regulation, the following definitions apply:\n\n(2) ‘risk’ means the combination of the probability of an occurrence of harm and the severity of that harm;\n\n(49) ‘serious incident’ means an incident or malfunctioning of an AI system that directly or indirectly leads to any of the following:\n(a) the death of a person, or serious harm to a person’s health;\n(b) a serious and irreversible disruption of the management or operation of critical infrastructure;\n(c) the infringement of obligations under Union law intended to protect fundamental rights;\n(d) serious harm to property or the environment;\n\n(64) ‘high-impact capabilities’ means capabilities that match or exceed the capabilities recorded in the most advanced general-purpose AI models;\n(65) ‘systemic risk’ means a risk that is specific to the high-impact capabilities of general-purpose AI models, having a significant impact on the Union market due to their reach, or due to actual or reasonably foreseeable negative effects on public health, safety, public security, fundamental rights, or the society as a whole, that can be propagated at scale across the value chain;",
      "original_content": "### Artikel 3: Begriffsbestimmungen\nFür die Zwecke dieser Verordnung bezeichnet der Ausdruck\n[...]\n2. „Risiko“ die Kombination aus der Wahrscheinlichkeit des Auftretens eines Schadens und der Schwere dieses Schadens;\n[...]\n49. „schwerwiegender Vorfall“ einen Vorfall oder eine Fehlfunktion bezüglich eines KI-Systems, das bzw. die direkt oder indirekt eine der nachstehenden Folgen hat:\na) den Tod oder die schwere gesundheitliche Schädigung einer Person;\nb) eine schwere und unumkehrbare Störung der Verwaltung oder des Betriebs kritischer Infrastrukturen;\nc) die Verletzung von Pflichten aus den Unionsrechtsvorschriften zum Schutz der Grundrechte;\nd) schwere Sach- oder Umweltschäden;\n[...]\n64. „Fähigkeiten mit hoher Wirkkraft“ bezeichnet Fähigkeiten, die den bei den fortschrittlichsten KI-Modellen mit allgemeinem Verwendungszweck festgestellten Fähigkeiten entsprechen oder diese übersteigen;\n65. „systemisches Risiko“ ein Risiko, das für die Fähigkeiten mit hoher Wirkkraft von KI-Modellen mit allgemeinem Verwendungszweck spezifisch ist und aufgrund deren Reichweite oder aufgrund tatsächlicher oder vernünftigerweise vorhersehbarer negativer Folgen für die öffentliche Gesundheit, die Sicherheit, die öffentliche Sicherheit, die Grundrechte oder die Gesellschaft insgesamt erhebliche Auswirkungen auf den Unionsmarkt hat, die sich in großem Umfang über die gesamte Wertschöpfungskette hinweg verbreiten können;"
    },
    {
      "chunk_idx": 187,
      "id": "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6",
      "title": "Art 3: Z3-Z11, Z68 Akteure",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 3: Definitions\nFor the purposes of this Regulation, the following definitions apply:\n\n(3) ‘provider’ means a natural or legal person, public authority, agency or other body that develops an AI system or a general-purpose AI model or that has an AI system or a general-purpose AI model developed and places it on the market or puts the AI system into service under its own name or trademark, whether for payment or free of charge;\n(4) ‘deployer’ means a natural or legal person, public authority, agency or other body using an AI system under its authority except where the AI system is used in the course of a personal non-professional activity;\n(5) ‘authorised representative’ means a natural or legal person located or established in the Union who has received and accepted a written mandate from a provider of an AI system or a general-purpose AI model to, respectively, perform and carry out on its behalf the obligations and procedures established by this Regulation;\n(6) ‘importer’ means a natural or legal person located or established in the Union that places on the market an AI system that bears the name or trademark of a natural or legal person established in a third country;\n(7) ‘distributor’ means a natural or legal person in the supply chain, other than the provider or the importer, that makes an AI system available on the Union market;\n(8) ‘operator’ means a provider, product manufacturer, deployer, authorised representative, importer or distributor;\n(9) ‘placing on the market’ means the first making available of an AI system or a general-purpose AI model on the Union market;\n(10) ‘making available on the market’ means the supply of an AI system or a general-purpose AI model for distribution or use on the Union market in the course of a commercial activity, whether in return for payment or free of charge;\n(11) ‘putting into service’ means the supply of an AI system for first use directly to the deployer or for own use in the Union for its intended purpose;\n\n(68) ‘downstream provider’ means a provider of an AI system, including a general-purpose AI system, which integrates an AI model, regardless of whether the AI model is provided by themselves and vertically integrated or provided by another entity based on contractual relations.",
      "original_content": "### Artikel 3: Begriffsbestimmungen\nFür die Zwecke dieser Verordnung bezeichnet der Ausdruck\n[...]\n3. „Anbieter“ eine natürliche oder juristische Person, Behörde, Einrichtung oder sonstige Stelle, die ein KI-System oder ein KI-Modell mit allgemeinem Verwendungszweck entwickelt oder entwickeln lässt und es unter ihrem eigenen Namen oder ihrer Handelsmarke in Verkehr bringt oder das KI-System unter ihrem eigenen Namen oder ihrer Handelsmarke in Betrieb nimmt, sei es entgeltlich oder unentgeltlich;\n4. „Betreiber“ eine natürliche oder juristische Person, Behörde, Einrichtung oder sonstige Stelle, die ein KI-System in eigener Verantwortung verwendet, es sei denn, das KI-System wird im Rahmen einer persönlichen und nicht beruflichen Tätigkeit verwendet;\n5. „Bevollmächtigter“ eine in der Union ansässige oder niedergelassene natürliche oder juristische Person, die vom Anbieter eines KI-Systems oder eines KI-Modells mit allgemeinem Verwendungszweck schriftlich dazu bevollmächtigt wurde und sich damit einverstanden erklärt hat, in seinem Namen die in dieser Verordnung festgelegten Pflichten zu erfüllen bzw. Verfahren durchzuführen;\n6. „Einführer“ eine in der Union ansässige oder niedergelassene natürliche oder juristische Person, die ein KI-System, das den Namen oder die Handelsmarke einer in einem Drittland niedergelassenen natürlichen oder juristischen Person trägt, in Verkehr bringt;\n7. „Händler“ eine natürliche oder juristische Person in der Lieferkette, die ein KI-System auf dem Unionsmarkt bereitstellt, mit Ausnahme des Anbieters oder des Einführers;\n8. „Akteur“ einen Anbieter, Produkthersteller, Betreiber, Bevollmächtigten, Einführer oder Händler;\n9. „Inverkehrbringen“ die erstmalige Bereitstellung eines KI-Systems oder eines KI-Modells mit allgemeinem Verwendungszweck auf dem Unionsmarkt;\n10. „Bereitstellung auf dem Markt“ die entgeltliche oder unentgeltliche Abgabe eines KI-Systems oder eines KI-Modells mit allgemeinem Verwendungszweck zum Vertrieb oder zur Verwendung auf dem Unionsmarkt im Rahmen einer Geschäftstätigkeit;\n11. „Inbetriebnahme“ die Bereitstellung eines KI-Systems in der Union zum Erstgebrauch direkt an den Betreiber oder zum Eigengebrauch entsprechend seiner Zweckbestimmung;\n[...]\n68. „nachgelagerter Anbieter“ einen Anbieter eines KI-Systems, einschließlich eines KI-Systems mit allgemeinem Verwendungszweck, das ein KI-Modell integriert, unabhängig davon, ob das KI-Modell von ihm selbst bereitgestellt und vertikal integriert wird oder von einer anderen Einrichtung auf der Grundlage vertraglicher Beziehungen bereitgestellt wird."
    },
    {
      "chunk_idx": 188,
      "id": "50b2ea2a-b39a-49b3-86b1-4b89a8b9dbf6",
      "title": "Art 3: Z19-Z22, Z26, Z47-Z48  Behördenzuständigkeiten",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 3: Definitions\nFor the purposes of this Regulation, the following definitions apply:\n\n(19) ‘notifying authority’ means the national authority responsible for setting up and carrying out the necessary procedures for the assessment, designation and notification of conformity assessment bodies and for their monitoring;\n(20) ‘conformity assessment’ means the process of demonstrating whether the requirements set out in Chapter III, Section 2 relating to a high-risk AI system have been fulfilled;\n(21) ‘conformity assessment body’ means a body that performs third-party conformity assessment activities, including testing, certification and inspection;\n(22) ‘notified body’ means a conformity assessment body notified in accordance with this Regulation and other relevant Union harmonisation legislation;\n\n(26) ‘market surveillance authority’ means the national authority carrying out the activities and taking the measures pursuant to Regulation (EU) 2019/1020;\n\n(47) ‘AI Office’ means the Commission’s function of contributing to the implementation, monitoring and supervision of AI systems and general-purpose AI models, and AI governance, provided for in Commission Decision of 24 January 2024; references in this Regulation to the AI Office shall be construed as references to the Commission;\n(48) ‘national competent authority’ means a notifying authority or a market surveillance authority; as regards AI systems put into service or used by Union institutions, agencies, offices and bodies, references to national competent authorities or market surveillance authorities in this Regulation shall be construed as references to the European Data Protection Supervisor;",
      "original_content": "### Artikel 3: Begriffsbestimmungen\nFür die Zwecke dieser Verordnung bezeichnet der Ausdruck\n[...]\n19. „notifizierende Behörde“ die nationale Behörde, die für die Einrichtung und Durchführung der erforderlichen Verfahren für die Bewertung, Benennung und Notifizierung von Konformitätsbewertungsstellen und für deren Überwachung zuständig ist;\n20. „Konformitätsbewertung“ ein Verfahren mit dem bewertet wird, ob die in Titel III Abschnitt 2 festgelegten Anforderungen an ein Hochrisiko-KI-System erfüllt wurden;\n21. „Konformitätsbewertungsstelle“ eine Stelle, die Konformitätsbewertungstätigkeiten einschließlich Prüfungen, Zertifizierungen und Inspektionen durchführt und dabei als Dritte auftritt;\n22. „notifizierte Stelle“ eine Konformitätsbewertungsstelle, die gemäß dieser Verordnung und den anderen einschlägigen Harmonisierungsrechtsvorschriften der Union notifiziert wurde;\n[...]\n26. „Marktüberwachungsbehörde“ die nationale Behörde, die die Tätigkeiten durchführt und die Maßnahmen ergreift, die in der Verordnung (EU) 2019/1020 vorgesehen sind;\n[...]\n47. „Büro für Künstliche Intelligenz“ die Aufgabe der Kommission, zur Umsetzung, Beobachtung und Überwachung von KI-Systemen und KI-Modellen mit allgemeinem Verwendungszweck und zu der im Beschluss der Kommission vom 24. Januar 2024 vorgesehenen KI-Governance beizutragen; Bezugnahmen in dieser Verordnung auf das Büro für Künstliche Intelligenz gelten als Bezugnahmen auf die Kommission;\n48. „zuständige nationale Behörde“ eine notifizierende Behörde oder eine Marktüberwachungsbehörde; in Bezug auf KI-Systeme, die von Organen, Einrichtungen und sonstigen Stellen der Union in Betrieb genommen oder verwendet werden, sind Bezugnahmen auf die zuständigen nationalen Behörden oder Marktüberwachungsbehörden in dieser Verordnung als Bezugnahmen auf den Europäischen Datenschutzbeauftragten auszulegen;"
    },
    {
      "chunk_idx": 189,
      "id": "53f9adb8-85c5-457c-9e77-8793acc99047",
      "title": "Art 3: Z27-Z28 EU-Normen",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 3: Definitions\nFor the purposes of this Regulation, the following definitions apply:\n\n(27) ‘harmonised standard’ means a harmonised standard as defined in Article 2(1), point (c), of Regulation (EU) No 1025/2012;\n(28) ‘common specification’ means a set of technical specifications as defined in Article 2, point (4) of Regulation (EU) No 1025/2012, providing means to comply with certain requirements established under this Regulation;",
      "original_content": "### Artikel 3: Begriffsbestimmungen\nFür die Zwecke dieser Verordnung bezeichnet der Ausdruck\n[...]\n27. „harmonisierte Norm“ bezeichnet eine harmonisierte Norm im Sinne des Artikels 2 Absatz 1 Buchstabe c der Verordnung (EU) Nr. 1025/2012;\n28. „gemeinsame Spezifikation“ eine Reihe technischer Spezifikationen im Sinne des Artikels 2 Nummer 4 der Verordnung (EU) Nr. 1025/2012, deren Befolgung es ermöglicht, bestimmte Anforderungen der vorliegenden Verordnung zu erfüllen;"
    },
    {
      "chunk_idx": 190,
      "id": "1d78d2f5-6bcc-41dc-9df9-80342afe9a47",
      "title": "Art3: Z29-Z33 Training von KI",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 3: Definitions\nFor the purposes of this Regulation, the following definitions apply:\n\n(29) ‘training data’ means data used for training an AI system through fitting its learnable parameters;\n(30) ‘validation data’ means data used for providing an evaluation of the trained AI system and for tuning its non-learnable parameters and its learning process in order, inter alia, to prevent underfitting or overfitting;\n(31) ‘validation data set’ means a separate data set or part of the training data set, either as a fixed or variable split;\n(32) ‘testing data’ means data used for providing an independent evaluation of the AI system in order to confirm the expected performance of that system before its placing on the market or putting into service;\n(33) ‘input data’ means data provided to or directly acquired by an AI system on the basis of which the system produces an output;",
      "original_content": "### Artikel 3: Begriffsbestimmungen\nFür die Zwecke dieser Verordnung bezeichnet der Ausdruck\n[...]\n29. „Trainingsdaten“ Daten, die zum Trainieren eines KI-Systems verwendet werden, wobei dessen lernbare Parameter angepasst werden;\n30. „Validierungsdaten“ Daten, die zur Evaluation des trainierten KI-Systems und zur Einstellung seiner nicht erlernbaren Parameter und seines Lernprozesses verwendet werden, um unter anderem eine Unter- oder Überanpassung zu vermeiden;\n31. „Validierungsdatensatz“ einen separaten Datensatz oder einen Teil des Trainingsdatensatzes mit fester oder variabler Aufteilung;\n32. „Testdaten“ Daten, die für eine unabhängige Bewertung des KI-Systems verwendet werden, um die erwartete Leistung dieses Systems vor dessen Inverkehrbringen oder Inbetriebnahme zu bestätigen;\n33. „Eingabedaten“ die in ein KI-System eingespeisten oder von diesem direkt erfassten Daten, auf deren Grundlage das System eine Ausgabe hervorbringt;"
    },
    {
      "chunk_idx": 191,
      "id": "87c3670b-a636-4431-aadd-9cfe780035b0",
      "title": "Art 3: Z34-Z36, Z39-Z44 Biometrie",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 3: Definitions\nFor the purposes of this Regulation, the following definitions apply:\n\n(34) ‘biometric data’ means personal data resulting from specific technical processing relating to the physical, physiological or behavioural characteristics of a natural person, such as facial images or dactyloscopic data;\n(35) ‘biometric identification’ means the automated recognition of physical, physiological, behavioural, or psychological human features for the purpose of establishing the identity of a natural person by comparing biometric data of that individual to biometric data of individuals stored in a database;\n(36) ‘biometric verification’ means the automated, one-to-one verification, including authentication, of the identity of natural persons by comparing their biometric data to previously provided biometric data;\n\n(39) ‘emotion recognition system’ means an AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data;\n(40) ‘biometric categorisation system’ means an AI system for the purpose of assigning natural persons to specific categories on the basis of their biometric data, unless it is ancillary to another commercial service and strictly necessary for objective technical reasons;\n(41) ‘remote biometric identification system’ means an AI system for the purpose of identifying natural persons, without their active involvement, typically at a distance through the comparison of a person’s biometric data with the biometric data contained in a reference database;\n(42) ‘real-time remote biometric identification system’ means a remote biometric identification system, whereby the capturing of biometric data, the comparison and the identification all occur without a significant delay, comprising not only instant identification, but also limited short delays in order to avoid circumvention;\n(43) ‘post-remote biometric identification system’ means a remote biometric identification system other than a real-time remote biometric identification system;\n(44) ‘publicly accessible space’ means any publicly or privately owned physical place accessible to an undetermined number of natural persons, regardless of whether certain conditions for access may apply, and regardless of the potential capacity restrictions;",
      "original_content": "### Artikel 3: Begriffsbestimmungen\nFür die Zwecke dieser Verordnung bezeichnet der Ausdruck\n[...]\n34. „biometrische Daten“ mit speziellen technischen Verfahren gewonnene personenbezogene Daten zu den physischen, physiologischen oder verhaltenstypischen Merkmalen einer natürlichen Person, wie etwa Gesichtsbilder oder daktyloskopische Daten;\n35. „biometrische Identifizierung“ die automatisierte Erkennung physischer, physiologischer, verhaltensbezogener oder psychologischer menschlicher Merkmale zum Zwecke der Feststellung der Identität einer natürlichen Person durch den Vergleich biometrischer Daten dieser Person mit biometrischen Daten von Personen, die in einer Datenbank gespeichert sind;\n36. „biometrische Verifizierung“ die automatisierte Eins-zu-eins-Verifizierung, einschließlich Authentifizierung, der Identität natürlicher Personen durch den Vergleich ihrer biometrischen Daten mit zuvor bereitgestellten biometrischen Daten;\n[...]\n39. „Emotionserkennungssystem“ ein KI-System, das dem Zweck dient, Emotionen oder Absichten natürlicher Personen auf der Grundlage ihrer biometrischen Daten festzustellen oder daraus abzuleiten;\n40. „System zur biometrischen Kategorisierung“ ein KI-System, das dem Zweck dient, natürliche Personen auf der Grundlage ihrer biometrischen Daten bestimmten Kategorien zuzuordnen, sofern es sich um eine Nebenfunktion eines anderen kommerziellen Dienstes handelt und aus objektiven technischen Gründen unbedingt erforderlich ist;\n41. „biometrisches Fernidentifizierungssystem“ ein KI-System, das dem Zweck dient, natürliche Personen ohne ihre aktive Einbeziehung und in der Regel aus der Ferne durch Abgleich der biometrischen Daten einer Person mit den in einer Referenzdatenbank gespeicherten biometrischen Daten zu identifizieren;\n42. „biometrisches Echtzeit-Fernidentifizierungssystem“ ein biometrisches Fernidentifizierungssystem, bei dem die Erfassung biometrischer Daten, der Abgleich und die Identifizierung ohne erhebliche Verzögerung erfolgen, und das zur Vermeidung einer Umgehung der Vorschriften nicht nur die sofortige Identifizierung, sondern auch eine Identifizierung mit begrenzten kurzen Verzögerungen umfasst;\n43. „System zur nachträglichen biometrischen Fernidentifizierung“ ein biometrisches Fernidentifizierungssystem, das kein biometrisches Echtzeit-Fernidentifizierungssystem ist;\n44. „öffentlich zugänglicher Raum“ einen einer unbestimmten Anzahl natürlicher Personen zugänglichen physischen Ort in privatem oder öffentlichem Eigentum, unabhängig davon, ob bestimmte Bedingungen für den Zugang gelten, und unabhängig von möglichen Kapazitätsbeschränkungen;"
    },
    {
      "chunk_idx": 192,
      "id": "efa08dbd-87ac-42d1-931e-7a2e3b7fea9b",
      "title": "Art 3: Z37-Z38, Z50-Z52 personenbezogene bzw sensible Daten",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 3: Definitions\nFor the purposes of this Regulation, the following definitions apply:\n\n(37) ‘special categories of personal data’ means the categories of personal data referred to in Article 9(1) of Regulation (EU) 2016/679, Article 10 of Directive (EU) 2016/680 and Article 10(1) of Regulation (EU) 2018/1725;\n(38) ‘sensitive operational data’ means operational data related to activities of prevention, detection, investigation or prosecution of criminal offences, the disclosure of which could jeopardise the integrity of criminal proceedings;\n\n(50) ‘personal data’ means personal data as defined in Article 4, point (1), of Regulation (EU) 2016/679;\n(51) ‘non-personal data’ means data other than personal data as defined in Article 4, point (1), of Regulation (EU) 2016/679;\n(52) ‘profiling’ means profiling as defined in Article 4, point (4), of Regulation (EU) 2016/679;",
      "original_content": "### Artikel 3: Begriffsbestimmungen\nFür die Zwecke dieser Verordnung bezeichnet der Ausdruck\n[...]\n37. „besondere Kategorien personenbezogener Daten“ die in Artikel 9 Absatz 1der Verordnung (EU) 2016/679, Artikel 10 der Richtlinie (EU) 2016/680 und Artikel 10 Absatz 1 der Verordnung (EU) 2018/1725 aufgeführten Kategorien personenbezogener Daten;\n38. „sensible operative Daten“ operative Daten im Zusammenhang mit Tätigkeiten zur Verhütung, Aufdeckung, Untersuchung oder Verfolgung von Straftaten, deren Offenlegung die Integrität von Strafverfahren gefährden könnte;\n[...]\n50. „personenbezogene Daten“ personenbezogene Daten im Sinne von Artikel 4 Nummer 1 der Verordnung (EU) 2016/679;\n51. „nicht personenbezogene Daten“ Daten, die keine personenbezogenen Daten im Sinne von Artikel 4 Nummer 1 der Verordnung (EU) 2016/679 sind;\n52. „Profiling“ das Profiling im Sinne von Artikel 4 Nummer 4 der Verordnung (EU) 2016/679;"
    },
    {
      "chunk_idx": 193,
      "id": "833b170d-faf1-4500-ba49-cbb176876838",
      "title": "Art 3: Z61 Verstoß",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 3: Definitions\nFor the purposes of this Regulation, the following definitions apply:\n\n(61) ‘widespread infringement’ means any act or omission contrary to Union law protecting the interest of individuals, which:\n(a) has harmed or is likely to harm the collective interests of individuals residing in at least two Member States other than the Member State in which:\n(i) the act or omission originated or took place;\n(ii) the provider concerned, or, where applicable, its authorised representative is located or established; or\n(iii) the deployer is established, when the infringement is committed by the deployer;\n(b) has caused, causes or is likely to cause harm to the collective interests of individuals and has common features, including the same unlawful practice or the same interest being infringed, and is occurring concurrently, committed by the same operator, in at least three Member States;",
      "original_content": "### Artikel 3: Begriffsbestimmungen\nFür die Zwecke dieser Verordnung bezeichnet der Ausdruck\n[...]\n61. „weitverbreiteter Verstoß“ jede Handlung oder Unterlassung, die gegen das Unionsrecht verstößt, das die Interessen von Einzelpersonen schützt, und die\na) die kollektiven Interessen von Einzelpersonen in mindestens zwei anderen Mitgliedstaaten als dem Mitgliedstaat schädigt oder zu schädigen droht, in dem\ni) die Handlung oder die Unterlassung ihren Ursprung hatte oder stattfand,\nii) der betreffende Anbieter oder gegebenenfalls sein Bevollmächtigter sich befindet oder niedergelassen ist oder\niii) der Betreiber niedergelassen ist, sofern der Verstoß vom Betreiber begangen wird,\nb) die kollektiven Interessen von Einzelpersonen geschädigt hat, schädigt oder schädigen könnte und allgemeine Merkmale aufweist, einschließlich derselben rechtswidrigen Praxis oder desselben verletzten Interesses, und gleichzeitig auftritt und von demselben Akteur in mindestens drei Mitgliedstaaten begangen wird;"
    },
    {
      "chunk_idx": 194,
      "id": "8a0af483-9e07-445f-8fa1-77555b40c583",
      "title": "Art 3: Z45-Z46 Strafverfolgung",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 3: Definitions\nFor the purposes of this Regulation, the following definitions apply:\n\n(45) ‘law enforcement authority’ means:\n(a) any public authority competent for the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security; or\n(b) any other body or entity entrusted by Member State law to exercise public authority and public powers for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security;\n(46) ‘law enforcement’ means activities carried out by law enforcement authorities or on their behalf for the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, including safeguarding against and preventing threats to public security;",
      "original_content": "### Artikel 3: Begriffsbestimmungen\nFür die Zwecke dieser Verordnung bezeichnet der Ausdruck\n[...]\n45. „Strafverfolgungsbehörde“\na) eine Behörde, die für die Verhütung, Ermittlung, Aufdeckung oder Verfolgung von Straftaten oder die Strafvollstreckung, einschließlich des Schutzes vor und der Abwehr von Gefahren für die öffentliche Sicherheit, zuständig ist, oder\nb) eine andere Stelle oder Einrichtung, der durch nationales Recht die Ausübung öffentlicher Gewalt und hoheitlicher Befugnisse zur Verhütung, Ermittlung, Aufdeckung oder Verfolgung von Straftaten oder zur Strafvollstreckung, einschließlich des Schutzes vor und der Abwehr von Gefahren für die öffentliche Sicherheit, übertragen wurde;\n46. „Strafverfolgung“ Tätigkeiten der Strafverfolgungsbehörden oder in deren Auftrag zur Verhütung, Ermittlung, Aufdeckung oder Verfolgung von Straftaten oder zur Strafvollstreckung, einschließlich des Schutzes vor und der Abwehr von Gefahren für die öffentliche Sicherheit;"
    },
    {
      "chunk_idx": 195,
      "id": "9bacce57-261f-4716-adfa-6f5d904a65b4",
      "title": "Art 3: Z53-Z55, Z57-Z59 KI-Reallabor",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 3: Definitions\nFor the purposes of this Regulation, the following definitions apply:\n\n(53) ‘real-world testing plan’ means a document that describes the objectives, methodology, geographical, population and temporal scope, monitoring, organisation and conduct of testing in real-world conditions;\n(54) ‘sandbox plan’ means a document agreed between the participating provider and the competent authority describing the objectives, conditions, timeframe, methodology and requirements for the activities carried out within the sandbox;\n(55) ‘AI regulatory sandbox’ means a controlled framework set up by a competent authority which offers providers or prospective providers of AI systems the possibility to develop, train, validate and test, where appropriate in real-world conditions, an innovative AI system, pursuant to a sandbox plan for a limited time under regulatory supervision;\n\n(57) ‘testing in real-world conditions’ means the temporary testing of an AI system for its intended purpose in real-world conditions outside a laboratory or otherwise simulated environment, with a view to gathering reliable and robust data and to assessing and verifying the conformity of the AI system with the requirements of this Regulation and it does not qualify as placing the AI system on the market or putting it into service within the meaning of this Regulation, provided that all the conditions laid down in Article 57 or 60 are fulfilled;\n(58) ‘subject’, for the purpose of real-world testing, means a natural person who participates in testing in real-world conditions;\n(59) ‘informed consent’ means a subject’s freely given, specific, unambiguous and voluntary expression of his or her willingness to participate in a particular testing in real-world conditions, after having been informed of all aspects of the testing that are relevant to the subject’s decision to participate;",
      "original_content": "### Artikel 3: Begriffsbestimmungen\nFür die Zwecke dieser Verordnung bezeichnet der Ausdruck\n[...]\n53. „Plan für einen Test unter Realbedingungen“ ein Dokument, in dem die Ziele, die Methodik, der geografische, bevölkerungsbezogene und zeitliche Umfang, die Überwachung, die Organisation und die Durchführung eines Tests unter Realbedingungen beschrieben werden;\n54. „Plan für das Reallabor“ ein zwischen dem teilnehmenden Anbieter und der zuständigen Behörde vereinbartes Dokument, in dem die Ziele, die Bedingungen, der Zeitrahmen, die Methodik und die Anforderungen für die im Reallabor durchgeführten Tätigkeiten beschrieben werden;\n55. „KI-Reallabor“ einen kontrollierten Rahmen, der von einer zuständigen Behörde geschaffen wird und den Anbieter oder zukünftige Anbieter von KI-Systemen nach einem Plan für das Reallabor einen begrenzten Zeitraum und unter regulatorischer Aufsicht nutzen können, um ein innovatives KI-System zu entwickeln, zu trainieren, zu validieren und — gegebenenfalls unter Realbedingungen — zu testen.\n[...]\n57. „Test unter Realbedingungen“ den befristeten Test eines KI-Systems auf seine Zweckbestimmung, der unter Realbedingungen außerhalb eines Labors oder einer anderweitig simulierten Umgebung erfolgt, um zuverlässige und belastbare Daten zu erheben und die Konformität des KI-Systems mit den Anforderungen der vorliegenden Verordnung zu bewerten und zu überprüfen, wobei dieser Test nicht als Inverkehrbringen oder Inbetriebnahme des KI-Systems im Sinne dieser Verordnung gilt, sofern alle Bedingungen nach Artikel 57 oder Artikel 60 erfüllt sind;\n58. „Testteilnehmer“ für die Zwecke eines Tests unter Realbedingungen eine natürliche Person, die an dem Test unter Realbedingungen teilnimmt;\n59. „informierte Einwilligung“ eine aus freien Stücken erfolgende, spezifische, eindeutige und freiwillige Erklärung der Bereitschaft, an einem bestimmten Test unter Realbedingungen teilzunehmen, durch einen Testteilnehmer, nachdem dieser über alle Aspekte des Tests, die für die Entscheidungsfindung des Testteilnehmers bezüglich der Teilnahme relevant sind, aufgeklärt wurde;"
    },
    {
      "chunk_idx": 196,
      "id": "6f3bcb0e-023a-4fae-b8ae-aff6dfd8c929",
      "title": "Art 3: Z56 KI-Kompetenz",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 3: Definitions\nFor the purposes of this Regulation, the following definitions apply:\n\n(56) ‘AI literacy’ means skills, knowledge and understanding that allow providers, deployers and affected persons, taking into account their respective rights and obligations in the context of this Regulation, to make an informed deployment of AI systems, as well as to gain awareness about the opportunities and risks of AI and possible harm it can cause;",
      "original_content": "### Artikel 3: Begriffsbestimmungen\nFür die Zwecke dieser Verordnung bezeichnet der Ausdruck\n[...]\n56. „KI-Kompetenz“ die Fähigkeiten, die Kenntnisse und das Verständnis, die es Anbietern, Betreibern und Betroffenen unter Berücksichtigung ihrer jeweiligen Rechte und Pflichten im Rahmen dieser Verordnung ermöglichen, KI-Systeme sachkundig einzusetzen sowie sich der Chancen und Risiken von KI und möglicher Schäden, die sie verursachen kann, bewusst zu werden."
    },
    {
      "chunk_idx": 197,
      "id": "71a61842-dac1-4a64-b31a-cf6c68220d51",
      "title": "Art 3: Z60 Deepfake",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 3: Definitions\nFor the purposes of this Regulation, the following definitions apply:\n\n(60) ‘deep fake’ means AI-generated or manipulated image, audio or video content that resembles existing persons, objects, places, entities or events and would falsely appear to a person to be authentic or truthful;",
      "original_content": "### Artikel 3: Begriffsbestimmungen\nFür die Zwecke dieser Verordnung bezeichnet der Ausdruck\n[...]\n60. „Deepfake“ einen durch KI erzeugten oder manipulierten Bild-, Ton- oder Videoinhalt, der wirklichen Personen, Gegenständen, Orten, Einrichtungen oder Ereignissen ähnelt und einer Person fälschlicherweise als echt oder wahrheitsgemäß erscheinen würde;"
    },
    {
      "chunk_idx": 198,
      "id": "55614e42-478f-480a-a5c3-2ecb6c402441",
      "title": "Art 3: Z63-67 KI-Systeme und KI-Modelle mit allg. Verwendungszweck",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 3: Definitions\nFor the purposes of this Regulation, the following definitions apply:\n\n(63) ‘general-purpose AI model’ means an AI model, including where such an AI model is trained with a large amount of data using self-supervision at scale, that displays significant generality and is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications, except AI models that are used for research, development or prototyping activities before they are placed on the market;\n(64) ‘high-impact capabilities’ means capabilities that match or exceed the capabilities recorded in the most advanced general-purpose AI models;\n(65) ‘systemic risk’ means a risk that is specific to the high-impact capabilities of general-purpose AI models, having a significant impact on the Union market due to their reach, or due to actual or reasonably foreseeable negative effects on public health, safety, public security, fundamental rights, or the society as a whole, that can be propagated at scale across the value chain;\n(66) ‘general-purpose AI system’ means an AI system which is based on a general-purpose AI model and which has the capability to serve a variety of purposes, both for direct use as well as for integration in other AI systems;\n(67) ‘floating-point operation’ means any mathematical operation or assignment involving floating-point numbers, which are a subset of the real numbers typically represented on computers by an integer of fixed precision scaled by an integer exponent of a fixed base;",
      "original_content": "### Artikel 3: Begriffsbestimmungen\nFür die Zwecke dieser Verordnung bezeichnet der Ausdruck\n[...]\n63. „KI-Modell mit allgemeinem Verwendungszweck“ ein KI-Modell — einschließlich der Fälle, in denen ein solches KI-Modell mit einer großen Datenmenge unter umfassender Selbstüberwachung trainiert wird —, das eine erhebliche allgemeine Verwendbarkeit aufweist und in der Lage ist, unabhängig von der Art und Weise seines Inverkehrbringens ein breites Spektrum unterschiedlicher Aufgaben kompetent zu erfüllen, und das in eine Vielzahl nachgelagerter Systeme oder Anwendungen integriert werden kann, ausgenommen KI-Modelle, die vor ihrem Inverkehrbringen für Forschungs- und Entwicklungstätigkeiten oder die Konzipierung von Prototypen eingesetzt werden;\n64. „Fähigkeiten mit hoher Wirkkraft“ bezeichnet Fähigkeiten, die den bei den fortschrittlichsten KI-Modellen mit allgemeinem Verwendungszweck festgestellten Fähigkeiten entsprechen oder diese übersteigen;\n65. „systemisches Risiko“ ein Risiko, das für die Fähigkeiten mit hoher Wirkkraft von KI-Modellen mit allgemeinem Verwendungszweck spezifisch ist und aufgrund deren Reichweite oder aufgrund tatsächlicher oder vernünftigerweise vorhersehbarer negativer Folgen für die öffentliche Gesundheit, die Sicherheit, die öffentliche Sicherheit, die Grundrechte oder die Gesellschaft insgesamt erhebliche Auswirkungen auf den Unionsmarkt hat, die sich in großem Umfang über die gesamte Wertschöpfungskette hinweg verbreiten können;\n66. „KI-System mit allgemeinem Verwendungszweck“ ein KI-System, das auf einem KI-Modell mit allgemeinem Verwendungszweck beruht und in der Lage ist, einer Vielzahl von Zwecken sowohl für die direkte Verwendung als auch für die Integration in andere KI-Systeme zu dienen;\n67. „Gleitkommaoperation“ jede Rechenoperation oder jede Zuweisung mit Gleitkommazahlen, bei denen es sich um eine Teilmenge der reellen Zahlen handelt, die auf Computern typischerweise durch das Produkt aus einer ganzen Zahl mit fester Genauigkeit und einer festen Basis mit ganzzahligem Exponenten dargestellt wird;"
    },
    {
      "chunk_idx": 199,
      "id": "09b711a2-571c-4a78-9b84-60675ea83b5e",
      "title": "Art 4",
      "relevantChunksIds": [
        "21ada015-1d50-4ddb-97d5-33b540e849cb",
        "f9b21785-6f87-4438-8810-147304eb4d7e",
        "550c995f-0a8e-4714-b73e-9418fb982eed",
        "ba7d5b49-ef13-4d72-8b07-4524ff3dc1e6",
        "b757b57f-4015-401c-8003-852ba5aaefc0",
        "ecfd2b0e-fbbd-4551-8c94-4eaef2c02ec5",
        "1b027697-3d10-4487-9e5c-0a9be1158e3f",
        "5a9f9935-184c-45e2-8864-b91d0b030612",
        "0018756c-5b2f-46b7-abb9-8a5e3ffc3054",
        "26497c14-895e-4da7-bf4a-018625e3e739",
        "14294dba-dcf9-475a-8f96-d58137d64c43",
        "f3529f8b-f673-4bc4-aa17-3022e4814d31"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 4: AI literacy\nProviders and deployers of AI systems shall take measures to ensure, to their best extent, a sufficient level of AI literacy of their staff and other persons dealing with the operation and use of AI systems on their behalf, taking into account their technical knowledge, experience, education and training and the context the AI systems are to be used in, and considering the persons or groups of persons on whom the AI systems are to be used.",
      "original_content": "### Artikel 4: KI-Kompetenz\nDie Anbieter und Betreiber von KI-Systemen ergreifen Maßnahmen, um nach besten Kräften sicherzustellen, dass ihr Personal und andere Personen, die in ihrem Auftrag mit dem Betrieb und der Nutzung von KI-Systemen befasst sind, über ein ausreichendes Maß an KI-Kompetenz verfügen, wobei ihre technischen Kenntnisse, ihre Erfahrung, ihre Ausbildung und Schulung und der Kontext, in dem die KI-Systeme eingesetzt werden sollen, sowie die Personen oder Personengruppen, bei denen die KI-Systeme eingesetzt werden sollen, zu berücksichtigen sind."
    },
    {
      "chunk_idx": 200,
      "id": "283d70b2-97f7-4252-9190-378e90b707bd",
      "title": "Art 5: Abs 1 lit a-c (i - ii) Manipulation, social scoring",
      "relevantChunksIds": [
        "c8096ef4-8680-44ed-b282-f02c6434e936",
        "479fd058-034c-4a38-b806-38bf6965c0b7",
        "32ec33e4-0b93-498d-bf92-8f561ac77fbd",
        "d6b1d3b4-a6fc-460d-adac-3810b3fd4b9a",
        "a9bf3bf8-40da-4c63-a908-e12625e1e946",
        "032907d6-0290-44ab-bee5-be05248e3ae2",
        "d9cb5958-fede-437d-a0f6-ad55c6d6db50"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "# CHAPTER II: PROHIBITED AI PRACTICES\n### Article 5: Prohibited AI practices\n1. The following AI practices shall be prohibited:\n(a) the placing on the market, the putting into service or the use of an AI system that deploys subliminal techniques beyond a person’s consciousness or purposefully manipulative or deceptive techniques, with the objective, or the effect of materially distorting the behaviour of a person or a group of persons by appreciably impairing their ability to make an informed decision, thereby causing them to take a decision that they would not have otherwise taken in a manner that causes or is reasonably likely to cause that person, another person or group of persons significant harm;\n(b) the placing on the market, the putting into service or the use of an AI system that exploits any of the vulnerabilities of a natural person or a specific group of persons due to their age, disability or a specific social or economic situation, with the objective, or the effect, of materially distorting the behaviour of that person or a person belonging to that group in a manner that causes or is reasonably likely to cause that person or another person significant harm;\n(c) the placing on the market, the putting into service or the use of AI systems for the evaluation or classification of natural persons or groups of persons over a certain period of time based on their social behaviour or known, inferred or predicted personal or personality characteristics, with the social score leading to either or both of the following:\n(i) detrimental or unfavourable treatment of certain natural persons or groups of persons in social contexts that are unrelated to the contexts in which the data was originally generated or collected;\n(ii) detrimental or unfavourable treatment of certain natural persons or groups of persons that is unjustified or disproportionate to their social behaviour or its gravity;",
      "original_content": "# KAPITEL II: VERBOTENE PRAKTIKEN IM KI-BEREICH\n### Artikel 5: Verbotene Praktiken im KI-Bereich\n(1) Folgende Praktiken im KI-Bereich sind verboten:\na) das Inverkehrbringen, die Inbetriebnahme oder die Verwendung eines KI-Systems, das Techniken der unterschwelligen Beeinflussung außerhalb des Bewusstseins einer Person oder absichtlich manipulative oder täuschende Techniken mit dem Ziel oder der Wirkung einsetzt, das Verhalten einer Person oder einer Gruppe von Personen wesentlich zu verändern, indem ihre Fähigkeit, eine fundierte Entscheidung zu treffen, deutlich beeinträchtigt wird, wodurch sie veranlasst wird, eine Entscheidung zu treffen, die sie andernfalls nicht getroffen hätte, und zwar in einer Weise, die dieser Person, einer anderen Person oder einer Gruppe von Personen erheblichen Schaden zufügt oder mit hinreichender Wahrscheinlichkeit zufügen wird.\nb) das Inverkehrbringen, die Inbetriebnahme oder die Verwendung eines KI-Systems, das eine Vulnerabilität oder Schutzbedürftigkeit einer natürlichen Person oder einer bestimmten Gruppe von Personen aufgrund ihres Alters, einer Behinderung oder einer bestimmten sozialen oder wirtschaftlichen Situation mit dem Ziel oder der Wirkung ausnutzt, das Verhalten dieser Person oder einer dieser Gruppe angehörenden Person in einer Weise wesentlich zu verändern, die dieser Person oder einer anderen Person erheblichen Schaden zufügt oder mit hinreichender Wahrscheinlichkeit zufügen wird;\nc) das Inverkehrbringen, die Inbetriebnahme oder die Verwendung von KI-Systemen zur Bewertung oder Einstufung von natürlichen Personen oder Gruppen von Personen über einen bestimmten Zeitraum auf der Grundlage ihres sozialen Verhaltens oder bekannter, abgeleiteter oder vorhergesagter persönlicher Eigenschaften oder Persönlichkeitsmerkmale, wobei die soziale Bewertung zu einem oder beiden der folgenden Ergebnisse führt:\ni) Schlechterstellung oder Benachteiligung bestimmter natürlicher Personen oder Gruppen von Personen in sozialen Zusammenhängen, die in keinem Zusammenhang zu den Umständen stehen, unter denen die Daten ursprünglich erzeugt oder erhoben wurden;\nii) Schlechterstellung oder Benachteiligung bestimmter natürlicher Personen oder Gruppen von Personen in einer Weise, die im Hinblick auf ihr soziales Verhalten oder dessen Tragweite ungerechtfertigt oder unverhältnismäßig ist;"
    },
    {
      "chunk_idx": 201,
      "id": "e87ad418-9936-47c0-b4ec-99f31c41a0a8",
      "title": "Art 5: Abs 1 lit d predictive policing",
      "relevantChunksIds": [
        "c8096ef4-8680-44ed-b282-f02c6434e936",
        "479fd058-034c-4a38-b806-38bf6965c0b7",
        "32ec33e4-0b93-498d-bf92-8f561ac77fbd",
        "d6b1d3b4-a6fc-460d-adac-3810b3fd4b9a",
        "a9bf3bf8-40da-4c63-a908-e12625e1e946",
        "032907d6-0290-44ab-bee5-be05248e3ae2",
        "d9cb5958-fede-437d-a0f6-ad55c6d6db50"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "# CHAPTER II: PROHIBITED AI PRACTICES\n### Article 5: Prohibited AI practices\n\n(d) the placing on the market, the putting into service for this specific purpose, or the use of an AI system for making risk assessments of natural persons in order to assess or predict the risk of a natural person committing a criminal offence, based solely on the profiling of a natural person or on assessing their personality traits and characteristics; this prohibition shall not apply to AI systems used to support the human assessment of the involvement of a person in a criminal activity, which is already based on objective and verifiable facts directly linked to a criminal activity;",
      "original_content": "# KAPITEL II: VERBOTENE PRAKTIKEN IM KI-BEREICH\n### Artikel 5: Verbotene Praktiken im KI-Bereich\n[...]\nd) das Inverkehrbringen, die Inbetriebnahme für diesen spezifischen Zweck oder die Verwendung eines KI-Systems zur Durchführung von Risikobewertungen in Bezug auf natürliche Personen, um das Risiko, dass eine natürliche Person eine Straftat begeht, ausschließlich auf der Grundlage des Profiling einer natürlichen Person oder der Bewertung ihrer persönlichen Merkmale und Eigenschaften zu bewerten oder vorherzusagen; dieses Verbot gilt nicht für KI-Systeme, die dazu verwendet werden, die durch Menschen durchgeführte Bewertung der Beteiligung einer Person an einer kriminellen Aktivität, die sich bereits auf objektive und überprüfbare Tatsachen stützt, die in unmittelbarem Zusammenhang mit einer kriminellen Aktivität stehen, zu unterstützen;"
    },
    {
      "chunk_idx": 202,
      "id": "24b748de-eebc-404f-8b36-d12eed0a6660",
      "title": "Art 5: Abs 1 lit e Datenbanken zur Gesichtserkennung",
      "relevantChunksIds": [
        "c8096ef4-8680-44ed-b282-f02c6434e936",
        "479fd058-034c-4a38-b806-38bf6965c0b7",
        "32ec33e4-0b93-498d-bf92-8f561ac77fbd",
        "d6b1d3b4-a6fc-460d-adac-3810b3fd4b9a",
        "a9bf3bf8-40da-4c63-a908-e12625e1e946",
        "032907d6-0290-44ab-bee5-be05248e3ae2",
        "d9cb5958-fede-437d-a0f6-ad55c6d6db50"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 5: Prohibited AI practices\n\n(e) the placing on the market, the putting into service for this specific purpose, or the use of AI systems that create or expand facial recognition databases through the untargeted scraping of facial images from the internet or CCTV footage;",
      "original_content": "### Artikel 5: Verbotene Praktiken im KI-Bereich\n[...]\ne) das Inverkehrbringen, die Inbetriebnahme für diesen spezifischen Zweck oder die Verwendung von KI-Systemen, die Datenbanken zur Gesichtserkennung durch das ungezielte Auslesen von Gesichtsbildern aus dem Internet oder von Überwachungsaufnahmen erstellen oder erweitern;"
    },
    {
      "chunk_idx": 203,
      "id": "85555ab3-0021-4e99-96f7-b8d2182b43f7",
      "title": "Art 5: Abs 1 lit f Emotionen in Arbeit und Bildung",
      "relevantChunksIds": [
        "c8096ef4-8680-44ed-b282-f02c6434e936",
        "479fd058-034c-4a38-b806-38bf6965c0b7",
        "32ec33e4-0b93-498d-bf92-8f561ac77fbd",
        "d6b1d3b4-a6fc-460d-adac-3810b3fd4b9a",
        "a9bf3bf8-40da-4c63-a908-e12625e1e946",
        "032907d6-0290-44ab-bee5-be05248e3ae2",
        "d9cb5958-fede-437d-a0f6-ad55c6d6db50"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 5: Prohibited AI practices\n\n(f) the placing on the market, the putting into service for this specific purpose, or the use of AI systems to infer emotions of a natural person in the areas of workplace and education institutions, except where the use of the AI system is intended to be put in place or into the market for medical or safety reasons;",
      "original_content": "### Artikel 5: Verbotene Praktiken im KI-Bereich\n[...]\nf) das Inverkehrbringen, die Inbetriebnahme für diesen spezifischen Zweck oder die Verwendung von KI-Systemen zur Ableitung von Emotionen einer natürlichen Person am Arbeitsplatz und in Bildungseinrichtungen, es sei denn, die Verwendung des KI-Systems soll aus medizinischen Gründen oder Sicherheitsgründen eingeführt oder auf den Markt gebracht werden;"
    },
    {
      "chunk_idx": 204,
      "id": "a6e85038-89e8-4790-9457-cb624ca42c00",
      "title": "Art 5: Abs 1 lit g-h (i - iii) Biometrie",
      "relevantChunksIds": [
        "c8096ef4-8680-44ed-b282-f02c6434e936",
        "479fd058-034c-4a38-b806-38bf6965c0b7",
        "32ec33e4-0b93-498d-bf92-8f561ac77fbd",
        "d6b1d3b4-a6fc-460d-adac-3810b3fd4b9a",
        "a9bf3bf8-40da-4c63-a908-e12625e1e946",
        "032907d6-0290-44ab-bee5-be05248e3ae2",
        "d9cb5958-fede-437d-a0f6-ad55c6d6db50"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 5: Prohibited AI practices\n\n(g) the placing on the market, the putting into service for this specific purpose, or the use of biometric categorisation systems that categorise individually natural persons based on their biometric data to deduce or infer their race, political opinions, trade union membership, religious or philosophical beliefs, sex life or sexual orientation; this prohibition does not cover any labelling or filtering of lawfully acquired biometric datasets, such as images, based on biometric data or categorizing of biometric data in the area of law enforcement;\n(h) the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement, unless and in so far as such use is strictly necessary for one of the following objectives:\n\n\n(ii) the prevention of a specific, substantial and imminent threat to the life or physical safety of natural persons or a genuine and present or genuine and foreseeable threat of a terrorist attack;\n(iii) the localisation or identification of a person suspected of having committed a criminal offence, for the purpose of conducting a criminal investigation or prosecution or executing a criminal penalty for offences referred to in Annex II and punishable in the Member State concerned by a custodial sentence or a detention order for a maximum period of at least four years.\nPoint (h) of the first subparagraph is without prejudice to Article 9 of Regulation (EU) 2016/679 for the processing of biometric data for purposes other than law enforcement.",
      "original_content": "### Artikel 5: Verbotene Praktiken im KI-Bereich\n[...]\ng) das Inverkehrbringen, die Inbetriebnahme für diesen spezifischen Zweck oder die Verwendung von Systemen zur biometrischen Kategorisierung, mit denen natürliche Personen individuell auf der Grundlage ihrer biometrischen Daten kategorisiert werden, um ihre Rasse, ihre politischen Einstellungen, ihre Gewerkschaftszugehörigkeit, ihre religiösen oder weltanschaulichen Überzeugungen, ihr Sexualleben oder ihre sexuelle Ausrichtung zu erschließen oder abzuleiten; dieses Verbot gilt nicht für die Kennzeichnung oder Filterung rechtmäßig erworbener biometrischer Datensätze, wie z. B. Bilder auf der Grundlage biometrischer Daten oder die Kategorisierung biometrischer Daten im Bereich der Strafverfolgung;\nh) die Verwendung biometrischer Echtzeit-Fernidentifizierungssysteme in öffentlich zugänglichen Räumen zu Strafverfolgungszwecken, außer wenn und insoweit dies im Hinblick auf eines der folgenden Ziele unbedingt erforderlich ist:\nist:i) gezielte Suche nach bestimmten Opfern von Entführung, Menschenhandel oder\nsexueller Ausbeutung sowie die Suche nach vermissten Personen;\nii) Abwenden einer konkreten, erheblichen und unmittelbaren Gefahr für das Leben oder die körperliche Unversehrtheit natürlicher Personen oder einer tatsächlichen und bestehenden oder tatsächlichen und vorhersehbaren Gefahr eines Terroranschlags;\niii) Aufspüren oder Identifizieren einer Person, die der Begehung einer Straftat verdächtigt wird, zum Zwecke der Durchführung von strafrechtlichen Ermittlungen oder von Strafverfahren oder der Vollstreckung einer Strafe für die in Anhang II aufgeführten Straftaten, die in dem betreffenden Mitgliedstaat nach dessen Recht mit einer Freiheitsstrafe oder einer freiheitsentziehenden Maßregel der Sicherung im Höchstmaß von mindestens vier Jahren bedroht ist.\nUnterabsatz 1 Buchstabe h gilt unbeschadet des Artikels 9 der Verordnung (EU) 2016/679 für die Verarbeitung biometrischer Daten zu anderen Zwecken als der Strafverfolgung."
    },
    {
      "chunk_idx": 205,
      "id": "e436d2fa-0d04-4566-931b-a1955edc7114",
      "title": "Art 5: Abs 2 Biometrie Identitätsfeststellung",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 5: Prohibited AI practices\n\n2. The use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement for any of the objectives referred to in paragraph 1, first subparagraph, point (h), shall be deployed for the purposes set out in that point only to confirm the identity of the specifically targeted individual, and it shall take into account the following elements:\n(a) the nature of the situation giving rise to the possible use, in particular the seriousness, probability and scale of the harm that would be caused if the system were not used;\n(b) the consequences of the use of the system for the rights and freedoms of all persons concerned, in particular the seriousness, probability and scale of those consequences.\nIn addition, the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement for any of the objectives referred to in paragraph 1, first subparagraph, point (h), of this Article shall comply with necessary and proportionate safeguards and conditions in relation to the use in accordance with the national law authorising the use thereof, in particular as regards the temporal, geographic and personal limitations. The use of the ‘real-time’ remote biometric identification system in publicly accessible spaces shall be authorised only if the law enforcement authority has completed a fundamental rights impact assessment as provided for in Article 27 and has registered the system in the EU database according to Article 49. However, in duly justified cases of urgency, the use of such systems may be commenced without the registration in the EU database, provided that such registration is completed without undue delay.",
      "original_content": "### Artikel 5: Verbotene Praktiken im KI-Bereich\n[...]\n(2) Die Verwendung biometrischer Echtzeit-Fernidentifizierungssysteme in öffentlich zugänglichen Räumen zu Strafverfolgungszwecken im Hinblick auf die in Absatz 1 Unterabsatz 1 Buchstabe h genannten Ziele darf für die in jenem Buchstaben genannten Zwecke nur zur Bestätigung der Identität der speziell betroffenen Person erfolgen, wobei folgende Elemente berücksichtigt werden:\na) die Art der Situation, die der möglichen Verwendung zugrunde liegt, insbesondere die Schwere, die Wahrscheinlichkeit und das Ausmaß des Schadens, der entstehen würde, wenn das System nicht eingesetzt würde;\nb) die Folgen der Verwendung des Systems für die Rechte und Freiheiten aller betroffenen Personen, insbesondere die Schwere, die Wahrscheinlichkeit und das Ausmaß solcher Folgen.\nDarüber hinaus sind bei der Verwendung biometrischer Echtzeit-Fernidentifizierungssysteme in öffentlich zugänglichen Räumen zu Strafverfolgungszwecken im Hinblick auf die in Absatz 1 Unterabsatz 1 Buchstabe h des vorliegenden Artikels genannten Ziele notwendige und verhältnismäßige Schutzvorkehrungen und Bedingungen für die Verwendung im Einklang mit nationalem Recht über die Ermächtigung ihrer Verwendung einzuhalten, insbesondere in Bezug auf die zeitlichen, geografischen und personenbezogenen Beschränkungen. Die Verwendung biometrischer Echtzeit-Fernidentifizierungssysteme in öffentlich zugänglichen Räumen ist nur dann zu gestatten, wenn die Strafverfolgungsbehörde eine Folgenabschätzung im Hinblick auf die Grundrechte gemäß Artikel 27 abgeschlossen und das System gemäß Artikel 49 in der EU-Datenbank registriert hat. In hinreichend begründeten dringenden Fällen kann jedoch mit der Verwendung solcher Systeme zunächst ohne Registrierung in der EU-Datenbank begonnen werden, sofern diese Registrierung unverzüglich erfolgt."
    },
    {
      "chunk_idx": 206,
      "id": "7e829946-fb22-41cb-8900-9ff0f0a32229",
      "title": "Art 5: Abs 3 - 4 Biometrie Behördenzuständigkeiten",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 5: Prohibited AI practices\n\n3. For the purposes of paragraph 1, first subparagraph, point (h) and paragraph 2, each use for the purposes of law enforcement of a ‘real-time’ remote biometric identification system in publicly accessible spaces shall be subject to a prior authorisation granted by a judicial authority or an independent administrative authority whose decision is binding of the Member State in which the use is to take place, issued upon a reasoned request and in accordance with the detailed rules of national law referred to in paragraph 5. However, in a duly justified situation of urgency, the use of such system may be commenced without an authorisation provided that such authorisation is requested without undue delay, at the latest within 24 hours. If such authorisation is rejected, the use shall be stopped with immediate effect and all the data, as well as the results and outputs of that use shall be immediately discarded and deleted.\nThe competent judicial authority or an independent administrative authority whose decision is binding shall grant the authorisation only where it is satisfied, on the basis of objective evidence or clear indications presented to it, that the use of the ‘real-time’ remote biometric identification system concerned is necessary for, and proportionate to, achieving one of the objectives specified in paragraph 1, first subparagraph, point (h), as identified in the request and, in particular, remains limited to what is strictly necessary concerning the period of time as well as the geographic and personal scope. In deciding on the request, that authority shall take into account the elements referred to in paragraph 2. No decision that produces an adverse legal effect on a person may be taken based solely on the output of the ‘real-time’ remote biometric identification system.\n4. Without prejudice to paragraph 3, each use of a ‘real-time’ remote biometric identification system in publicly accessible spaces for law enforcement purposes shall be notified to the relevant market surveillance authority and the national data protection authority in accordance with the national rules referred to in paragraph 5. The notification shall, as a minimum, contain the information specified under paragraph 6 and shall not include sensitive operational data.",
      "original_content": "### Artikel 5: Verbotene Praktiken im KI-Bereich\n[...]\n(3) Für die Zwecke des Absatz 1 Unterabsatz 1 Buchstabe h und des Absatzes 2 ist für jede Verwendung eines biometrischen Echtzeit-Fernidentifizierungssystems in öffentlich zugänglichen Räumen zu Strafverfolgungszwecken eine vorherige Genehmigung erforderlich, die von einer Justizbehörde oder einer unabhängigen Verwaltungsbehörde des Mitgliedstaats, in dem die Verwendung erfolgen soll, auf begründeten Antrag und gemäß den in Absatz 5 genannten detaillierten nationalen Rechtsvorschriften erteilt wird, wobei deren Entscheidung bindend ist. In hinreichend begründeten dringenden Fällen kann jedoch mit der Verwendung eines solchen Systems zunächst ohne Genehmigung begonnen werden, sofern eine solche Genehmigung unverzüglich, spätestens jedoch innerhalb von 24 Stunden beantragt wird. Wird eine solche Genehmigung abgelehnt, so wird die Verwendung mit sofortiger Wirkung eingestellt und werden alle Daten sowie die Ergebnisse und Ausgaben dieser Verwendung unverzüglich verworfen und gelöscht.\nDie zuständige Justizbehörde oder eine unabhängige Verwaltungsbehörde, deren Entscheidung bindend ist, erteilt die Genehmigung nur dann, wenn sie auf der Grundlage objektiver Nachweise oder eindeutiger Hinweise, die ihr vorgelegt werden, davon überzeugt ist, dass die Verwendung des betreffenden biometrischen Echtzeit-Fernidentifizierungssystems für das Erreichen eines der in Absatz 1 Unterabsatz 1 Buchstabe h genannten Ziele — wie im Antrag angegeben — notwendig und verhältnismäßig ist und insbesondere auf das in Bezug auf den Zeitraum sowie den geografischen und persönlichen Anwendungsbereich unbedingt erforderliche Maß beschränkt bleibt. Bei ihrer Entscheidung über den Antrag berücksichtigt diese Behörde die in Absatz 2 genannten Elemente. Eine Entscheidung, aus der sich eine nachteilige Rechtsfolge für eine Person ergibt, darf nicht ausschließlich auf der Grundlage der Ausgabe des biometrischen Echtzeit-Fernidentifizierungssystems getroffen werden.\n(4) Unbeschadet des Absatzes 3 wird jede Verwendung eines biometrischen Echtzeit-Fernidentifizierungssystems in öffentlich zugänglichen Räumen zu Strafverfolgungszwecken der zuständigen Marktüberwachungsbehörde und der nationalen Datenschutzbehörde gemäß den in Absatz 5 genannten nationalen Vorschriften mitgeteilt. Die Mitteilung muss mindestens die in Absatz 6 genannten Angaben enthalten und darf keine sensiblen operativen Daten enthalten."
    },
    {
      "chunk_idx": 207,
      "id": "80b0fa01-7245-4610-a673-5d369d0e9f9a",
      "title": "Art 5: Abs 5 Biometrie nationale Regelungen",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 5: Prohibited AI practices\n\n5. A Member State may decide to provide for the possibility to fully or partially authorise the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement within the limits and under the conditions listed in paragraph 1, first subparagraph, point (h), and paragraphs 2 and 3. Member States concerned shall lay down in their national law the necessary detailed rules for the request, issuance and exercise of, as well as supervision and reporting relating to, the authorisations referred to in paragraph 3. Those rules shall also specify in respect of which of the objectives listed in paragraph 1, first subparagraph, point (h), including which of the criminal offences referred to in point (h)(iii) thereof, the competent authorities may be authorised to use those systems for the purposes of law enforcement. Member States shall notify those rules to the Commission at the latest 30 days following the adoption thereof. Member States may introduce, in accordance with Union law, more restrictive laws on the use of remote biometric identification systems.",
      "original_content": "### Artikel 5: Verbotene Praktiken im KI-Bereich\n[...]\n(5) Ein Mitgliedstaat kann die Möglichkeit einer vollständigen oder teilweisen Ermächtigung zur Verwendung biometrischer Echtzeit-Fernidentifizierungssysteme in öffentlich zugänglichen Räumen zu Strafverfolgungszwecken innerhalb der in Absatz 1 Unterabsatz 1 Buchstabe h sowie Absätze 2 und 3 aufgeführten Grenzen und unter den dort genannten Bedingungen vorsehen. Die betreffenden Mitgliedstaaten legen in ihrem nationalen Recht die erforderlichen detaillierten Vorschriften für die Beantragung, Erteilung und Ausübung der in Absatz 3 genannten Genehmigungen sowie für die entsprechende Beaufsichtigung und Berichterstattung fest. In diesen Vorschriften wird auch festgelegt, im Hinblick auf welche der in Absatz 1 Unterabsatz 1 Buchstabe h aufgeführten Ziele und welche der unter Buchstabe h Ziffer iii genannten Straftaten die zuständigen Behörden ermächtigt werden können, diese Systeme zu Strafverfolgungszwecken zu verwenden. Die Mitgliedstaaten teilen der Kommission diese Vorschriften spätestens 30 Tage nach ihrem Erlass mit. Die Mitgliedstaaten können im Einklang mit dem Unionsrecht strengere Rechtsvorschriften für die Verwendung biometrischer Fernidentifizierungssysteme erlassen."
    },
    {
      "chunk_idx": 208,
      "id": "c9d80e98-9396-4526-ad4f-1e57ebf765f0",
      "title": "Art 5: Abs 6-7 Biometrie Jahresbericht",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 5: Prohibited AI practices\n\n6. National market surveillance authorities and the national data protection authorities of Member States that have been notified of the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for law enforcement purposes pursuant to paragraph 4 shall submit to the Commission annual reports on such use. For that purpose, the Commission shall provide Member States and national market surveillance and data protection authorities with a template, including information on the number of the decisions taken by competent judicial authorities or an independent administrative authority whose decision is binding upon requests for authorisations in accordance with paragraph 3 and their result.\n7. The Commission shall publish annual reports on the use of real-time remote biometric identification systems in publicly accessible spaces for law enforcement purposes, based on aggregated data in Member States on the basis of the annual reports referred to in paragraph 6. Those annual reports shall not include sensitive operational data of the related law enforcement activities.",
      "original_content": "### Artikel 5: Verbotene Praktiken im KI-Bereich\n[...]\n(6) Die nationalen Marktüberwachungsbehörden und die nationalen Datenschutzbehörden der Mitgliedstaaten, denen gemäß Absatz 4 die Verwendung biometrischer Echtzeit-Fernidentifizierungssysteme in öffentlich zugänglichen Räumen zu Strafverfolgungszwecken mitgeteilt wurden, legen der Kommission Jahresberichte über diese Verwendung vor. Zu diesem Zweck stellt die Kommission den Mitgliedstaaten und den nationalen Marktüberwachungs- und Datenschutzbehörden ein Muster zur Verfügung, das Angaben über die Anzahl der Entscheidungen der zuständigen Justizbehörden oder einer unabhängigen Verwaltungsbehörde, deren Entscheidung über Genehmigungsanträge gemäß Absatz 3 bindend ist, und deren Ergebnis enthält.\n(7) Die Kommission veröffentlicht Jahresberichte über die Verwendung biometrischer Echtzeit-Fernidentifizierungssysteme in öffentlich zugänglichen Räumen zu Strafverfolgungszwecken, die auf aggregierten Daten aus den Mitgliedstaaten auf der Grundlage der in Absatz 6 genannten Jahresberichte beruhen. Diese Jahresberichte dürfen keine sensiblen operativen Daten im Zusammenhang mit den damit verbundenen Strafverfolgungsmaßnahmen enthalten."
    },
    {
      "chunk_idx": 209,
      "id": "239278ca-b87b-4431-b7c9-71fd1a54f9ff",
      "title": "Art 5: Abs 8 Sonstiges",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 5: Prohibited AI practices\n\n8. This Article shall not affect the prohibitions that apply where an AI practice infringes other Union law.",
      "original_content": "### Artikel 5: Verbotene Praktiken im KI-Bereich\n[...]\n(8) Dieser Artikel berührt nicht die Verbote, die gelten, wenn KI-Praktiken gegen andere Rechtsvorschriften der Union verstoßen."
    },
    {
      "chunk_idx": 210,
      "id": "6b073c5f-3422-4b34-b139-b8645e89b463",
      "title": "Art 6",
      "relevantChunksIds": [
        "16b5ff49-dbab-4e7e-861a-7e1d4cd7cb03",
        "070576ca-434c-4246-a1bf-ba25d2a45285",
        "cfa2dd1a-cc63-4053-9da5-b7fddaa3ae9c",
        "d027c4c8-3e9c-44bf-8ac9-72c2959acd00",
        "0b3d80d6-3316-4d5d-85a8-6477c568dfbe",
        "be8e58b9-7976-45e8-9b6e-ae5d17d5f7cb",
        "44ee4963-2543-4ba3-8550-cf2a21c2224c",
        "8a27e104-58f9-4f9d-8bbb-aff055c16634",
        "c810069a-3ec4-433a-8bff-0faacb8796dd",
        "09d31d87-cf07-441e-a4c2-84ae865db022",
        "d89b42c0-fb0b-40c3-a99d-40aae0d9dea9",
        "332aaf1e-5867-464e-b22e-94979182c753",
        "396c4d54-1dac-4555-bd49-3204cc61a7f3",
        "8321c080-a535-45b2-a8d8-2201f5fb36c4",
        "d47b3160-2283-4955-bec9-6ae45acbef8b",
        "7729fd74-193d-4df9-b6a1-c3c6d261090f",
        "8a98c63a-448a-4311-9ab0-7e6c56c2a6f8",
        "20cafabb-1545-4c9a-b9fd-5adeb4509205",
        "c8096ef4-8680-44ed-b282-f02c6434e936",
        "479fd058-034c-4a38-b806-38bf6965c0b7",
        "32ec33e4-0b93-498d-bf92-8f561ac77fbd",
        "d6b1d3b4-a6fc-460d-adac-3810b3fd4b9a",
        "a9bf3bf8-40da-4c63-a908-e12625e1e946",
        "032907d6-0290-44ab-bee5-be05248e3ae2",
        "d9cb5958-fede-437d-a0f6-ad55c6d6db50"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "# CHAPTER III: HIGH-RISK AI SYSTEMS\n## SECTION 1: Classification of AI systems as high-risk\n### Article 6: Classification rules for high-risk AI systems\n1. Irrespective of whether an AI system is placed on the market or put into service independently of the products referred to in points (a) and (b), that AI system shall be considered to be high-risk where both of the following conditions are fulfilled:\n(a) the AI system is intended to be used as a safety component of a product, or the AI system is itself a product, covered by the Union harmonisation legislation listed in Annex I;\n(b) the product whose safety component pursuant to point (a) is the AI system, or the AI system itself as a product, is required to undergo a third-party conformity assessment, with a view to the placing on the market or the putting into service of that product pursuant to the Union harmonisation legislation listed in Annex I.\n2. In addition to the high-risk AI systems referred to in paragraph 1, AI systems referred to in Annex III shall be considered to be high-risk.\n3. By derogation from paragraph 2, an AI system referred to in Annex III shall not be considered to be high-risk where it does not pose a significant risk of harm to the health, safety or fundamental rights of natural persons, including by not materially influencing the outcome of decision making.\nThe first subparagraph shall apply where any of the following conditions is fulfilled:\n(a) the AI system is intended to perform a narrow procedural task;\n(b) the AI system is intended to improve the result of a previously completed human activity;\n(c) the AI system is intended to detect decision-making patterns or deviations from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment, without proper human review; or\n(d) the AI system is intended to perform a preparatory task to an assessment relevant for the purposes of the use cases listed in Annex III.\nNotwithstanding the first subparagraph, an AI system referred to in Annex III shall always be considered to be high-risk where the AI system performs profiling of natural persons.\n4. A provider who considers that an AI system referred to in Annex III is not high-risk shall document its assessment before that system is placed on the market or put into service. Such provider shall be subject to the registration obligation set out in Article 49(2). Upon request of national competent authorities, the provider shall provide the documentation of the assessment.\n5. The Commission shall, after consulting the European Artificial Intelligence Board (the ‘Board’), and no later than 2 February 2026, provide guidelines specifying the practical implementation of this Article in line with Article 96 together with a comprehensive list of practical examples of use cases of AI systems that are high-risk and not high-risk.\n6. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend paragraph 3, second subparagraph, of this Article by adding new conditions to those laid down therein, or by modifying them, where there is concrete and reliable evidence of the existence of AI systems that fall under the scope of Annex III, but do not pose a significant risk of harm to the health, safety or fundamental rights of natural persons.\n7. The Commission shall adopt delegated acts in accordance with Article 97 in order to amend paragraph 3, second subparagraph, of this Article by deleting any of the conditions laid down therein, where there is concrete and reliable evidence that this is necessary to maintain the level of protection of health, safety and fundamental rights provided for by this Regulation.\n8. Any amendment to the conditions laid down in paragraph 3, second subparagraph, adopted in accordance with paragraphs 6 and 7 of this Article shall not decrease the overall level of protection of health, safety and fundamental rights provided for by this Regulation and shall ensure consistency with the delegated acts adopted pursuant to Article 7(1), and take account of market and technological developments.",
      "original_content": "# KAPITEL III: HOCHRISIKO-KI-SYSTEME\n## ABSCHNITT 1: Einstufung von KI-Systemen als Hochrisiko-KI-Systeme\n### Artikel 6: Einstufungsvorschriften für Hochrisiko-KI-Systeme\n(1) Ungeachtet dessen, ob ein KI-System unabhängig von den unter den Buchstaben a und b genannten Produkten in Verkehr gebracht oder in Betrieb genommen wird, gilt es als Hochrisiko-KI-System, wenn die beiden folgenden Bedingungen erfüllt sind:\na) das KI-System soll als Sicherheitsbauteil eines unter die in Anhang I aufgeführten Harmonisierungsrechtsvorschriften der Union fallenden Produkts verwendet werden oder das KI-System ist selbst ein solches Produkt;\nb) das Produkt, dessen Sicherheitsbauteil gemäß Buchstabe a das KI-System ist, oder das KI-System selbst als Produkt muss einer Konformitätsbewertung durch Dritte im Hinblick auf das Inverkehrbringen oder die Inbetriebnahme dieses Produkts gemäß den in Anhang I aufgeführten Harmonisierungsrechtsvorschriften der Union unterzogen werden.\n(2) Zusätzlich zu den in Absatz 1 genannten Hochrisiko-KI-Systemen gelten die in Anhang III genannten KI-Systeme als hochriskant.\n(3) Abweichend von Absatz 2 gilt ein in Anhang III genanntes KI-System nicht als hochriskant, wenn es kein erhebliches Risiko der Beeinträchtigung in Bezug auf die Gesundheit, Sicherheit oder Grundrechte natürlicher Personen birgt, indem es unter anderem nicht das Ergebnis der Entscheidungsfindung wesentlich beeinflusst.\nUnterabsatz 1 gilt, wenn eine der folgenden Bedingungen erfüllt ist:\na) das KI-System ist dazu bestimmt, eine eng gefasste Verfahrensaufgabe durchzuführen;\nb) das KI-System ist dazu bestimmt, das Ergebnis einer zuvor abgeschlossenen menschlichen Tätigkeit zu verbessern;\nc) das KI-System ist dazu bestimmt, Entscheidungsmuster oder Abweichungen von früheren Entscheidungsmustern zu erkennen, und ist nicht dazu gedacht, die zuvor abgeschlossene menschliche Bewertung ohne eine angemessene menschliche Überprüfung zu ersetzen oder zu beeinflussen; oder\nd) das KI-System ist dazu bestimmt, eine vorbereitende Aufgabe für eine Bewertung durchzuführen, die für die Zwecke der in Anhang III aufgeführten Anwendungsfälle relevant ist.\nUngeachtet des Unterabsatzes 1 gilt ein in Anhang III aufgeführtes KI-System immer dann als hochriskant, wenn es ein Profiling natürlicher Personen vornimmt.\n(4) Ein Anbieter, der der Auffassung ist, dass ein in Anhang III aufgeführtes KI-System nicht hochriskant ist, dokumentiert seine Bewertung, bevor dieses System in Verkehr gebracht oder in Betrieb genommen wird. Dieser Anbieter unterliegt der Registrierungspflicht gemäß Artikel 49 Absatz 2. Auf Verlangen der zuständigen nationalen Behörden legt der Anbieter die Dokumentation der Bewertung vor.\n(5) Die Kommission stellt nach Konsultation des Europäischen Gremiums für Künstliche Intelligenz (im Folgenden „KI-Gremium“) spätestens bis zum 2. Februar 2026 Leitlinien zur praktischen Umsetzung dieses Artikels gemäß Artikel 96 und eine umfassende Liste praktischer Beispiele für Anwendungsfälle für KI-Systeme, die hochriskant oder nicht hochriskant sind, bereit.\n(6) Die Kommission ist befugt, gemäß Artikel 97 delegierte Rechtsakte zu erlassen, um Absatz 3 Unterabsatz 2 des vorliegenden Artikels zu ändern, indem neue Bedingungen zu den darin genannten Bedingungen hinzugefügt oder diese geändert werden, wenn konkrete und zuverlässige Beweise für das Vorhandensein von KI-Systemen vorliegen, die in den Anwendungsbereich von Anhang III fallen, jedoch kein erhebliches Risiko der Beeinträchtigung in Bezug auf die Gesundheit, Sicherheit oder Grundrechte natürlicher Personen bergen.\n(7) Die Kommission erlässt gemäß Artikel 97 delegierte Rechtsakte, um Absatz 3 Unterabsatz 2 des vorliegenden Artikels zu ändern, indem eine der darin festgelegten Bedingungen gestrichen wird, wenn konkrete und zuverlässige Beweise dafür vorliegen, dass dies für die Aufrechterhaltung des Schutzniveaus in Bezug auf Gesundheit, Sicherheit und die in dieser Verordnung vorgesehenen Grundrechte erforderlich ist.\n(8) Eine Änderung der in Absatz 3 Unterabsatz 2 festgelegten Bedingungen, die gemäß den Absätzen 6 und 7 des vorliegenden Artikels erlassen wurde, darf das allgemeine Schutzniveau in Bezug auf Gesundheit, Sicherheit und die in dieser Verordnung vorgesehenen Grundrechte nicht senken; dabei ist die Kohärenz mit den gemäß Artikel 7 Absatz 1 erlassenen delegierten Rechtsakten sicherzustellen und die Marktentwicklungen und die technologischen Entwicklungen sind zu berücksichtigen."
    },
    {
      "chunk_idx": 211,
      "id": "aef2917e-0009-4636-8202-ef41bb6b33ff",
      "title": "Art 7",
      "relevantChunksIds": [
        "0b3d80d6-3316-4d5d-85a8-6477c568dfbe",
        "8a27e104-58f9-4f9d-8bbb-aff055c16634"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 7: Amendments to Annex III\n1. The Commission is empowered to adopt delegated acts in accordance with Article 97 to amend Annex III by adding or modifying use-cases of high-risk AI systems where both of the following conditions are fulfilled:\n(a) the AI systems are intended to be used in any of the areas listed in Annex III;\n(b) the AI systems pose a risk of harm to health and safety, or an adverse impact on fundamental rights, and that risk is equivalent to, or greater than, the risk of harm or of adverse impact posed by the high-risk AI systems already referred to in Annex III.\n2. When assessing the condition under paragraph 1, point (b), the Commission shall take into account the following criteria:\n(a) the intended purpose of the AI system;\n(b) the extent to which an AI system has been used or is likely to be used;\n(c) the nature and amount of the data processed and used by the AI system, in particular whether special categories of personal data are processed;\n(d) the extent to which the AI system acts autonomously and the possibility for a human to override a decision or recommendations that may lead to potential harm;\n(e) the extent to which the use of an AI system has already caused harm to health and safety, has had an adverse impact on fundamental rights or has given rise to significant concerns in relation to the likelihood of such harm or adverse impact, as demonstrated, for example, by reports or documented allegations submitted to national competent authorities or by other reports, as appropriate;\n(f) the potential extent of such harm or such adverse impact, in particular in terms of its intensity and its ability to affect multiple persons or to disproportionately affect a particular group of persons;\n(g) the extent to which persons who are potentially harmed or suffer an adverse impact are dependent on the outcome produced with an AI system, in particular because for practical or legal reasons it is not reasonably possible to opt-out from that outcome;\n(h) the extent to which there is an imbalance of power, or the persons who are potentially harmed or suffer an adverse impact are in a vulnerable position in relation to the deployer of an AI system, in particular due to status, authority, knowledge, economic or social circumstances, or age;\n(i) the extent to which the outcome produced involving an AI system is easily corrigible or reversible, taking into account the technical solutions available to correct or reverse it, whereby outcomes having an adverse impact on health, safety or fundamental rights, shall not be considered to be easily corrigible or reversible;\n(j) the magnitude and likelihood of benefit of the deployment of the AI system for individuals, groups, or society at large, including possible improvements in product safety;\n(k) the extent to which existing Union law provides for:\n(i) effective measures of redress in relation to the risks posed by an AI system, with the exclusion of claims for damages;\n(ii) effective measures to prevent or substantially minimise those risks.\n3. The Commission is empowered to adopt delegated acts in accordance with Article 97 to amend the list in Annex III by removing high-risk AI systems where both of the following conditions are fulfilled:\n(a) the high-risk AI system concerned no longer poses any significant risks to fundamental rights, health or safety, taking into account the criteria listed in paragraph 2;\n(b) the deletion does not decrease the overall level of protection of health, safety and fundamental rights under Union law.",
      "original_content": "### Artikel 7: Änderungen des Anhangs III\n(1) Die Kommission ist befugt, gemäß Artikel 97 delegierte Rechtsakte zur Änderung von Anhang III durch Hinzufügung oder Änderung von Anwendungsfällen für Hochrisiko-KI-Systeme zu erlassen, die beide der folgenden Bedingungen erfüllen:\na) Die KI-Systeme sollen in einem der in Anhang III aufgeführten Bereiche eingesetzt werden;\nb) die KI-Systeme bergen ein Risiko der Schädigung in Bezug auf die Gesundheit und Sicherheit oder haben nachteilige Auswirkungen auf die Grundrechte und dieses Risiko gleicht dem Risiko der Schädigung oder den nachteiligen Auswirkungen, das bzw. die von den in Anhang III bereits genannten Hochrisiko-KI-Systemen ausgeht bzw. ausgehen, oder übersteigt diese.\n(2) Bei der Bewertung der Bedingung gemäß Absatz 1 Buchstabe b berücksichtigt die Kommission folgende Kriterien:\na) die Zweckbestimmung des KI-Systems;\nb) das Ausmaß, in dem ein KI-System verwendet wird oder voraussichtlich verwendet werden wird;\nc) die Art und den Umfang der vom KI-System verarbeiteten und verwendeten Daten, insbesondere die Frage, ob besondere Kategorien personenbezogener Daten verarbeitet werden;\nd) das Ausmaß, in dem das KI-System autonom handelt, und die Möglichkeit, dass ein Mensch eine Entscheidung oder Empfehlungen, die zu einem potenziellen Schaden führen können, außer Kraft setzt;\ne) das Ausmaß, in dem durch die Verwendung eines KI-Systems schon die Gesundheit und Sicherheit geschädigt wurden, es nachteilige Auswirkungen auf die Grundrechte gab oder z. B. nach Berichten oder dokumentierten Behauptungen, die den zuständigen nationalen Behörden übermittelt werden, oder gegebenenfalls anderen Berichten Anlass zu erheblichen Bedenken hinsichtlich der Wahrscheinlichkeit eines solchen Schadens oder solcher nachteiligen Auswirkungen besteht;\nf) das potenzielle Ausmaß solcher Schäden oder nachteiligen Auswirkungen, insbesondere hinsichtlich ihrer Intensität und ihrer Eignung, mehrere Personen zu beeinträchtigen oder eine bestimmte Gruppe von Personen unverhältnismäßig stark zu beeinträchtigen;\ng) das Ausmaß, in dem Personen, die potenziell geschädigt oder negative Auswirkungen erleiden werden, von dem von einem KI-System hervorgebrachten Ergebnis abhängen, weil es insbesondere aus praktischen oder rechtlichen Gründen nach vernünftigem Ermessen unmöglich ist, sich diesem Ergebnis zu entziehen;\nh) das Ausmaß, in dem ein Machtungleichgewicht besteht oder in dem Personen, die potenziell geschädigt oder negative Auswirkungen erleiden werden, gegenüber dem Betreiber eines KI-Systems schutzbedürftig sind, insbesondere aufgrund von Status, Autorität, Wissen, wirtschaftlichen oder sozialen Umständen oder Alter;\ni) das Ausmaß, in dem das mithilfe eines KI-Systems hervorgebrachte Ergebnis unter Berücksichtigung der verfügbaren technischen Lösungen für seine Korrektur oder Rückgängigmachung leicht zu korrigieren oder rückgängig zu machen ist, wobei Ergebnisse, die sich auf die Gesundheit, Sicherheit oder Grundrechte von Personen negativ auswirken, nicht als leicht korrigierbar oder rückgängig zu machen gelten;\nj) das Ausmaß und die Wahrscheinlichkeit, dass der Einsatz des KI-Systems für Einzelpersonen, Gruppen oder die Gesellschaft im Allgemeinen, einschließlich möglicher Verbesserungen der Produktsicherheit, nützlich ist;\nk) das Ausmaß, in dem bestehendes Unionsrecht Folgendes vorsieht:\ni) wirksame Abhilfemaßnahmen in Bezug auf die Risiken, die von einem KI-System ausgehen, mit Ausnahme von Schadenersatzansprüchen;\nii) wirksame Maßnahmen zur Vermeidung oder wesentlichen Verringerung dieser Risiken.\n(3) Die Kommission ist befugt, gemäß Artikel 97 delegierte Rechtsakte zur Änderung der Liste in Anhang III zu erlassen, um Hochrisiko-KI-Systeme zu streichen, die beide der folgenden Bedingungen erfüllen:\na) Das betreffende Hochrisiko-KI-System weist unter Berücksichtigung der in Absatz 2 aufgeführten Kriterien keine erheblichen Risiken mehr für die Grundrechte, Gesundheit oder Sicherheit auf;\nb) durch die Streichung wird das allgemeine Schutzniveau in Bezug auf Gesundheit, Sicherheit und Grundrechte im Rahmen des Unionsrechts nicht gesenkt."
    },
    {
      "chunk_idx": 212,
      "id": "181144f1-acf1-42ef-a1a2-aca3c1a52b7d",
      "title": "Art 8",
      "relevantChunksIds": [
        "3be859ed-68c0-4253-864e-73cecf5b5500",
        "3ebbd833-32ab-4a37-87be-8bae31dbef92",
        "03a48ad3-88c0-41d1-be59-9032cdfa8840"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "## SECTION 2: Requirements for high-risk AI systems\n### Article 8: Compliance with the requirements\n1. High-risk AI systems shall comply with the requirements laid down in this Section, taking into account their intended purpose as well as the generally acknowledged state of the art on AI and AI-related technologies. The risk management system referred to in Article 9 shall be taken into account when ensuring compliance with those requirements.\n2. Where a product contains an AI system, to which the requirements of this Regulation as well as requirements of the Union harmonisation legislation listed in Section A of Annex I apply, providers shall be responsible for ensuring that their product is fully compliant with all applicable requirements under applicable Union harmonisation legislation. In ensuring the compliance of high-risk AI systems referred to in paragraph 1 with the requirements set out in this Section, and in order to ensure consistency, avoid duplication and minimise additional burdens, providers shall have a choice of integrating, as appropriate, the necessary testing and reporting processes, information and documentation they provide with regard to their product into documentation and procedures that already exist and are required under the Union harmonisation legislation listed in Section A of Annex I.",
      "original_content": "## ABSCHNITT 2: Anforderungen an Hochrisiko-KI-Systeme\n### Artikel 8: Einhaltung der Anforderungen\n(1) Hochrisiko-KI-Systeme müssen die in diesem Abschnitt festgelegten Anforderungen erfüllen, wobei ihrer Zweckbestimmung sowie dem allgemein anerkannten Stand der Technik in Bezug auf KI und KI-bezogene Technologien Rechnung zu tragen ist. Bei der Gewährleistung der Einhaltung dieser Anforderungen wird dem in Artikel 9 genannten Risikomanagementsystem Rechnung getragen.\n(2) Enthält ein Produkt ein KI-System, für das die Anforderungen dieser Verordnung und die Anforderungen der in Anhang I Abschnitt A aufgeführten Harmonisierungsrechtsvorschriften der Union gelten, so sind die Anbieter dafür verantwortlich, sicherzustellen, dass ihr Produkt alle geltenden Anforderungen der geltenden Harmonisierungsrechtsvorschriften der Union vollständig erfüllt. Bei der Gewährleistung der Erfüllung der in diesem Abschnitt festgelegten Anforderungen durch die in Absatz 1 genannten Hochrisiko-KI-Systeme und im Hinblick auf die Gewährleistung der Kohärenz, der Vermeidung von Doppelarbeit und der Minimierung zusätzlicher Belastungen haben die Anbieter die Wahl, die erforderlichen Test- und Berichterstattungsverfahren, Informationen und Dokumentationen, die sie im Zusammenhang mit ihrem Produkt bereitstellen, gegebenenfalls in Dokumentationen und Verfahren zu integrieren, die bereits bestehen und gemäß den in Anhang I Abschnitt A aufgeführten Harmonisierungsrechtsvorschriften der Union vorgeschrieben sind."
    },
    {
      "chunk_idx": 213,
      "id": "6d7d2b9c-9224-4f54-ad18-671918597d0c",
      "title": "Art 9",
      "relevantChunksIds": [
        "1d3e3007-9327-4f7b-ba9b-44bd93c2f4ae",
        "25407f28-0e07-4101-a714-b193ca14783e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 9: Risk management system\n1. A risk management system shall be established, implemented, documented and maintained in relation to high-risk AI systems.\n2. The risk management system shall be understood as a continuous iterative process planned and run throughout the entire lifecycle of a high-risk AI system, requiring regular systematic review and updating. It shall comprise the following steps:\n(a) the identification and analysis of the known and the reasonably foreseeable risks that the high-risk AI system can pose to health, safety or fundamental rights when the high-risk AI system is used in accordance with its intended purpose;\n(b) the estimation and evaluation of the risks that may emerge when the high-risk AI system is used in accordance with its intended purpose, and under conditions of reasonably foreseeable misuse;\n(c) the evaluation of other risks possibly arising, based on the analysis of data gathered from the post-market monitoring system referred to in Article 72;\n(d) the adoption of appropriate and targeted risk management measures designed to address the risks identified pursuant to point (a).\n3. The risks referred to in this Article shall concern only those which may be reasonably mitigated or eliminated through the development or design of the high-risk AI system, or the provision of adequate technical information.\n4. The risk management measures referred to in paragraph 2, point (d), shall give due consideration to the effects and possible interaction resulting from the combined application of the requirements set out in this Section, with a view to minimising risks more effectively while achieving an appropriate balance in implementing the measures to fulfil those requirements.\n5. The risk management measures referred to in paragraph 2, point (d), shall be such that the relevant residual risk associated with each hazard, as well as the overall residual risk of the high-risk AI systems is judged to be acceptable.\nIn identifying the most appropriate risk management measures, the following shall be ensured:\n(a) elimination or reduction of risks identified and evaluated pursuant to paragraph 2 in as far as technically feasible through adequate design and development of the high-risk AI system;\n(b) where appropriate, implementation of adequate mitigation and control measures addressing risks that cannot be eliminated;\n(c) provision of information required pursuant to Article 13 and, where appropriate, training to deployers.\nWith a view to eliminating or reducing risks related to the use of the high-risk AI system, due consideration shall be given to the technical knowledge, experience, education, the training to be expected by the deployer, and the presumable context in which the system is intended to be used.\n6. High-risk AI systems shall be tested for the purpose of identifying the most appropriate and targeted risk management measures. Testing shall ensure that high-risk AI systems perform consistently for their intended purpose and that they are in compliance with the requirements set out in this Section.\n7. Testing procedures may include testing in real-world conditions in accordance with Article 60.\n8. The testing of high-risk AI systems shall be performed, as appropriate, at any time throughout the development process, and, in any event, prior to their being placed on the market or put into service. Testing shall be carried out against prior defined metrics and probabilistic thresholds that are appropriate to the intended purpose of the high-risk AI system.\n9. When implementing the risk management system as provided for in paragraphs 1 to 7, providers shall give consideration to whether in view of its intended purpose the high-risk AI system is likely to have an adverse impact on persons under the age of 18 and, as appropriate, other vulnerable groups.\n10. For providers of high-risk AI systems that are subject to requirements regarding internal risk management processes under other relevant provisions of Union law, the aspects provided in paragraphs 1 to 9 may be part of, or combined with, the risk management procedures established pursuant to that law.",
      "original_content": "### Artikel 9: Risikomanagementsystem\n(1) Für Hochrisiko-KI-Systeme wird ein Risikomanagementsystem eingerichtet, angewandt, dokumentiert und aufrechterhalten.\n(2) Das Risikomanagementsystem versteht sich als ein kontinuierlicher iterativer Prozess, der während des gesamten Lebenszyklus eines Hochrisiko-KI-Systems geplant und durchgeführt wird und eine regelmäßige systematische Überprüfung und Aktualisierung erfordert. Es umfasst folgende Schritte:\na) die Ermittlung und Analyse der bekannten und vernünftigerweise vorhersehbaren Risiken, die vom Hochrisiko-KI-System für die Gesundheit, Sicherheit oder Grundrechte ausgehen können, wenn es entsprechend seiner Zweckbestimmung verwendet wird;\nb) die Abschätzung und Bewertung der Risiken, die entstehen können, wenn das Hochrisiko-KI-System entsprechend seiner Zweckbestimmung oder im Rahmen einer vernünftigerweise vorhersehbaren Fehlanwendung verwendet wird;\nc) die Bewertung anderer möglicherweise auftretender Risiken auf der Grundlage der Auswertung der Daten aus dem in Artikel 72 genannten System zur Beobachtung nach dem Inverkehrbringen;\nd) die Ergreifung geeigneter und gezielter Risikomanagementmaßnahmen zur Bewältigung der gemäß Buchstabe a ermittelten Risiken.\n(3) Die in diesem Artikel genannten Risiken betreffen nur solche Risiken, die durch die Entwicklung oder Konzeption des Hochrisiko-KI-Systems oder durch die Bereitstellung ausreichender technischer Informationen angemessen gemindert oder behoben werden können.\n(4) Bei den in Absatz 2 Buchstabe d genannten Risikomanagementmaßnahmen werden die Auswirkungen und möglichen Wechselwirkungen, die sich aus der kombinierten Anwendung der Anforderungen dieses Abschnitts ergeben, gebührend berücksichtigt, um die Risiken wirksamer zu minimieren und gleichzeitig ein angemessenes Gleichgewicht bei der Durchführung der Maßnahmen zur Erfüllung dieser Anforderungen sicherzustellen.\n(5) Die in Absatz 2 Buchstabe d genannten Risikomanagementmaßnahmen werden so gestaltet, dass jedes mit einer bestimmten Gefahr verbundene relevante Restrisiko sowie das Gesamtrestrisiko der Hochrisiko-KI-Systeme als vertretbar beurteilt wird.\nBei der Festlegung der am besten geeigneten Risikomanagementmaßnahmen ist Folgendes sicherzustellen:\na) soweit technisch möglich, Beseitigung oder Verringerung der gemäß Absatz 2 ermittelten und bewerteten Risiken durch eine geeignete Konzeption und Entwicklung des Hochrisiko-KI-Systems;\nb) gegebenenfalls Anwendung angemessener Minderungs- und Kontrollmaßnahmen zur Bewältigung nicht auszuschließender Risiken;\nc) Bereitstellung der gemäß Artikel 13 erforderlichen Informationen und gegebenenfalls entsprechende Schulung der Betreiber.\nZur Beseitigung oder Verringerung der Risiken im Zusammenhang mit der Verwendung des Hochrisiko-KI-Systems werden die technischen Kenntnisse, die Erfahrungen und der Bildungsstand, die vom Betreiber erwartet werden können, sowie der voraussichtliche Kontext, in dem das System eingesetzt werden soll, gebührend berücksichtigt.\n(6) Hochrisiko-KI-Systeme müssen getestet werden, um die am besten geeigneten gezielten Risikomanagementmaßnahmen zu ermitteln. Durch das Testen wird sichergestellt, dass Hochrisiko-KI-Systeme stets im Einklang mit ihrer Zweckbestimmung funktionieren und die Anforderungen dieses Abschnitts erfüllen.\n(7) Die Testverfahren können einen Test unter Realbedingungen gemäß Artikel 60 umfassen.\n(8) Das Testen von Hochrisiko-KI-Systemen erfolgt zu jedem geeigneten Zeitpunkt während des gesamten Entwicklungsprozesses und in jedem Fall vor ihrem Inverkehrbringen oder ihrer Inbetriebnahme. Das Testen erfolgt anhand vorab festgelegter Metriken und Wahrscheinlichkeitsschwellenwerte, die für die Zweckbestimmung des Hochrisiko-KI-Systems geeignet sind.\n(9) Bei der Umsetzung des in den Absätzen 1 bis 7 vorgesehenen Risikomanagementsystems berücksichtigen die Anbieter, ob angesichts seiner Zweckbestimmung das Hochrisiko-KI-System wahrscheinlich nachteilige Auswirkungen auf Personen unter 18 Jahren oder gegebenenfalls andere schutzbedürftige Gruppen haben wird.\n(10) Bei Anbietern von Hochrisiko-KI-Systemen, die den Anforderungen an interne Risikomanagementprozesse gemäß anderen einschlägigen Bestimmungen des Unionsrechts unterliegen, können die in den Absätzen 1 bis 9 enthaltenen Aspekte Bestandteil der nach diesem Recht festgelegten Risikomanagementverfahren sein oder mit diesen Verfahren kombiniert werden."
    },
    {
      "chunk_idx": 214,
      "id": "69b883ca-3e2d-46ed-b797-e2c62832a376",
      "title": "Art 10",
      "relevantChunksIds": [
        "2aedd016-7002-471a-af6e-131b4f9f1f54",
        "316fe982-49dc-4467-a230-7b526feb2812",
        "53ee5174-71e9-4bc8-81f2-251f7a1bc278"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 10: Data and data governance\n1. High-risk AI systems which make use of techniques involving the training of AI models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5 whenever such data sets are used.\n2. Training, validation and testing data sets shall be subject to data governance and management practices appropriate for the intended purpose of the high-risk AI system. Those practices shall concern in particular:\n(a) the relevant design choices;\n(b) data collection processes and the origin of data, and in the case of personal data, the original purpose of the data collection;\n(c) relevant data-preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and aggregation;\n(d) the formulation of assumptions, in particular with respect to the information that the data are supposed to measure and represent;\n(e) an assessment of the availability, quantity and suitability of the data sets that are needed;\n(f) examination in view of possible biases that are likely to affect the health and safety of persons, have a negative impact on fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence inputs for future operations;\n(g) appropriate measures to detect, prevent and mitigate possible biases identified according to point (f);\n(h) the identification of relevant data gaps or shortcomings that prevent compliance with this Regulation, and how those gaps and shortcomings can be addressed.\n3. Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible, free of errors and complete in view of the intended purpose. They shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons in relation to whom the high-risk AI system is intended to be used. Those characteristics of the data sets may be met at the level of individual data sets or at the level of a combination thereof.\n4. Data sets shall take into account, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, contextual, behavioural or functional setting within which the high-risk AI system is intended to be used.\n5. To the extent that it is strictly necessary for the purpose of ensuring bias detection and correction in relation to the high-risk AI systems in accordance with paragraph (2), points (f) and (g) of this Article, the providers of such systems may exceptionally process special categories of personal data, subject to appropriate safeguards for the fundamental rights and freedoms of natural persons. In addition to the provisions set out in Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, all the following conditions must be met in order for such processing to occur:\n(a) the bias detection and correction cannot be effectively fulfilled by processing other data, including synthetic or anonymised data;\n(b) the special categories of personal data are subject to technical limitations on the re-use of the personal data, and state-of-the-art security and privacy-preserving measures, including pseudonymisation;\n(c) the special categories of personal data are subject to measures to ensure that the personal data processed are secured, protected, subject to suitable safeguards, including strict controls and documentation of the access, to avoid misuse and ensure that only authorised persons have access to those personal data with appropriate confidentiality obligations;\n(d) the special categories of personal data are not to be transmitted, transferred or otherwise accessed by other parties;\n(e) the special categories of personal data are deleted once the bias has been corrected or the personal data has reached the end of its retention period, whichever comes first;\n(f) the records of processing activities pursuant to Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680 include the reasons why the processing of special categories of personal data was strictly necessary to detect and correct biases, and why that objective could not be achieved by processing other data.\n6. For the development of high-risk AI systems not using techniques involving the training of AI models, paragraphs 2 to 5 apply only to the testing data sets.",
      "original_content": "### Artikel 10: Daten und Daten-Governance\n(1) Hochrisiko-KI-Systeme, in denen Techniken eingesetzt werden, bei denen KI-Modelle mit Daten trainiert werden, müssen mit Trainings-, Validierungs- und Testdatensätzen entwickelt werden, die den in den Absätzen 2 bis 5 genannten Qualitätskriterien entsprechen, wenn solche Datensätze verwendet werden.\n(2) Für Trainings-, Validierungs- und Testdatensätze gelten Daten-Governance- und Datenverwaltungsverfahren, die für die Zweckbestimmung des Hochrisiko-KI-Systems geeignet sind. Diese Verfahren betreffen insbesondere\na) die einschlägigen konzeptionellen Entscheidungen,\nb) die Datenerhebungsverfahren und die Herkunft der Daten und im Falle personenbezogener Daten den ursprünglichen Zweck der Datenerhebung,\nc) relevante Datenaufbereitungsvorgänge wie Annotation, Kennzeichnung, Bereinigung, Aktualisierung, Anreicherung und Aggregierung,\nd) die Aufstellung von Annahmen, insbesondere in Bezug auf die Informationen, die mit den Daten erfasst und dargestellt werden sollen,\ne) eine Bewertung der Verfügbarkeit, Menge und Eignung der benötigten Datensätze,\nf) eine Untersuchung im Hinblick auf mögliche Verzerrungen (Bias), die die Gesundheit und Sicherheit von Personen beeinträchtigen, sich negativ auf die Grundrechte auswirken oder zu einer nach den Rechtsvorschriften der Union verbotenen Diskriminierung führen könnten, insbesondere wenn die Datenausgaben die Eingaben für künftige Operationen beeinflussen,\ng) geeignete Maßnahmen zur Erkennung, Verhinderung und Abschwächung möglicher gemäß Buchstabe f ermittelter Verzerrungen,\nh) die Ermittlung relevanter Datenlücken oder Mängel, die der Einhaltung dieser Verordnung entgegenstehen, und wie diese Lücken und Mängel behoben werden können.\n(3) Die Trainings-, Validierungs- und Testdatensätze müssen im Hinblick auf die Zweckbestimmung relevant, hinreichend repräsentativ und so weit wie möglich fehlerfrei und vollständig sein. Sie müssen die geeigneten statistischen Merkmale, gegebenenfalls auch bezüglich der Personen oder Personengruppen, für die das Hochrisiko-KI-System bestimmungsgemäß verwendet werden soll, haben. Diese Merkmale der Datensätze können auf der Ebene einzelner Datensätze oder auf der Ebene einer Kombination davon erfüllt werden.\n(4) Die Datensätze müssen, soweit dies für die Zweckbestimmung erforderlich ist, die entsprechenden Merkmale oder Elemente berücksichtigen, die für die besonderen geografischen, kontextuellen, verhaltensbezogenen oder funktionalen Rahmenbedingungen, unter denen das Hochrisiko-KI-System bestimmungsgemäß verwendet werden soll, typisch sind.\n(5) Soweit dies für die Erkennung und Korrektur von Verzerrungen im Zusammenhang mit Hochrisiko-KI-Systemen im Einklang mit Absatz 2 Buchstaben f und g dieses Artikels unbedingt erforderlich ist, dürfen die Anbieter solcher Systeme ausnahmsweise besondere Kategorien personenbezogener Daten verarbeiten, wobei sie angemessene Vorkehrungen für den Schutz der Grundrechte und Grundfreiheiten natürlicher Personen treffen müssen. Zusätzlich zu den Bestimmungen der Verordnungen (EU) 2016/679 und (EU) 2018/1725 und der Richtlinie (EU) 2016/680 müssen alle folgenden Bedingungen erfüllt sein, damit eine solche Verarbeitung stattfinden kann:\na) Die Erkennung und Korrektur von Verzerrungen kann durch die Verarbeitung anderer Daten, einschließlich synthetischer oder anonymisierter Daten, nicht effektiv durchgeführt werden;\nb) die besonderen Kategorien personenbezogener Daten unterliegen technischen Beschränkungen einer Weiterverwendung der personenbezogenen Daten und modernsten Sicherheits- und Datenschutzmaßnahmen, einschließlich Pseudonymisierung;\nc) die besonderen Kategorien personenbezogener Daten unterliegen Maßnahmen, mit denen sichergestellt wird, dass die verarbeiteten personenbezogenen Daten gesichert, geschützt und Gegenstand angemessener Sicherheitsvorkehrungen sind, wozu auch strenge Kontrollen des Zugriffs und seine Dokumentation gehören, um Missbrauch zu verhindern und sicherzustellen, dass nur befugte Personen Zugang zu diesen personenbezogenen Daten mit angemessenen Vertraulichkeitspflichten haben;\nd) die besonderen Kategorien personenbezogener Daten werden nicht an Dritte übermittelt oder übertragen, noch haben diese Dritten anderweitigen Zugang zu diesen Daten;\ne) die besonderen Kategorien personenbezogener Daten werden gelöscht, sobald die Verzerrung korrigiert wurde oder das Ende der Speicherfrist für die personenbezogenen Daten erreicht ist, je nachdem, was zuerst eintritt;\nf) die Aufzeichnungen über Verarbeitungstätigkeiten gemäß den Verordnungen (EU) 2016/679 und (EU) 2018/1725 und der Richtlinie (EU) 2016/680 enthalten die Gründe, warum die Verarbeitung besonderer Kategorien personenbezogener Daten für die Erkennung und Korrektur von Verzerrungen unbedingt erforderlich war und warum dieses Ziel mit der Verarbeitung anderer Daten nicht erreicht werden konnte.\n(6) Bei der Entwicklung von Hochrisiko-KI-Systemen, in denen keine Techniken eingesetzt werden, bei denen KI-Modelle trainiert werden, gelten die Absätze 2 bis 5 nur für Testdatensätze."
    },
    {
      "chunk_idx": 215,
      "id": "355be535-f654-47e6-9608-95369e856b37",
      "title": "Art 11",
      "relevantChunksIds": [
        "817c9521-a9db-4149-99df-86ce3f02b215"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 11: Technical documentation\n1. The technical documentation of a high-risk AI system shall be drawn up before that system is placed on the market or put into service and shall be kept up-to date.\nThe technical documentation shall be drawn up in such a way as to demonstrate that the high-risk AI system complies with the requirements set out in this Section and to provide national competent authorities and notified bodies with the necessary information in a clear and comprehensive form to assess the compliance of the AI system with those requirements. It shall contain, at a minimum, the elements set out in Annex IV. SMEs, including start-ups, may provide the elements of the technical documentation specified in Annex IV in a simplified manner. To that end, the Commission shall establish a simplified technical documentation form targeted at the needs of small and microenterprises. Where an SME, including a start-up, opts to provide the information required in Annex IV in a simplified manner, it shall use the form referred to in this paragraph. Notified bodies shall accept the form for the purposes of the conformity assessment.\n2. Where a high-risk AI system related to a product covered by the Union harmonisation legislation listed in Section A of Annex I is placed on the market or put into service, a single set of technical documentation shall be drawn up containing all the information set out in paragraph 1, as well as the information required under those legal acts.\n3. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend Annex IV, where necessary, to ensure that, in light of technical progress, the technical documentation provides all the information necessary to assess the compliance of the system with the requirements set out in this Section.",
      "original_content": "### Artikel 11: Technische Dokumentation\n(1) Die technische Dokumentation eines Hochrisiko-KI-Systems wird erstellt, bevor dieses System in Verkehr gebracht oder in Betrieb genommen wird, und ist auf dem neuesten Stand zu halten.\nDie technische Dokumentation wird so erstellt, dass aus ihr der Nachweis hervorgeht, wie das Hochrisiko-KI-System die Anforderungen dieses Abschnitts erfüllt, und dass den zuständigen nationalen Behörden und den notifizierten Stellen die Informationen in klarer und verständlicher Form zur Verfügung stehen, die erforderlich sind, um zu beurteilen, ob das KI-System diese Anforderungen erfüllt. Sie enthält zumindest die in Anhang IV genannten Angaben. KMU, einschließlich Start-up-Unternehmen, können die in Anhang IV aufgeführten Elemente der technischen Dokumentation in vereinfachter Weise bereitstellen. Zu diesem Zweck erstellt die Kommission ein vereinfachtes Formular für die technische Dokumentation, das auf die Bedürfnisse von kleinen Unternehmen und Kleinstunternehmen zugeschnitten ist. Entscheidet sich ein KMU, einschließlich Start-up-Unternehmen, für eine vereinfachte Bereitstellung der in Anhang IV vorgeschriebenen Angaben, so verwendet es das in diesem Absatz genannte Formular. Die notifizierten Stellen akzeptieren das Formular für die Zwecke der Konformitätsbewertung.\n(2) Wird ein Hochrisiko-KI-System, das mit einem Produkt verbunden ist, das unter die in Anhang I Abschnitt A aufgeführten Harmonisierungsrechtsvorschriften der Union fällt, in Verkehr gebracht oder in Betrieb genommen, so wird eine einzige technische Dokumentation erstellt, die alle in Absatz 1 genannten Informationen sowie die nach diesen Rechtsakten erforderlichen Informationen enthält.\n(3) Die Kommission ist befugt, wenn dies nötig ist, gemäß Artikel 97 delegierte Rechtsakte zur Änderung des Anhangs IV zu erlassen, damit die technische Dokumentation in Anbetracht des technischen Fortschritts stets alle Informationen enthält, die erforderlich sind, um zu beurteilen, ob das System die Anforderungen dieses Abschnitts erfüllt."
    },
    {
      "chunk_idx": 216,
      "id": "40622ac2-1630-4d8b-a8a6-0aadd9c20248",
      "title": "Art 12",
      "relevantChunksIds": [
        "817c9521-a9db-4149-99df-86ce3f02b215"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 12: Record-keeping\n1. High-risk AI systems shall technically allow for the automatic recording of events (logs) over the lifetime of the system.\n2. In order to ensure a level of traceability of the functioning of a high-risk AI system that is appropriate to the intended purpose of the system, logging capabilities shall enable the recording of events relevant for:\n(a) identifying situations that may result in the high-risk AI system presenting a risk within the meaning of Article 79(1) or in a substantial modification;\n(b) facilitating the post-market monitoring referred to in Article 72; and\n\n(a) recording of the period of each use of the system (start date and time and end date and time of each use);\n(b) the reference database against which input data has been checked by the system;\n(c) the input data for which the search has led to a match;\n(d) the identification of the natural persons involved in the verification of the results, as referred to in Article 14(5).",
      "original_content": "### Artikel 12: Aufzeichnungspflichten\n(1) Die Technik der Hochrisiko-KI-Systeme muss die automatische Aufzeichnung von Ereignissen (im Folgenden „Protokollierung“) während des Lebenszyklus des Systems ermöglichen.\n(2) Zur Gewährleistung, dass das Funktionieren des Hochrisiko-KI-Systems in einem der Zweckbestimmung des Systems angemessenen Maße rückverfolgbar ist, ermöglichen die Protokollierungsfunktionen die Aufzeichnung von Ereignissen, die für Folgendes relevant sind:\na) die Ermittlung von Situationen, die dazu führen können, dass das Hochrisiko-KI-System ein Risiko im Sinne des Artikels 79 Absatz 1 birgt oder dass es zu einer wesentlichen Änderung kommt,\nb) die Erleichterung der Beobachtung nach dem Inverkehrbringen gemäß Artikel 72 und\nc) die Überwachung des Betriebs der Hochrisiko-KI-Systeme gemäß Artikel 26 Absatz 5. (3) Die Protokollierungsfunktionen der in Anhang III Nummer 1 Buchstabe a genannten Hochrisiko-KI-Systeme müssen zumindest Folgendes umfassen:\na) Aufzeichnung jedes Zeitraums der Verwendung des Systems (Datum und Uhrzeit des Beginns und des Endes jeder Verwendung);\nb) die Referenzdatenbank, mit der das System die Eingabedaten abgleicht;\nc) die Eingabedaten, mit denen die Abfrage zu einer Übereinstimmung geführt hat;\nd) die Identität der gemäß Artikel 14 Absatz 5 an der Überprüfung der Ergebnisse beteiligten natürlichen Personen."
    },
    {
      "chunk_idx": 217,
      "id": "e36b6aab-d7e1-44ce-8d20-d6bee82bff7d",
      "title": "Art 13",
      "relevantChunksIds": [
        "9032a3a6-c85d-43bc-b8da-fb8d7a40e48c"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 13: Transparency and provision of information to deployers\n1. High-risk AI systems shall be designed and developed in such a way as to ensure that their operation is sufficiently transparent to enable deployers to interpret a system’s output and use it appropriately. An appropriate type and degree of transparency shall be ensured with a view to achieving compliance with the relevant obligations of the provider and deployer set out in Section 3.\n2. High-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include concise, complete, correct and clear information that is relevant, accessible and comprehensible to deployers.\n3. The instructions for use shall contain at least the following information:\n(a) the identity and the contact details of the provider and, where applicable, of its authorised representative;\n(b) the characteristics, capabilities and limitations of performance of the high-risk AI system, including:\n(i) its intended purpose;\n(ii) the level of accuracy, including its metrics, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected, and any known and foreseeable circumstances that may have an impact on that expected level of accuracy, robustness and cybersecurity;\n(iii) any known or foreseeable circumstance, related to the use of the high-risk AI system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety or fundamental rights referred to in Article 9(2);\n(iv) where applicable, the technical capabilities and characteristics of the high-risk AI system to provide information that is relevant to explain its output;\n(v) when appropriate, its performance regarding specific persons or groups of persons on which the system is intended to be used;\n(vi) when appropriate, specifications for the input data, or any other relevant information in terms of the training, validation and testing data sets used, taking into account the intended purpose of the high-risk AI system;\n(vii) where applicable, information to enable deployers to interpret the output of the high-risk AI system and use it appropriately;\n(c) the changes to the high-risk AI system and its performance which have been pre-determined by the provider at the moment of the initial conformity assessment, if any;\n(d) the human oversight measures referred to in Article 14, including the technical measures put in place to facilitate the interpretation of the outputs of the high-risk AI systems by the deployers;\n(e) the computational and hardware resources needed, the expected lifetime of the high-risk AI system and any necessary maintenance and care measures, including their frequency, to ensure the proper functioning of that AI system, including as regards software updates;\n(f) where relevant, a description of the mechanisms included within the high-risk AI system that allows deployers to properly collect, store and interpret the logs in accordance with Article 12.",
      "original_content": "### Artikel 13: Transparenz und Bereitstellung von Informationen für die Betreiber\n(1) Hochrisiko-KI-Systeme werden so konzipiert und entwickelt, dass ihr Betrieb hinreichend transparent ist, damit die Betreiber die Ausgaben eines Systems angemessen interpretieren und verwenden können. Die Transparenz wird auf eine geeignete Art und in einem angemessenen Maß gewährleistet, damit die Anbieter und Betreiber ihre in Abschnitt 3 festgelegten einschlägigen Pflichten erfüllen können.\n(2) Hochrisiko-KI-Systeme werden mit Betriebsanleitungen in einem geeigneten digitalen Format bereitgestellt oder auf andere Weise mit Betriebsanleitungen versehen, die präzise, vollständige, korrekte und eindeutige Informationen in einer für die Betreiber relevanten, barrierefrei zugänglichen und verständlichen Form enthalten.\n(3) Die Betriebsanleitungen enthalten mindestens folgende Informationen:\na) den Namen und die Kontaktangaben des Anbieters sowie gegebenenfalls seines Bevollmächtigten;\nb) die Merkmale, Fähigkeiten und Leistungsgrenzen des Hochrisiko-KI-Systems, einschließlich\ni) seiner Zweckbestimmung,\nii) des Maßes an Genauigkeit — einschließlich diesbezüglicher Metriken —, Robustheit und Cybersicherheit gemäß Artikel 15, für das das Hochrisiko-KI-System getestet und validiert wurde und das zu erwarten ist, sowie aller bekannten und vorhersehbaren Umstände, die sich auf das erwartete Maß an Genauigkeit, Robustheit und Cybersicherheit auswirken können;\niii) aller bekannten oder vorhersehbaren Umstände bezüglich der Verwendung des Hochrisiko-KI-Systems im Einklang mit seiner Zweckbestimmung oder einer vernünftigerweise vorhersehbaren Fehlanwendung, die zu den in Artikel 9 Absatz 2 genannten Risiken für die Gesundheit und Sicherheit oder die Grundrechte führen können,\niv) gegebenenfalls der technischen Fähigkeiten und Merkmale des Hochrisiko-KI-Systems, um Informationen bereitzustellen, die zur Erläuterung seiner Ausgaben relevant sind;\nv) gegebenenfalls seiner Leistung in Bezug auf bestimmte Personen oder Personengruppen, auf die das System bestimmungsgemäß angewandt werden soll;\nvi) gegebenenfalls der Spezifikationen für die Eingabedaten oder sonstiger relevanter Informationen über die verwendeten Trainings-, Validierungs- und Testdatensätze, unter Berücksichtigung der Zweckbestimmung des Hochrisiko-KI-Systems;\nvii) gegebenenfalls Informationen, die es den Betreibern ermöglichen, die Ausgabe des Hochrisiko-KI-Systems zu interpretieren und es angemessen zu nutzen;\nc) etwaige Änderungen des Hochrisiko-KI-Systems und seiner Leistung, die der Anbieter zum Zeitpunkt der ersten Konformitätsbewertung vorab bestimmt hat;\nd) die in Artikel 14 genannten Maßnahmen zur Gewährleistung der menschlichen Aufsicht, einschließlich der technischen Maßnahmen, die getroffen wurden, um den Betreibern die Interpretation der Ausgaben von Hochrisiko-KI-Systemen zu erleichtern;\ne) die erforderlichen Rechen- und Hardware-Ressourcen, die erwartete Lebensdauer des Hochrisiko-KI-Systems und alle erforderlichen Wartungs- und Pflegemaßnahmen einschließlich deren Häufigkeit zur Gewährleistung des ordnungsgemäßen Funktionierens dieses KI-Systems, auch in Bezug auf Software-Updates;\nf) gegebenenfalls eine Beschreibung der in das Hochrisiko-KI-System integrierten Mechanismen, die es den Betreibern ermöglicht, die Protokolle im Einklang mit Artikel 12 ordnungsgemäß zu erfassen, zu speichern und auszuwerten."
    },
    {
      "chunk_idx": 218,
      "id": "67156c3d-d006-41f0-87e3-725cd933dbdb",
      "title": "Art 14",
      "relevantChunksIds": [
        "550c995f-0a8e-4714-b73e-9418fb982eed"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 14: Human oversight\n1. High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which they are in use.\n2. Human oversight shall aim to prevent or minimise the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular where such risks persist despite the application of other requirements set out in this Section.\n3. The oversight measures shall be commensurate with the risks, level of autonomy and context of use of the high-risk AI system, and shall be ensured through either one or both of the following types of measures:\n(a) measures identified and built, when technically feasible, into the high-risk AI system by the provider before it is placed on the market or put into service;\n(b) measures identified by the provider before placing the high-risk AI system on the market or putting it into service and that are appropriate to be implemented by the deployer.\n4. For the purpose of implementing paragraphs 1, 2 and 3, the high-risk AI system shall be provided to the deployer in such a way that natural persons to whom human oversight is assigned are enabled, as appropriate and proportionate:\n(a) to properly understand the relevant capacities and limitations of the high-risk AI system and be able to duly monitor its operation, including in view of detecting and addressing anomalies, dysfunctions and unexpected performance;\n(b) to remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system (automation bias), in particular for high-risk AI systems used to provide information or recommendations for decisions to be taken by natural persons;\n(c) to correctly interpret the high-risk AI system’s output, taking into account, for example, the interpretation tools and methods available;\n(d) to decide, in any particular situation, not to use the high-risk AI system or to otherwise disregard, override or reverse the output of the high-risk AI system;\n(e) to intervene in the operation of the high-risk AI system or interrupt the system through a ‘stop’ button or a similar procedure that allows the system to come to a halt in a safe state.\n5. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 of this Article shall be such as to ensure that, in addition, no action or decision is taken by the deployer on the basis of the identification resulting from the system unless that identification has been separately verified and confirmed by at least two natural persons with the necessary competence, training and authority.\nThe requirement for a separate verification by at least two natural persons shall not apply to high-risk AI systems used for the purposes of law enforcement, migration, border control or asylum, where Union or national law considers the application of this requirement to be disproportionate.",
      "original_content": "### Artikel 14: Menschliche Aufsicht\n(1) Hochrisiko-KI-Systeme werden so konzipiert und entwickelt, dass sie während der Dauer ihrer Verwendung — auch mit geeigneten Instrumenten einer Mensch-Maschine-Schnittstelle — von natürlichen Personen wirksam beaufsichtigt werden können.\n(2) Die menschliche Aufsicht dient der Verhinderung oder Minimierung der Risiken für Gesundheit, Sicherheit oder Grundrechte, die entstehen können, wenn ein Hochrisiko-KI-System im Einklang mit seiner Zweckbestimmung oder im Rahmen einer vernünftigerweise vorhersehbaren Fehlanwendung verwendet wird, insbesondere wenn solche Risiken trotz der Einhaltung anderer Anforderungen dieses Abschnitts fortbestehen.\n(3) Die Aufsichtsmaßnahmen müssen den Risiken, dem Grad der Autonomie und dem Kontext der Nutzung des Hochrisiko-KI-Systems angemessen sein und werden durch eine oder beide der folgenden Arten von Vorkehrungen gewährleistet:\na) Vorkehrungen, die vor dem Inverkehrbringen oder der Inbetriebnahme vom Anbieter bestimmt und, sofern technisch machbar, in das Hochrisiko-KI-System eingebaut werden;\nb) Vorkehrungen, die vor dem Inverkehrbringen oder der Inbetriebnahme des Hochrisiko-KI-Systems vom Anbieter bestimmt werden und dazu geeignet sind, vom Betreiber umgesetzt zu werden.\n(4) Für die Zwecke der Durchführung der Absätze 1, 2 und 3 wird das Hochrisiko-KI-System dem Betreiber so zur Verfügung gestellt, dass die natürlichen Personen, denen die menschliche Aufsicht übertragen wurde, angemessen und verhältnismäßig in der Lage sind,\na) die einschlägigen Fähigkeiten und Grenzen des Hochrisiko-KI-Systems angemessen zu verstehen und seinen Betrieb ordnungsgemäß zu überwachen, einschließlich in Bezug auf das Erkennen und Beheben von Anomalien, Fehlfunktionen und unerwarteter Leistung;\nb) sich einer möglichen Neigung zu einem automatischen oder übermäßigen Vertrauen in die von einem Hochrisiko-KI-System hervorgebrachte Ausgabe („Automatisierungsbias“) bewusst zu bleiben, insbesondere wenn Hochrisiko-KI-Systeme Informationen oder Empfehlungen ausgeben, auf deren Grundlage natürliche Personen Entscheidungen treffen;\nc) die Ausgabe des Hochrisiko-KI-Systems richtig zu interpretieren, wobei beispielsweise die vorhandenen Interpretationsinstrumente und -methoden zu berücksichtigen sind;\nd) in einer bestimmten Situation zu beschließen, das Hochrisiko-KI-System nicht zu verwenden oder die Ausgabe des Hochrisiko-KI-Systems außer Acht zu lassen, außer Kraft zu setzen oder rückgängig zu machen;\ne) in den Betrieb des Hochrisiko-KI-Systems einzugreifen oder den Systembetrieb mit einer „Stopptaste“ oder einem ähnlichen Verfahren zu unterbrechen, was dem System ermöglicht, in einem sicheren Zustand zum Stillstand zu kommen.\n(5) Bei den in Anhang III Nummer 1 Buchstabe a genannten Hochrisiko-KI-Systemen müssen die in Absatz 3 des vorliegenden Artikels genannten Vorkehrungen so gestaltet sein, dass außerdem der Betreiber keine Maßnahmen oder Entscheidungen allein aufgrund des vom System hervorgebrachten Identifizierungsergebnisses trifft, solange diese Identifizierung nicht von mindestens zwei natürlichen Personen, die die notwendige Kompetenz, Ausbildung und Befugnis besitzen, getrennt überprüft und bestätigt wurde.\nDie Anforderung einer getrennten Überprüfung durch mindestens zwei natürliche Personen gilt nicht für Hochrisiko-KI-Systeme, die für Zwecke in den Bereichen Strafverfolgung, Migration, Grenzkontrolle oder Asyl verwendet werden, wenn die Anwendung dieser Anforderung nach Unionsrecht oder nationalem Recht unverhältnismäßig wäre."
    },
    {
      "chunk_idx": 219,
      "id": "91bb9b2c-b571-4042-8143-f399e538ffe2",
      "title": "Art 15",
      "relevantChunksIds": [
        "3b55e0a7-53c5-443d-a628-f905ea635201",
        "c9ddca61-d229-4d40-b57a-f7c249377ecb",
        "f13362fc-1e4e-4ac2-80ec-5def8ff64f2a",
        "3c1ccff0-1d4e-4217-9828-90e5bb65d611",
        "63c8bc3d-ba5f-4134-97e6-4294cf1e2697"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 15: Accuracy, robustness and cybersecurity\n1. High-risk AI systems shall be designed and developed in such a way that they achieve an appropriate level of accuracy, robustness, and cybersecurity, and that they perform consistently in those respects throughout their lifecycle.\n2. To address the technical aspects of how to measure the appropriate levels of accuracy and robustness set out in paragraph 1 and any other relevant performance metrics, the Commission shall, in cooperation with relevant stakeholders and organisations such as metrology and benchmarking authorities, encourage, as appropriate, the development of benchmarks and measurement methodologies.\n3. The levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be declared in the accompanying instructions of use.\n4. High-risk AI systems shall be as resilient as possible regarding errors, faults or inconsistencies that may occur within the system or the environment in which the system operates, in particular due to their interaction with natural persons or other systems. Technical and organisational measures shall be taken in this regard.\nThe robustness of high-risk AI systems may be achieved through technical redundancy solutions, which may include backup or fail-safe plans.\nHigh-risk AI systems that continue to learn after being placed on the market or put into service shall be developed in such a way as to eliminate or reduce as far as possible the risk of possibly biased outputs influencing input for future operations (feedback loops), and as to ensure that any such feedback loops are duly addressed with appropriate mitigation measures.\n5. High-risk AI systems shall be resilient against attempts by unauthorised third parties to alter their use, outputs or performance by exploiting system vulnerabilities.\nThe technical solutions aiming to ensure the cybersecurity of high-risk AI systems shall be appropriate to the relevant circumstances and the risks.\nThe technical solutions to address AI specific vulnerabilities shall include, where appropriate, measures to prevent, detect, respond to, resolve and control for attacks trying to manipulate the training data set (data poisoning), or pre-trained components used in training (model poisoning), inputs designed to cause the AI model to make a mistake (adversarial examples or model evasion), confidentiality attacks or model flaws.",
      "original_content": "### Artikel 15: Genauigkeit, Robustheit und Cybersicherheit\n(1) Hochrisiko-KI-Systeme werden so konzipiert und entwickelt, dass sie ein angemessenes Maß an Genauigkeit, Robustheit und Cybersicherheit erreichen und in dieser Hinsicht während ihres gesamten Lebenszyklus beständig funktionieren.\n(2) Um die technischen Aspekte der Art und Weise der Messung des angemessenen Maßes an Genauigkeit und Robustheit gemäß Absatz 1 und anderer einschlägiger Leistungsmetriken anzugehen, fördert die Kommission in Zusammenarbeit mit einschlägigen Interessenträgern und Organisationen wie Metrologie- und Benchmarking-Behörden gegebenenfalls die Entwicklung von Benchmarks und Messmethoden.\n(3) Die Maße an Genauigkeit und die relevanten Genauigkeitsmetriken von Hochrisiko-KI-Systemen werden in den ihnen beigefügten Betriebsanleitungen angegeben.\n(4) Hochrisiko-KI-Systeme müssen so widerstandsfähig wie möglich gegenüber Fehlern, Störungen oder Unstimmigkeiten sein, die innerhalb des Systems oder der Umgebung, in der das System betrieben wird, insbesondere wegen seiner Interaktion mit natürlichen Personen oder anderen Systemen, auftreten können. In diesem Zusammenhang sind technische und organisatorische Maßnahmen zu ergreifen.\nDie Robustheit von Hochrisiko-KI-Systemen kann durch technische Redundanz erreicht werden, was auch Sicherungs- oder Störungssicherheitspläne umfassen kann.\nHochrisiko-KI-Systeme, die nach dem Inverkehrbringen oder der Inbetriebnahme weiterhin dazulernen, sind so zu entwickeln, dass das Risiko möglicherweise verzerrter Ausgaben, die künftige Vorgänge beeinflussen („Rückkopplungsschleifen“), beseitigt oder so gering wie möglich gehalten wird und sichergestellt wird, dass auf solche Rückkopplungsschleifen angemessen mit geeigneten Risikominderungsmaßnahmen eingegangen wird.\n(5) Hochrisiko-KI-Systeme müssen widerstandsfähig gegen Versuche unbefugter Dritter sein, ihre Verwendung, Ausgaben oder Leistung durch Ausnutzung von Systemschwachstellen zu verändern.\nDie technischen Lösungen zur Gewährleistung der Cybersicherheit von Hochrisiko-KI-Systemen müssen den jeweiligen Umständen und Risiken angemessen sein.\nDie technischen Lösungen für den Umgang mit KI-spezifischen Schwachstellen umfassen gegebenenfalls Maßnahmen, um Angriffe, mit denen versucht wird, eine Manipulation des Trainingsdatensatzes („data poisoning“) oder vortrainierter Komponenten, die beim Training verwendet werden („model poisoning“), vorzunehmen, Eingabedaten, die das KI-Modell zu Fehlern verleiten sollen („adversarial examples“ oder „model evasions“), Angriffe auf vertrauliche Daten oder Modellmängel zu verhüten, zu erkennen, darauf zu reagieren, sie zu beseitigen und zu kontrollieren."
    },
    {
      "chunk_idx": 220,
      "id": "86806ab4-b869-4e14-8e9f-39249fac5a50",
      "title": "Art 16",
      "relevantChunksIds": [
        "d5118a3b-1529-4362-9b1b-c99a8dd748ec",
        "0fa0541d-b4e5-48ce-b134-aff5e22c8eec"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "## SECTION 3: Obligations of providers and deployers of high-risk AI systems and other parties\n### Article 16: Obligations of providers of high-risk AI systems\nProviders of high-risk AI systems shall:\n(a) ensure that their high-risk AI systems are compliant with the requirements set out in Section 2;\n(b) indicate on the high-risk AI system or, where that is not possible, on its packaging or its accompanying documentation, as applicable, their name, registered trade name or registered trade mark, the address at which they can be contacted;\n(c) have a quality management system in place which complies with Article 17;\n(d) keep the documentation referred to in Article 18;\n(e) when under their control, keep the logs automatically generated by their high-risk AI systems as referred to in Article 19;\n(f) ensure that the high-risk AI system undergoes the relevant conformity assessment procedure as referred to in Article 43, prior to its being placed on the market or put into service;\n(g) draw up an EU declaration of conformity in accordance with Article 47;\n(h) affix the CE marking to the high-risk AI system or, where that is not possible, on its packaging or its accompanying documentation, to indicate conformity with this Regulation, in accordance with Article 48;\n(i) comply with the registration obligations referred to in Article 49(1);\n(j) take the necessary corrective actions and provide information as required in Article 20;\n(k) upon a reasoned request of a national competent authority, demonstrate the conformity of the high-risk AI system with the requirements set out in Section 2;\n(l) ensure that the high-risk AI system complies with accessibility requirements in accordance with Directives (EU) 2016/2102 and (EU) 2019/882.",
      "original_content": "## ABSCHNITT 3: Pflichten der Anbieter und Betreiber von Hochrisiko-KI-Systemen und anderer Beteiligter\n### Artikel 16: Pflichten der Anbieter von Hochrisiko-KI-Systemen\nAnbieter von Hochrisiko-KI-Systemen müssen\na) sicherstellen, dass ihre Hochrisiko-KI-Systeme die in Abschnitt 2 festgelegten Anforderungen erfüllen;\nb) auf dem Hochrisiko-KI-System oder, falls dies nicht möglich ist, auf seiner Verpackung oder in der beigefügten Dokumentation ihren Namen, ihren eingetragenen Handelsnamen bzw. ihre eingetragene Handelsmarke und ihre Kontaktanschrift angeben;\nc) über ein Qualitätsmanagementsystem verfügen, das Artikel 17 entspricht;\nd) die in Artikel 18 genannte Dokumentation aufbewahren;\ne) die von ihren Hochrisiko-KI-Systemen automatisch erzeugten Protokolle gemäß Artikel 19 aufbewahren, wenn diese ihrer Kontrolle unterliegen;\nf) sicherstellen, dass das Hochrisiko-KI-System dem betreffenden Konformitätsbewertungsverfahren gemäß Artikel 43 unterzogen wird, bevor es in Verkehr gebracht oder in Betrieb genommen wird;\ng) eine EU-Konformitätserklärung gemäß Artikel 47 ausstellen;\nh) die CE-Kennzeichnung an das Hochrisiko-KI-System oder, falls dies nicht möglich ist, auf seiner Verpackung oder in der beigefügten Dokumentation anbringen, um Konformität mit dieser Verordnung gemäß Artikel 48 anzuzeigen;\ni) den in Artikel 49 Absatz 1 genannten Registrierungspflichten nachkommen;\nj) die erforderlichen Korrekturmaßnahmen ergreifen und die gemäß Artikel 20 erforderlichen Informationen bereitstellen;\nk) auf begründete Anfrage einer zuständigen nationalen Behörde nachweisen, dass das Hochrisiko-KI-System die Anforderungen in Abschnitt 2 erfüllt;\nl) sicherstellen, dass das Hochrisiko-KI-System die Barrierefreiheitsanforderungen gemäß den Richtlinien (EU) 2016/2102 und (EU) 2019/882 erfüllt."
    },
    {
      "chunk_idx": 221,
      "id": "9cd5b042-8095-49be-8ad3-51f2fb94e473",
      "title": "Art 17",
      "relevantChunksIds": [
        "da5fac2f-a0db-40a4-8e4c-03c6125fa585"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 17: Quality management system\n1. Providers of high-risk AI systems shall put a quality management system in place that ensures compliance with this Regulation. That system shall be documented in a systematic and orderly manner in the form of written policies, procedures and instructions, and shall include at least the following aspects:\n(a) a strategy for regulatory compliance, including compliance with conformity assessment procedures and procedures for the management of modifications to the high-risk AI system;\n(b) techniques, procedures and systematic actions to be used for the design, design control and design verification of the high-risk AI system;\n(c) techniques, procedures and systematic actions to be used for the development, quality control and quality assurance of the high-risk AI system;\n(d) examination, test and validation procedures to be carried out before, during and after the development of the high-risk AI system, and the frequency with which they have to be carried out;\n(e) technical specifications, including standards, to be applied and, where the relevant harmonised standards are not applied in full or do not cover all of the relevant requirements set out in Section 2, the means to be used to ensure that the high-risk AI system complies with those requirements;\n(f) systems and procedures for data management, including data acquisition, data collection, data analysis, data labelling, data storage, data filtration, data mining, data aggregation, data retention and any other operation regarding the data that is performed before and for the purpose of the placing on the market or the putting into service of high-risk AI systems;\n(g) the risk management system referred to in Article 9;\n(h) the setting-up, implementation and maintenance of a post-market monitoring system, in accordance with Article 72;\n(i) procedures related to the reporting of a serious incident in accordance with Article 73;\n(j) the handling of communication with national competent authorities, other relevant authorities, including those providing or supporting the access to data, notified bodies, other operators, customers or other interested parties;\n(k) systems and procedures for record-keeping of all relevant documentation and information;\n(l) resource management, including security-of-supply related measures;\n(m) an accountability framework setting out the responsibilities of the management and other staff with regard to all the aspects listed in this paragraph.\n2. The implementation of the aspects referred to in paragraph 1 shall be proportionate to the size of the provider’s organisation. Providers shall, in any event, respect the degree of rigour and the level of protection required to ensure the compliance of their high-risk AI systems with this Regulation.\n3. Providers of high-risk AI systems that are subject to obligations regarding quality management systems or an equivalent function under relevant sectoral Union law may include the aspects listed in paragraph 1 as part of the quality management systems pursuant to that law.\n4. For providers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law, the obligation to put in place a quality management system, with the exception of paragraph 1, points (g), (h) and (i) of this Article, shall be deemed to be fulfilled by complying with the rules on internal governance arrangements or processes pursuant to the relevant Union financial services law. To that end, any harmonised standards referred to in Article 40 shall be taken into account.",
      "original_content": "### Artikel 17: Qualitätsmanagementsystem\n(1) Anbieter von Hochrisiko-KI-Systemen richten ein Qualitätsmanagementsystem ein, das die Einhaltung dieser Verordnung gewährleistet. Dieses System wird systematisch und ordnungsgemäß in Form schriftlicher Regeln, Verfahren und Anweisungen dokumentiert und umfasst mindestens folgende Aspekte:\na) ein Konzept zur Einhaltung der Regulierungsvorschriften, was die Einhaltung der Konformitätsbewertungsverfahren und der Verfahren für das Management von Änderungen an dem Hochrisiko-KI-System miteinschließt;\nb) Techniken, Verfahren und systematische Maßnahmen für den Entwurf, die Entwurfskontrolle und die Entwurfsprüfung des Hochrisiko-KI-Systems;\nc) Techniken, Verfahren und systematische Maßnahmen für die Entwicklung, Qualitätskontrolle und Qualitätssicherung des Hochrisiko-KI-Systems;\nd) Untersuchungs-, Test- und Validierungsverfahren, die vor, während und nach der Entwicklung des Hochrisiko-KI-Systems durchzuführen sind, und die Häufigkeit der Durchführung;\ne) die technischen Spezifikationen und Normen, die anzuwenden sind und, falls die einschlägigen harmonisierten Normen nicht vollständig angewandt werden oder sie nicht alle relevanten Anforderungen gemäß Abschnitt 2 abdecken, die Mittel, mit denen gewährleistet werden soll, dass das Hochrisiko-KI-System diese Anforderungen erfüllt;\nf) Systeme und Verfahren für das Datenmanagement, einschließlich Datengewinnung, Datenerhebung, Datenanalyse, Datenkennzeichnung, Datenspeicherung, Datenfilterung, Datenauswertung, Datenaggregation, Vorratsdatenspeicherung und sonstiger Vorgänge in Bezug auf die Daten, die im Vorfeld und für die Zwecke des Inverkehrbringens oder der Inbetriebnahme von Hochrisiko-KI-Systemen durchgeführt werden;\ng) das in Artikel 9 genannte Risikomanagementsystem;\nh) die Einrichtung, Anwendung und Aufrechterhaltung eines Systems zur Beobachtung nach dem Inverkehrbringen gemäß Artikel 72;\ni) Verfahren zur Meldung eines schwerwiegenden Vorfalls gemäß Artikel 73;\nj) die Handhabung der Kommunikation mit zuständigen nationalen Behörden, anderen einschlägigen Behörden, auch Behörden, die den Zugang zu Daten gewähren oder erleichtern, notifizierten Stellen, anderen Akteuren, Kunden oder sonstigen interessierten Kreisen;\nk) Systeme und Verfahren für die Aufzeichnung sämtlicher einschlägigen Dokumentation und Informationen;\nl) Ressourcenmanagement, einschließlich Maßnahmen im Hinblick auf die Versorgungssicherheit;\nm) einen Rechenschaftsrahmen, der die Verantwortlichkeiten der Leitung und des sonstigen Personals in Bezug auf alle in diesem Absatz aufgeführten Aspekte regelt.\n(2) Die Umsetzung der in Absatz 1 genannten Aspekte erfolgt in einem angemessenen Verhältnis zur Größe der Organisation des Anbieters. Die Anbieter müssen in jedem Fall den Grad der Strenge und das Schutzniveau einhalten, die erforderlich sind, um die Übereinstimmung ihrer Hochrisiko-KI-Systeme mit dieser Verordnung sicherzustellen.\n(3) Anbieter von Hochrisiko-KI-Systemen, die Pflichten in Bezug auf Qualitätsmanagementsysteme oder eine gleichwertige Funktion gemäß den sektorspezifischen Rechtsvorschriften der Union unterliegen, können die in Absatz 1 aufgeführten Aspekte als Bestandteil der nach den genannten Rechtsvorschriften festgelegten Qualitätsmanagementsysteme einbeziehen.\n(4) Bei Anbietern, die Finanzinstitute sind und gemäß den Rechtsvorschriften der Union über Finanzdienstleistungen Anforderungen in Bezug auf ihre Regelungen oder Verfahren der internen Unternehmensführung unterliegen, gilt die Pflicht zur Einrichtung eines Qualitätsmanagementsystems — mit Ausnahme des Absatzes 1 Buchstaben g, h und i des vorliegenden Artikels — als erfüllt, wenn die Vorschriften über Regelungen oder Verfahren der internen Unternehmensführung gemäß dem einschlägigen Unionsrecht über Finanzdienstleistungen eingehalten werden. Zu diesem Zweck werden die in Artikel 40 genannten harmonisierten Normen berücksichtigt."
    },
    {
      "chunk_idx": 222,
      "id": "b4faa5d7-2c2b-43c9-b793-47d6579e2fc8",
      "title": "Art 18",
      "relevantChunksIds": [
        "817c9521-a9db-4149-99df-86ce3f02b215",
        "da5fac2f-a0db-40a4-8e4c-03c6125fa585"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 18: Documentation keeping\n1. The provider shall, for a period ending 10 years after the high-risk AI system has been placed on the market or put into service, keep at the disposal of the national competent authorities:\n(a) the technical documentation referred to in Article 11;\n(b) the documentation concerning the quality management system referred to in Article 17;\n(c) the documentation concerning the changes approved by notified bodies, where applicable;\n(d) the decisions and other documents issued by the notified bodies, where applicable;\n(e) the EU declaration of conformity referred to in Article 47.\n2. Each Member State shall determine conditions under which the documentation referred to in paragraph 1 remains at the disposal of the national competent authorities for the period indicated in that paragraph for the cases when a provider or its authorised representative established on its territory goes bankrupt or ceases its activity prior to the end of that period.\n3. Providers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law shall maintain the technical documentation as part of the documentation kept under the relevant Union financial services law.",
      "original_content": "### Artikel 18: Aufbewahrung der Dokumentation\n(1) Der Anbieter hält für einen Zeitraum von zehn Jahren ab dem Inverkehrbringen oder der Inbetriebnahme des Hochrisiko-KI-Systems folgende Unterlagen für die zuständigen nationalen Behörden bereit:\na) die in Artikel 11 genannte technische Dokumentation;\nb) die Dokumentation zu dem in Artikel 17 genannten Qualitätsmanagementsystem;\nc) die Dokumentation über etwaige von notifizierten Stellen genehmigte Änderungen;\nd) gegebenenfalls die von den notifizierten Stellen ausgestellten Entscheidungen und sonstigen Dokumente;\ne) die in Artikel 47 genannte EU-Konformitätserklärung.\n(2) Jeder Mitgliedstaat legt die Bedingungen fest, unter denen die in Absatz 1 genannte Dokumentation für die zuständigen nationalen Behörden für den in dem genannten Absatz angegebenen Zeitraum bereitgehalten wird, für den Fall, dass ein Anbieter oder sein in demselben Hoheitsgebiet niedergelassener Bevollmächtigter vor Ende dieses Zeitraums in Konkurs geht oder seine Tätigkeit aufgibt.\n(3) Anbieter, die Finanzinstitute sind und gemäß dem Unionsrecht über Finanzdienstleistungen Anforderungen in Bezug auf ihre Regelungen oder Verfahren der internen Unternehmensführung unterliegen, pflegen die technische Dokumentation als Teil der gemäß dem Unionsrecht über Finanzdienstleistungen aufzubewahrenden Dokumentation."
    },
    {
      "chunk_idx": 223,
      "id": "45c3bd82-bba0-434b-96d1-a4036c68969c",
      "title": "Art 19",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 19: Automatically generated logs\n1. Providers of high-risk AI systems shall keep the logs referred to in Article 12(1), automatically generated by their high-risk AI systems, to the extent such logs are under their control. Without prejudice to applicable Union or national law, the logs shall be kept for a period appropriate to the intended purpose of the high-risk AI system, of at least six months, unless provided otherwise in the applicable Union or national law, in particular in Union law on the protection of personal data.\n2. Providers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law shall maintain the logs automatically generated by their high-risk AI systems as part of the documentation kept under the relevant financial services law.",
      "original_content": "### Artikel 19: Automatisch erzeugte Protokolle\n(1) Anbieter von Hochrisiko-KI-Systemen bewahren die von ihren Hochrisiko-KI-Systemen automatisch erzeugten Protokolle gemäß Artikel 12 Absatz 1 auf, soweit diese Protokolle ihrer Kontrolle unterliegen. Unbeschadet des geltenden Unionsrechts oder nationalen Rechts werden die Protokolle für einen der Zweckbestimmung des Hochrisiko-KI-Systems angemessenen Zeitraum von mindestens sechs Monaten aufbewahrt, sofern in den geltenden Rechtsvorschriften der Union, insbesondere im Unionsrecht zum Schutz personenbezogener Daten, oder im geltenden nationalen Recht nichts anderes vorgesehen ist.\n(2) Anbieter, die Finanzinstitute sind und gemäß den Rechtsvorschriften der Union über Finanzdienstleistungen Anforderungen in Bezug auf ihre Regelungen oder Verfahren der internen Unternehmensführung, unterliegen, bewahren die von ihren Hochrisiko-KI-Systemen automatisch erzeugten Protokolle als Teil der gemäß dem einschlägigen Unionsrecht über Finanzdienstleistungen aufzubewahrenden Dokumentation auf."
    },
    {
      "chunk_idx": 224,
      "id": "d74611d3-9a18-4ee9-86e8-953a959a7350",
      "title": "Art 20",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 20: Corrective actions and duty of information\n1. Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system that they have placed on the market or put into service is not in conformity with this Regulation shall immediately take the necessary corrective actions to bring that system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They shall inform the distributors of the high-risk AI system concerned and, where applicable, the deployers, the authorised representative and importers accordingly.\n2. Where the high-risk AI system presents a risk within the meaning of Article 79(1) and the provider becomes aware of that risk, it shall immediately investigate the causes, in collaboration with the reporting deployer, where applicable, and inform the market surveillance authorities competent for the high-risk AI system concerned and, where applicable, the notified body that issued a certificate for that high-risk AI system in accordance with Article 44, in particular, of the nature of the non-compliance and of any relevant corrective action taken.",
      "original_content": "### Artikel 20: Korrekturmaßnahmen und Informationspflicht\n(1) Anbieter von Hochrisiko-KI-Systemen, die der Auffassung sind oder Grund zu der Annahme haben, dass ein von ihnen in Verkehr gebrachtes oder in Betrieb genommenes Hochrisiko-KI-System nicht dieser Verordnung entspricht, ergreifen unverzüglich die erforderlichen Korrekturmaßnahmen, um die Konformität dieses Systems herzustellen oder es gegebenenfalls zurückzunehmen, zu deaktivieren oder zurückzurufen. Sie informieren die Händler des betreffenden Hochrisiko-KI-Systems und gegebenenfalls die Betreiber, den Bevollmächtigten und die Einführer darüber.\n(2) Birgt das Hochrisiko-KI-System ein Risiko im Sinne des Artikels 79 Absatz 1 und wird sich der Anbieter des Systems dieses Risikos bewusst, so führt er unverzüglich gegebenenfalls gemeinsam mit dem meldenden Betreiber eine Untersuchung der Ursachen durch und informiert er die Marktüberwachungsbehörden, in deren Zuständigkeit das betroffene Hochrisiko-KI-System fällt, und gegebenenfalls die notifizierte Stelle, die eine Bescheinigung für dieses Hochrisiko-KI-System gemäß Artikel 44 ausgestellt hat, insbesondere über die Art der Nichtkonformität und über bereits ergriffene relevante Korrekturmaßnahmen."
    },
    {
      "chunk_idx": 225,
      "id": "ff73c457-0b7d-4690-9417-d68464a484ea",
      "title": "Art 21",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 21: Cooperation with competent authorities\n1. Providers of high-risk AI systems shall, upon a reasoned request by a competent authority, provide that authority all the information and documentation necessary to demonstrate the conformity of the high-risk AI system with the requirements set out in Section 2, in a language which can be easily understood by the authority in one of the official languages of the institutions of the Union as indicated by the Member State concerned.\n2. Upon a reasoned request by a competent authority, providers shall also give the requesting competent authority, as applicable, access to the automatically generated logs of the high-risk AI system referred to in Article 12(1), to the extent such logs are under their control.\n3. Any information obtained by a competent authority pursuant to this Article shall be treated in accordance with the confidentiality obligations set out in Article 78.",
      "original_content": "### Artikel 21: Zusammenarbeit mit den zuständigen Behörden\n(1) Anbieter von Hochrisiko-KI-Systemen übermitteln einer zuständigen Behörde auf deren begründete Anfrage sämtliche Informationen und Dokumentation, die erforderlich sind, um die Konformität des Hochrisiko-KI-Systems mit den in Abschnitt 2 festgelegten Anforderungen nachzuweisen, und zwar in einer Sprache, die für die Behörde leicht verständlich ist und bei der es sich um eine der von dem betreffenden Mitgliedstaat angegebenen Amtssprachen der Institutionen der Union handelt.\n(2) Auf begründete Anfrage einer zuständigen Behörde gewähren die Anbieter der anfragenden zuständigen Behörde gegebenenfalls auch Zugang zu den automatisch erzeugten Protokollen des Hochrisiko-KI-Systems gemäß Artikel 12 Absatz 1, soweit diese Protokolle ihrer Kontrolle unterliegen.\n(3) Alle Informationen, die eine zuständige Behörde aufgrund dieses Artikels erhält, werden im Einklang mit den in Artikel 78 festgelegten Vertraulichkeitspflichten behandelt."
    },
    {
      "chunk_idx": 226,
      "id": "19b32d95-81d7-4097-8ca6-ca2c6c0358df",
      "title": "Art 22",
      "relevantChunksIds": [
        "86e3f5bd-eef6-4109-ab46-aedd548f4a71"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 22: Authorised representatives of providers of high-risk AI systems\n1. Prior to making their high-risk AI systems available on the Union market, providers established in third countries shall, by written mandate, appoint an authorised representative which is established in the Union.\n2. The provider shall enable its authorised representative to perform the tasks specified in the mandate received from the provider.\n3. The authorised representative shall perform the tasks specified in the mandate received from the provider. It shall provide a copy of the mandate to the market surveillance authorities upon request, in one of the official languages of the institutions of the Union, as indicated by the competent authority. For the purposes of this Regulation, the mandate shall empower the authorised representative to carry out the following tasks:\n(a) verify that the EU declaration of conformity referred to in Article 47 and the technical documentation referred to in Article 11 have been drawn up and that an appropriate conformity assessment procedure has been carried out by the provider;\n(b) keep at the disposal of the competent authorities and national authorities or bodies referred to in Article 74(10), for a period of 10 years after the high-risk AI system has been placed on the market or put into service, the contact details of the provider that appointed the authorised representative, a copy of the EU declaration of conformity referred to in Article 47, the technical documentation and, if applicable, the certificate issued by the notified body;\n(c) provide a competent authority, upon a reasoned request, with all the information and documentation, including that referred to in point (b) of this subparagraph, necessary to demonstrate the conformity of a high-risk AI system with the requirements set out in Section 2, including access to the logs, as referred to in Article 12(1), automatically generated by the high-risk AI system, to the extent such logs are under the control of the provider;\n(d) cooperate with competent authorities, upon a reasoned request, in any action the latter take in relation to the high-risk AI system, in particular to reduce and mitigate the risks posed by the high-risk AI system;\n(e) where applicable, comply with the registration obligations referred to in Article 49(1), or, if the registration is carried out by the provider itself, ensure that the information referred to in point 3 of Section A of Annex VIII is correct.\nThe mandate shall empower the authorised representative to be addressed, in addition to or instead of the provider, by the competent authorities, on all issues related to ensuring compliance with this Regulation.\n4. The authorised representative shall terminate the mandate if it considers or has reason to consider the provider to be acting contrary to its obligations pursuant to this Regulation. In such a case, it shall immediately inform the relevant market surveillance authority, as well as, where applicable, the relevant notified body, about the termination of the mandate and the reasons therefor.",
      "original_content": "### Artikel 22: Bevollmächtigte der Anbieter von Hochrisiko-KI-Systemen\n(1) Anbieter, die in Drittländern niedergelassen sind, benennen vor der Bereitstellung ihrer Hochrisiko-KI-Systeme auf dem Unionsmarkt schriftlich einen in der Union niedergelassenen Bevollmächtigten.\n(2) Der Anbieter muss seinem Bevollmächtigten ermöglichen, die Aufgaben wahrzunehmen, die im vom Anbieter erhaltenen Auftrag festgelegt sind.\n(3) Der Bevollmächtigte nimmt die Aufgaben wahr, die in seinem vom Anbieter erhaltenen Auftrag festgelegt sind. Er stellt den Marktüberwachungsbehörden auf Anfrage eine Kopie des Auftrags in einer von der zuständigen Behörde angegebenen Amtssprache der Institutionen der Union bereit. Für die Zwecke dieser Verordnung ermächtigt der Auftrag den Bevollmächtigten zumindest zur Wahrnehmung folgender Aufgaben:\na) Überprüfung, ob die in Artikel 47 genannte EU-Konformitätserklärung und die technische Dokumentation gemäß Artikel 11 erstellt wurden und ob der Anbieter ein angemessenes Konformitätsbewertungsverfahren durchgeführt hat;\nb) Bereithaltung — für einen Zeitraum von zehn Jahren ab dem Inverkehrbringen oder der Inbetriebnahme des Hochrisiko-KI-Systems — der Kontaktdaten des Anbieters, der den Bevollmächtigten benannt hat, eines Exemplars der in Artikel 47 genannten EU-Konformitätserklärung, der technischen Dokumentation und gegebenenfalls der von der notifizierten Stelle ausgestellten Bescheinigung für die zuständigen Behörden und die in Artikel 74 Absatz 10 genannten nationalen Behörden oder Stellen;\nc) Übermittlung sämtlicher — auch der unter Buchstabe b dieses Unterabsatzes genannten — Informationen und Dokumentation, die erforderlich sind, um die Konformität eines Hochrisiko-KI-Systems mit den in Abschnitt 2 festgelegten Anforderungen nachzuweisen, an eine zuständige Behörde auf deren begründete Anfrage, einschließlich der Gewährung des Zugangs zu den vom Hochrisiko-KI-System automatisch erzeugten Protokollen gemäß Artikel 12 Absatz 1, soweit diese Protokolle der Kontrolle des Anbieters unterliegen;\nd) Zusammenarbeit mit den zuständigen Behörden auf deren begründete Anfrage bei allen Maßnahmen, die Letztere im Zusammenhang mit dem Hochrisiko-KI-System ergreifen, um insbesondere die von dem Hochrisiko-KI-System ausgehenden Risiken zu verringern und abzumildern;\ne) gegebenenfalls die Einhaltung der Registrierungspflichten gemäß Artikel 49 Absatz 1 oder, falls die Registrierung vom Anbieter selbst vorgenommen wird, Sicherstellung der Richtigkeit der in Anhang VIII Abschnitt A Nummer 3 aufgeführten Informationen.\nMit dem Auftrag wird der Bevollmächtigte ermächtigt, neben oder anstelle des Anbieters als Ansprechpartner für die zuständigen Behörden in allen Fragen zu dienen, die die Gewährleistung der Einhaltung dieser Verordnung betreffen.\n(4) Der Bevollmächtigte beendet den Auftrag, wenn er der Auffassung ist oder Grund zu der Annahme hat, dass der Anbieter gegen seine Pflichten gemäß dieser Verordnung verstößt. In diesem Fall informiert er unverzüglich die betreffende Marktüberwachungsbehörde und gegebenenfalls die betreffende notifizierte Stelle über die Beendigung des Auftrags und deren Gründe."
    },
    {
      "chunk_idx": 227,
      "id": "9d898b6d-782d-4626-8838-1296950b5b7c",
      "title": "Art 23",
      "relevantChunksIds": [
        "0a83841c-2931-4e56-a212-b73e3ab410f1"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 23: Obligations of importers\n1. Before placing a high-risk AI system on the market, importers shall ensure that the system is in conformity with this Regulation by verifying that:\n(a) the relevant conformity assessment procedure referred to in Article 43 has been carried out by the provider of the high-risk AI system;\n(b) the provider has drawn up the technical documentation in accordance with Article 11 and Annex IV;\n(c) the system bears the required CE marking and is accompanied by the EU declaration of conformity referred to in Article 47 and instructions for use;\n(d) the provider has appointed an authorised representative in accordance with Article 22(1).\n2. Where an importer has sufficient reason to consider that a high-risk AI system is not in conformity with this Regulation, or is falsified, or accompanied by falsified documentation, it shall not place the system on the market until it has been brought into conformity. Where the high-risk AI system presents a risk within the meaning of Article 79(1), the importer shall inform the provider of the system, the authorised representative and the market surveillance authorities to that effect.\n3. Importers shall indicate their name, registered trade name or registered trade mark, and the address at which they can be contacted on the high-risk AI system and on its packaging or its accompanying documentation, where applicable.\n4. Importers shall ensure that, while a high-risk AI system is under their responsibility, storage or transport conditions, where applicable, do not jeopardise its compliance with the requirements set out in Section 2.\n5. Importers shall keep, for a period of 10 years after the high-risk AI system has been placed on the market or put into service, a copy of the certificate issued by the notified body, where applicable, of the instructions for use, and of the EU declaration of conformity referred to in Article 47.\n6. Importers shall provide the relevant competent authorities, upon a reasoned request, with all the necessary information and documentation, including that referred to in paragraph 5, to demonstrate the conformity of a high-risk AI system with the requirements set out in Section 2 in a language which can be easily understood by them. For this purpose, they shall also ensure that the technical documentation can be made available to those authorities.\n7. Importers shall cooperate with the relevant competent authorities in any action those authorities take in relation to a high-risk AI system placed on the market by the importers, in particular to reduce and mitigate the risks posed by it.",
      "original_content": "### Artikel 23: Pflichten der Einführer\n(1) Bevor sie ein Hochrisiko-KI-System in Verkehr bringen, stellen die Einführer sicher, dass das System dieser Verordnung entspricht, indem sie überprüfen, ob\na) der Anbieter des Hochrisiko-KI-Systems das entsprechende Konformitätsbewertungsverfahren gemäß Artikel 43 durchgeführt hat;\nb) der Anbieter die technische Dokumentation gemäß Artikel 11 und Anhang IV erstellt hat;\nc) das System mit der erforderlichen CE-Kennzeichnung versehen ist und ihm die in Artikel 47 genannte EU-Konformitätserklärung und Betriebsanleitungen beigefügt sind;\nd) der Anbieter einen Bevollmächtigten gemäß Artikel 22 Absatz 1 benannt hat.\n(2) Hat ein Einführer hinreichenden Grund zu der Annahme, dass ein Hochrisiko-KI-System nicht dieser Verordnung entspricht oder gefälscht ist oder diesem eine gefälschte Dokumentation beigefügt ist, so bringt er das System erst in Verkehr, nachdem dessen Konformität hergestellt wurde. Birgt das Hochrisiko-KI-System ein Risiko im Sinne des Artikels 79 Absatz 1, so informiert der Einführer den Anbieter des Systems, die Bevollmächtigten und die Marktüberwachungsbehörden darüber.\n(3) Die Einführer geben ihren Namen, ihren eingetragenen Handelsnamen oder ihre eingetragene Handelsmarke und die Anschrift, unter der sie in Bezug auf das Hochrisiko-KI-System kontaktiert werden können, auf der Verpackung oder gegebenenfalls in der beigefügten Dokumentation an.\n(4) Solange sich ein Hochrisiko-KI-System in ihrer Verantwortung befindet, gewährleisten Einführer, dass — soweit zutreffend — die Lagerungs- oder Transportbedingungen seine Konformität mit den in Abschnitt 2 festgelegten Anforderungen nicht beeinträchtigen.\n(5) Die Einführer halten für einen Zeitraum von zehn Jahren ab dem Inverkehrbringen oder der Inbetriebnahme des Hochrisiko-KI-Systems ein Exemplar der von der notifizierten Stelle ausgestellten Bescheinigung sowie gegebenenfalls die Betriebsanleitungen und die in Artikel 47 genannte EU-Konformitätserklärung bereit.\n(6) Die Einführer übermitteln den betreffenden nationalen Behörden auf deren begründete Anfrage sämtliche — auch die in Absatz 5 genannten — Informationen und Dokumentation, die erforderlich sind, um die Konformität des Hochrisiko-KI-Systems mit den in Abschnitt 2 festgelegten Anforderungen nachzuweisen, und zwar in einer Sprache, die für jene leicht verständlich ist. Zu diesem Zweck stellen sie auch sicher, dass diesen Behörden die technische Dokumentation zur Verfügung gestellt werden kann.\n(7) Die Einführer arbeiten mit den betreffenden nationalen Behörden bei allen Maßnahmen zusammen, die diese Behörden im Zusammenhang mit einem von den Einführern in Verkehr gebrachten Hochrisiko-KI-System ergreifen, um insbesondere die von diesem System ausgehenden Risiken zu verringern und abzumildern."
    },
    {
      "chunk_idx": 228,
      "id": "984b9ce1-d58b-474f-8062-48d47f385b5d",
      "title": "Art 24",
      "relevantChunksIds": [
        "0a83841c-2931-4e56-a212-b73e3ab410f1"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 24: Obligations of distributors\n1. Before making a high-risk AI system available on the market, distributors shall verify that it bears the required CE marking, that it is accompanied by a copy of the EU declaration of conformity referred to in Article 47 and instructions for use, and that the provider and the importer of that system, as applicable, have complied with their respective obligations as laid down in Article 16, points (b) and (c) and Article 23(3).\n2. Where a distributor considers or has reason to consider, on the basis of the information in its possession, that a high-risk AI system is not in conformity with the requirements set out in Section 2, it shall not make the high-risk AI system available on the market until the system has been brought into conformity with those requirements. Furthermore, where the high-risk AI system presents a risk within the meaning of Article 79(1), the distributor shall inform the provider or the importer of the system, as applicable, to that effect.\n3. Distributors shall ensure that, while a high-risk AI system is under their responsibility, storage or transport conditions, where applicable, do not jeopardise the compliance of the system with the requirements set out in Section 2.\n4. A distributor that considers or has reason to consider, on the basis of the information in its possession, a high-risk AI system which it has made available on the market not to be in conformity with the requirements set out in Section 2, shall take the corrective actions necessary to bring that system into conformity with those requirements, to withdraw it or recall it, or shall ensure that the provider, the importer or any relevant operator, as appropriate, takes those corrective actions. Where the high-risk AI system presents a risk within the meaning of Article 79(1), the distributor shall immediately inform the provider or importer of the system and the authorities competent for the high-risk AI system concerned, giving details, in particular, of the non-compliance and of any corrective actions taken.\n5. Upon a reasoned request from a relevant competent authority, distributors of a high-risk AI system shall provide that authority with all the information and documentation regarding their actions pursuant to paragraphs 1 to 4 necessary to demonstrate the conformity of that system with the requirements set out in Section 2.\n6. Distributors shall cooperate with the relevant competent authorities in any action those authorities take in relation to a high-risk AI system made available on the market by the distributors, in particular to reduce or mitigate the risk posed by it.",
      "original_content": "### Artikel 24: Pflichten der Händler\n(1) Bevor Händler ein Hochrisiko-KI-System auf dem Markt bereitstellen, überprüfen sie, ob es mit der erforderlichen CE-Kennzeichnung versehen ist, ob ihm eine Kopie der in Artikel 47 genannten EU-Konformitätserklärung und Betriebsanleitungen beigefügt sind und ob der Anbieter und gegebenenfalls der Einführer dieses Systems ihre in Artikel 16 Buchstaben b und c sowie Artikel 23 Absatz 3 festgelegten jeweiligen Pflichten erfüllt haben.\n(2) Ist ein Händler der Auffassung oder hat er aufgrund von Informationen, die ihm zur Verfügung stehen, Grund zu der Annahme, dass ein Hochrisiko-KI-System nicht den Anforderungen in Abschnitt 2 entspricht, so stellt er das Hochrisiko-KI-System erst auf dem Markt bereit, nachdem die Konformität des Systems mit den Anforderungen hergestellt wurde. Birgt das Hochrisiko-IT-System zudem ein Risiko im Sinne des Artikels 79 Absatz 1, so informiert der Händler den Anbieter bzw. den Einführer des Systems darüber.\n(3) Solange sich ein Hochrisiko-KI-System in ihrer Verantwortung befindet, gewährleisten Händler, dass — soweit zutreffend — die Lagerungs- oder Transportbedingungen die Konformität des Systems mit den in Abschnitt 2 festgelegten Anforderungen nicht beeinträchtigen.\n(4) Ein Händler, der aufgrund von Informationen, die ihm zur Verfügung stehen, der Auffassung ist oder Grund zu der Annahme hat, dass ein von ihm auf dem Markt bereitgestelltes Hochrisiko-KI-System nicht den Anforderungen in Abschnitt 2 entspricht, ergreift die erforderlichen Korrekturmaßnahmen, um die Konformität dieses Systems mit diesen Anforderungen herzustellen, es zurückzunehmen oder zurückzurufen, oder er stellt sicher, dass der Anbieter, der Einführer oder gegebenenfalls jeder relevante Akteur diese Korrekturmaßnahmen ergreift. Birgt das Hochrisiko-KI-System ein Risiko im Sinne des Artikels 79 Absatz 1, so informiert der Händler unverzüglich den Anbieter bzw. den Einführer des Systems sowie die für das betroffene Hochrisiko-KI-System zuständigen Behörden und macht dabei ausführliche Angaben, insbesondere zur Nichtkonformität und zu bereits ergriffenen Korrekturmaßnahmen.\n(5) Auf begründete Anfrage einer betreffenden zuständigen Behörde übermitteln die Händler eines Hochrisiko-KI-Systems dieser Behörde sämtliche Informationen und Dokumentation in Bezug auf ihre Maßnahmen gemäß den Absätzen 1 bis 4, die erforderlich sind, um die Konformität dieses Systems mit den in Abschnitt 2 festgelegten Anforderungen nachzuweisen.\n(6) Die Händler arbeiten mit den betreffenden zuständigen Behörden bei allen Maßnahmen zusammen, die diese Behörden im Zusammenhang mit einem von den Händlern auf dem Markt bereitgestellten Hochrisiko-KI-System ergreifen, um insbesondere das von diesem System ausgehende Risiko zu verringern oder abzumildern."
    },
    {
      "chunk_idx": 229,
      "id": "63f69c82-a393-43d5-bf47-0ea7b61acc5f",
      "title": "Art 25",
      "relevantChunksIds": [
        "0a83841c-2931-4e56-a212-b73e3ab410f1",
        "89b37334-a989-4eca-bb8e-55165382038d",
        "3ae98876-3fe7-4515-8012-b96f73db643a",
        "116d04ba-85cc-42e0-9e8e-ac7f1460a22d",
        "4123b06a-0100-490b-9631-66e3f5f66ba2",
        "d5cb7544-49fd-4991-8ab4-23d8dab0ff08",
        "dfe0e9c1-22e5-4ba8-ad77-a65c24cee14d",
        "a513a6fc-8abc-4dd4-b7b9-1b2c4520a19c"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 25: Responsibilities along the AI value chain\n1. Any distributor, importer, deployer or other third-party shall be considered to be a provider of a high-risk AI system for the purposes of this Regulation and shall be subject to the obligations of the provider under Article 16, in any of the following circumstances:\n(a) they put their name or trademark on a high-risk AI system already placed on the market or put into service, without prejudice to contractual arrangements stipulating that the obligations are otherwise allocated;\n(b) they make a substantial modification to a high-risk AI system that has already been placed on the market or has already been put into service in such a way that it remains a high-risk AI system pursuant to Article 6;\n(c) they modify the intended purpose of an AI system, including a general-purpose AI system, which has not been classified as high-risk and has already been placed on the market or put into service in such a way that the AI system concerned becomes a high-risk AI system in accordance with Article 6.\n2. Where the circumstances referred to in paragraph 1 occur, the provider that initially placed the AI system on the market or put it into service shall no longer be considered to be a provider of that specific AI system for the purposes of this Regulation. That initial provider shall closely cooperate with new providers and shall make available the necessary information and provide the reasonably expected technical access and other assistance that are required for the fulfilment of the obligations set out in this Regulation, in particular regarding the compliance with the conformity assessment of high-risk AI systems. This paragraph shall not apply in cases where the initial provider has clearly specified that its AI system is not to be changed into a high-risk AI system and therefore does not fall under the obligation to hand over the documentation.\n3. In the case of high-risk AI systems that are safety components of products covered by the Union harmonisation legislation listed in Section A of Annex I, the product manufacturer shall be considered to be the provider of the high-risk AI system, and shall be subject to the obligations under Article 16 under either of the following circumstances:\n(a) the high-risk AI system is placed on the market together with the product under the name or trademark of the product manufacturer;\n(b) the high-risk AI system is put into service under the name or trademark of the product manufacturer after the product has been placed on the market.\n4. The provider of a high-risk AI system and the third party that supplies an AI system, tools, services, components, or processes that are used or integrated in a high-risk AI system shall, by written agreement, specify the necessary information, capabilities, technical access and other assistance based on the generally acknowledged state of the art, in order to enable the provider of the high-risk AI system to fully comply with the obligations set out in this Regulation. This paragraph shall not apply to third parties making accessible to the public tools, services, processes, or components, other than general-purpose AI models, under a free and open-source licence.\nThe AI Office may develop and recommend voluntary model terms for contracts between providers of high-risk AI systems and third parties that supply tools, services, components or processes that are used for or integrated into high-risk AI systems. When developing those voluntary model terms, the AI Office shall take into account possible contractual requirements applicable in specific sectors or business cases. The voluntary model terms shall be published and be available free of charge in an easily usable electronic format.\n5. Paragraphs 2 and 3 are without prejudice to the need to observe and protect intellectual property rights, confidential business information and trade secrets in accordance with Union and national law.",
      "original_content": "### Artikel 25: Verantwortlichkeiten entlang der KI-Wertschöpfungskette\n(1) In den folgenden Fällen gelten Händler, Einführer, Betreiber oder sonstige Dritte als Anbieter eines Hochrisiko-KI-Systems für die Zwecke dieser Verordnung und unterliegen den Anbieterpflichten gemäß Artikel 16:\na) wenn sie ein bereits in Verkehr gebrachtes oder in Betrieb genommenes Hochrisiko-KI-System mit ihrem Namen oder ihrer Handelsmarke versehen, unbeschadet vertraglicher Vereinbarungen, die eine andere Aufteilung der Pflichten vorsehen;\nb) wenn sie eine wesentliche Veränderung eines Hochrisiko-KI-Systems, das bereits in Verkehr gebracht oder in Betrieb genommen wurde, so vornehmen, dass es weiterhin ein Hochrisiko-KI-System gemäß Artikel 6 bleibt;\nc) wenn sie die Zweckbestimmung eines KI-Systems, einschließlich eines KI-Systems mit allgemeinem Verwendungszweck, das nicht als hochriskant eingestuft wurde und bereits in Verkehr gebracht oder in Betrieb genommen wurde, so verändern, dass das betreffende KI-System zu einem Hochrisiko-KI-System im Sinne von Artikel 6 wird.\n(2) Unter den in Absatz 1 genannten Umständen gilt der Anbieter, der das KI-System ursprünglich in Verkehr gebracht oder in Betrieb genommen hatte, nicht mehr als Anbieter dieses spezifischen KI-Systems für die Zwecke dieser Verordnung. Dieser Erstanbieter arbeitet eng mit neuen Anbietern zusammen, stellt die erforderlichen Informationen zur Verfügung und sorgt für den vernünftigerweise zu erwartenden technischen Zugang und sonstige Unterstützung, die für die Erfüllung der in dieser Verordnung festgelegten Pflichten, insbesondere in Bezug auf die Konformitätsbewertung von Hochrisiko-KI-Systemen, erforderlich sind. Dieser Absatz gilt nicht in Fällen, in denen der Erstanbieter eindeutig festgelegt hat, dass sein KI-System nicht in ein Hochrisiko-KI-System umgewandelt werden darf und daher nicht der Pflicht zur Übergabe der Dokumentation unterliegt.\n(3) Im Falle von Hochrisiko-KI-Systemen, bei denen es sich um Sicherheitsbauteile von Produkten handelt, die unter die in Anhang I Abschnitt A aufgeführten Harmonisierungsrechtsvorschriften der Union fallen, gilt der Produkthersteller als Anbieter des Hochrisiko-KI-Systems und unterliegt in den beiden nachfolgenden Fällen den Pflichten nach Artikel 16:\na) Das Hochrisiko-KI-System wird zusammen mit dem Produkt unter dem Namen oder der Handelsmarke des Produktherstellers in Verkehr gebracht;\nb) das Hochrisiko-KI-System wird unter dem Namen oder der Handelsmarke des Produktherstellers in Betrieb genommen, nachdem das Produkt in Verkehr gebracht wurde.\n(4) Der Anbieter eines Hochrisiko-KI-Systems und der Dritte, der ein KI-System, Instrumente, Dienste, Komponenten oder Verfahren bereitstellt, die in einem Hochrisiko-KI-System verwendet oder integriert werden, legen in einer schriftlichen Vereinbarung die Informationen, die Fähigkeiten, den technischen Zugang und die sonstige Unterstützung nach dem allgemein anerkannten Stand der Technik fest, die erforderlich sind, damit der Anbieter des Hochrisiko-KI-Systems die in dieser Verordnung festgelegten Pflichten vollständig erfüllen kann. Dieser Absatz gilt nicht für Dritte, die Instrumente, Dienste, Verfahren oder Komponenten, bei denen es sich nicht um KI-Modelle mit allgemeinem Verwendungszweck handelt, im Rahmen einer freien und quelloffenen Lizenz öffentlich zugänglich machen.\nDas Büro für Künstliche Intelligenz kann freiwillige Musterbedingungen für Verträge zwischen Anbietern von Hochrisiko-KI-Systemen und Dritten, die Instrumente, Dienste, Komponenten oder Verfahren bereitstellen, die für Hochrisiko-KI-Systeme verwendet oder in diese integriert werden, ausarbeiten und empfehlen. Bei der Ausarbeitung dieser freiwilligen Musterbedingungen berücksichtigt das Büro für Künstliche Intelligenz mögliche vertragliche Anforderungen, die in bestimmten Sektoren oder Geschäftsfällen gelten. Die freiwilligen Musterbedingungen werden veröffentlicht und sind kostenlos in einem leicht nutzbaren elektronischen Format verfügbar.\n(5) Die Absätze 2 und 3 berühren nicht die Notwendigkeit, Rechte des geistigen Eigentums, vertrauliche Geschäftsinformationen und Geschäftsgeheimnisse im Einklang mit dem Unionsrecht und dem nationalen Recht zu achten und zu schützen."
    },
    {
      "chunk_idx": 230,
      "id": "54d92cd3-752d-4303-b8e9-f8c7efac0e64",
      "title": "Art 26",
      "relevantChunksIds": [
        "c7976729-6c7e-451a-8143-a8a52257644d",
        "b43808c6-abf1-4af0-8243-6e7e62e8e30b",
        "7188729e-4dec-4ecf-a3b9-8494a9fc716e",
        "90669d84-c0e3-4ecb-9c0f-f8bbef4679aa",
        "4441c4d9-747f-4f05-8fc8-b67033b9273f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 26: Obligations of deployers of high-risk AI systems\n1. Deployers of high-risk AI systems shall take appropriate technical and organisational measures to ensure they use such systems in accordance with the instructions for use accompanying the systems, pursuant to paragraphs 3 and 6.\n2. Deployers shall assign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support.\n3. The obligations set out in paragraphs 1 and 2, are without prejudice to other deployer obligations under Union or national law and to the deployer’s freedom to organise its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider.\n4. Without prejudice to paragraphs 1 and 2, to the extent the deployer exercises control over the input data, that deployer shall ensure that input data is relevant and sufficiently representative in view of the intended purpose of the high-risk AI system.\n5. Deployers shall monitor the operation of the high-risk AI system on the basis of the instructions for use and, where relevant, inform providers in accordance with Article 72. Where deployers have reason to consider that the use of the high-risk AI system in accordance with the instructions may result in that AI system presenting a risk within the meaning of Article 79(1), they shall, without undue delay, inform the provider or distributor and the relevant market surveillance authority, and shall suspend the use of that system. Where deployers have identified a serious incident, they shall also immediately inform first the provider, and then the importer or distributor and the relevant market surveillance authorities of that incident. If the deployer is not able to reach the provider, Article 73 shall apply mutatis mutandis. This obligation shall not cover sensitive operational data of deployers of AI systems which are law enforcement authorities.\nFor deployers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law, the monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled by complying with the rules on internal governance arrangements, processes and mechanisms pursuant to the relevant financial service law.\n6. Deployers of high-risk AI systems shall keep the logs automatically generated by that high-risk AI system to the extent such logs are under their control, for a period appropriate to the intended purpose of the high-risk AI system, of at least six months, unless provided otherwise in applicable Union or national law, in particular in Union law on the protection of personal data.\nDeployers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law shall maintain the logs as part of the documentation kept pursuant to the relevant Union financial service law.\n7. Before putting into service or using a high-risk AI system at the workplace, deployers who are employers shall inform workers’ representatives and the affected workers that they will be subject to the use of the high-risk AI system. This information shall be provided, where applicable, in accordance with the rules and procedures laid down in Union and national law and practice on information of workers and their representatives.\n8. Deployers of high-risk AI systems that are public authorities, or Union institutions, bodies, offices or agencies shall comply with the registration obligations referred to in Article 49. When such deployers find that the high-risk AI system that they envisage using has not been registered in the EU database referred to in Article 71, they shall not use that system and shall inform the provider or the distributor.\n9. Where applicable, deployers of high-risk AI systems shall use the information provided under Article 13 of this Regulation to comply with their obligation to carry out a data protection impact assessment under Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680.\n10. Without prejudice to Directive (EU) 2016/680, in the framework of an investigation for the targeted search of a person suspected or convicted of having committed a criminal offence, the deployer of a high-risk AI system for post-remote biometric identification shall request an authorisation, ex ante, or without undue delay and no later than 48 hours, by a judicial authority or an administrative authority whose decision is binding and subject to judicial review, for the use of that system, except when it is used for the initial identification of a potential suspect based on objective and verifiable facts directly linked to the offence. Each use shall be limited to what is strictly necessary for the investigation of a specific criminal offence.\nIf the authorisation requested pursuant to the first subparagraph is rejected, the use of the post-remote biometric identification system linked to that requested authorisation shall be stopped with immediate effect and the personal data linked to the use of the high-risk AI system for which the authorisation was requested shall be deleted.\nIn no case shall such high-risk AI system for post-remote biometric identification be used for law enforcement purposes in an untargeted way, without any link to a criminal offence, a criminal proceeding, a genuine and present or genuine and foreseeable threat of a criminal offence, or the search for a specific missing person. It shall be ensured that no decision that produces an adverse legal effect on a person may be taken by the law enforcement authorities based solely on the output of such post-remote biometric identification systems.\nThis paragraph is without prejudice to Article 9 of Regulation (EU) 2016/679 and Article 10 of Directive (EU) 2016/680 for the processing of biometric data.\nRegardless of the purpose or deployer, each use of such high-risk AI systems shall be documented in the relevant police file and shall be made available to the relevant market surveillance authority and the national data protection authority upon request, excluding the disclosure of sensitive operational data related to law enforcement. This subparagraph shall be without prejudice to the powers conferred by Directive (EU) 2016/680 on supervisory authorities.\nDeployers shall submit annual reports to the relevant market surveillance and national data protection authorities on their use of post-remote biometric identification systems, excluding the disclosure of sensitive operational data related to law enforcement. The reports may be aggregated to cover more than one deployment.\nMember States may introduce, in accordance with Union law, more restrictive laws on the use of post-remote biometric identification systems.\n",
      "original_content": "### Artikel 26: Pflichten der Betreiber von Hochrisiko-KI-Systemen\n(1) Die Betreiber von Hochrisiko-KI-Systemen treffen geeignete technische und organisatorische Maßnahmen, um sicherzustellen, dass sie solche Systeme entsprechend der den Systemen beigefügten Betriebsanleitungen und gemäß den Absätzen 3 und 6 verwenden.\n(2) Die Betreiber übertragen natürlichen Personen, die über die erforderliche Kompetenz, Ausbildung und Befugnis verfügen, die menschliche Aufsicht und lassen ihnen die erforderliche Unterstützung zukommen.\n(3) Die Pflichten nach den Absätzen 1 und 2 lassen sonstige Pflichten der Betreiber nach Unionsrecht oder nationalem Recht sowie die Freiheit der Betreiber bei der Organisation ihrer eigenen Ressourcen und Tätigkeiten zur Wahrnehmung der vom Anbieter angegebenen Maßnahmen der menschlichen Aufsicht unberührt.\n(4) Unbeschadet der Absätze 1 und 2 und soweit die Eingabedaten ihrer Kontrolle unterliegen, sorgen die Betreiber dafür, dass die Eingabedaten der Zweckbestimmung des Hochrisiko-KI-Systems entsprechen und ausreichend repräsentativ sind.\n(5) Die Betreiber überwachen den Betrieb des Hochrisiko-KI-Systems anhand der Betriebsanleitung und informieren gegebenenfalls die Anbieter gemäß Artikel 72. Haben Betreiber Grund zu der Annahme, dass die Verwendung gemäß der Betriebsanleitung dazu führen kann, dass dieses Hochrisiko-KI-System ein Risiko im Sinne des Artikels 79 Absatz 1 birgt, so informieren sie unverzüglich den Anbieter oder Händler und die zuständige Marktüberwachungsbehörde und setzen die Verwendung dieses Systems aus. Haben die Betreiber einen schwerwiegenden Vorfall festgestellt, informieren sie auch unverzüglich zuerst den Anbieter und dann den Einführer oder Händler und die zuständigen Marktüberwachungsbehörden über diesen Vorfall. Kann der Betreiber den Anbieter nicht erreichen, so gilt Artikel 73 entsprechend. Diese Pflicht gilt nicht für sensible operative Daten von Betreibern von KI-Systemen, die Strafverfolgungsbehörden sind.\nBei Betreibern, die Finanzinstitute sind und gemäß den Rechtsvorschriften der Union über Finanzdienstleistungen Anforderungen in Bezug auf ihre Regelungen oder Verfahren der internen Unternehmensführung, unterliegen, gilt die in Unterabsatz 1 festgelegte Überwachungspflicht als erfüllt, wenn die Vorschriften über Regelungen, Verfahren oder Mechanismen der internen Unternehmensführung gemäß einschlägigem Recht über Finanzdienstleistungen eingehalten werden.\n(6) Betreiber von Hochrisiko-KI-Systemen bewahren die von ihrem Hochrisiko-KI-System automatisch erzeugten Protokolle, soweit diese Protokolle ihrer Kontrolle unterliegen, für einen der Zweckbestimmung des Hochrisiko-KI-Systems angemessenen Zeitraum von mindestens sechs Monaten auf, sofern im geltenden Unionsrecht, insbesondere im Unionsrecht über den Schutz personenbezogener Daten, oder im geltenden nationalen Recht nichts anderes bestimmt ist.\nBetreiber, die Finanzinstitute sind und gemäß den Rechtsvorschriften der Union über Finanzdienstleistungen Anforderungen in Bezug auf ihre Regelungen oder Verfahren der internen Unternehmensführung unterliegen, bewahren die Protokolle als Teil der gemäß einschlägigem Unionsecht über Finanzdienstleistungen aufzubewahrenden Dokumentation auf.\n(7) Vor der Inbetriebnahme oder Verwendung eines Hochrisiko-KI-Systems am Arbeitsplatz informieren Betreiber, die Arbeitgeber sind, die Arbeitnehmervertreter und die betroffenen Arbeitnehmer darüber, dass sie der Verwendung des Hochrisiko-KI-Systems unterliegen werden. Diese Informationen werden gegebenenfalls im Einklang mit den Vorschriften und Gepflogenheiten auf Unionsebene und nationaler Ebene in Bezug auf die Unterrichtung der Arbeitnehmer und ihrer Vertreter bereitgestellt.\n(8) Betreiber von Hochrisiko-KI-Systemen, bei denen es sich um Organe, Einrichtungen oder sonstige Stellen der Union handelt, müssen den Registrierungspflichten gemäß Artikel 49 nachkommen. Stellen diese Betreiber fest, dass das Hochrisiko-KI-System, dessen Verwendung sie planen, nicht in der in Artikel 71 genannten EU-Datenbank registriert wurde, sehen sie von der Verwendung dieses Systems ab und informieren den Anbieter oder den Händler.\n(9) Die Betreiber von Hochrisiko-KI-Systemen verwenden gegebenenfalls die gemäß Artikel 13 der vorliegenden Verordnung bereitgestellten Informationen, um ihrer Pflicht zur Durchführung einer Datenschutz-Folgenabschätzung gemäß Artikel 35 der Verordnung (EU) 2016/679 oder Artikel 27 der Richtlinie (EU) 2016/680 nachzukommen.\n(10) Unbeschadet der Richtlinie (EU) 2016/680 beantragt der Betreiber eines Hochrisiko-KI-Systems zur nachträglichen biometrischen Fernfernidentifizierung im Rahmen von Ermittlungen zur gezielten Suche einer Person, die der Begehung einer Straftat verdächtigt wird oder aufgrund einer solchen verurteilt wurde, vorab oder unverzüglich, spätestens jedoch binnen 48 Stunden bei einer Justizbehörde oder einer Verwaltungsbehörde, deren Entscheidung bindend ist und einer justiziellen Überprüfung unterliegt, die Genehmigung für die Nutzung dieses Systems, es sei denn, es wird zur erstmaligen Identifizierung eines potenziellen Verdächtigen auf der Grundlage objektiver und nachprüfbarer Tatsachen, die in unmittelbarem Zusammenhang mit der Straftat stehen, verwendet. Jede Verwendung ist auf das für die Ermittlung einer bestimmten Straftat unbedingt erforderliche Maß zu beschränken.\nWird die gemäß Unterabsatz 1 beantragte Genehmigung abgelehnt, so wird die Verwendung des mit dieser beantragten Genehmigung verbundenen Systems zur nachträglichen biometrischen Fernidentifizierung mit sofortiger Wirkung eingestellt und werden die personenbezogenen Daten, die im Zusammenhang mit der Verwendung des Hochrisiko-KI-Systems stehen, für die die Genehmigung beantragt wurde, gelöscht.\nIn keinem Fall darf ein solches Hochrisiko-KI-System zur nachträglichen biometrischen Fernidentifizierung zu Strafverfolgungszwecken in nicht zielgerichteter Weise und ohne jeglichen Zusammenhang mit einer Straftat, einem Strafverfahren, einer tatsächlichen und bestehenden oder tatsächlichen und vorhersehbaren Gefahr einer Straftat oder der Suche nach einer bestimmten vermissten Person verwendet werden. Es muss sichergestellt werden, dass die Strafverfolgungsbehörden keine ausschließlich auf der Grundlage der Ausgabe solcher Systeme zur nachträglichen biometrischen Fernidentifizierung beruhende Entscheidung, aus der sich eine nachteilige Rechtsfolge für eine Person ergibt, treffen.\nDieser Absatz gilt unbeschadet des Artikels 9 der Verordnung (EU) 2016/679 und des Artikels 10 der Richtlinie (EU) 2016/680 für die Verarbeitung biometrischer Daten.\nUnabhängig vom Zweck oder Betreiber wird jede Verwendung solcher Hochrisiko-KI-Systeme in der einschlägigen Polizeiakte dokumentiert und der zuständigen Marktüberwachungsbehörde und der nationalen Datenschutzbehörde auf Anfrage zur Verfügung gestellt, wovon die Offenlegung sensibler operativer Daten im Zusammenhang mit der Strafverfolgung ausgenommen ist. Dieser Unterabsatz berührt nicht die den Aufsichtsbehörden durch die Richtlinie (EU) 2016/680 übertragenen Befugnisse.\nDie Betreiber legen den zuständigen Marktüberwachungsbehörden und den nationalen Datenschutzbehörden Jahresberichte über ihre Verwendung von Systemen zur nachträglichen biometrischen Fernidentifizierung vor, wovon die Offenlegung sensibler operativer Daten im Zusammenhang mit der Strafverfolgung ausgenommen ist. Die Berichte können eine Zusammenfassung sein, damit sie mehr als einen Einsatz abdecken.\nDie Mitgliedstaaten können im Einklang mit dem Unionsrecht strengere Rechtsvorschriften für die Verwendung von Systemen zur nachträglichen biometrischen Fernidentifizierung erlassen.\n(11) Unbeschadet des Artikels 50 der vorliegenden Verordnung informieren die Betreiber der in Anhang III aufgeführten Hochrisiko-KI-Systeme, die natürliche Personen betreffende Entscheidungen treffen oder bei solchen Entscheidungen Unterstützung leisten, die natürlichen Personen darüber, dass sie der Verwendung des Hochrisiko-KI-Systems unterliegen. Für Hochrisiko-KI-Systeme, die zu Strafverfolgungszwecken verwendet werden, gilt Artikel 13 der Richtlinie (EU) 2016/680. (12) Die Betreiber arbeiten mit den zuständigen Behörden bei allen Maßnahmen zusammen, die diese Behörden im Zusammenhang mit dem Hochrisiko-KI-System zur Umsetzung dieser Verordnung ergreifen."
    },
    {
      "chunk_idx": 231,
      "id": "1a8cb301-fedf-4e50-a05e-2d3ad00b5264",
      "title": "Art 27",
      "relevantChunksIds": [
        "90ac7181-45af-41f1-817b-5a51223d7825"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 27: Fundamental rights impact assessment for high-risk AI systems\n1. Prior to deploying a high-risk AI system referred to in Article 6(2), with the exception of high-risk AI systems intended to be used in the area listed in point 2 of Annex III, deployers that are bodies governed by public law, or are private entities providing public services, and deployers of high-risk AI systems referred to in points 5 (b) and (c) of Annex III, shall perform an assessment of the impact on fundamental rights that the use of such system may produce. For that purpose, deployers shall perform an assessment consisting of:\n(a) a description of the deployer’s processes in which the high-risk AI system will be used in line with its intended purpose;\n(b) a description of the period of time within which, and the frequency with which, each high-risk AI system is intended to be used;\n(c) the categories of natural persons and groups likely to be affected by its use in the specific context;\n(d) the specific risks of harm likely to have an impact on the categories of natural persons or groups of persons identified pursuant to point (c) of this paragraph, taking into account the information given by the provider pursuant to Article 13;\n(e) a description of the implementation of human oversight measures, according to the instructions for use;\n(f) the measures to be taken in the case of the materialisation of those risks, including the arrangements for internal governance and complaint mechanisms.\n2. The obligation laid down in paragraph 1 applies to the first use of the high-risk AI system. The deployer may, in similar cases, rely on previously conducted fundamental rights impact assessments or existing impact assessments carried out by provider. If, during the use of the high-risk AI system, the deployer considers that any of the elements listed in paragraph 1 has changed or is no longer up to date, the deployer shall take the necessary steps to update the information.\n3. Once the assessment referred to in paragraph 1 of this Article has been performed, the deployer shall notify the market surveillance authority of its results, submitting the filled-out template referred to in paragraph 5 of this Article as part of the notification. In the case referred to in Article 46(1), deployers may be exempt from that obligation to notify.\n4. If any of the obligations laid down in this Article is already met through the data protection impact assessment conducted pursuant to Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680, the fundamental rights impact assessment referred to in paragraph 1 of this Article shall complement that data protection impact assessment.\n5. The AI Office shall develop a template for a questionnaire, including through an automated tool, to facilitate deployers in complying with their obligations under this Article in a simplified manner.",
      "original_content": "### Artikel 27: Grundrechte-Folgenabschätzung für Hochrisiko-KI-Systeme\n(1) Vor der Inbetriebnahme eines Hochrisiko-KI-Systems gemäß Artikel 6 Absatz 2 — mit Ausnahme von Hochrisiko-KI-Systemen, die in dem in Anhang III Nummer 2 aufgeführten Bereich verwendet werden sollen — führen Betreiber, bei denen es sich um Einrichtungen des öffentlichen Rechts oder private Einrichtungen, die öffentliche Dienste erbringen, handelt, und Betreiber von Hochrisiko-KI-Systemen gemäß Anhang III Nummer 5 Buchstaben b und c eine Abschätzung der Auswirkungen, die die Verwendung eines solchen Systems auf die Grundrechte haben kann, durch. Zu diesem Zweck führen die Betreiber eine Abschätzung durch, die Folgendes umfasst:\na) eine Beschreibung der Verfahren des Betreibers, bei denen das Hochrisiko-KI-System im Einklang mit seiner Zweckbestimmung verwendet wird;\nb) eine Beschreibung des Zeitraums und der Häufigkeit, innerhalb dessen bzw. mit der jedes Hochrisiko-KI-System verwendet werden soll;\nc) die Kategorien der natürlichen Personen und Personengruppen, die von seiner Verwendung im spezifischen Kontext betroffen sein könnten;\nd) die spezifischen Schadensrisiken, die sich auf die gemäß Buchstabe c dieses Absatzes ermittelten Kategorien natürlicher Personen oder Personengruppen auswirken könnten, unter Berücksichtigung der vom Anbieter gemäß Artikel 13 bereitgestellten Informationen;\ne) eine Beschreibung der Umsetzung von Maßnahmen der menschlichen Aufsicht entsprechend den Betriebsanleitungen;\nf) die Maßnahmen, die im Falle des Eintretens dieser Risiken zu ergreifen sind, einschließlich der Regelungen für die interne Unternehmensführung und Beschwerdemechanismen.\n(2) Die in Absatz 1 festgelegte Pflicht gilt für die erste Verwendung eines Hochrisiko-KI-Systems. Der Betreiber kann sich in ähnlichen Fällen auf zuvor durchgeführte Grundrechte-Folgenabschätzungen oder bereits vorhandene Folgenabschätzungen, die vom Anbieter durchgeführt wurden, stützen. Gelangt der Betreiber während der Verwendung des Hochrisiko-KI-Systems zur Auffassung, dass sich eines der in Absatz 1 aufgeführten Elemente geändert hat oder nicht mehr auf dem neuesten Stand ist, so unternimmt der Betreiber die erforderlichen Schritte, um die Informationen zu aktualisieren.\n(3) Sobald die Abschätzung gemäß Absatz 1 des vorliegenden Artikels durchgeführt wurde, teilt der Betreiber der Marktüberwachungsbehörde ihre Ergebnisse mit, indem er das ausgefüllte, in Absatz 5 des vorliegenden Artikels genannte Muster als Teil der Mitteilung übermittelt. In dem in Artikel 46 Absatz 1 genannten Fall können die Betreiber von der Mitteilungspflicht befreit werden.\n(4) Wird eine der in diesem Artikel festgelegten Pflichten bereits infolge einer gemäß Artikel 35 der Verordnung (EU) 2016/679 oder Artikel 27 der Richtlinie (EU) 2016/680 durchgeführten Datenschutz-Folgenabschätzung erfüllt, so ergänzt die Grundrechte-Folgenabschätzung gemäß Absatz 1 des vorliegenden Artikels diese Datenschutz-Folgenabschätzung.\n(5) Das Büro für Künstliche Intelligenz arbeitet ein Muster für einen Fragebogen — auch mithilfe eines automatisierten Instruments — aus, um die Betreiber in die Lage zu versetzen, ihren Pflichten gemäß diesem Artikel in vereinfachter Weise nachzukommen."
    },
    {
      "chunk_idx": 232,
      "id": "1d1f418c-443a-4e77-b70a-5415bad671e6",
      "title": "Art 28",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "## SECTION 4: Notifying authorities and notified bodies\n### Article 28: Notifying authorities\n1. Each Member State shall designate or establish at least one notifying authority responsible for setting up and carrying out the necessary procedures for the assessment, designation and notification of conformity assessment bodies and for their monitoring. Those procedures shall be developed in cooperation between the notifying authorities of all Member States.\n2. Member States may decide that the assessment and monitoring referred to in paragraph 1 is to be carried out by a national accreditation body within the meaning of, and in accordance with, Regulation (EC) No 765/2008.\n3. Notifying authorities shall be established, organised and operated in such a way that no conflict of interest arises with conformity assessment bodies, and that the objectivity and impartiality of their activities are safeguarded.\n4. Notifying authorities shall be organised in such a way that decisions relating to the notification of conformity assessment bodies are taken by competent persons different from those who carried out the assessment of those bodies.\n5. Notifying authorities shall offer or provide neither any activities that conformity assessment bodies perform, nor any consultancy services on a commercial or competitive basis.\n6. Notifying authorities shall safeguard the confidentiality of the information that they obtain, in accordance with Article 78.\n7. Notifying authorities shall have an adequate number of competent personnel at their disposal for the proper performance of their tasks. Competent personnel shall have the necessary expertise, where applicable, for their function, in fields such as information technologies, AI and law, including the supervision of fundamental rights.",
      "original_content": "## ABSCHNITT 4: Notifizierende Behörden und notifizierte Stellen\n### Artikel 28: Notifizierende Behörden\n(1) Jeder Mitgliedstaat sorgt für die Benennung oder Schaffung mindestens einer notifizierenden Behörde, die für die Einrichtung und Durchführung der erforderlichen Verfahren zur Bewertung, Benennung und Notifizierung von Konformitätsbewertungsstellen und für deren Überwachung zuständig ist. Diese Verfahren werden in Zusammenarbeit zwischen den notifizierenden Behörden aller Mitgliedstaaten entwickelt.\n(2) Die Mitgliedstaaten können entscheiden, dass die Bewertung und Überwachung nach Absatz 1 von einer nationalen Akkreditierungsstelle im Sinne und gemäß der Verordnung (EG) Nr. 765/2008 durchzuführen ist.\n(3) Notifizierende Behörden werden so eingerichtet, organisiert und geführt, dass jegliche Interessenkonflikte mit Konformitätsbewertungsstellen vermieden werden und die Objektivität und die Unparteilichkeit ihrer Tätigkeiten gewährleistet sind.\n(4) Notifizierende Behörden werden so organisiert, dass Entscheidungen über die Notifizierung von Konformitätsbewertungsstellen von kompetenten Personen getroffen werden, die nicht mit den Personen identisch sind, die die Bewertung dieser Stellen durchgeführt haben.\n(5) Notifizierende Behörden dürfen weder Tätigkeiten, die Konformitätsbewertungsstellen durchführen, noch Beratungsleistungen auf einer gewerblichen oder wettbewerblichen Basis anbieten oder erbringen.\n(6) Notifizierende Behörden gewährleisten gemäß Artikel 78 die Vertraulichkeit der von ihnen erlangten Informationen.\n(7) Notifizierende Behörden verfügen über eine angemessene Anzahl kompetenter Mitarbeiter, sodass sie ihre Aufgaben ordnungsgemäß wahrnehmen können. Die kompetenten Mitarbeiter verfügen — wo erforderlich — über das für ihre Funktion erforderliche Fachwissen in Bereichen wie Informationstechnologie sowie KI und Recht, einschließlich der Überwachung der Grundrechte."
    },
    {
      "chunk_idx": 233,
      "id": "897ec415-d93f-4928-9ce9-712efa9fd275",
      "title": "Art 29",
      "relevantChunksIds": [
        "c7c4a6e2-92a4-40d5-9243-1d698685d1a0"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 29: Application of a conformity assessment body for notification\n1. Conformity assessment bodies shall submit an application for notification to the notifying authority of the Member State in which they are established.\n2. The application for notification shall be accompanied by a description of the conformity assessment activities, the conformity assessment module or modules and the types of AI systems for which the conformity assessment body claims to be competent, as well as by an accreditation certificate, where one exists, issued by a national accreditation body attesting that the conformity assessment body fulfils the requirements laid down in Article 31.\nAny valid document related to existing designations of the applicant notified body under any other Union harmonisation legislation shall be added.\n3. Where the conformity assessment body concerned cannot provide an accreditation certificate, it shall provide the notifying authority with all the documentary evidence necessary for the verification, recognition and regular monitoring of its compliance with the requirements laid down in Article 31.\n4. For notified bodies which are designated under any other Union harmonisation legislation, all documents and certificates linked to those designations may be used to support their designation procedure under this Regulation, as appropriate. The notified body shall update the documentation referred to in paragraphs 2 and 3 of this Article whenever relevant changes occur, in order to enable the authority responsible for notified bodies to monitor and verify continuous compliance with all the requirements laid down in Article 31.",
      "original_content": "### Artikel 29: Antrag einer Konformitätsbewertungsstelle auf Notifizierung\n(1) Konformitätsbewertungsstellen beantragen ihre Notifizierung bei der notifizierenden Behörde des Mitgliedstaats, in dem sie niedergelassen sind.\n(2) Dem Antrag auf Notifizierung legen sie eine Beschreibung der Konformitätsbewertungstätigkeiten, des bzw. der Konformitätsbewertungsmodule und der Art der KI-Systeme, für die diese Konformitätsbewertungsstelle Kompetenz beansprucht, sowie, falls vorhanden, eine Akkreditierungsurkunde bei, die von einer nationalen Akkreditierungsstelle ausgestellt wurde und in der bescheinigt wird, dass die Konformitätsbewertungsstelle die Anforderungen des Artikels 31 erfüllt.\nSonstige gültige Dokumente in Bezug auf bestehende Benennungen der antragstellenden notifizierten Stelle im Rahmen anderer Harmonisierungsrechtsvorschriften der Union sind ebenfalls beizufügen.\n(3) Kann die betreffende Konformitätsbewertungsstelle keine Akkreditierungsurkunde vorweisen, so legt sie der notifizierenden Behörde als Nachweis alle Unterlagen vor, die erforderlich sind, um zu überprüfen, festzustellen und regelmäßig zu überwachen, ob sie die Anforderungen des Artikels 31 erfüllt.\n(4) Bei notifizierten Stellen, die im Rahmen anderer Harmonisierungsrechtsvorschriften der Union benannt wurden, können alle Unterlagen und Bescheinigungen im Zusammenhang mit solchen Benennungen zur Unterstützung ihres Benennungsverfahrens nach dieser Verordnung verwendet werden. Die notifizierte Stelle aktualisiert die in den Absätzen 2 und 3 des vorliegenden Artikels genannte Dokumentation immer dann, wenn sich relevante Änderungen ergeben, damit die für notifizierte Stellen zuständige Behörde überwachen und überprüfen kann, ob die Anforderungen des Artikels 31 kontinuierlich erfüllt sind."
    },
    {
      "chunk_idx": 234,
      "id": "d2fac56c-1843-421c-baa8-2d3e0178bec6",
      "title": "Art 30",
      "relevantChunksIds": [
        "577d5a3c-36b6-48b6-a3fd-27b111efe5fd",
        "491d84d6-b754-4408-8bbd-21c789451f0f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 30: Notification procedure\n1. Notifying authorities may notify only conformity assessment bodies which have satisfied the requirements laid down in Article 31.\n\n4. The conformity assessment body concerned may perform the activities of a notified body only where no objections are raised by the Commission or the other Member States within two weeks of a notification by a notifying authority where it includes an accreditation certificate referred to in Article 29(2), or within two months of a notification by the notifying authority where it includes documentary evidence referred to in Article 29(3).\n5. Where objections are raised, the Commission shall, without delay, enter into consultations with the relevant Member States and the conformity assessment body. In view thereof, the Commission shall decide whether the authorisation is justified. The Commission shall address its decision to the Member State concerned and to the relevant conformity assessment body.",
      "original_content": "### Artikel 30: Notifizierungsverfahren\n(1) Die notifizierenden Behörden dürfen nur Konformitätsbewertungsstellen notifizieren, die die Anforderungen des Artikels 31 erfüllen.\n(2) Die notifizierenden Behörden unterrichten die Kommission und die übrigen Mitgliedstaaten mithilfe des elektronischen Notifizierungsinstruments, das von der Kommission entwickelt und verwaltet wird, über jede Konformitätsbewertungsstelle gemäß Absatz 1. (3) Die Notifizierung gemäß Absatz 2 des vorliegenden Artikels enthält vollständige Angaben zu den Konformitätsbewertungstätigkeiten, dem betreffenden Konformitätsbewertungsmodul oder den betreffenden Konformitätsbewertungsmodulen, den betreffenden Arten der KI-Systeme und der einschlägigen Bestätigung der Kompetenz. Beruht eine Notifizierung nicht auf einer Akkreditierungsurkunde gemäß Artikel 29 Absatz 2, so legt die notifizierende Behörde der Kommission und den anderen Mitgliedstaaten die Unterlagen vor, die die Kompetenz der Konformitätsbewertungsstelle und die Vereinbarungen nachweisen, die getroffen wurden, um sicherzustellen, dass die Stelle regelmäßig überwacht wird und weiterhin die Anforderungen des Artikels 31 erfüllt.\n(4) Die betreffende Konformitätsbewertungsstelle darf die Tätigkeiten einer notifizierten Stelle nur dann wahrnehmen, wenn weder die Kommission noch die anderen Mitgliedstaaten innerhalb von zwei Wochen nach einer Notifizierung durch eine notifizierende Behörde, falls eine Akkreditierungsurkunde gemäß Artikel 29 Absatz 2 vorgelegt wird, oder innerhalb von zwei Monaten nach einer Notifizierung durch eine notifizierende Behörde, falls als Nachweis Unterlagen gemäß Artikel 29 Absatz 3 vorgelegt werden, Einwände erhoben haben.\n(5) Werden Einwände erhoben, konsultiert die Kommission unverzüglich die betreffenden Mitgliedstaaten und die Konformitätsbewertungsstelle. Im Hinblick darauf entscheidet die Kommission, ob die Genehmigung gerechtfertigt ist. Die Kommission richtet ihren Beschluss an die betroffenen Mitgliedstaaten und an die zuständige Konformitätsbewertungsstelle."
    },
    {
      "chunk_idx": 235,
      "id": "a95f9b76-59d0-4655-9859-ce20a0190849",
      "title": "Art 31",
      "relevantChunksIds": [
        "ab48c7d3-9758-42ad-80bd-2aef56bf20b4",
        "577d5a3c-36b6-48b6-a3fd-27b111efe5fd",
        "491d84d6-b754-4408-8bbd-21c789451f0f",
        "2aedd016-7002-471a-af6e-131b4f9f1f54"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 31: Requirements relating to notified bodies\n1. A notified body shall be established under the national law of a Member State and shall have legal personality.\n2. Notified bodies shall satisfy the organisational, quality management, resources and process requirements that are necessary to fulfil their tasks, as well as suitable cybersecurity requirements.\n3. The organisational structure, allocation of responsibilities, reporting lines and operation of notified bodies shall ensure confidence in their performance, and in the results of the conformity assessment activities that the notified bodies conduct.\n4. Notified bodies shall be independent of the provider of a high-risk AI system in relation to which they perform conformity assessment activities. Notified bodies shall also be independent of any other operator having an economic interest in high-risk AI systems assessed, as well as of any competitors of the provider. This shall not preclude the use of assessed high-risk AI systems that are necessary for the operations of the conformity assessment body, or the use of such high-risk AI systems for personal purposes.\n5. Neither a conformity assessment body, its top-level management nor the personnel responsible for carrying out its conformity assessment tasks shall be directly involved in the design, development, marketing or use of high-risk AI systems, nor shall they represent the parties engaged in those activities. They shall not engage in any activity that might conflict with their independence of judgement or integrity in relation to conformity assessment activities for which they are notified. This shall, in particular, apply to consultancy services.\n6. Notified bodies shall be organised and operated so as to safeguard the independence, objectivity and impartiality of their activities. Notified bodies shall document and implement a structure and procedures to safeguard impartiality and to promote and apply the principles of impartiality throughout their organisation, personnel and assessment activities.\n7. Notified bodies shall have documented procedures in place ensuring that their personnel, committees, subsidiaries, subcontractors and any associated body or personnel of external bodies maintain, in accordance with Article 78, the confidentiality of the information which comes into their possession during the performance of conformity assessment activities, except when its disclosure is required by law. The staff of notified bodies shall be bound to observe professional secrecy with regard to all information obtained in carrying out their tasks under this Regulation, except in relation to the notifying authorities of the Member State in which their activities are carried out.\n8. Notified bodies shall have procedures for the performance of activities which take due account of the size of a provider, the sector in which it operates, its structure, and the degree of complexity of the AI system concerned.\n9. Notified bodies shall take out appropriate liability insurance for their conformity assessment activities, unless liability is assumed by the Member State in which they are established in accordance with national law or that Member State is itself directly responsible for the conformity assessment.\n10. Notified bodies shall be capable of carrying out all their tasks under this Regulation with the highest degree of professional integrity and the requisite competence in the specific field, whether those tasks are carried out by notified bodies themselves or on their behalf and under their responsibility.\n11. Notified bodies shall have sufficient internal competences to be able effectively to evaluate the tasks conducted by external parties on their behalf. The notified body shall have permanent availability of sufficient administrative, technical, legal and scientific personnel who possess experience and knowledge relating to the relevant types of AI systems, data and data computing, and relating to the requirements set out in Section 2.\n12. Notified bodies shall participate in coordination activities as referred to in Article 38. They shall also take part directly, or be represented in, European standardisation organisations, or ensure that they are aware and up to date in respect of relevant standards.",
      "original_content": "### Artikel 31: Anforderungen an notifizierte Stellen\n(1) Eine notifizierte Stelle wird nach dem nationalen Recht eines Mitgliedstaats gegründet und muss mit Rechtspersönlichkeit ausgestattet sein.\n(2) Die notifizierten Stellen müssen die zur Wahrnehmung ihrer Aufgaben erforderlichen Anforderungen an die Organisation, das Qualitätsmanagement, die Ressourcenausstattung und die Verfahren sowie angemessene Cybersicherheitsanforderungen erfüllen.\n(3) Die Organisationsstruktur, die Zuweisung der Zuständigkeiten, die Berichtslinien und die Funktionsweise der notifizierten Stellen müssen das Vertrauen in ihre Leistung und in die Ergebnisse der von ihnen durchgeführten Konformitätsbewertungstätigkeiten gewährleisten.\n(4) Die notifizierten Stellen müssen von dem Anbieter eines Hochrisiko-KI-Systems, zu dem sie Konformitätsbewertungstätigkeiten durchführen, unabhängig sein. Außerdem müssen die notifizierten Stellen von allen anderen Akteuren, die ein wirtschaftliches Interesse an den bewerteten Hochrisiko-KI-Systemen haben, und von allen Wettbewerbern des Anbieters unabhängig sein. Dies schließt die Verwendung von bewerteten Hochrisiko-KI-Systemen, die für die Tätigkeit der Konformitätsbewertungsstelle nötig sind, oder die Verwendung solcher Hochrisiko-KI-Systeme zum persönlichen Gebrauch nicht aus.\n(5) Weder die Konformitätsbewertungsstelle, ihre oberste Leitungsebene noch die für die Erfüllung ihrer Konformitätsbewertungsaufgaben zuständigen Mitarbeiter dürfen direkt an Entwurf, Entwicklung, Vermarktung oder Verwendung von Hochrisiko-KI-Systemen beteiligt sein oder die an diesen Tätigkeiten beteiligten Parteien vertreten. Sie dürfen sich nicht mit Tätigkeiten befassen, die ihre Unabhängigkeit bei der Beurteilung oder ihre Integrität im Zusammenhang mit den Konformitätsbewertungstätigkeiten, für die sie notifiziert sind, beeinträchtigen könnten. Dies gilt besonders für Beratungsdienstleistungen.\n(6) Notifizierte Stellen werden so organisiert und geführt, dass bei der Ausübung ihrer Tätigkeit Unabhängigkeit, Objektivität und Unparteilichkeit gewahrt sind. Von den notifizierten Stellen werden eine Struktur und Verfahren dokumentiert und umgesetzt, die ihre Unparteilichkeit gewährleisten und sicherstellen, dass die Grundsätze der Unparteilichkeit in ihrer gesamten Organisation, von allen Mitarbeitern und bei allen Bewertungstätigkeiten gefördert und angewandt werden.\n(7) Die notifizierten Stellen gewährleisten durch dokumentierte Verfahren, dass ihre Mitarbeiter, Ausschüsse, Zweigstellen, Unterauftragnehmer sowie alle zugeordneten Stellen oder Mitarbeiter externer Einrichtungen die Vertraulichkeit der Informationen, die bei der Durchführung der Konformitätsbewertungstätigkeiten in ihren Besitz gelangen, im Einklang mit Artikel 78 wahren, außer wenn ihre Offenlegung gesetzlich vorgeschrieben ist. Informationen, von denen Mitarbeiter der notifizierten Stellen bei der Durchführung ihrer Aufgaben gemäß dieser Verordnung Kenntnis erlangen, unterliegen der beruflichen Schweigepflicht, außer gegenüber den notifizierenden Behörden des Mitgliedstaats, in dem sie ihre Tätigkeiten ausüben.\n(8) Die notifizierten Stellen verfügen über Verfahren zur Durchführung ihrer Tätigkeiten unter gebührender Berücksichtigung der Größe eines Betreibers, des Sektors, in dem er tätig ist, seiner Struktur sowie der Komplexität des betreffenden KI-Systems.\n(9) Die notifizierten Stellen schließen eine angemessene Haftpflichtversicherung für ihre Konformitätsbewertungstätigkeiten ab, es sei denn, diese Haftpflicht wird aufgrund nationalen Rechts von dem Mitgliedstaat, in dem sie niedergelassen sind, gedeckt oder dieser Mitgliedstaat ist selbst unmittelbar für die Konformitätsbewertung zuständig.\n(10) Die notifizierten Stellen müssen in der Lage sein, ihre Aufgaben gemäß dieser Verordnung mit höchster beruflicher Integrität und der erforderlichen Fachkompetenz in dem betreffenden Bereich auszuführen, gleichgültig, ob diese Aufgaben von den notifizierten Stellen selbst oder in ihrem Auftrag und in ihrer Verantwortung durchgeführt werden.\n(11) Die notifizierten Stellen müssen über ausreichende interne Kompetenzen verfügen, um die von externen Stellen in ihrem Namen wahrgenommen Aufgaben wirksam beurteilen zu können. Die notifizierten Stellen müssen ständig über ausreichendes administratives, technisches, juristisches und wissenschaftliches Personal verfügen, das Erfahrungen und Kenntnisse in Bezug auf einschlägige Arten der KI-Systeme, Daten und Datenverarbeitung sowie die in Abschnitt 2 festgelegten Anforderungen besitzt.\n(12) Die notifizierten Stellen wirken an den in Artikel 38 genannten Koordinierungstätigkeiten mit. Sie wirken außerdem unmittelbar oder mittelbar an der Arbeit der europäischen Normungsorganisationen mit oder stellen sicher, dass sie stets über den Stand der einschlägigen Normen unterrichtet sind."
    },
    {
      "chunk_idx": 236,
      "id": "2c00d4ce-c731-4696-9693-eda24b9eaf27",
      "title": "Art 32",
      "relevantChunksIds": [
        "4f2899cb-4889-4299-8ac6-1c62a78e48ba",
        "8b015b72-9483-4675-bf5a-02c2ddc3a267"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 32: Presumption of conformity with requirements relating to notified bodies\nWhere a conformity assessment body demonstrates its conformity with the criteria laid down in the relevant harmonised standards or parts thereof, the references of which have been published in the Official Journal of the European Union, it shall be presumed to comply with the requirements set out in Article 31 in so far as the applicable harmonised standards cover those requirements.",
      "original_content": "### Artikel 32: Vermutung der Konformität mit den Anforderungen an notifizierte Stellen\nWeist eine Konformitätsbewertungsstelle nach, dass sie die Kriterien der einschlägigen harmonisierten Normen, deren Fundstellen im Amtsblatt der Europäischen Union veröffentlicht wurden, oder Teile dieser Normen erfüllt, so wird davon ausgegangen, dass sie die Anforderungen des Artikels 31, soweit diese von den geltenden harmonisierten Normen erfasst werden, erfüllt."
    },
    {
      "chunk_idx": 237,
      "id": "8828f983-a392-468e-9e97-1217cb1ded61",
      "title": "Art 33",
      "relevantChunksIds": [
        "8d281ca3-ab5a-4f6e-8b3b-e59fe81002df"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 33: Subsidiaries of notified bodies and subcontracting\n1. Where a notified body subcontracts specific tasks connected with the conformity assessment or has recourse to a subsidiary, it shall ensure that the subcontractor or the subsidiary meets the requirements laid down in Article 31, and shall inform the notifying authority accordingly.\n2. Notified bodies shall take full responsibility for the tasks performed by any subcontractors or subsidiaries.\n3. Activities may be subcontracted or carried out by a subsidiary only with the agreement of the provider. Notified bodies shall make a list of their subsidiaries publicly available.\n4. The relevant documents concerning the assessment of the qualifications of the subcontractor or the subsidiary and the work carried out by them under this Regulation shall be kept at the disposal of the notifying authority for a period of five years from the termination date of the subcontracting.",
      "original_content": "### Artikel 33: Zweigstellen notifizierter Stellen und Vergabe von Unteraufträgen\n(1) Vergibt eine notifizierte Stelle bestimmte mit der Konformitätsbewertung verbundene Aufgaben an Unterauftragnehmer oder überträgt sie diese einer Zweigstelle, so stellt sie sicher, dass der Unterauftragnehmer oder die Zweigstelle die Anforderungen des Artikels 31 erfüllt, und informiert die notifizierende Behörde darüber.\n(2) Die notifizierten Stellen tragen die volle Verantwortung für Arbeiten, die von jedweden Unterauftragnehmern oder Zweigstellen ausgeführt werden.\n(3) Tätigkeiten dürfen nur mit Zustimmung des Anbieters an einen Unterauftragnehmer vergeben oder einer Zweigstelle übertragen werden. Die notifizierten Stellen veröffentlichen ein Verzeichnis ihrer Zweigstellen.\n(4) Die einschlägigen Unterlagen über die Bewertung der Qualifikation des Unterauftragnehmers oder der Zweigstelle und die von ihnen gemäß dieser Verordnung ausgeführten Arbeiten werden für einen Zeitraum von fünf Jahren ab dem Datum der Beendigung der Unterauftragsvergabe für die notifizierende Behörde bereitgehalten."
    },
    {
      "chunk_idx": 238,
      "id": "0d4481cd-fcdd-40f0-9386-6c5ccf4eeb97",
      "title": "Art 34",
      "relevantChunksIds": [
        "c87d7613-0407-4eb6-8bb8-5dad9dd7e48d"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 34: Operational obligations of notified bodies\n1. Notified bodies shall verify the conformity of high-risk AI systems in accordance with the conformity assessment procedures set out in Article 43.\n2. Notified bodies shall avoid unnecessary burdens for providers when performing their activities, and take due account of the size of the provider, the sector in which it operates, its structure and the degree of complexity of the high-risk AI system concerned, in particular in view of minimising administrative burdens and compliance costs for micro- and small enterprises within the meaning of Recommendation 2003/361/EC. The notified body shall, nevertheless, respect the degree of rigour and the level of protection required for the compliance of the high-risk AI system with the requirements of this Regulation.\n3. Notified bodies shall make available and submit upon request all relevant documentation, including the providers’ documentation, to the notifying authority referred to in Article 28 to allow that authority to conduct its assessment, designation, notification and monitoring activities, and to facilitate the assessment outlined in this Section.",
      "original_content": "### Artikel 34: Operative Pflichten der notifizierten Stellen\n(1) Die notifizierten Stellen überprüfen die Konformität von Hochrisiko-KI-Systemen nach den in Artikel 43 festgelegten Konformitätsbewertungsverfahren.\n(2) Die notifizierten Stellen vermeiden bei der Durchführung ihrer Tätigkeiten unnötige Belastungen für die Anbieter und berücksichtigen gebührend die Größe des Anbieters, den Sektor, in dem er tätig ist, seine Struktur sowie die Komplexität des betreffenden Hochrisiko-KI-Systems, um insbesondere den Verwaltungsaufwand und die Befolgungskosten für Kleinstunternehmen und kleine Unternehmen im Sinne der Empfehlung 2003/361/EG zu minimieren. Die notifizierte Stelle muss jedoch den Grad der Strenge und das Schutzniveau einhalten, die für die Konformität des Hochrisiko-KI-Systems mit den Anforderungen dieser Verordnung erforderlich sind.\n(3) Die notifizierten Stellen machen der in Artikel 28 genannten notifizierenden Behörde sämtliche einschlägige Dokumentation, einschließlich der Dokumentation des Anbieters, zugänglich bzw. übermitteln diese auf Anfrage, damit diese Behörde ihre Bewertungs-, Benennungs-, Notifizierungs- und Überwachungstätigkeiten durchführen kann und die Bewertung gemäß diesem Abschnitt erleichtert wird."
    },
    {
      "chunk_idx": 239,
      "id": "460c66b2-dd41-4bfb-8928-ac5a8fe9f816",
      "title": "Art 35",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 35: Identification numbers and lists of notified bodies\n1. The Commission shall assign a single identification number to each notified body, even where a body is notified under more than one Union act.\n2. The Commission shall make publicly available the list of the bodies notified under this Regulation, including their identification numbers and the activities for which they have been notified. The Commission shall ensure that the list is kept up to date.",
      "original_content": "### Artikel 35: Identifizierungsnummern und Verzeichnisse notifizierter Stellen\n(1) Die Kommission weist jeder notifizierten Stelle eine einzige Identifizierungsnummer zu, selbst wenn eine Stelle nach mehr als einem Rechtsakt der Union notifiziert wurde.\n(2) Die Kommission veröffentlicht das Verzeichnis der nach dieser Verordnung notifizierten Stellen samt ihren Identifizierungsnummern und den Tätigkeiten, für die sie notifiziert wurden. Die Kommission stellt sicher, dass das Verzeichnis auf dem neuesten Stand gehalten wird."
    },
    {
      "chunk_idx": 240,
      "id": "49563d87-4fce-4a1e-adc5-839e980a9965",
      "title": "Art 36",
      "relevantChunksIds": [
        "cfa2dd1a-cc63-4053-9da5-b7fddaa3ae9c",
        "c7c4a6e2-92a4-40d5-9243-1d698685d1a0",
        "ab48c7d3-9758-42ad-80bd-2aef56bf20b4"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 36: Changes to notifications\n1. The notifying authority shall notify the Commission and the other Member States of any relevant changes to the notification of a notified body via the electronic notification tool referred to in Article 30(2).\n2. The procedures laid down in Articles 29 and 30 shall apply to extensions of the scope of the notification.\nFor changes to the notification other than extensions of its scope, the procedures laid down in paragraphs (3) to (9) shall apply.\n3. Where a notified body decides to cease its conformity assessment activities, it shall inform the notifying authority and the providers concerned as soon as possible and, in the case of a planned cessation, at least one year before ceasing its activities. The certificates of the notified body may remain valid for a period of nine months after cessation of the notified body’s activities, on condition that another notified body has confirmed in writing that it will assume responsibilities for the high-risk AI systems covered by those certificates. The latter notified body shall complete a full assessment of the high-risk AI systems affected by the end of that nine-month-period before issuing new certificates for those systems. Where the notified body has ceased its activity, the notifying authority shall withdraw the designation.\n4. Where a notifying authority has sufficient reason to consider that a notified body no longer meets the requirements laid down in Article 31, or that it is failing to fulfil its obligations, the notifying authority shall without delay investigate the matter with the utmost diligence. In that context, it shall inform the notified body concerned about the objections raised and give it the possibility to make its views known. If the notifying authority comes to the conclusion that the notified body no longer meets the requirements laid down in Article 31 or that it is failing to fulfil its obligations, it shall restrict, suspend or withdraw the designation as appropriate, depending on the seriousness of the failure to meet those requirements or fulfil those obligations. It shall immediately inform the Commission and the other Member States accordingly.\n5. Where its designation has been suspended, restricted, or fully or partially withdrawn, the notified body shall inform the providers concerned within 10 days.\n6. In the event of the restriction, suspension or withdrawal of a designation, the notifying authority shall take appropriate steps to ensure that the files of the notified body concerned are kept, and to make them available to notifying authorities in other Member States and to market surveillance authorities at their request.\n7. In the event of the restriction, suspension or withdrawal of a designation, the notifying authority shall:\n(a) assess the impact on the certificates issued by the notified body;\n(b) submit a report on its findings to the Commission and the other Member States within three months of having notified the changes to the designation;\n(c) require the notified body to suspend or withdraw, within a reasonable period of time determined by the authority, any certificates which were unduly issued, in order to ensure the continuing conformity of high-risk AI systems on the market;\n(d) inform the Commission and the Member States about certificates the suspension or withdrawal of which it has required;\n(e) provide the national competent authorities of the Member State in which the provider has its registered place of business with all relevant information about the certificates of which it has required the suspension or withdrawal; that authority shall take the appropriate measures, where necessary, to avoid a potential risk to health, safety or fundamental rights.\n8. With the exception of certificates unduly issued, and where a designation has been suspended or restricted, the certificates shall remain valid in one of the following circumstances:\n(a) the notifying authority has confirmed, within one month of the suspension or restriction, that there is no risk to health, safety or fundamental rights in relation to certificates affected by the suspension or restriction, and the notifying authority has outlined a timeline for actions to remedy the suspension or restriction; or\n(b) the notifying authority has confirmed that no certificates relevant to the suspension will be issued, amended or re-issued during the course of the suspension or restriction, and states whether the notified body has the capability of continuing to monitor and remain responsible for existing certificates issued for the period of the suspension or restriction; in the event that the notifying authority determines that the notified body does not have the capability to support existing certificates issued, the provider of the system covered by the certificate shall confirm in writing to the national competent authorities of the Member State in which it has its registered place of business, within three months of the suspension or restriction, that another qualified notified body is temporarily assuming the functions of the notified body to monitor and remain responsible for the certificates during the period of suspension or restriction.\n9. With the exception of certificates unduly issued, and where a designation has been withdrawn, the certificates shall remain valid for a period of nine months under the following circumstances:\n(a) the national competent authority of the Member State in which the provider of the high-risk AI system covered by the certificate has its registered place of business has confirmed that there is no risk to health, safety or fundamental rights associated with the high-risk AI systems concerned; and\n(b) another notified body has confirmed in writing that it will assume immediate responsibility for those AI systems and completes its assessment within 12 months of the withdrawal of the designation.\nIn the circumstances referred to in the first subparagraph, the national competent authority of the Member State in which the provider of the system covered by the certificate has its place of business may extend the provisional validity of the certificates for additional periods of three months, which shall not exceed 12 months in total.\nThe national competent authority or the notified body assuming the functions of the notified body affected by the change of designation shall immediately inform the Commission, the other Member States and the other notified bodies thereof.",
      "original_content": "### Artikel 36: Änderungen der Notifizierungen\n(1) Die notifizierende Behörde unterrichtet die Kommission und die anderen Mitgliedstaaten mithilfe des in Artikel 30 Absatz 2 genannten elektronischen Notifizierungsinstruments über alle relevanten Änderungen der Notifizierung einer notifizierten Stelle.\n(2) Für Erweiterungen des Anwendungsbereichs der Notifizierung gelten die in den Artikeln 29 und 30 festgelegten Verfahren.\nFür andere Änderungen der Notifizierung als Erweiterungen ihres Anwendungsbereichs gelten die in den Absätzen 3 bis 9 dargelegten Verfahren.\n(3) Beschließt eine notifizierte Stelle die Einstellung ihrer Konformitätsbewertungstätigkeiten, so informiert sie die betreffende notifizierende Behörde und die betreffenden Anbieter so bald wie möglich und im Falle einer geplanten Einstellung ihrer Tätigkeiten mindestens ein Jahr vor deren Einstellung darüber. Die Bescheinigungen der notifizierten Stelle können für einen Zeitraum von neun Monaten nach Einstellung der Tätigkeiten der notifizierten Stelle gültig bleiben, sofern eine andere notifizierte Stelle schriftlich bestätigt hat, dass sie die Verantwortung für die von diesen Bescheinigungen abgedeckten Hochrisiko-KI-Systeme übernimmt. Die letztgenannte notifizierte Stelle führt vor Ablauf dieser Frist von neun Monaten eine vollständige Bewertung der betroffenen Hochrisiko-KI-Systeme durch, bevor sie für diese neue Bescheinigungen ausstellt. Stellt die notifizierte Stelle ihre Tätigkeit ein, so widerruft die notifizierende Behörde die Benennung.\n(4) Hat eine notifizierende Behörde hinreichenden Grund zu der Annahme, dass eine notifizierte Stelle die in Artikel 31 festgelegten Anforderungen nicht mehr erfüllt oder dass sie ihren Pflichten nicht nachkommt, so untersucht die notifizierende Behörde den Sachverhalt unverzüglich und mit äußerster Sorgfalt. In diesem Zusammenhang teilt sie der betreffenden notifizierten Stelle die erhobenen Einwände mit und gibt ihr die Möglichkeit, dazu Stellung zu nehmen. Kommt die notifizierende Behörde zu dem Schluss, dass die notifizierte Stelle die in Artikel 31 festgelegten Anforderungen nicht mehr erfüllt oder dass sie ihren Pflichten nicht nachkommt, schränkt sie die Benennung gegebenenfalls ein, setzt sie aus oder widerruft sie, je nach Schwere der Nichterfüllung dieser Anforderungen oder Pflichtverletzung. Sie informiert die Kommission und die anderen Mitgliedstaaten unverzüglich darüber.\n(5) Wird die Benennung einer notifizierten Stelle ausgesetzt, eingeschränkt oder vollständig oder teilweise widerrufen, so informiert die notifizierte Stelle die betreffenden Anbieter innerhalb von zehn Tagen darüber.\n(6) Wird eine Benennung eingeschränkt, ausgesetzt oder widerrufen, so ergreift die notifizierende Behörde geeignete Maßnahmen, um sicherzustellen, dass die Akten der betreffenden notifizierten Stelle für die notifizierenden Behörden in anderen Mitgliedstaaten und die Marktüberwachungsbehörden bereitgehalten und ihnen auf deren Anfrage zur Verfügung gestellt werden.\n(7) Wird eine Benennung eingeschränkt, ausgesetzt oder widerrufen, so geht die notifizierende Behörde wie folgt vor:\na) Sie bewertet die Auswirkungen auf die von der notifizierten Stelle ausgestellten Bescheinigungen;\nb) sie legt der Kommission und den anderen Mitgliedstaaten innerhalb von drei Monaten nach Notifizierung der Änderungen der Benennung einen Bericht über ihre diesbezüglichen Ergebnisse vor;\nc) sie weist die notifizierte Stelle zur Gewährleistung der fortlaufenden Konformität der im Verkehr befindlichen Hochrisiko-KI-Systeme an, sämtliche nicht ordnungsgemäß ausgestellten Bescheinigungen innerhalb einer von der Behörde festgelegten angemessenen Frist auszusetzen oder zu widerrufen;\nd) sie informiert die Kommission und die Mitgliedstaaten über Bescheinigungen, deren Aussetzung oder Widerruf sie angewiesen hat;\ne) sie stellt den zuständigen nationalen Behörden des Mitgliedstaats, in dem der Anbieter seine eingetragene Niederlassung hat, alle relevanten Informationen über Bescheinigungen, deren Aussetzung oder Widerruf sie angewiesen hat, zur Verfügung; diese Behörde ergreift erforderlichenfalls geeignete Maßnahmen, um ein mögliches Risiko für Gesundheit, Sicherheit oder Grundrechte zu verhindern.\n(8) Abgesehen von den Fällen, in denen Bescheinigungen nicht ordnungsgemäß ausgestellt wurden und in denen eine Benennung ausgesetzt oder eingeschränkt wurde, bleiben die Bescheinigungen unter einem der folgenden Umstände gültig:\na) Die notifizierende Behörde hat innerhalb eines Monats nach der Aussetzung oder Einschränkung bestätigt, dass im Zusammenhang mit den von der Aussetzung oder Einschränkung betroffenen Bescheinigungen kein Risiko für Gesundheit, Sicherheit oder Grundrechte besteht, und die notifizierende Behörde hat einen Zeitplan für Maßnahmen zur Aufhebung der Aussetzung oder Einschränkung genannt oder\nb) die notifizierende Behörde hat bestätigt, dass keine von der Aussetzung betroffenen Bescheinigungen während der Dauer der Aussetzung oder Einschränkung ausgestellt, geändert oder erneut ausgestellt werden, und gibt an, ob die notifizierte Stelle in der Lage ist, bestehende ausgestellte Bescheinigungen während der Dauer der Aussetzung oder Einschränkung weiterhin zu überwachen und die Verantwortung dafür zu übernehmen; falls die notifizierende Behörde feststellt, dass die notifizierte Stelle nicht in der Lage ist, bestehende ausgestellte Bescheinigungen weiterzuführen, so bestätigt der Anbieter des von der Bescheinigung abgedeckten Systems den zuständigen nationalen Behörden des Mitgliedstaats, in dem er seine eingetragene Niederlassung hat, innerhalb von drei Monaten nach der Aussetzung oder Einschränkung schriftlich, dass eine andere qualifizierte notifizierte Stelle vorübergehend die Aufgaben der notifizierten Stelle zur Überwachung der Bescheinigung übernimmt und dass sie während der Dauer der Aussetzung oder Einschränkung für die Bescheinigung verantwortlich bleibt.\n(9) Abgesehen von den Fällen, in denen Bescheinigungen nicht ordnungsgemäß ausgestellt wurden und in denen eine Benennung widerrufen wurde, bleiben die Bescheinigungen unter folgenden Umständen für eine Dauer von neun Monaten gültig:\na) Die zuständige nationale Behörde des Mitgliedstaats, in dem der Anbieter des von der Bescheinigung abgedeckten Hochrisiko-KI-Systems seine eingetragene Niederlassung hat, hat bestätigt, dass im Zusammenhang mit den betreffenden Hochrisiko-KI-Systemen kein Risiko für Gesundheit, Sicherheit oder Grundrechte besteht, und\nb) eine andere notifizierte Stelle hat schriftlich bestätigt, dass sie die unmittelbare Verantwortung für diese KI-Systeme übernehmen und deren Bewertung innerhalb von 12 Monaten ab dem Widerruf der Benennung abgeschlossen haben wird.\nUnter den in Unterabsatz 1 genannten Umständen kann die zuständige nationale Behörde des Mitgliedstaats, in dem der Anbieter des von der Bescheinigung abgedeckten Systems seine Niederlassung hat, die vorläufige Gültigkeit der Bescheinigungen um zusätzliche Zeiträume von je drei Monaten, jedoch nicht um insgesamt mehr als 12 Monate, verlängern.\nDie zuständige nationale Behörde oder die notifizierte Stelle, die die Aufgaben der von der Benennungsänderung betroffenen notifizierten Stelle übernimmt, informiert unverzüglich die Kommission, die anderen Mitgliedstaaten und die anderen notifizierten Stellen darüber."
    },
    {
      "chunk_idx": 241,
      "id": "59668afb-a6c5-4909-9fbd-3c58436a069a",
      "title": "Art 37",
      "relevantChunksIds": [
        "491d84d6-b754-4408-8bbd-21c789451f0f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 37: Challenge to the competence of notified bodies\n1. The Commission shall, where necessary, investigate all cases where there are reasons to doubt the competence of a notified body or the continued fulfilment by a notified body of the requirements laid down in Article 31 and of its applicable responsibilities.\n2. The notifying authority shall provide the Commission, on request, with all relevant information relating to the notification or the maintenance of the competence of the notified body concerned.\n3. The Commission shall ensure that all sensitive information obtained in the course of its investigations pursuant to this Article is treated confidentially in accordance with Article 78.\n4. Where the Commission ascertains that a notified body does not meet or no longer meets the requirements for its notification, it shall inform the notifying Member State accordingly and request it to take the necessary corrective measures, including the suspension or withdrawal of the notification if necessary. Where the Member State fails to take the necessary corrective measures, the Commission may, by means of an implementing act, suspend, restrict or withdraw the designation. That implementing act shall be adopted in accordance with the examination procedure referred to in Article 98(2).",
      "original_content": "### Artikel 37: Anfechtungen der Kompetenz notifizierter Stellen\n(1) Die Kommission untersucht erforderlichenfalls alle Fälle, in denen begründete Zweifel an der Kompetenz einer notifizierten Stelle oder daran bestehen, dass eine notifizierte Stelle die in Artikel 31 festgelegten Anforderungen und ihre geltenden Pflichten weiterhin erfüllt.\n(2) Die notifizierende Behörde stellt der Kommission auf Anfrage alle Informationen über die Notifizierung oder die Aufrechterhaltung der Kompetenz der betreffenden notifizierten Stelle zur Verfügung.\n(3) Die Kommission stellt sicher, dass alle im Verlauf ihrer Untersuchungen gemäß diesem Artikel erlangten sensiblen Informationen gemäß Artikel 78 vertraulich behandelt werden.\n(4) Stellt die Kommission fest, dass eine notifizierte Stelle die Anforderungen für ihre Notifizierung nicht oder nicht mehr erfüllt, so informiert sie den notifizierenden Mitgliedstaat entsprechend und fordert ihn auf, die erforderlichen Abhilfemaßnahmen zu treffen, einschließlich einer Aussetzung oder eines Widerrufs der Notifizierung, sofern dies nötig ist. Versäumt es ein Mitgliedstaat, die erforderlichen Abhilfemaßnahmen zu ergreifen, kann die Kommission die Benennung im Wege eines Durchführungsrechtsakts aussetzen, einschränken oder widerrufen. Dieser Durchführungsrechtsakt wird gemäß dem in Artikel 98 Absatz 2 genannten Prüfverfahren erlassen."
    },
    {
      "chunk_idx": 242,
      "id": "8eb6153c-6d1f-4d6b-b0ec-1b046b06ee6b",
      "title": "Art 38",
      "relevantChunksIds": [
        "c7c4a6e2-92a4-40d5-9243-1d698685d1a0",
        "ab48c7d3-9758-42ad-80bd-2aef56bf20b4",
        "5668652e-ee66-42a3-9cd2-d8f70595e52e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 38: Coordination of notified bodies\n1. The Commission shall ensure that, with regard to high-risk AI systems, appropriate coordination and cooperation between notified bodies active in the conformity assessment procedures pursuant to this Regulation are put in place and properly operated in the form of a sectoral group of notified bodies.\n2. Each notifying authority shall ensure that the bodies notified by it participate in the work of a group referred to in paragraph 1, directly or through designated representatives.\n3. The Commission shall provide for the exchange of knowledge and best practices between notifying authorities.",
      "original_content": "### Artikel 38: Koordinierung der notifizierten Stellen\n(1) Die Kommission sorgt dafür, dass in Bezug auf Hochrisiko-KI-Systeme eine zweckmäßige Koordinierung und Zusammenarbeit zwischen den an den Konformitätsbewertungsverfahren im Rahmen dieser Verordnung beteiligten notifizierten Stellen in Form einer sektoralen Gruppe notifizierter Stellen eingerichtet und ordnungsgemäß weitergeführt wird.\n(2) Jede notifizierende Behörde sorgt dafür, dass sich die von ihr notifizierten Stellen direkt oder über benannte Vertreter an der Arbeit der in Absatz 1 genannten Gruppe beteiligen.\n(3) Die Kommission sorgt für den Austausch von Wissen und bewährten Verfahren zwischen den notifizierenden Behörden."
    },
    {
      "chunk_idx": 243,
      "id": "57f3c023-6709-4194-b70c-d00ff5c769b5",
      "title": "Art 39",
      "relevantChunksIds": [
        "c7c4a6e2-92a4-40d5-9243-1d698685d1a0",
        "ab48c7d3-9758-42ad-80bd-2aef56bf20b4"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 39: Conformity assessment bodies of third countries\nConformity assessment bodies established under the law of a third country with which the Union has concluded an agreement may be authorised to carry out the activities of notified bodies under this Regulation, provided that they meet the requirements laid down in Article 31 or they ensure an equivalent level of compliance.",
      "original_content": "### Artikel 39: Konformitätsbewertungsstellen in Drittländern\nKonformitätsbewertungsstellen, die nach dem Recht eines Drittlands errichtet wurden, mit dem die Union ein Abkommen geschlossen hat, können ermächtigt werden, die Tätigkeiten notifizierter Stellen gemäß dieser Verordnung durchzuführen, sofern sie die Anforderungen gemäß Artikel 31 erfüllen oder das gleiche Maß an Konformität gewährleisten."
    },
    {
      "chunk_idx": 244,
      "id": "e7a7f354-d2ea-480e-80eb-6f29beca4569",
      "title": "Art 40",
      "relevantChunksIds": [
        "4f2899cb-4889-4299-8ac6-1c62a78e48ba",
        "8b015b72-9483-4675-bf5a-02c2ddc3a267",
        "59e64fd2-e26e-426d-be65-48f42d265850"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "## SECTION 5: Standards, conformity assessment, certificates, registration\n### Article 40: Harmonised standards and standardisation deliverables\n1. High-risk AI systems or general-purpose AI models which are in conformity with harmonised standards or parts thereof the references of which have been published in the Official Journal of the European Union in accordance with Regulation (EU) No 1025/2012 shall be presumed to be in conformity with the requirements set out in Section 2 of this Chapter or, as applicable, with the obligations set out in of Chapter V, Sections 2 and 3, of this Regulation, to the extent that those standards cover those requirements or obligations.\n2. In accordance with Article 10 of Regulation (EU) No 1025/2012, the Commission shall issue, without undue delay, standardisation requests covering all requirements set out in Section 2 of this Chapter and, as applicable, standardisation requests covering obligations set out in Chapter V, Sections 2 and 3, of this Regulation. The standardisation request shall also ask for deliverables on reporting and documentation processes to improve AI systems’ resource performance, such as reducing the high-risk AI system’s consumption of energy and of other resources during its lifecycle, and on the energy-efficient development of general-purpose AI models. When preparing a standardisation request, the Commission shall consult the Board and relevant stakeholders, including the advisory forum.\nWhen issuing a standardisation request to European standardisation organisations, the Commission shall specify that standards have to be clear, consistent, including with the standards developed in the various sectors for products covered by the existing Union harmonisation legislation listed in Annex I, and aiming to ensure that high-risk AI systems or general-purpose AI models placed on the market or put into service in the Union meet the relevant requirements or obligations laid down in this Regulation.\nThe Commission shall request the European standardisation organisations to provide evidence of their best efforts to fulfil the objectives referred to in the first and the second subparagraph of this paragraph in accordance with Article 24 of Regulation (EU) No 1025/2012.\n3. The participants in the standardisation process shall seek to promote investment and innovation in AI, including through increasing legal certainty, as well as the competitiveness and growth of the Union market, to contribute to strengthening global cooperation on standardisation and taking into account existing international standards in the field of AI that are consistent with Union values, fundamental rights and interests, and to enhance multi-stakeholder governance ensuring a balanced representation of interests and the effective participation of all relevant stakeholders in accordance with Articles 5, 6, and 7 of Regulation (EU) No 1025/2012.",
      "original_content": "## ABSCHNITT 5: Normen, Konformitätsbewertung, Bescheinigungen, Registrierung\n### Artikel 40: Harmonisierte Normen und Normungsdokumente\n(1) Bei Hochrisiko-KI-Systemen oder KI-Modellen mit allgemeinem Verwendungszweck, die mit harmonisierten Normen oder Teilen davon, deren Fundstellen gemäß der Verordnung (EU) Nr. 1025/2012 im Amtsblatt der Europäischen Union veröffentlicht wurden, übereinstimmen, wird eine Konformität mit den Anforderungen gemäß Abschnitt 2 des vorliegenden Kapitels oder gegebenenfalls mit den Pflichten gemäß Kapitel V Abschnitte 2 und 3 der vorliegenden Verordnung vermutet, soweit diese Anforderungen oder Verpflichtungen von den Normen abgedeckt sind.\n(2) Gemäß Artikel 10 der Verordnung (EU) Nr. 1025/2012 erteilt die Kommission unverzüglich Normungsaufträge, die alle Anforderungen gemäß Abschnitt 2 des vorliegenden Kapitels abdecken und gegebenenfalls Normungsaufträge, die Pflichten gemäß Kapitel V Abschnitte 2 und 3 der vorliegenden Verordnung abdecken. In dem Normungsauftrag werden auch Dokumente zu den Berichterstattungs- und Dokumentationsverfahren im Hinblick auf die Verbesserung der Ressourcenleistung von KI-Systemen z. B. durch die Verringerung des Energie- und sonstigen Ressourcenverbrauchs des Hochrisiko-KI-Systems während seines gesamten Lebenszyklus und zu der energieeffizienten Entwicklung von KI-Modellen mit allgemeinem Verwendungszweck verlangt. Bei der Ausarbeitung des Normungsauftrags konsultiert die Kommission das KI-Gremium und die einschlägigen Interessenträger, darunter das Beratungsforum.\nBei der Erteilung eines Normungsauftrags an die europäischen Normungsorganisationen gibt die Kommission an, dass die Normen klar und — u. a. mit den Normen, die in den verschiedenen Sektoren für Produkte entwickelt wurden, die unter die in Anhang I aufgeführten geltenden Harmonisierungsrechtsvorschriften der Union fallen — konsistent sein müssen und sicherstellen sollen, dass die in der Union in Verkehr gebrachten oder in Betrieb genommenen Hochrisiko-KI-Systeme oder KI-Modelle mit allgemeinem Verwendungszweck die in dieser Verordnung festgelegten einschlägigen Anforderungen oder Pflichten erfüllen.\nDie Kommission fordert die europäischen Normungsorganisationen auf, Nachweise dafür vorzulegen, dass sie sich nach besten Kräften bemühen, die in den Unterabsätzen 1 und 2 dieses Absatzes genannten Ziele im Einklang mit Artikel 24 der Verordnung (EU) Nr. 1025/2012 zu erreichen.\n(3) Die am Normungsprozess Beteiligten bemühen sich, Investitionen und Innovationen im Bereich der KI, u. a. durch Erhöhung der Rechtssicherheit, sowie der Wettbewerbsfähigkeit und des Wachstums des Unionsmarktes zu fördern und zur Stärkung der weltweiten Zusammenarbeit bei der Normung und zur Berücksichtigung bestehender internationaler Normen im Bereich der KI, die mit den Werten, Grundrechten und Interessen der Union im Einklang stehen, beizutragen und die Multi-Stakeholder-Governance zu verbessern, indem eine ausgewogene Vertretung der Interessen und eine wirksame Beteiligung aller relevanten Interessenträger gemäß den Artikeln 5, 6 und 7 der Verordnung (EU) Nr. 1025/2012 sichergestellt werden."
    },
    {
      "chunk_idx": 245,
      "id": "80666904-0b31-4c56-8bd9-2d238d969a2f",
      "title": "Art 41",
      "relevantChunksIds": [
        "90893d61-25fd-4ae1-8232-5e4cdb5675e9"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 41: Common specifications\n1. The Commission may adopt, implementing acts establishing common specifications for the requirements set out in Section 2 of this Chapter or, as applicable, for the obligations set out in Sections 2 and 3 of Chapter V where the following conditions have been fulfilled:\n(a) the Commission has requested, pursuant to Article 10(1) of Regulation (EU) No 1025/2012, one or more European standardisation organisations to draft a harmonised standard for the requirements set out in Section 2 of this Chapter, or, as applicable, for the obligations set out in Sections 2 and 3 of Chapter V, and:\n(i) the request has not been accepted by any of the European standardisation organisations; or\n(ii) the harmonised standards addressing that request are not delivered within the deadline set in accordance with Article 10(1) of Regulation (EU) No 1025/2012; or\n(iii) the relevant harmonised standards insufficiently address fundamental rights concerns; or\n(iv) the harmonised standards do not comply with the request; and\n(b) no reference to harmonised standards covering the requirements referred to in Section 2 of this Chapter or, as applicable, the obligations referred to in Sections 2 and 3 of Chapter V has been published in the Official Journal of the European Union in accordance with Regulation (EU) No 1025/2012, and no such reference is expected to be published within a reasonable period.\nWhen drafting the common specifications, the Commission shall consult the advisory forum referred to in Article 67.\nThe implementing acts referred to in the first subparagraph of this paragraph shall be adopted in accordance with the examination procedure referred to in Article 98(2).\n2. Before preparing a draft implementing act, the Commission shall inform the committee referred to in Article 22 of Regulation (EU) No 1025/2012 that it considers the conditions laid down in paragraph 1 of this Article to be fulfilled.\n3. High-risk AI systems or general-purpose AI models which are in conformity with the common specifications referred to in paragraph 1, or parts of those specifications, shall be presumed to be in conformity with the requirements set out in Section 2 of this Chapter or, as applicable, to comply with the obligations referred to in Sections 2 and 3 of Chapter V, to the extent those common specifications cover those requirements or those obligations.\n4. Where a harmonised standard is adopted by a European standardisation organisation and proposed to the Commission for the publication of its reference in the Official Journal of the European Union, the Commission shall assess the harmonised standard in accordance with Regulation (EU) No 1025/2012. When reference to a harmonised standard is published in the Official Journal of the European Union, the Commission shall repeal the implementing acts referred to in paragraph 1, or parts thereof which cover the same requirements set out in Section 2 of this Chapter or, as applicable, the same obligations set out in Sections 2 and 3 of Chapter V.\n5. Where providers of high-risk AI systems or general-purpose AI models do not comply with the common specifications referred to in paragraph 1, they shall duly justify that they have adopted technical solutions that meet the requirements referred to in Section 2 of this Chapter or, as applicable, comply with the obligations set out in Sections 2 and 3 of Chapter V to a level at least equivalent thereto.\n6. Where a Member State considers that a common specification does not entirely meet the requirements set out in Section 2 or, as applicable, comply with obligations set out in Sections 2 and 3 of Chapter V, it shall inform the Commission thereof with a detailed explanation. The Commission shall assess that information and, if appropriate, amend the implementing act establishing the common specification concerned.",
      "original_content": "### Artikel 41: Gemeinsame Spezifikationen\n(1) Die Kommission kann Durchführungsrechtsakte zur Festlegung gemeinsamer Spezifikationen für die Anforderungen gemäß Abschnitt 2 dieses Kapitels oder gegebenenfalls die Pflichten gemäß Kapitel V Abschnitte 2 und 3 erlassen, wenn die folgenden Bedingungen erfüllt sind:\na) Die Kommission hat gemäß Artikel 10 Absatz 1 der Verordnung (EU) Nr. 1025/2012 eine oder mehrere europäische Normungsorganisationen damit beauftragt, eine harmonisierte Norm für die in Abschnitt 2 dieses Kapitels festgelegten Anforderungen oder gegebenenfalls für die in Kapitel V Abschnitte 2 und 3 festgelegten Pflichten zu erarbeiten, und\ni) der Auftrag wurde von keiner der europäischen Normungsorganisationen angenommen oder\nii) die harmonisierten Normen, die Gegenstand dieses Auftrags sind, werden nicht innerhalb der gemäß Artikel 10 Absatz 1 der Verordnung (EU) Nr. 1025/2012 festgelegten Frist erarbeitet oder\niii) die einschlägigen harmonisierten Normen tragen den Bedenken im Bereich der Grundrechte nicht ausreichend Rechnung oder\niv) die harmonisierten Normen entsprechen nicht dem Auftrag und\nb) im Amtsblatt der Europäischen Union sind keine Fundstellen zu harmonisierten Normen gemäß der Verordnung (EU) Nr. 1025/2012 veröffentlicht, die den in Abschnitt 2 dieses Kapitels aufgeführten Anforderungen oder gegebenenfalls den in Kapitel V Abschnitte 2 und 3 aufgeführten Pflichten genügen, und es ist nicht zu erwarten, dass eine solche Fundstelle innerhalb eines angemessenen Zeitraums veröffentlicht wird.\nBei der Verfassung der gemeinsamen Spezifikationen konsultiert die Kommission das in Artikel 67 genannte Beratungsforum.\nDie in Unterabsatz 1 des vorliegenden Absatzes genannten Durchführungsrechtsakte werden gemäß dem in Artikel 98 Absatz 2 genannten Prüfverfahren erlassen.\n(2) Vor der Ausarbeitung eines Entwurfs eines Durchführungsrechtsakts informiert die Kommission den in Artikel 22 der Verordnung (EU) Nr. 1025/2012 genannten Ausschuss darüber, dass sie die in Absatz 1 des vorliegenden Artikels festgelegten Bedingungen als erfüllt erachtet.\n(3) Bei Hochrisiko-KI-Systemen oder KI-Modellen mit allgemeinem Verwendungszweck, die mit den in Absatz 1 genannten gemeinsamen Spezifikationen oder Teilen dieser Spezifikationen übereinstimmen, wird eine Konformität mit den Anforderungen in Abschnitt 2 dieses Kapitels oder gegebenenfalls die Einhaltung der in Kapitel V Abschnitte 2 und 3 genannten Pflichten vermutet, soweit diese Anforderungen oder diese Pflichten von den gemeinsamen Spezifikationen abgedeckt sind.\n(4) Wird eine harmonisierte Norm von einer europäischen Normungsorganisation angenommen und der Kommission zur Veröffentlichung ihrer Fundstelle im Amtsblatt der Europäischen Union vorgeschlagen, so bewertet die Kommission die harmonisierte Norm gemäß der Verordnung (EU) Nr. 1025/2012. Wird die Fundstelle zu einer harmonisierten Norm im Amtsblatt der Europäischen Union veröffentlicht, so werden die in Absatz 1 genannten Durchführungsrechtsakte, die dieselben Anforderungen gemäß Abschnitt 2 dieses Kapitels oder gegebenenfalls dieselben Pflichten gemäß Kapitel V Abschnitte 2 und 3 erfassen, von der Kommission ganz oder teilweise aufgehoben.\n(5) Wenn Anbieter von Hochrisiko-KI-Systemen oder KI-Modellen mit allgemeinem Verwendungszweck die in Absatz 1 genannten gemeinsamen Spezifikationen nicht befolgen, müssen sie hinreichend nachweisen, dass sie technische Lösungen verwenden, die die in Abschnitt 2 dieses Kapitels aufgeführten Anforderungen oder gegebenenfalls die Pflichten gemäß Kapitel V Abschnitte 2 und 3 zumindest in gleichem Maße erfüllen;\n(6) Ist ein Mitgliedstaat der Auffassung, dass eine gemeinsame Spezifikation den Anforderungen gemäß Abschnitt 2 nicht vollständig entspricht oder gegebenenfalls die Pflichten gemäß Kapitel V Abschnitte 2 und 3 nicht vollständig erfüllt, so setzt er die Kommission im Rahmen einer ausführlichen Erläuterung davon in Kenntnis. Die Kommission bewertet die betreffende Information und ändert gegebenenfalls den Durchführungsrechtsakt, durch den die betreffende gemeinsame Spezifikation festgelegt wurde."
    },
    {
      "chunk_idx": 246,
      "id": "7461fda7-1c33-468c-9be3-4f3ad8d29452",
      "title": "Art 42",
      "relevantChunksIds": [
        "b7a14263-88d3-44cc-9341-5e2d1a7ddc73"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 42: Presumption of conformity with certain requirements\n1. High-risk AI systems that have been trained and tested on data reflecting the specific geographical, behavioural, contextual or functional setting within which they are intended to be used shall be presumed to comply with the relevant requirements laid down in Article 10(4).\n2. High-risk AI systems that have been certified or for which a statement of conformity has been issued under a cybersecurity scheme pursuant to Regulation (EU) 2019/881 and the references of which have been published in the Official Journal of the European Union shall be presumed to comply with the cybersecurity requirements set out in Article 15 of this Regulation in so far as the cybersecurity certificate or statement of conformity or parts thereof cover those requirements.",
      "original_content": "### Artikel 42: Vermutung der Konformität mit bestimmten Anforderungen\n(1) Für Hochrisiko-KI-Systeme, die mit Daten, in denen sich die besonderen geografischen, verhaltensbezogenen, kontextuellen oder funktionalen Rahmenbedingungen niederschlagen, unter denen sie verwendet werden sollen, trainiert und getestet wurden, gilt die Vermutung, dass sie die in Artikel 10 Absatz 4 festgelegten einschlägigen Anforderungen erfüllen.\n(2) Für Hochrisiko-KI-Systeme, die im Rahmen eines der Cybersicherheitszertifizierungssysteme gemäß der Verordnung (EU) 2019/881, deren Fundstellen im Amtsblatt der Europäischen Union veröffentlicht wurden, zertifiziert wurden oder für die eine solche Konformitätserklärung erstellt wurde, gilt die Vermutung, dass sie die in Artikel 15 der vorliegenden Verordnung festgelegten Cybersicherheitsanforderungen erfüllen, sofern diese Anforderungen von der Cybersicherheitszertifizierung oder der Konformitätserklärung oder Teilen davon abdeckt sind."
    },
    {
      "chunk_idx": 247,
      "id": "aeae4d53-6d81-4c49-843d-b93704615c05",
      "title": "Art 43",
      "relevantChunksIds": [
        "b7a14263-88d3-44cc-9341-5e2d1a7ddc73",
        "dffe5a54-c8df-4391-9a4c-7180f3dbda14",
        "33677500-8ade-4c9b-96ec-eff3f9ffb701",
        "c7c4a6e2-92a4-40d5-9243-1d698685d1a0",
        "ab48c7d3-9758-42ad-80bd-2aef56bf20b4",
        "a709f623-ae8c-4f40-97fc-ce028af806d3",
        "16e58dc6-88e2-4ab6-8bb3-6f208fb0c25d",
        "2ffebd22-e662-4762-80e2-63f3edced074"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 43: Conformity assessment\n1. For high-risk AI systems listed in point 1 of Annex III, where, in demonstrating the compliance of a high-risk AI system with the requirements set out in Section 2, the provider has applied harmonised standards referred to in Article 40, or, where applicable, common specifications referred to in Article 41, the provider shall opt for one of the following conformity assessment procedures based on:\n(a) the internal control referred to in Annex VI; or\n(b) the assessment of the quality management system and the assessment of the technical documentation, with the involvement of a notified body, referred to in Annex VII.\nIn demonstrating the compliance of a high-risk AI system with the requirements set out in Section 2, the provider shall follow the conformity assessment procedure set out in Annex VII where:\n(a) harmonised standards referred to in Article 40 do not exist, and common specifications referred to in Article 41 are not available;\n(b) the provider has not applied, or has applied only part of, the harmonised standard;\n(c) the common specifications referred to in point (a) exist, but the provider has not applied them;\n(d) one or more of the harmonised standards referred to in point (a) has been published with a restriction, and only on the part of the standard that was restricted.\nFor the purposes of the conformity assessment procedure referred to in Annex VII, the provider may choose any of the notified bodies. However, where the high-risk AI system is intended to be put into service by law enforcement, immigration or asylum authorities or by Union institutions, bodies, offices or agencies, the market surveillance authority referred to in Article 74(8) or (9), as applicable, shall act as a notified body.\n2. For high-risk AI systems referred to in points 2 to 8 of Annex III, providers shall follow the conformity assessment procedure based on internal control as referred to in Annex VI, which does not provide for the involvement of a notified body.\n3. For high-risk AI systems covered by the Union harmonisation legislation listed in Section A of Annex I, the provider shall follow the relevant conformity assessment procedure as required under those legal acts. The requirements set out in Section 2 of this Chapter shall apply to those high-risk AI systems and shall be part of that assessment. Points 4.3., 4.4., 4.5. and the fifth paragraph of point 4.6 of Annex VII shall also apply.\nFor the purposes of that assessment, notified bodies which have been notified under those legal acts shall be entitled to control the conformity of the high-risk AI systems with the requirements set out in Section 2, provided that the compliance of those notified bodies with requirements laid down in Article 31(4), (5), (10) and (11) has been assessed in the context of the notification procedure under those legal acts.\nWhere a legal act listed in Section A of Annex I enables the product manufacturer to opt out from a third-party conformity assessment, provided that that manufacturer has applied all harmonised standards covering all the relevant requirements, that manufacturer may use that option only if it has also applied harmonised standards or, where applicable, common specifications referred to in Article 41, covering all requirements set out in Section 2 of this Chapter.\n4. High-risk AI systems that have already been subject to a conformity assessment procedure shall undergo a new conformity assessment procedure in the event of a substantial modification, regardless of whether the modified system is intended to be further distributed or continues to be used by the current deployer.\nFor high-risk AI systems that continue to learn after being placed on the market or put into service, changes to the high-risk AI system and its performance that have been pre-determined by the provider at the moment of the initial conformity assessment and are part of the information contained in the technical documentation referred to in point 2(f) of Annex IV, shall not constitute a substantial modification.\n5. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend Annexes VI and VII by updating them in light of technical progress.\n6. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend paragraphs 1 and 2 of this Article in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to the conformity assessment procedure referred to in Annex VII or parts thereof. The Commission shall adopt such delegated acts taking into account the effectiveness of the conformity assessment procedure based on internal control referred to in Annex VI in preventing or minimising the risks to health and safety and protection of fundamental rights posed by such systems, as well as the availability of adequate capacities and resources among notified bodies.",
      "original_content": "### Artikel 43: Konformitätsbewertung\n(1) Hat ein Anbieter zum Nachweis, dass ein in Anhang III Nummer 1 aufgeführtes Hochrisiko-KI-System die in Abschnitt 2 festgelegten Anforderungen erfüllt, harmonisierte Normen gemäß Artikel 40 oder gegebenenfalls gemeinsame Spezifikationen gemäß Artikel 41 angewandt, so entscheidet er sich für eines der folgenden Konformitätsbewertungsverfahren auf der Grundlage\na) der internen Kontrolle gemäß Anhang VI oder\nb) der Bewertung des Qualitätsmanagementsystems und der Bewertung der technischen Dokumentation unter Beteiligung einer notifizierten Stelle gemäß Anhang VII.\nZum Nachweis, dass sein Hochrisiko-KI-System die in Abschnitt 2 festgelegten Anforderungen erfüllt, befolgt der Anbieter das Konformitätsbewertungsverfahren gemäß Anhang VII, wenn\na) es harmonisierte Normen gemäß Artikel 40 nicht gibt und keine gemeinsamen Spezifikationen gemäß Artikel 41 vorliegen,\nb) der Anbieter die harmonisierte Norm nicht oder nur teilweise angewandt hat;\nc) die unter Buchstabe a genannten gemeinsamen Spezifikationen zwar vorliegen, der Anbieter sie jedoch nicht angewandt hat;\nd) eine oder mehrere der unter Buchstabe a genannten harmonisierten Normen mit einer Einschränkung und nur für den eingeschränkten Teil der Norm veröffentlicht wurden.\nFür die Zwecke des Konformitätsbewertungsverfahrens gemäß Anhang VII kann der Anbieter eine der notifizierten Stellen auswählen. Soll das Hochrisiko-KI-System jedoch von Strafverfolgungs-, Einwanderungs- oder Asylbehörden oder von Organen, Einrichtungen oder sonstigen Stellen der Union in Betrieb genommen werden, so übernimmt die in Artikel 74 Absatz 8 bzw. 9 genannte Marktüberwachungsbehörde die Funktion der notifizierten Stelle.\n(2) Bei den in Anhang III Nummern 2 bis 8 aufgeführten Hochrisiko-KI-Systemen befolgen die Anbieter das Konformitätsbewertungsverfahren auf der Grundlage einer internen Kontrolle gemäß Anhang VI, das keine Beteiligung einer notifizierten Stelle vorsieht.\n(3) Bei den Hochrisiko-KI-Systemen, die unter die in Anhang I Abschnitt A aufgeführten Harmonisierungsrechtsakte der Union fallen, befolgt der Anbieter die einschlägigen Konformitätsbewertungsverfahren, die nach diesen Rechtsakten erforderlich sind. Die in Abschnitt 2 dieses Kapitels festgelegten Anforderungen gelten für diese Hochrisiko-KI-Systeme und werden in diese Bewertung einbezogen. Anhang VII Nummern 4.3, 4.4 und 4.5 sowie Nummer 4.6 Absatz 5 finden ebenfalls Anwendung.\nFür die Zwecke dieser Bewertung sind die notifizierten Stellen, die gemäß diesen Rechtsakten notifiziert wurden, berechtigt, die Konformität der Hochrisiko-KI-Systeme mit den in Abschnitt 2 festgelegten Anforderungen zu kontrollieren, sofern im Rahmen des gemäß diesen Rechtsakten durchgeführten Notifizierungsverfahrens geprüft wurde, dass diese notifizierten Stellen die in Artikel 31 Absätze 4, 5, 10 und 11 festgelegten Anforderungen erfüllen.\nWenn ein in Anhang I Abschnitt A aufgeführter Rechtsakte es dem Hersteller des Produkts ermöglicht, auf eine Konformitätsbewertung durch Dritte zu verzichten, sofern dieser Hersteller alle harmonisierten Normen, die alle einschlägigen Anforderungen abdecken, angewandt hat, so darf dieser Hersteller nur dann von dieser Möglichkeit Gebrauch machen, wenn er auch harmonisierte Normen oder gegebenenfalls gemeinsame Spezifikationen gemäß Artikel 41, die alle in Abschnitt 2 dieses Kapitels festgelegten Anforderungen abdecken, angewandt hat.\n(4) Hochrisiko-KI-Systeme, die bereits Gegenstand eines Konformitätsbewertungsverfahren gewesen sind, werden im Falle einer wesentlichen Änderung einem neuen Konformitätsbewertungsverfahren unterzogen, unabhängig davon, ob das geänderte System noch weiter in Verkehr gebracht oder vom derzeitigen Betreiber weitergenutzt werden soll.\nBei Hochrisiko-KI-Systemen, die nach dem Inverkehrbringen oder der Inbetriebnahme weiterhin dazulernen, gelten Änderungen des Hochrisiko-KI-Systems und seiner Leistung, die vom Anbieter zum Zeitpunkt der ursprünglichen Konformitätsbewertung vorab festgelegt wurden und in den Informationen der technischen Dokumentation gemäß Anhang IV Nummer 2 Buchstabe f enthalten sind, nicht als wesentliche Veränderung;\n(5) Die Kommission ist befugt, gemäß Artikel 97 delegierte Rechtsakte zu erlassen, um die Anhänge VI und VII zu ändern, indem sie sie angesichts des technischen Fortschritts aktualisiert.\n(6) Die Kommission ist befugt, gemäß Artikel 97 delegierte Rechtsakte zur Änderung der Absätze 1 und 2 des vorliegenden Artikels zu erlassen, um die in Anhang III Nummern 2 bis 8 genannten Hochrisiko-KI-Systeme dem Konformitätsbewertungsverfahren gemäß Anhang VII oder Teilen davon zu unterwerfen. Die Kommission erlässt solche delegierten Rechtsakte unter Berücksichtigung der Wirksamkeit des Konformitätsbewertungsverfahrens auf der Grundlage einer internen Kontrolle gemäß Anhang VI hinsichtlich der Vermeidung oder Minimierung der von solchen Systemen ausgehenden Risiken für die Gesundheit und Sicherheit und den Schutz der Grundrechte sowie hinsichtlich der Verfügbarkeit angemessener Kapazitäten und Ressourcen in den notifizierten Stellen."
    },
    {
      "chunk_idx": 248,
      "id": "2e2eba3d-b388-48a7-99a6-b31c4ea1f382",
      "title": "Art 44",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 44: Certificates\n1. Certificates issued by notified bodies in accordance with Annex VII shall be drawn-up in a language which can be easily understood by the relevant authorities in the Member State in which the notified body is established.\n2. Certificates shall be valid for the period they indicate, which shall not exceed five years for AI systems covered by Annex I, and four years for AI systems covered by Annex III. At the request of the provider, the validity of a certificate may be extended for further periods, each not exceeding five years for AI systems covered by Annex I, and four years for AI systems covered by Annex III, based on a re-assessment in accordance with the applicable conformity assessment procedures. Any supplement to a certificate shall remain valid, provided that the certificate which it supplements is valid.\n3. Where a notified body finds that an AI system no longer meets the requirements set out in Section 2, it shall, taking account of the principle of proportionality, suspend or withdraw the certificate issued or impose restrictions on it, unless compliance with those requirements is ensured by appropriate corrective action taken by the provider of the system within an appropriate deadline set by the notified body. The notified body shall give reasons for its decision.\nAn appeal procedure against decisions of the notified bodies, including on conformity certificates issued, shall be available.",
      "original_content": "### Artikel 44: Bescheinigungen\n(1) Die von notifizierten Stellen gemäß Anhang VII ausgestellten Bescheinigungen werden in einer Sprache ausgefertigt, die für die einschlägigen Behörden des Mitgliedstaats, in dem die notifizierte Stelle niedergelassen ist, leicht verständlich ist.\n(2) Die Bescheinigungen sind für die darin genannte Dauer gültig, die maximal fünf Jahre für unter Anhang I fallende KI-Systeme und maximal vier Jahre für unter Anhang III fallende KI-Systeme beträgt. Auf Antrag des Anbieters kann die Gültigkeit einer Bescheinigung auf der Grundlage einer Neubewertung gemäß den geltenden Konformitätsbewertungsverfahren um weitere Zeiträume von jeweils höchstens fünf Jahren für unter Anhang I fallende KI-Systeme und höchstens vier Jahre für unter Anhang III fallende KI-Systeme verlängert werden. Eine Ergänzung zu einer Bescheinigung bleibt gültig, sofern die Bescheinigung, zu der sie gehört, gültig ist.\n(3) Stellt eine notifizierte Stelle fest, dass ein KI-System die in Abschnitt 2 festgelegten Anforderungen nicht mehr erfüllt, so setzt sie die ausgestellte Bescheinigung aus, widerruft sie oder schränkt sie ein, jeweils unter Berücksichtigung des Grundsatzes der Verhältnismäßigkeit, sofern die Einhaltung der Anforderungen nicht durch geeignete Korrekturmaßnahmen des Anbieters des Systems innerhalb einer von der notifizierten Stelle gesetzten angemessenen Frist wiederhergestellt wird. Die notifizierte Stelle begründet ihre Entscheidung.\nEs muss ein Einspruchsverfahren gegen die Entscheidungen der notifizierten Stellen, auch solche über ausgestellte Konformitätsbescheinigungen, vorgesehen sein."
    },
    {
      "chunk_idx": 249,
      "id": "6132fe3a-5c8b-4fbe-8bd9-0fd34eea0e03",
      "title": "Art 45",
      "relevantChunksIds": [
        "ab48c7d3-9758-42ad-80bd-2aef56bf20b4"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 45: Information obligations of notified bodies\n1. Notified bodies shall inform the notifying authority of the following:\n(a) any Union technical documentation assessment certificates, any supplements to those certificates, and any quality management system approvals issued in accordance with the requirements of Annex VII;\n(b) any refusal, restriction, suspension or withdrawal of a Union technical documentation assessment certificate or a quality management system approval issued in accordance with the requirements of Annex VII;\n(c) any circumstances affecting the scope of or conditions for notification;\n(d) any request for information which they have received from market surveillance authorities regarding conformity assessment activities;\n(e) on request, conformity assessment activities performed within the scope of their notification and any other activity performed, including cross-border activities and subcontracting.\n2. Each notified body shall inform the other notified bodies of:\n(a) quality management system approvals which it has refused, suspended or withdrawn, and, upon request, of quality system approvals which it has issued;\n(b) Union technical documentation assessment certificates or any supplements thereto which it has refused, withdrawn, suspended or otherwise restricted, and, upon request, of the certificates and/or supplements thereto which it has issued.\n3. Each notified body shall provide the other notified bodies carrying out similar conformity assessment activities covering the same types of AI systems with relevant information on issues relating to negative and, on request, positive conformity assessment results.\n4. Notified bodies shall safeguard the confidentiality of the information that they obtain, in accordance with Article 78.",
      "original_content": "### Artikel 45: Informationspflichten der notifizierten Stellen\n(1) Die notifizierten Stellen informieren die notifizierende Behörde über\na) alle Unionsbescheinigungen über die Bewertung der technischen Dokumentation, etwaige Ergänzungen dieser Bescheinigungen und alle Genehmigungen von Qualitätsmanagementsystemen, die gemäß den Anforderungen des Anhangs VII erteilt wurden;\nb) alle Verweigerungen, Einschränkungen, Aussetzungen oder Rücknahmen von Unionsbescheinigungen über die Bewertung der technischen Dokumentation oder Genehmigungen von Qualitätsmanagementsystemen, die gemäß den Anforderungen des Anhangs VII erteilt wurden;\nc) alle Umstände, die Folgen für den Anwendungsbereich oder die Bedingungen der Notifizierung haben;\nd) alle Auskunftsersuchen über Konformitätsbewertungstätigkeiten, die sie von den Marktüberwachungsbehörden erhalten haben;\ne) auf Anfrage die Konformitätsbewertungstätigkeiten, denen sie im Anwendungsbereich ihrer Notifizierung nachgegangen sind, und sonstige Tätigkeiten, einschließlich grenzüberschreitender Tätigkeiten und Vergabe von Unteraufträgen, die sie durchgeführt haben.\n(2) Jede notifizierte Stelle informiert die anderen notifizierten Stellen über\na) die Genehmigungen von Qualitätsmanagementsystemen, die sie verweigert, ausgesetzt oder zurückgenommen hat, und auf Anfrage die Genehmigungen von Qualitätsmanagementsystemen, die sie erteilt hat;\nb) die Bescheinigungen der Union über die Bewertung der technischen Dokumentation und deren etwaige Ergänzungen, die sie verweigert, ausgesetzt oder zurückgenommen oder anderweitig eingeschränkt hat, und auf Anfrage die Bescheinigungen und/oder deren Ergänzungen, die sie ausgestellt hat.\n(3) Jede notifizierte Stelle übermittelt den anderen notifizierten Stellen, die ähnlichen Konformitätsbewertungstätigkeiten für die gleichen Arten der KI-Systeme nachgehen, einschlägige Informationen über negative und auf Anfrage über positive Konformitätsbewertungsergebnisse.\n(4) Notifizierende Behörden gewährleisten gemäß Artikel 78 die Vertraulichkeit der von ihnen erlangten Informationen."
    },
    {
      "chunk_idx": 250,
      "id": "2a95bf85-4c68-478b-b8d0-0d8874b0a5c5",
      "title": "Art 46",
      "relevantChunksIds": [
        "2484d9b1-5411-470b-8e36-171561caba05"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 46: Derogation from conformity assessment procedure\n1. By way of derogation from Article 43 and upon a duly justified request, any market surveillance authority may authorise the placing on the market or the putting into service of specific high-risk AI systems within the territory of the Member State concerned, for exceptional reasons of public security or the protection of life and health of persons, environmental protection or the protection of key industrial and infrastructural assets. That authorisation shall be for a limited period while the necessary conformity assessment procedures are being carried out, taking into account the exceptional reasons justifying the derogation. The completion of those procedures shall be undertaken without undue delay.\n2. In a duly justified situation of urgency for exceptional reasons of public security or in the case of specific, substantial and imminent threat to the life or physical safety of natural persons, law-enforcement authorities or civil protection authorities may put a specific high-risk AI system into service without the authorisation referred to in paragraph 1, provided that such authorisation is requested during or after the use without undue delay. If the authorisation referred to in paragraph 1 is refused, the use of the high-risk AI system shall be stopped with immediate effect and all the results and outputs of such use shall be immediately discarded.\n3. The authorisation referred to in paragraph 1 shall be issued only if the market surveillance authority concludes that the high-risk AI system complies with the requirements of Section 2. The market surveillance authority shall inform the Commission and the other Member States of any authorisation issued pursuant to paragraphs 1 and 2. This obligation shall not cover sensitive operational data in relation to the activities of law-enforcement authorities.\n4. Where, within 15 calendar days of receipt of the information referred to in paragraph 3, no objection has been raised by either a Member State or the Commission in respect of an authorisation issued by a market surveillance authority of a Member State in accordance with paragraph 1, that authorisation shall be deemed justified.\n5. Where, within 15 calendar days of receipt of the notification referred to in paragraph 3, objections are raised by a Member State against an authorisation issued by a market surveillance authority of another Member State, or where the Commission considers the authorisation to be contrary to Union law, or the conclusion of the Member States regarding the compliance of the system as referred to in paragraph 3 to be unfounded, the Commission shall, without delay, enter into consultations with the relevant Member State. The operators concerned shall be consulted and have the possibility to present their views. Having regard thereto, the Commission shall decide whether the authorisation is justified. The Commission shall address its decision to the Member State concerned and to the relevant operators.\n6. Where the Commission considers the authorisation unjustified, it shall be withdrawn by the market surveillance authority of the Member State concerned.\n7. For high-risk AI systems related to products covered by Union harmonisation legislation listed in Section A of Annex I, only the derogations from the conformity assessment established in that Union harmonisation legislation shall apply.",
      "original_content": "### Artikel 46: Ausnahme vom Konformitätsbewertungsverfahren\n(1) Abweichend von Artikel 43 und auf ein hinreichend begründetes Ersuchen kann eine Marktüberwachungsbehörde das Inverkehrbringen oder die Inbetriebnahme bestimmter Hochrisiko-KI-Systeme im Hoheitsgebiet des betreffenden Mitgliedstaats aus außergewöhnlichen Gründen der öffentlichen Sicherheit, des Schutzes des Lebens und der Gesundheit von Personen, des Umweltschutzes oder des Schutzes wichtiger Industrie- und Infrastrukturanlagen genehmigen. Diese Genehmigung wird auf die Dauer der erforderlichen Konformitätsbewertungsverfahren befristet, wobei den außergewöhnlichen Gründen für die Ausnahme Rechnung getragen wird. Der Abschluss dieser Verfahren erfolgt unverzüglich.\n(2) In hinreichend begründeten dringenden Fällen aus außergewöhnlichen Gründen der öffentlichen Sicherheit oder in Fällen einer konkreten, erheblichen und unmittelbaren Gefahr für das Leben oder die körperliche Unversehrtheit natürlicher Personen können Strafverfolgungsbehörden oder Katastrophenschutzbehörden ein bestimmtes Hochrisiko-KI-System ohne die in Absatz 1 genannte Genehmigung in Betrieb nehmen, sofern diese Genehmigung während der Verwendung oder im Anschluss daran unverzüglich beantragt wird. Falls die Genehmigung gemäß Absatz 1 abgelehnt wird, wird Verwendung des Hochrisiko-KI-Systems mit sofortiger Wirkung eingestellt und sämtliche Ergebnisse und Ausgaben dieser Verwendung werden unverzüglich verworfen.\n(3) Die in Absatz 1 genannte Genehmigung wird nur erteilt, wenn die Marktüberwachungsbehörde zu dem Schluss gelangt, dass das Hochrisiko-KI-System die Anforderungen des Abschnitts 2 erfüllt. Die Marktüberwachungsbehörde informiert die Kommission und die anderen Mitgliedstaaten über alle von ihr gemäß den Absätzen 1 und 2 erteilten Genehmigungen. Diese Pflicht erstreckt sich nicht auf sensible operative Daten zu den Tätigkeiten von Strafverfolgungsbehörden.\n(4) Erhebt weder ein Mitgliedstaat noch die Kommission innerhalb von 15 Kalendertagen nach Erhalt der in Absatz 3 genannten Mitteilung Einwände gegen die von einer Marktüberwachungsbehörde eines Mitgliedstaats gemäß Absatz 1 erteilte Genehmigung, so gilt diese Genehmigung als gerechtfertigt.\n(5) Erhebt innerhalb von 15 Kalendertagen nach Erhalt der in Absatz 3 genannten Mitteilung ein Mitgliedstaat Einwände gegen eine von einer Marktüberwachungsbehörde eines anderen Mitgliedstaats erteilte Genehmigung oder ist die Kommission der Auffassung, dass die Genehmigung mit dem Unionsrecht unvereinbar ist oder dass die Schlussfolgerung der Mitgliedstaaten in Bezug auf die Konformität des in Absatz 3 genannten Systems unbegründet ist, so nimmt die Kommission unverzüglich Konsultationen mit dem betreffenden Mitgliedstaat auf. Die betroffenen Akteure werden konsultiert und erhalten Gelegenheit, dazu Stellung zu nehmen. In Anbetracht dessen entscheidet die Kommission, ob die Genehmigung gerechtfertigt ist. Die Kommission richtet ihren Beschluss an den betroffenen Mitgliedstaat und an die betroffenen Akteure.\n(6) Wird die Genehmigung von der Kommission als ungerechtfertigt erachtet, so muss sie von der Marktüberwachungsbehörde des betreffenden Mitgliedstaats zurückgenommen werden.\n(7) Für Hochrisiko-KI-Systeme im Zusammenhang mit Produkten, die unter die in Anhang I Abschnitt A aufgeführten Harmonisierungsrechtsvorschriften der Union fallen, gelten nur die in diesen Harmonisierungsrechtsvorschriften der Union festgelegten Ausnahmen von den Konformitätsbewertungsverfahren."
    },
    {
      "chunk_idx": 251,
      "id": "53305eb0-be55-40af-8070-94d5214e877c",
      "title": "Art 47",
      "relevantChunksIds": [
        "90893d61-25fd-4ae1-8232-5e4cdb5675e9"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 47: EU declaration of conformity\n1. The provider shall draw up a written machine readable, physical or electronically signed EU declaration of conformity for each high-risk AI system, and keep it at the disposal of the national competent authorities for 10 years after the high-risk AI system has been placed on the market or put into service. The EU declaration of conformity shall identify the high-risk AI system for which it has been drawn up. A copy of the EU declaration of conformity shall be submitted to the relevant national competent authorities upon request.\n2. The EU declaration of conformity shall state that the high-risk AI system concerned meets the requirements set out in Section 2. The EU declaration of conformity shall contain the information set out in Annex V, and shall be translated into a language that can be easily understood by the national competent authorities of the Member States in which the high-risk AI system is placed on the market or made available.\n3. Where high-risk AI systems are subject to other Union harmonisation legislation which also requires an EU declaration of conformity, a single EU declaration of conformity shall be drawn up in respect of all Union law applicable to the high-risk AI system. The declaration shall contain all the information required to identify the Union harmonisation legislation to which the declaration relates.\n4. By drawing up the EU declaration of conformity, the provider shall assume responsibility for compliance with the requirements set out in Section 2. The provider shall keep the EU declaration of conformity up-to-date as appropriate.\n5. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend Annex V by updating the content of the EU declaration of conformity set out in that Annex, in order to introduce elements that become necessary in light of technical progress.",
      "original_content": "### Artikel 47: EU-Konformitätserklärung\n(1) Der Anbieter stellt für jedes Hochrisiko-KI-System eine schriftliche maschinenlesbare, physische oder elektronisch unterzeichnete EU-Konformitätserklärung aus und hält sie für einen Zeitraum von 10 Jahren ab dem Inverkehrbringen oder der Inbetriebnahme des Hochrisiko-KI-Systems für die zuständigen nationalen Behörden bereit. Aus der EU-Konformitätserklärung geht hervor, für welches Hochrisiko-KI-System sie ausgestellt wurde. Eine Kopie der EU-Konformitätserklärung wird den zuständigen nationalen Behörden auf Anfrage übermittelt.\n(2) Die EU-Konformitätserklärung muss feststellen, dass das betreffende Hochrisiko-KI-System die in Abschnitt 2 festgelegten Anforderungen erfüllt. Die EU-Konformitätserklärung enthält die in Anhang V festgelegten Informationen und wird in eine Sprache übersetzt, die für die zuständigen nationalen Behörden der Mitgliedstaaten, in denen das Hochrisiko-KI-System in Verkehr gebracht oder bereitgestellt wird, leicht verständlich ist.\n(3) Unterliegen Hochrisiko-KI-Systeme anderen Harmonisierungsrechtsvorschriften der Union, die ebenfalls eine EU-Konformitätserklärung vorschreiben, so wird eine einzige EU-Konformitätserklärung ausgestellt, die sich auf alle für das Hochrisiko-KI-System geltenden Rechtsvorschriften der Union bezieht. Die Erklärung enthält alle erforderlichen Informationen zur Feststellung der Harmonisierungsrechtsvorschriften der Union, auf die sich die Erklärung bezieht.\n(4) Mit der Ausstellung der EU-Konformitätserklärung übernimmt der Anbieter die Verantwortung für die Erfüllung der in Abschnitt 2 festgelegten Anforderungen. Der Anbieter hält die EU-Konformitätserklärung gegebenenfalls auf dem neuesten Stand.\n(5) Der Kommission ist befugt, gemäß Artikel 97 delegierte Rechtsakte zur Aktualisierung des in Anhang V festgelegten Inhalts der EU-Konformitätserklärung zu erlassen, um den genannten Anhang durch die Einführung von Elementen zu ändern, die angesichts des technischen Fortschritts erforderlich werden."
    },
    {
      "chunk_idx": 252,
      "id": "9819f1c8-9e45-4ff4-8edf-575415272e9f",
      "title": "Art 48",
      "relevantChunksIds": [
        "5668652e-ee66-42a3-9cd2-d8f70595e52e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 48: CE marking\n1. The CE marking shall be subject to the general principles set out in Article 30 of Regulation (EC) No 765/2008.\n2. For high-risk AI systems provided digitally, a digital CE marking shall be used, only if it can easily be accessed via the interface from which that system is accessed or via an easily accessible machine-readable code or other electronic means.\n3. The CE marking shall be affixed visibly, legibly and indelibly for high-risk AI systems. Where that is not possible or not warranted on account of the nature of the high-risk AI system, it shall be affixed to the packaging or to the accompanying documentation, as appropriate.\n4. Where applicable, the CE marking shall be followed by the identification number of the notified body responsible for the conformity assessment procedures set out in Article 43. The identification number of the notified body shall be affixed by the body itself or, under its instructions, by the provider or by the provider’s authorised representative. The identification number shall also be indicated in any promotional material which mentions that the high-risk AI system fulfils the requirements for CE marking.\n5. Where high-risk AI systems are subject to other Union law which also provides for the affixing of the CE marking, the CE marking shall indicate that the high-risk AI system also fulfil the requirements of that other law.",
      "original_content": "### Artikel 48: CE-Kennzeichnung\n(1) Für die CE-Kennzeichnung gelten die in Artikel 30 der Verordnung (EG) Nr. 765/2008 festgelegten allgemeinen Grundsätze.\n(2) Bei digital bereitgestellten Hochrisiko-KI-Systemen wird eine digitale CE-Kennzeichnung nur dann verwendet, wenn sie über die Schnittstelle, von der aus auf dieses System zugegriffen wird, oder über einen leicht zugänglichen maschinenlesbaren Code oder andere elektronische Mittel leicht zugänglich ist.\n(3) Die CE-Kennzeichnung wird gut sichtbar, leserlich und dauerhaft an Hochrisiko-KI-Systemen angebracht. Falls die Art des Hochrisiko-KI-Systems dies nicht zulässt oder nicht rechtfertigt, wird sie auf der Verpackung bzw. der beigefügten Dokumentation angebracht.\n(4) Gegebenenfalls wird der CE-Kennzeichnung die Identifizierungsnummer der für die in Artikel 43 festgelegten Konformitätsbewertungsverfahren zuständigen notifizierten Stelle hinzugefügt. Die Identifizierungsnummer der notifizierten Stelle ist entweder von der Stelle selbst oder nach ihren Anweisungen durch den Anbieter oder den Bevollmächtigten des Anbieters anzubringen. Diese Identifizierungsnummer wird auch auf jeglichem Werbematerial angegeben, in dem darauf hingewiesen wird, dass das Hochrisiko-KI-System die Anforderungen für die CE-Kennzeichnung erfüllt.\n(5) Falls Hochrisiko-KI-Systeme ferner unter andere Rechtsvorschriften der Union fallen, in denen die CE-Kennzeichnung auch vorgesehen ist, bedeutet die CE-Kennzeichnung, dass das Hochrisiko-KI-System auch die Anforderungen dieser anderen Rechtsvorschriften erfüllt."
    },
    {
      "chunk_idx": 253,
      "id": "bea96906-be50-4eb8-9cbe-cf07816645c5",
      "title": "Art 49",
      "relevantChunksIds": [
        "90893d61-25fd-4ae1-8232-5e4cdb5675e9"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 49: Registration\n1. Before placing on the market or putting into service a high-risk AI system listed in Annex III, with the exception of high-risk AI systems referred to in point 2 of Annex III, the provider or, where applicable, the authorised representative shall register themselves and their system in the EU database referred to in Article 71.\n2. Before placing on the market or putting into service an AI system for which the provider has concluded that it is not high-risk according to Article 6(3), that provider or, where applicable, the authorised representative shall register themselves and that system in the EU database referred to in Article 71.\n3. Before putting into service or using a high-risk AI system listed in Annex III, with the exception of high-risk AI systems listed in point 2 of Annex III, deployers that are public authorities, Union institutions, bodies, offices or agencies or persons acting on their behalf shall register themselves, select the system and register its use in the EU database referred to in Article 71.\n4. For high-risk AI systems referred to in points 1, 6 and 7 of Annex III, in the areas of law enforcement, migration, asylum and border control management, the registration referred to in paragraphs 1, 2 and 3 of this Article shall be in a secure non-public section of the EU database referred to in Article 71 and shall include only the following information, as applicable, referred to in:\n(a) Section A, points 1 to 10, of Annex VIII, with the exception of points 6, 8 and 9;\n(b) Section B, points 1 to 5, and points 8 and 9 of Annex VIII;\n(c) Section C, points 1 to 3, of Annex VIII;\n\n5. High-risk AI systems referred to in point 2 of Annex III shall be registered at national level.",
      "original_content": "### Artikel 49: Registrierung\n(1) Vor dem Inverkehrbringen oder der Inbetriebnahme eines in Anhang III aufgeführten Hochrisiko-KI-Systems — mit Ausnahme der in Anhang III Nummer 2 genannten Hochrisiko-KI-Systeme — registriert der Anbieter oder gegebenenfalls sein Bevollmächtigter sich und sein System in der in Artikel 71 genannten EU-Datenbank.\n(2) Vor dem Inverkehrbringen oder der Inbetriebnahme eines Hochrisiko-KI-Systems, bei dem der Anbieter zu dem Schluss gelangt ist, dass es nicht hochriskant gemäß Artikel 6 Absatz 3 ist, registriert dieser Anbieter oder gegebenenfalls sein Bevollmächtigter sich und dieses System in der in Artikel 71 genannten EU-Datenbank.\n(3) Vor der Inbetriebnahme oder Verwendung eines in Anhang III aufgeführten Hochrisiko-KI-Systems — mit Ausnahme der in Anhang III Nummer 2 aufgeführten Hochrisiko-KI-Systeme — registrieren sich Betreiber, bei denen es sich um Behörden oder Organe, Einrichtungen oder sonstige Stellen der Union oder in ihrem Namen handelnde Personen handelt, in der in Artikel 71 genannten EU-Datenbank, wählen das System aus und registrieren es dort.\n(4) Bei den in Anhang III Nummern 1, 6 und 7 genannten Hochrisiko-KI-Systemen erfolgt in den Bereichen Strafverfolgung, Migration, Asyl und Grenzkontrolle die Registrierung gemäß den Absätzen 1, 2 und 3 des vorliegenden Artikels in einem sicheren nicht öffentlichen Teil der in Artikel 71 genannten EU-Datenbank und enthält, soweit zutreffend, lediglich die Informationen gemäß\na) Anhang VIII Abschnitt A Nummern 1 bis 10 mit Ausnahme der Nummern 6, 8 und 9,\nb) Anhang VIII Abschnitt B Nummern 1 bis 5 sowie Nummern 8 und 9,\nc) Anhang VIII Abschnitt C Nummern 1 bis 3,\nd) Anhang IX Nummern 1, 2, 3 und Nummer 5. Nur die Kommission und die in Artikel 74 Absatz 8 genannten nationalen Behörden haben Zugang zu den jeweiligen beschränkten Teilen der EU-Datenbank gemäß Unterabsatz 1 dieses Absatzes.\n(5) Die in Anhang III Nummer 2 genannten Hochrisiko-KI-Systeme werden auf nationaler Ebene registriert."
    },
    {
      "chunk_idx": 254,
      "id": "986b540c-851f-4387-b2d1-e9b5d9bed869",
      "title": "Art 50",
      "relevantChunksIds": [
        "42b80e11-733d-441e-98e1-d79837892537"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "# CHAPTER IV: TRANSPARENCY OBLIGATIONS FOR PROVIDERS AND DEPLOYERS OF CERTAIN AI SYSTEMS\n### Article 50: Transparency obligations for providers and deployers of certain AI systems\n1. Providers shall ensure that AI systems intended to interact directly with natural persons are designed and developed in such a way that the natural persons concerned are informed that they are interacting with an AI system, unless this is obvious from the point of view of a natural person who is reasonably well-informed, observant and circumspect, taking into account the circumstances and the context of use. This obligation shall not apply to AI systems authorised by law to detect, prevent, investigate or prosecute criminal offences, subject to appropriate safeguards for the rights and freedoms of third parties, unless those systems are available for the public to report a criminal offence.\n2. Providers of AI systems, including general-purpose AI systems, generating synthetic audio, image, video or text content, shall ensure that the outputs of the AI system are marked in a machine-readable format and detectable as artificially generated or manipulated. Providers shall ensure their technical solutions are effective, interoperable, robust and reliable as far as this is technically feasible, taking into account the specificities and limitations of various types of content, the costs of implementation and the generally acknowledged state of the art, as may be reflected in relevant technical standards. This obligation shall not apply to the extent the AI systems perform an assistive function for standard editing or do not substantially alter the input data provided by the deployer or the semantics thereof, or where authorised by law to detect, prevent, investigate or prosecute criminal offences.\n3. Deployers of an emotion recognition system or a biometric categorisation system shall inform the natural persons exposed thereto of the operation of the system, and shall process the personal data in accordance with Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, as applicable. This obligation shall not apply to AI systems used for biometric categorisation and emotion recognition, which are permitted by law to detect, prevent or investigate criminal offences, subject to appropriate safeguards for the rights and freedoms of third parties, and in accordance with Union law.\n4. Deployers of an AI system that generates or manipulates image, audio or video content constituting a deep fake, shall disclose that the content has been artificially generated or manipulated. This obligation shall not apply where the use is authorised by law to detect, prevent, investigate or prosecute criminal offence. Where the content forms part of an evidently artistic, creative, satirical, fictional or analogous work or programme, the transparency obligations set out in this paragraph are limited to disclosure of the existence of such generated or manipulated content in an appropriate manner that does not hamper the display or enjoyment of the work.\nDeployers of an AI system that generates or manipulates text which is published with the purpose of informing the public on matters of public interest shall disclose that the text has been artificially generated or manipulated. This obligation shall not apply where the use is authorised by law to detect, prevent, investigate or prosecute criminal offences or where the AI-generated content has undergone a process of human review or editorial control and where a natural or legal person holds editorial responsibility for the publication of the content.\n5. The information referred to in paragraphs 1 to 4 shall be provided to the natural persons concerned in a clear and distinguishable manner at the latest at the time of the first interaction or exposure. The information shall conform to the applicable accessibility requirements.\n6. Paragraphs 1 to 4 shall not affect the requirements and obligations set out in Chapter III, and shall be without prejudice to other transparency obligations laid down in Union or national law for deployers of AI systems.\n7. The AI Office shall encourage and facilitate the drawing up of codes of practice at Union level to facilitate the effective implementation of the obligations regarding the detection and labelling of artificially generated or manipulated content. The Commission may adopt implementing acts to approve those codes of practice in accordance with the procedure laid down in Article 56 (6). If it deems the code is not adequate, the Commission may adopt an implementing act specifying common rules for the implementation of those obligations in accordance with the examination procedure laid down in Article 98(2).",
      "original_content": "# KAPITEL IV: TRANSPARENZPFLICHTEN FÜR ANBIETER UND BETREIBER BESTIMMTER KI-SYSTEME\n### Artikel 50: Transparenzpflichten für Anbieter und Betreiber bestimmter KI-Systeme\n(1) Die Anbieter stellen sicher, dass KI-Systeme, die für die direkte Interaktion mit natürlichen Personen bestimmt sind, so konzipiert und entwickelt werden, dass die betreffenden natürlichen Personen informiert werden, dass sie mit einem KI-System interagieren, es sei denn, dies ist aus Sicht einer angemessen informierten, aufmerksamen und verständigen natürlichen Person aufgrund der Umstände und des Kontexts der Nutzung offensichtlich. Diese Pflicht gilt nicht für gesetzlich zur Aufdeckung, Verhütung, Ermittlung oder Verfolgung von Straftaten zugelassene KI-Systeme, wenn geeignete Schutzvorkehrungen für die Rechte und Freiheiten Dritter bestehen, es sei denn, diese Systeme stehen der Öffentlichkeit zur Anzeige einer Straftat zur Verfügung.\n(2) Anbieter von KI-Systemen, einschließlich KI-Systemen mit allgemeinem Verwendungszweck, die synthetische Audio-, Bild-, Video- oder Textinhalte erzeugen, stellen sicher, dass die Ausgaben des KI-Systems in einem maschinenlesbaren Format gekennzeichnet und als künstlich erzeugt oder manipuliert erkennbar sind. Die Anbieter sorgen dafür, dass — soweit technisch möglich — ihre technischen Lösungen wirksam, interoperabel, belastbar und zuverlässig sind und berücksichtigen dabei die Besonderheiten und Beschränkungen der verschiedenen Arten von Inhalten, die Umsetzungskosten und den allgemein anerkannten Stand der Technik, wie er in den einschlägigen technischen Normen zum Ausdruck kommen kann. Diese Pflicht gilt nicht, soweit die KI-Systeme eine unterstützende Funktion für die Standardbearbeitung ausführen oder die vom Betreiber bereitgestellten Eingabedaten oder deren Semantik nicht wesentlich verändern oder wenn sie zur Aufdeckung, Verhütung, Ermittlung oder Verfolgung von Straftaten gesetzlich zugelassen sind.\n(3) Die Betreiber eines Emotionserkennungssystems oder eines Systems zur biometrischen Kategorisierung informieren die davon betroffenen natürlichen Personen über den Betrieb des Systems und verarbeiten personenbezogene Daten gemäß den Verordnungen (EU) 2016/679 und (EU) 2018/1725 und der Richtlinie (EU) 2016/680. Diese Pflicht gilt nicht für gesetzlich zur Aufdeckung, Verhütung oder Ermittlung von Straftaten zugelassene KI-Systeme, die zur biometrischen Kategorisierung und Emotionserkennung im Einklang mit dem Unionsrecht verwendet werden, sofern geeignete Schutzvorkehrungen für die Rechte und Freiheiten Dritter bestehen.\n(4) Betreiber eines KI-Systems, das Bild-, Ton- oder Videoinhalte erzeugt oder manipuliert, die ein Deepfake sind, müssen offenlegen, dass die Inhalte künstlich erzeugt oder manipuliert wurden. Diese Pflicht gilt nicht, wenn die Verwendung zur Aufdeckung, Verhütung, Ermittlung oder Verfolgung von Straftaten gesetzlich zugelassen ist. Ist der Inhalt Teil eines offensichtlich künstlerischen, kreativen, satirischen, fiktionalen oder analogen Werks oder Programms, so beschränken sich die in diesem Absatz festgelegten Transparenzpflichten darauf, das Vorhandensein solcher erzeugten oder manipulierten Inhalte in geeigneter Weise offenzulegen, die die Darstellung oder den Genuss des Werks nicht beeinträchtigt.\nBetreiber eines KI-Systems, das Text erzeugt oder manipuliert, der veröffentlicht wird, um die Öffentlichkeit über Angelegenheiten von öffentlichem Interesse zu informieren, müssen offenlegen, dass der Text künstlich erzeugt oder manipuliert wurde. Diese Pflicht gilt nicht, wenn die Verwendung zur Aufdeckung, Verhütung, Ermittlung oder Verfolgung von Straftaten gesetzlich zugelassen ist oder wenn die durch KI erzeugten Inhalte einem Verfahren der menschlichen Überprüfung oder redaktionellen Kontrolle unterzogen wurden und wenn eine natürliche oder juristische Person die redaktionelle Verantwortung für die Veröffentlichung der Inhalte trägt.\n(5) Die in den Absätzen 1 bis 4 genannten Informationen werden den betreffenden natürlichen Personen spätestens zum Zeitpunkt der ersten Interaktion oder Aussetzung in klarer und eindeutiger Weise bereitgestellt. Die Informationen müssen den geltenden Barrierefreiheitsanforderungen entsprechen.\n(6) Die Absätze 1 bis 4 lassen die in Kapitel III festgelegten Anforderungen und Pflichten unberührt und berühren nicht andere Transparenzpflichten, die im Unionsrecht oder dem nationalen Recht für Betreiber von KI-Systemen festgelegt sind.\n(7) Das Büro für Künstliche Intelligenz fördert und erleichtert die Ausarbeitung von Praxisleitfäden auf Unionsebene, um die wirksame Umsetzung der Pflichten in Bezug auf die Feststellung und Kennzeichnung künstlich erzeugter oder manipulierter Inhalte zu erleichtern. Die Kommission kann Durchführungsrechtsakte zur Genehmigung dieser Praxisleitfäden nach dem in Artikel 56 Absatz 6 festgelegten Verfahren erlassen. Hält sie einen Kodex für nicht angemessen, so kann die Kommission einen Durchführungsrechtsakt gemäß dem in Artikel 98 Absatz 2 genannten Prüfverfahren erlassen, in dem gemeinsame Vorschriften für die Umsetzung dieser Pflichten festgelegt werden."
    },
    {
      "chunk_idx": 255,
      "id": "18f063a4-31d6-40a9-b45a-da3f602f920f",
      "title": "Art 51",
      "relevantChunksIds": [
        "8543b14a-dfa5-4fd3-9673-e854faf06935",
        "aa588b7b-71e5-42c1-9adc-5aaa8688320a",
        "c9a0bc19-9064-435f-a8d1-73a20a754192"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "# CHAPTER V: GENERAL-PURPOSE AI MODELS\n## SECTION 1: Classification rules\n### Article 51: Classification of general-purpose AI models as general-purpose AI models with systemic risk\n1. A general-purpose AI model shall be classified as a general-purpose AI model with systemic risk if it meets any of the following conditions:\n(a) it has high impact capabilities evaluated on the basis of appropriate technical tools and methodologies, including indicators and benchmarks;\n(b) based on a decision of the Commission, ex officio or following a qualified alert from the scientific panel, it has capabilities or an impact equivalent to those set out in point (a) having regard to the criteria set out in Annex XIII.\n\n3. The Commission shall adopt delegated acts in accordance with Article 97 to amend the thresholds listed in paragraphs 1 and 2 of this Article, as well as to supplement benchmarks and indicators in light of evolving technological developments, such as algorithmic improvements or increased hardware efficiency, when necessary, for these thresholds to reflect the state of the art.",
      "original_content": "# KAPITEL V: KI-MODELLE MIT ALLGEMEINEM VERWENDUNGSZWECK\n## ABSCHNITT 1: Einstufungsvorschriften\n### Artikel 51: Einstufung von KI-Modellen mit allgemeinem Verwendungszweck als KI-Modelle mit allgemeinem Verwendungszweck mit systemischem Risiko\n(1) Ein KI-Modell mit allgemeinem Verwendungszweck wird als KI-Modell mit allgemeinem Verwendungszweck mit systemischem Risiko eingestuft, wenn eine der folgenden Bedingungen erfüllt ist:\na) Es verfügt über Fähigkeiten mit hohem Wirkungsgrad, die mithilfe geeigneter technischer Instrumente und Methoden, einschließlich Indikatoren und Benchmarks, bewertet werden;\nb) einem unter Berücksichtigung der in Anhang XIII festgelegten Kriterien von der Kommission von Amts wegen oder aufgrund einer qualifizierten Warnung des wissenschaftlichen Gremiums getroffenen Entscheidung zufolge verfügt es über Fähigkeiten oder eine Wirkung, die denen gemäß Buchstabe a entsprechen.\n(2) Bei einem KI-Modell mit allgemeinem Verwendungszweck wird angenommen, dass es über Fähigkeiten mit hohem Wirkungsgrad gemäß Absatz 1 Buchstabe a verfügt, wenn die kumulierte Menge der für sein Training verwendeten Berechnungen, gemessen in Gleitkommaoperationen, mehr als 10^25 beträgt.\n(3) Die Kommission erlässt gemäß Artikel 97 delegierte Rechtsakte zur Änderung der in den Absätzen 1 und 2 des vorliegenden Artikels aufgeführten Schwellenwerte sowie zur Ergänzung von Benchmarks und Indikatoren vor dem Hintergrund sich wandelnder technologischer Entwicklungen, wie z. B. algorithmische Verbesserungen oder erhöhte Hardwareeffizienz, wenn dies erforderlich ist, damit diese Schwellenwerte dem Stand der Technik entsprechen."
    },
    {
      "chunk_idx": 256,
      "id": "e4d5fea4-38ce-4829-87ad-748c71e5bfd1",
      "title": "Art 52",
      "relevantChunksIds": [
        "2e8446c1-8259-4a99-8353-ae8eee2bf2ab"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 52: Procedure\n1. Where a general-purpose AI model meets the condition referred to in Article 51(1), point (a), the relevant provider shall notify the Commission without delay and in any event within two weeks after that requirement is met or it becomes known that it will be met. That notification shall include the information necessary to demonstrate that the relevant requirement has been met. If the Commission becomes aware of a general-purpose AI model presenting systemic risks of which it has not been notified, it may decide to designate it as a model with systemic risk.\n2. The provider of a general-purpose AI model that meets the condition referred to in Article 51(1), point (a), may present, with its notification, sufficiently substantiated arguments to demonstrate that, exceptionally, although it meets that requirement, the general-purpose AI model does not present, due to its specific characteristics, systemic risks and therefore should not be classified as a general-purpose AI model with systemic risk.\n3. Where the Commission concludes that the arguments submitted pursuant to paragraph 2 are not sufficiently substantiated and the relevant provider was not able to demonstrate that the general-purpose AI model does not present, due to its specific characteristics, systemic risks, it shall reject those arguments, and the general-purpose AI model shall be considered to be a general-purpose AI model with systemic risk.\n4. The Commission may designate a general-purpose AI model as presenting systemic risks, ex officio or following a qualified alert from the scientific panel pursuant to Article 90(1), point (a), on the basis of criteria set out in Annex XIII.\nThe Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend Annex XIII by specifying and updating the criteria set out in that Annex.\n5. Upon a reasoned request of a provider whose model has been designated as a general-purpose AI model with systemic risk pursuant to paragraph 4, the Commission shall take the request into account and may decide to reassess whether the general-purpose AI model can still be considered to present systemic risks on the basis of the criteria set out in Annex XIII. Such a request shall contain objective, detailed and new reasons that have arisen since the designation decision. Providers may request reassessment at the earliest six months after the designation decision. Where the Commission, following its reassessment, decides to maintain the designation as a general-purpose AI model with systemic risk, providers may request reassessment at the earliest six months after that decision.\n6. The Commission shall ensure that a list of general-purpose AI models with systemic risk is published and shall keep that list up to date, without prejudice to the need to observe and protect intellectual property rights and confidential business information or trade secrets in accordance with Union and national law.",
      "original_content": "### Artikel 52: Verfahren\n(1) Erfüllt ein KI-Modell mit allgemeinem Verwendungszweck die Bedingung gemäß Artikel 51 Absatz 1 Buchstabe a, so teilt der betreffende Anbieter dies der Kommission unverzüglich, in jedem Fall jedoch innerhalb von zwei Wochen, nachdem diese Bedingung erfüllt ist oder bekannt wird, dass sie erfüllt wird, mit. Diese Mitteilung muss die Informationen enthalten, die erforderlich sind, um nachzuweisen, dass die betreffende Bedingung erfüllt ist. Erlangt die Kommission Kenntnis von einem KI-Modell mit allgemeinem Verwendungszweck, das systemische Risiken birgt, die ihr nicht mitgeteilt wurden, so kann sie entscheiden, es als Modell mit systemischen Risiken auszuweisen.\n(2) Der Anbieter eines KI-Modells mit allgemeinem Verwendungszweck, das die in Artikel 51 Absatz 1 Buchstabe a genannte Bedingung erfüllt, kann in seiner Mitteilung hinreichend begründete Argumente vorbringen, um nachzuweisen, dass das KI-Modell mit allgemeinem Verwendungszweck, obwohl es diese Bedingung erfüllt, aufgrund seiner besonderen Merkmale außerordentlicherweise keine systemischen Risiken birgt und daher nicht als KI-Modell mit allgemeinem Verwendungszweck mit systemischem Risiko eingestuft werden sollte.\n(3) Gelangt die Kommission zu dem Schluss, dass die gemäß Absatz 2 vorgebrachten Argumente nicht hinreichend begründet sind, und konnte der betreffende Anbieter nicht nachweisen, dass das KI-Modell mit allgemeinem Verwendungszweck aufgrund seiner besonderen Merkmale keine systemischen Risiken aufweist, weist sie diese Argumente zurück, und das KI-Modell mit allgemeinem Verwendungszweck gilt als KI-Modell mit allgemeiner Zweckbestimmung mit systemischem Risiko.\n(4) Die Kommission kann ein KI-Modell mit allgemeinem Verwendungszweck von Amts wegen oder aufgrund einer qualifizierten Warnung des wissenschaftlichen Gremiums gemäß Artikel 90 Absatz 1 Buchstabe a auf der Grundlage der in Anhang XIII festgelegten Kriterien als KI-Modell mit systemischen Risiken ausweisen.\nDie Kommission ist befugt, gemäß Artikel 97 delegierte Rechtsakte zu erlassen, um Anhang XIII zu ändern, indem die in dem genannten Anhang genannten Indikatoren präzisiert und aktualisiert werden.\n(5) Stellt der Anbieter, dessen Modell gemäß Absatz 4 als KI-Modell mit allgemeinem Verwendungszweck mit systemischem Risiko ausgewiesen wurde, einen entsprechenden Antrag, berücksichtigt die Kommission den Antrag und kann entscheiden, erneut zu prüfen, ob beim KI-Modell mit allgemeinem Verwendungszweck auf der Grundlage der in Anhang XIII festgelegten Kriterien immer noch davon ausgegangen werden kann, dass es systemische Risiken aufweist. Dieser Antrag muss objektive, detaillierte und neue Gründe enthalten, die sich seit der Entscheidung zur Ausweisung ergeben haben. Die Anbieter können frühestens sechs Monate nach der Entscheidung zur Ausweisung eine Neubewertung beantragen. Entscheidet die Kommission nach ihrer Neubewertung, die Ausweisung als KI-Modell mit allgemeiner Zweckbestimmung mit systemischem Risiko beizubehalten, können die Anbieter frühestens sechs Monate nach dieser Entscheidung eine Neubewertung beantragen.\n(6) Die Kommission stellt sicher, dass eine Liste von KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko veröffentlicht wird, und hält diese Liste unbeschadet der Notwendigkeit, Rechte des geistigen Eigentums und vertrauliche Geschäftsinformationen oder Geschäftsgeheimnisse im Einklang mit dem Unionsrecht und dem nationalen Recht zu achten und zu schützen, auf dem neuesten Stand."
    },
    {
      "chunk_idx": 257,
      "id": "7bed8e5e-3585-48d6-889d-68089e0581a4",
      "title": "Art 53",
      "relevantChunksIds": [
        "e4baece6-7335-43c7-bc93-770f04b64211",
        "b908b1fb-c1a0-41d0-85cd-96d3d195d360",
        "0a6d98df-01bf-4a5c-b316-f87ecb2b8f17",
        "b5f38203-47bc-4aba-a5fb-0270a939ad11",
        "867f6662-88e1-490c-9191-9ba91afbd52d"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "## SECTION 2: Obligations for providers of general-purpose AI models\n### Article 53: Obligations for providers of general-purpose AI models\n1. Providers of general-purpose AI models shall:\n(a) draw up and keep up-to-date the technical documentation of the model, including its training and testing process and the results of its evaluation, which shall contain, at a minimum, the information set out in Annex XI for the purpose of providing it, upon request, to the AI Office and the national competent authorities;\n(b) draw up, keep up-to-date and make available information and documentation to providers of AI systems who intend to integrate the general-purpose AI model into their AI systems. Without prejudice to the need to observe and protect intellectual property rights and confidential business information or trade secrets in accordance with Union and national law, the information and documentation shall:\n(i) enable providers of AI systems to have a good understanding of the capabilities and limitations of the general-purpose AI model and to comply with their obligations pursuant to this Regulation; and\n(ii) contain, at a minimum, the elements set out in Annex XII;\n(c) put in place a policy to comply with Union law on copyright and related rights, and in particular to identify and comply with, including through state-of-the-art technologies, a reservation of rights expressed pursuant to Article 4(3) of Directive (EU) 2019/790;\n(d) draw up and make publicly available a sufficiently detailed summary about the content used for training of the general-purpose AI model, according to a template provided by the AI Office.\n2. The obligations set out in paragraph 1, points (a) and (b), shall not apply to providers of AI models that are released under a free and open-source licence that allows for the access, usage, modification, and distribution of the model, and whose parameters, including the weights, the information on the model architecture, and the information on model usage, are made publicly available. This exception shall not apply to general-purpose AI models with systemic risks.\n3. Providers of general-purpose AI models shall cooperate as necessary with the Commission and the national competent authorities in the exercise of their competences and powers pursuant to this Regulation.\n4. Providers of general-purpose AI models may rely on codes of practice within the meaning of Article 56 to demonstrate compliance with the obligations set out in paragraph 1 of this Article, until a harmonised standard is published. Compliance with European harmonised standards grants providers the presumption of conformity to the extent that those standards cover those obligations. Providers of general-purpose AI models who do not adhere to an approved code of practice or do not comply with a European harmonised standard shall demonstrate alternative adequate means of compliance for assessment by the Commission.\n5. For the purpose of facilitating compliance with Annex XI, in particular points 2 (d) and (e) thereof, the Commission is empowered to adopt delegated acts in accordance with Article 97 to detail measurement and calculation methodologies with a view to allowing for comparable and verifiable documentation.\n6. The Commission is empowered to adopt delegated acts in accordance with Article 97(2) to amend Annexes XI and XII in light of evolving technological developments.\n7. Any information or documentation obtained pursuant to this Article, including trade secrets, shall be treated in accordance with the confidentiality obligations set out in Article 78.",
      "original_content": "## ABSCHNITT 2: Pflichten für Anbieter von KI-Modellen mit allgemeinem Verwendungszweck\n### Artikel 53: Pflichten für Anbieter von KI-Modellen mit allgemeinem Verwendungszweck\n(1) Anbieter von KI-Modellen mit allgemeinem Verwendungszweck\na) erstellen und aktualisieren die technische Dokumentation des Modells, einschließlich seines Trainings- und Testverfahrens und der Ergebnisse seiner Bewertung, die mindestens die in Anhang XI aufgeführten Informationen enthält, damit sie dem Büro für Künstliche Intelligenz und den zuständigen nationalen Behörden auf Anfrage zur Verfügung gestellt werden kann;\nb) erstellen und aktualisieren Informationen und die Dokumentation und stellen sie Anbietern von KI-Systemen zur Verfügung, die beabsichtigen, das KI-Modell mit allgemeinem Verwendungszweck in ihre KI-Systeme zu integrieren. Unbeschadet der Notwendigkeit, die Rechte des geistigen Eigentums und vertrauliche Geschäftsinformationen oder Geschäftsgeheimnisse im Einklang mit dem Unionsrecht und dem nationalen Recht zu achten und zu schützen, müssen die Informationen und die Dokumentation\ni) die Anbieter von KI-Systemen in die Lage versetzen, die Fähigkeiten und Grenzen des KI-Modells mit allgemeinem Verwendungszweck gut zu verstehen und ihren Pflichten gemäß dieser Verordnung nachzukommen, und\nii) zumindest die in Anhang XII genannten Elemente enthalten;\nc) bringen eine Strategie zur Einhaltung des Urheberrechts der Union und damit zusammenhängender Rechte und insbesondere zur Ermittlung und Einhaltung eines gemäß Artikel 4 Absatz 3 der Richtlinie (EU) 2019/790 geltend gemachten Rechtsvorbehalts, auch durch modernste Technologien, auf den Weg;\nd) erstellen und veröffentlichen eine hinreichend detaillierte Zusammenfassung der für das Training des KI-Modells mit allgemeinem Verwendungszweck verwendeten Inhalte nach einer vom Büro für Künstliche Intelligenz bereitgestellten Vorlage.\n(2) Die Pflichten gemäß Absatz 1 Buchstaben a und b gelten nicht für Anbieter von KI-Modellen, die im Rahmen einer freien und quelloffenen Lizenz bereitgestellt werden, die den Zugang, die Nutzung, die Änderung und die Verbreitung des Modells ermöglicht und deren Parameter, einschließlich Gewichte, Informationen über die Modellarchitektur und Informationen über die Modellnutzung, öffentlich zugänglich gemacht werden. Diese Ausnahme gilt nicht für KI-Modellen mit allgemeinem Verwendungszweck mit systemischen Risiken.\n(3) Anbieter von KI-Modellen mit allgemeinem Verwendungszweck arbeiten bei der Ausübung ihrer Zuständigkeiten und Befugnisse gemäß dieser Verordnung erforderlichenfalls mit der Kommission und den zuständigen nationalen Behörden zusammen.\n(4) Anbieter von KI-Modellen mit allgemeinem Verwendungszweck können sich bis zur Veröffentlichung einer harmonisierten Norm auf Praxisleitfäden im Sinne des Artikels 56 stützen, um die Einhaltung der in Absatz 1 des vorliegenden Artikels genannten Pflichten nachzuweisen. Die Einhaltung der harmonisierten europäischen Norm begründet für die Anbieter die Vermutung der Konformität, insoweit diese Normen diese Verpflichtungen abdecken. Anbieter von KI-Modellen mit allgemeinem Verwendungszweck, die keinen genehmigten Praxisleitfaden befolgen oder eine harmonisierte europäische Norm nicht einhalten, müssen geeignete alternative Verfahren der Einhaltung aufzeigen, die von der Kommission zu bewerten sind.\n(5) Um die Einhaltung von Anhang XI, insbesondere Nummer 2 Buchstaben d und e, zu erleichtern, ist die Kommission befugt, gemäß Artikel 97 delegierte Rechtsakte zu erlassen, um die Mess- und Berechnungsmethoden im Einzelnen festzulegen, damit eine vergleichbare und überprüfbare Dokumentation ermöglicht wird.\n(6) Die Kommission ist befugt, gemäß Artikel 97 Absatz 2 delegierte Rechtsakte zu erlassen, um die Anhänge XI und XII vor dem Hintergrund sich wandelnder technologischer Entwicklungen zu ändern.\n(7) Jegliche Informationen oder Dokumentation, die gemäß diesem Artikel erlangt werden, einschließlich Geschäftsgeheimnisse, werden im Einklang mit den in Artikel 78 festgelegten Vertraulichkeitspflichten behandelt."
    },
    {
      "chunk_idx": 258,
      "id": "49e88b0c-2141-4832-bc84-ee8b5fef7d4f",
      "title": "Art 54",
      "relevantChunksIds": [
        "86e3f5bd-eef6-4109-ab46-aedd548f4a71"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 54: Authorised representatives of providers of general-purpose AI models\n1. Prior to placing a general-purpose AI model on the Union market, providers established in third countries shall, by written mandate, appoint an authorised representative which is established in the Union.\n2. The provider shall enable its authorised representative to perform the tasks specified in the mandate received from the provider.\n3. The authorised representative shall perform the tasks specified in the mandate received from the provider. It shall provide a copy of the mandate to the AI Office upon request, in one of the official languages of the institutions of the Union. For the purposes of this Regulation, the mandate shall empower the authorised representative to carry out the following tasks:\n(a) verify that the technical documentation specified in Annex XI has been drawn up and all obligations referred to in Article 53 and, where applicable, Article 55 have been fulfilled by the provider;\n(b) keep a copy of the technical documentation specified in Annex XI at the disposal of the AI Office and national competent authorities, for a period of 10 years after the general-purpose AI model has been placed on the market, and the contact details of the provider that appointed the authorised representative;\n(c) provide the AI Office, upon a reasoned request, with all the information and documentation, including that referred to in point (b), necessary to demonstrate compliance with the obligations in this Chapter;\n(d) cooperate with the AI Office and competent authorities, upon a reasoned request, in any action they take in relation to the general-purpose AI model, including when the model is integrated into AI systems placed on the market or put into service in the Union.\n4. The mandate shall empower the authorised representative to be addressed, in addition to or instead of the provider, by the AI Office or the competent authorities, on all issues related to ensuring compliance with this Regulation.\n5. The authorised representative shall terminate the mandate if it considers or has reason to consider the provider to be acting contrary to its obligations pursuant to this Regulation. In such a case, it shall also immediately inform the AI Office about the termination of the mandate and the reasons therefor.\n6. The obligation set out in this Article shall not apply to providers of general-purpose AI models that are released under a free and open-source licence that allows for the access, usage, modification, and distribution of the model, and whose parameters, including the weights, the information on the model architecture, and the information on model usage, are made publicly available, unless the general-purpose AI models present systemic risks.",
      "original_content": "### Artikel 54: Bevollmächtigte der Anbieter von KI-Modellen mit allgemeinem Verwendungszweck\n(1) Anbieter, die in Drittländern niedergelassen sind, benennen vor dem Inverkehrbringen eines KI-Modells mit allgemeinem Verwendungszweck auf dem Unionsmarkt schriftlich einen in der Union niedergelassenen Bevollmächtigten.\n(2) Der Anbieter muss seinem Bevollmächtigten ermöglichen, die Aufgaben wahrzunehmen, die im vom Anbieter erhaltenen Auftrag festgelegt sind.\n(3) Der Bevollmächtigte nimmt die Aufgaben wahr, die in seinem vom Anbieter erhaltenen Auftrag festgelegt sind. Er stellt dem Büro für Künstliche Intelligenz auf Anfrage eine Kopie des Auftrags in einer der Amtssprachen der Institutionen der Union bereit. Für die Zwecke dieser Verordnung ermächtigt der Auftrag den Bevollmächtigten zumindest zur Wahrnehmung folgender Aufgaben:\na) Überprüfung, ob die technische Dokumentation gemäß Anhang XI erstellt wurde und alle Pflichten gemäß Artikel 53 und gegebenenfalls gemäß Artikel 55 vom Anbieter erfüllt wurden;\nb) Bereithaltung einer Kopie der technischen Dokumentation gemäß Anhang XI für das Büro für Künstliche Intelligenz und die zuständigen nationalen Behörden für einen Zeitraum von zehn Jahren nach dem Inverkehrbringen des KI-Modells mit allgemeinem Verwendungszweck und der Kontaktdaten des Anbieters, der den Bevollmächtigten benannt hat;\nc) Bereitstellung sämtlicher zum Nachweis der Einhaltung der Pflichten gemäß diesem Kapitel erforderlichen Informationen und Dokumentation, einschließlich der unter Buchstabe b genannten Informationen und Dokumentation, an das Büro für Künstliche Intelligenz auf begründeten Antrag;\nd) Zusammenarbeit mit dem Büro für Künstliche Intelligenz und den zuständigen Behörden auf begründeten Antrag bei allen Maßnahmen, die sie im Zusammenhang mit einem KI-Modell mit allgemeinem Verwendungszweck ergreifen, auch wenn das Modell in KI-Systeme integriert ist, die in der Union in Verkehr gebracht oder in Betrieb genommen werden.\n(4) Mit dem Auftrag wird der Bevollmächtigte ermächtigt, neben oder anstelle des Anbieters als Ansprechpartner für das Büro für Künstliche Intelligenz oder die zuständigen Behörden in allen Fragen zu dienen, die die Gewährleistung der Einhaltung dieser Verordnung betreffen.\n(5) Der Bevollmächtigte beendet den Auftrag, wenn er der Auffassung ist oder Grund zu der Annahme hat, dass der Anbieter gegen seine Pflichten gemäß dieser Verordnung verstößt. In einem solchen Fall informiert er auch das Büro für Künstliche Intelligenz unverzüglich über die Beendigung des Auftrags und die Gründe dafür.\n(6) Die Pflicht gemäß diesem Artikel gilt nicht für Anbieter von KI-Modellen, die im Rahmen einer freien und quelloffenen Lizenz bereitgestellt werden, die den Zugang, die Nutzung, die Änderung und die Verbreitung des Modells ermöglicht und deren Parameter, einschließlich Gewichte, Informationen über die Modellarchitektur und Informationen über die Modellnutzung, öffentlich zugänglich gemacht werden, es sei denn, die KI-Modelle mit allgemeinem Verwendungszweck bergen systemische Risiken."
    },
    {
      "chunk_idx": 259,
      "id": "16cd04eb-0068-4a58-9062-a5e44f5f15a0",
      "title": "Art 55",
      "relevantChunksIds": [
        "89e675f5-d397-4279-80b8-8d3635280832"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "## SECTION 3: Obligations of providers of general-purpose AI models with systemic risk\n### Article 55: Obligations of providers of general-purpose AI models with systemic risk\n1. In addition to the obligations listed in Articles 53 and 54, providers of general-purpose AI models with systemic risk shall:\n(a) perform model evaluation in accordance with standardised protocols and tools reflecting the state of the art, including conducting and documenting adversarial testing of the model with a view to identifying and mitigating systemic risks;\n(b) assess and mitigate possible systemic risks at Union level, including their sources, that may stem from the development, the placing on the market, or the use of general-purpose AI models with systemic risk;\n(c) keep track of, document, and report, without undue delay, to the AI Office and, as appropriate, to national competent authorities, relevant information about serious incidents and possible corrective measures to address them;\n(d) ensure an adequate level of cybersecurity protection for the general-purpose AI model with systemic risk and the physical infrastructure of the model.\n2. Providers of general-purpose AI models with systemic risk may rely on codes of practice within the meaning of Article 56 to demonstrate compliance with the obligations set out in paragraph 1 of this Article, until a harmonised standard is published. Compliance with European harmonised standards grants providers the presumption of conformity to the extent that those standards cover those obligations. Providers of general-purpose AI models with systemic risks who do not adhere to an approved code of practice or do not comply with a European harmonised standard shall demonstrate alternative adequate means of compliance for assessment by the Commission.\n3. Any information or documentation obtained pursuant to this Article, including trade secrets, shall be treated in accordance with the confidentiality obligations set out in Article 78.",
      "original_content": "## ABSCHNITT 3: Pflichten der Anbieter von KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko\n### Artikel 55: Pflichten der Anbieter von KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko\n(1) Zusätzlich zu den in den Artikeln 53 und 54 aufgeführten Pflichten müssen Anbieter von KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko\na) eine Modellbewertung mit standardisierten Protokollen und Instrumenten, die dem Stand der Technik entsprechen, durchführen, wozu auch die Durchführung und Dokumentation von Angriffstests beim Modell gehören, um systemische Risiken zu ermitteln und zu mindern,\nb) mögliche systemische Risiken auf Unionsebene — einschließlich ihrer Ursachen —, die sich aus der Entwicklung, dem Inverkehrbringen oder der Verwendung von KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko ergeben können, bewerten und mindern,\nc) einschlägige Informationen über schwerwiegende Vorfälle und mögliche Abhilfemaßnahmen erfassen und dokumentieren und das Büro für Künstliche Intelligenz und gegebenenfalls die zuständigen nationalen Behörden unverzüglich darüber unterrichten,\nd) ein angemessenes Maß an Cybersicherheit für die KI-Modelle mit allgemeinem Verwendungszweck mit systemischem Risiko und die physische Infrastruktur des Modells gewährleisten.\n(2) Anbieter von KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko können sich bis zur Veröffentlichung einer harmonisierten Norm auf Praxisleitfäden im Sinne des Artikels 56 stützen, um die Einhaltung der in Absatz 1 des vorliegenden Artikels genannten Pflichten nachzuweisen. Die Einhaltung der harmonisierten europäischen Norm begründet für die Anbieter die Vermutung der Konformität, insoweit diese Normen diese Verpflichtungen abdecken. Anbieter von KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko, die einen genehmigten Praxisleitfaden nicht befolgen oder eine harmonisierte europäische Norm nicht einhalten, müssen geeignete alternative Verfahren der Einhaltung aufzeigen, die von der Kommission zu bewerten sind.\n(3) Jegliche Informationen oder Dokumentation, die gemäß diesem Artikel erlangt werden, werden im Einklang mit den in Artikel 78 festgelegten Vertraulichkeitspflichten behandelt."
    },
    {
      "chunk_idx": 260,
      "id": "b47202cc-5133-4ed0-aca4-2ec68b90632f",
      "title": "Art 56",
      "relevantChunksIds": [
        "8f179a57-c1c8-4502-b569-01a45d4dbd4b"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "## SECTION 4: Codes of practice\n### Article 56: Codes of practice\n1. The AI Office shall encourage and facilitate the drawing up of codes of practice at Union level in order to contribute to the proper application of this Regulation, taking into account international approaches.\n2. The AI Office and the Board shall aim to ensure that the codes of practice cover at least the obligations provided for in Articles 53 and 55, including the following issues:\n(a) the means to ensure that the information referred to in Article 53(1), points (a) and (b), is kept up to date in light of market and technological developments;\n(b) the adequate level of detail for the summary about the content used for training;\n(c) the identification of the type and nature of the systemic risks at Union level, including their sources, where appropriate;\n(d) the measures, procedures and modalities for the assessment and management of the systemic risks at Union level, including the documentation thereof, which shall be proportionate to the risks, take into consideration their severity and probability and take into account the specific challenges of tackling those risks in light of the possible ways in which such risks may emerge and materialise along the AI value chain.\n3. The AI Office may invite all providers of general-purpose AI models, as well as relevant national competent authorities, to participate in the drawing-up of codes of practice. Civil society organisations, industry, academia and other relevant stakeholders, such as downstream providers and independent experts, may support the process.\n4. The AI Office and the Board shall aim to ensure that the codes of practice clearly set out their specific objectives and contain commitments or measures, including key performance indicators as appropriate, to ensure the achievement of those objectives, and that they take due account of the needs and interests of all interested parties, including affected persons, at Union level.\n5. The AI Office shall aim to ensure that participants to the codes of practice report regularly to the AI Office on the implementation of the commitments and the measures taken and their outcomes, including as measured against the key performance indicators as appropriate. Key performance indicators and reporting commitments shall reflect differences in size and capacity between various participants.\n6. The AI Office and the Board shall regularly monitor and evaluate the achievement of the objectives of the codes of practice by the participants and their contribution to the proper application of this Regulation. The AI Office and the Board shall assess whether the codes of practice cover the obligations provided for in Articles 53 and 55, and shall regularly monitor and evaluate the achievement of their objectives. They shall publish their assessment of the adequacy of the codes of practice.\nThe Commission may, by way of an implementing act, approve a code of practice and give it a general validity within the Union. That implementing act shall be adopted in accordance with the examination procedure referred to in Article 98(2).\n7. The AI Office may invite all providers of general-purpose AI models to adhere to the codes of practice. For providers of general-purpose AI models not presenting systemic risks this adherence may be limited to the obligations provided for in Article 53, unless they declare explicitly their interest to join the full code.\n8. The AI Office shall, as appropriate, also encourage and facilitate the review and adaptation of the codes of practice, in particular in light of emerging standards. The AI Office shall assist in the assessment of available standards.\n",
      "original_content": "## ABSCHNITT 4: Praxisleitfäden\n### Artikel 56: Praxisleitfäden\n(1) Das Büro für Künstliche Intelligenz fördert und erleichtert die Ausarbeitung von Praxisleitfäden auf Unionsebene, um unter Berücksichtigung internationaler Ansätze zur ordnungsgemäßen Anwendung dieser Verordnung beizutragen.\n(2) Das Büro für Künstliche Intelligenz und das KI-Gremium streben an, sicherzustellen, dass die Praxisleitfäden mindestens die in den Artikeln 53 und 55 vorgesehenen Pflichten abdecken, einschließlich der folgenden Aspekte:\na) Mittel, mit denen sichergestellt wird, dass die in Artikel 53 Absatz 1 Buchstaben a und b genannten Informationen vor dem Hintergrund der Marktentwicklungen und technologischen Entwicklungen auf dem neuesten Stand gehalten werden;\nb) die angemessene Detailgenauigkeit bei der Zusammenfassung der für das Training verwendeten Inhalte;\nc) die Ermittlung von Art und Wesen der systemischen Risiken auf Unionsebene, gegebenenfalls einschließlich ihrer Ursachen;\nd) die Maßnahmen, Verfahren und Modalitäten für die Bewertung und das Management der systemischen Risiken auf Unionsebene, einschließlich ihrer Dokumentation, die in einem angemessenen Verhältnis zu den Risiken stehen, ihrer Schwere und Wahrscheinlichkeit Rechnung tragen und die spezifischen Herausforderungen bei der Bewältigung dieser Risiken vor dem Hintergrund der möglichen Arten der Entstehung und des Eintretens solcher Risiken entlang der KI-Wertschöpfungskette berücksichtigen.\n(3) Das Büro für Künstliche Intelligenz kann alle Anbieter von KI-Modellen mit allgemeinem Verwendungszweck sowie die einschlägigen zuständigen nationalen Behörden ersuchen, sich an der Ausarbeitung von Praxisleitfäden zu beteiligen. Organisationen der Zivilgesellschaft, die Industrie, die Wissenschaft und andere einschlägige Interessenträger wie nachgelagerte Anbieter und unabhängige Sachverständige können den Prozess unterstützen.\n(4) Das Büro für Künstliche Intelligenz und das KI-Gremium streben an, sicherzustellen, dass in den Praxisleitfäden ihre spezifischen Ziele eindeutig festgelegt sind und Verpflichtungen oder Maßnahmen, gegebenenfalls einschließlich wesentlicher Leistungsindikatoren, enthalten, um die Verwirklichung dieser Ziele gewährleisten, und dass sie den Bedürfnissen und Interessen aller interessierten Kreise, einschließlich betroffener Personen, auf Unionsebene gebührend Rechnung tragen.\n(5) Das Büro für Künstliche Intelligenz strebt an, sicherzustellen, dass die an Praxisleitfäden Beteiligten dem Büro für Künstliche Intelligenz regelmäßig über die Umsetzung der Verpflichtungen, die ergriffenen Maßnahmen und deren Ergebnisse, die gegebenenfalls auch anhand der wesentlichen Leistungsindikatoren gemessen werden, Bericht erstatten. Bei den wesentlichen Leistungsindikatoren und den Berichtspflichten wird den Größen- und Kapazitätsunterschieden zwischen den verschiedenen Beteiligten Rechnung getragen.\n(6) Das Büro für Künstliche Intelligenz und KI-Gremium überwachen und bewerten regelmäßig die Verwirklichung der Ziele der Praxisleitfäden durch die Beteiligten und deren Beitrag zur ordnungsgemäßen Anwendung dieser Verordnung. Das Büro für Künstliche Intelligenz und das KI-Gremium bewerten, ob die Praxisleitfäden die in den Artikeln 53 und 55 vorgesehenen Pflichten abdecken, und überwachen und bewerten regelmäßig die Verwirklichung von deren Zielen. Sie veröffentlichen ihre Bewertung der Angemessenheit der Praxisleitfäden.\nDie Kommission kann im Wege eines Durchführungsrechtsakts einen Praxisleitfaden genehmigen und ihm in der Union allgemeine Gültigkeit verleihen. Dieser Durchführungsrechtsakt wird gemäß dem in Artikel 98 Absatz 2 genannten Prüfverfahren erlassen.\n(7) Das Büro für Künstliche Intelligenz kann alle Anbieter von KI-Modellen mit allgemeinem Verwendungszweck ersuchen, die Praxisleitfäden zu befolgen. Für Anbieter von KI-Modellen mit allgemeinem Verwendungszweck, die keine systemischen Risiken bergen, kann diese Befolgung auf die in Artikel 53 vorgesehenen Pflichten beschränkt werden, es sei denn, sie erklären ausdrücklich ihr Interesse, sich dem ganzen Kodex anzuschließen.\n(8) Das Büro für Künstliche Intelligenz fördert und erleichtert gegebenenfalls auch die Überprüfung und Anpassung der Praxisleitfäden, insbesondere vor dem Hintergrund neuer Normen. Das Büro für Künstliche Intelligenz unterstützt bei der Bewertung der verfügbaren Normen.\n(9) Praxisleitfäden müssen spätestens am 2. Mai 2025 vorliegen. Das Büro für Künstliche Intelligenz unternimmt die erforderlichen Schritte, einschließlich des Ersuchens von Anbietern gemäß Absatz 7. Kann bis zum 2. August 2025 ein Verhaltenskodex nicht fertiggestellt werden oder erachtet das Büro für Künstliche Intelligenz dies nach seiner Bewertung gemäß Absatz 6 des vorliegenden Artikels für nicht angemessen, kann die Kommission im Wege von Durchführungsrechtsakten gemeinsame Vorschriften für die Umsetzung der in den Artikeln 53 und 55 vorgesehenen Pflichten, einschließlich der in Absatz 2 des vorliegenden Artikels genannten Aspekte, festlegen. Diese Durchführungsrechtsakte werden gemäß dem in Artikel 98 Absatz 2 genannten Prüfverfahren erlassen."
    },
    {
      "chunk_idx": 261,
      "id": "8c75b44d-51ac-48e9-b7d7-45c0362eaefb",
      "title": "Art 57",
      "relevantChunksIds": [
        "0af8ee0f-22ec-4daa-92dd-589a6b0f94f1",
        "ca5614f2-4cf4-460e-addd-b5609143a0cd",
        "b0f84d4d-7c65-4840-a9aa-aa32b68acc74"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "# CHAPTER VI: MEASURES IN SUPPORT OF INNOVATION\n### Article 57: AI regulatory sandboxes\n1. Member States shall ensure that their competent authorities establish at least one AI regulatory sandbox at national level, which shall be operational by 2 August 2026. That sandbox may also be established jointly with the competent authorities of other Member States. The Commission may provide technical support, advice and tools for the establishment and operation of AI regulatory sandboxes.\nThe obligation under the first subparagraph may also be fulfilled by participating in an existing sandbox in so far as that participation provides an equivalent level of national coverage for the participating Member States.\n2. Additional AI regulatory sandboxes at regional or local level, or established jointly with the competent authorities of other Member States may also be established.\n3. The European Data Protection Supervisor may also establish an AI regulatory sandbox for Union institutions, bodies, offices and agencies, and may exercise the roles and the tasks of national competent authorities in accordance with this Chapter.\n4. Member States shall ensure that the competent authorities referred to in paragraphs 1 and 2 allocate sufficient resources to comply with this Article effectively and in a timely manner. Where appropriate, national competent authorities shall cooperate with other relevant authorities, and may allow for the involvement of other actors within the AI ecosystem. This Article shall not affect other regulatory sandboxes established under Union or national law. Member States shall ensure an appropriate level of cooperation between the authorities supervising those other sandboxes and the national competent authorities.\n5. AI regulatory sandboxes established under paragraph 1 shall provide for a controlled environment that fosters innovation and facilitates the development, training, testing and validation of innovative AI systems for a limited time before their being placed on the market or put into service pursuant to a specific sandbox plan agreed between the providers or prospective providers and the competent authority. Such sandboxes may include testing in real world conditions supervised therein.\n6. Competent authorities shall provide, as appropriate, guidance, supervision and support within the AI regulatory sandbox with a view to identifying risks, in particular to fundamental rights, health and safety, testing, mitigation measures, and their effectiveness in relation to the obligations and requirements of this Regulation and, where relevant, other Union and national law supervised within the sandbox.\n7. Competent authorities shall provide providers and prospective providers participating in the AI regulatory sandbox with guidance on regulatory expectations and how to fulfil the requirements and obligations set out in this Regulation.\nUpon request of the provider or prospective provider of the AI system, the competent authority shall provide a written proof of the activities successfully carried out in the sandbox. The competent authority shall also provide an exit report detailing the activities carried out in the sandbox and the related results and learning outcomes. Providers may use such documentation to demonstrate their compliance with this Regulation through the conformity assessment process or relevant market surveillance activities. In this regard, the exit reports and the written proof provided by the national competent authority shall be taken positively into account by market surveillance authorities and notified bodies, with a view to accelerating conformity assessment procedures to a reasonable extent.\n8. Subject to the confidentiality provisions in Article 78, and with the agreement of the provider or prospective provider, the Commission and the Board shall be authorised to access the exit reports and shall take them into account, as appropriate, when exercising their tasks under this Regulation. If both the provider or prospective provider and the national competent authority explicitly agree, the exit report may be made publicly available through the single information platform referred to in this Article.\n9. The establishment of AI regulatory sandboxes shall aim to contribute to the following objectives:\n(a) improving legal certainty to achieve regulatory compliance with this Regulation or, where relevant, other applicable Union and national law;\n(b) supporting the sharing of best practices through cooperation with the authorities involved in the AI regulatory sandbox;\n(c) fostering innovation and competitiveness and facilitating the development of an AI ecosystem;\n(d) contributing to evidence-based regulatory learning;\n(e) facilitating and accelerating access to the Union market for AI systems, in particular when provided by SMEs, including start-ups.\n10. National competent authorities shall ensure that, to the extent the innovative AI systems involve the processing of personal data or otherwise fall under the supervisory remit of other national authorities or competent authorities providing or supporting access to data, the national data protection authorities and those other national or competent authorities are associated with the operation of the AI regulatory sandbox and involved in the supervision of those aspects to the extent of their respective tasks and powers.\n11. The AI regulatory sandboxes shall not affect the supervisory or corrective powers of the competent authorities supervising the sandboxes, including at regional or local level. Any significant risks to health and safety and fundamental rights identified during the development and testing of such AI systems shall result in an adequate mitigation. National competent authorities shall have the power to temporarily or permanently suspend the testing process, or the participation in the sandbox if no effective mitigation is possible, and shall inform the AI Office of such decision. National competent authorities shall exercise their supervisory powers within the limits of the relevant law, using their discretionary powers when implementing legal provisions in respect of a specific AI regulatory sandbox project, with the objective of supporting innovation in AI in the Union.\n12. Providers and prospective providers participating in the AI regulatory sandbox shall remain liable under applicable Union and national liability law for any damage inflicted on third parties as a result of the experimentation taking place in the sandbox. However, provided that the prospective providers observe the specific plan and the terms and conditions for their participation and follow in good faith the guidance given by the national competent authority, no administrative fines shall be imposed by the authorities for infringements of this Regulation. Where other competent authorities responsible for other Union and national law were actively involved in the supervision of the AI system in the sandbox and provided guidance for compliance, no administrative fines shall be imposed regarding that law.\n13. The AI regulatory sandboxes shall be designed and implemented in such a way that, where relevant, they facilitate cross-border cooperation between national competent authorities.\n14. National competent authorities shall coordinate their activities and cooperate within the framework of the Board.\n15. National competent authorities shall inform the AI Office and the Board of the establishment of a sandbox, and may ask them for support and guidance. The AI Office shall make publicly available a list of planned and existing sandboxes and keep it up to date in order to encourage more interaction in the AI regulatory sandboxes and cross-border cooperation.\n16. National competent authorities shall submit annual reports to the AI Office and to the Board, from one year after the establishment of the AI regulatory sandbox and every year thereafter until its termination, and a final report. Those reports shall provide information on the progress and results of the implementation of those sandboxes, including best practices, incidents, lessons learnt and recommendations on their setup and, where relevant, on the application and possible revision of this Regulation, including its delegated and implementing acts, and on the application of other Union law supervised by the competent authorities within the sandbox. The national competent authorities shall make those annual reports or abstracts thereof available to the public, online. The Commission shall, where appropriate, take the annual reports into account when exercising its tasks under this Regulation.\n17. The Commission shall develop a single and dedicated interface containing all relevant information related to AI regulatory sandboxes to allow stakeholders to interact with AI regulatory sandboxes and to raise enquiries with competent authorities, and to seek non-binding guidance on the conformity of innovative products, services, business models embedding AI technologies, in accordance with Article 62(1), point (c). The Commission shall proactively coordinate with national competent authorities, where relevant.",
      "original_content": "# KAPITEL VI: MASSNAHMEN ZUR INNOVATIONSFÖRDERUNG\n### Artikel 57: KI-Reallabore\n(1) Die Mitgliedstaaten sorgen dafür, dass ihre zuständigen Behörden mindestens ein KI-Reallabor auf nationaler Ebene einrichten, das bis zum 2. August 2026 einsatzbereit sein muss. Dieses Reallabor kann auch gemeinsam mit den zuständigen Behörden anderer Mitgliedstaaten eingerichtet werden. Die Kommission kann technische Unterstützung, Beratung und Instrumente für die Einrichtung und den Betrieb von KI-Reallaboren bereitstellen.\nDie Verpflichtung nach Unterabsatz 1 kann auch durch Beteiligung an einem bestehenden Reallabor erfüllt werden, sofern eine solche Beteiligung die nationale Abdeckung der teilnehmenden Mitgliedstaaten in gleichwertigem Maße gewährleistet.\n(2) Es können auch zusätzliche KI-Reallabore auf regionaler oder lokaler Ebene oder gemeinsam mit den zuständigen Behörden anderer Mitgliedstaaten eingerichtet werden;\n(3) Der Europäische Datenschutzbeauftragte kann auch ein KI-Reallabor für Organe, Einrichtungen und sonstige Stellen der Union einrichten und die Rollen und Aufgaben der zuständigen nationalen Behörden im Einklang mit diesem Kapitel wahrnehmen.\n(4) Die Mitgliedstaaten stellen sicher, dass die in den Absätzen 1 und 2 genannten zuständigen Behörden ausreichende Mittel bereitstellen, um diesem Artikel wirksam und zeitnah nachzukommen. Gegebenenfalls arbeiten die zuständigen nationalen Behörden mit anderen einschlägigen Behörden zusammen und können die Einbeziehung anderer Akteure des KI-Ökosystems gestatten. Andere Reallabore, die im Rahmen des Unionsrechts oder des nationalen Rechts eingerichtet wurden, bleiben von diesem Artikel unberührt. Die Mitgliedstaaten sorgen dafür, dass die diese anderen Reallabore beaufsichtigenden Behörden und die zuständigen nationalen Behörden angemessen zusammenarbeiten.\n(5) Die nach Absatz 1 eingerichteten KI-Reallabore bieten eine kontrollierte Umgebung, um Innovation zu fördern und die Entwicklung, das Training, das Testen und die Validierung innovativer KI-Systeme für einen begrenzten Zeitraum vor ihrem Inverkehrbringen oder ihrer Inbetriebnahme nach einem bestimmten zwischen den Anbietern oder zukünftigen Anbietern und der zuständigen Behörde vereinbarten Reallabor-Plan zu erleichtern. In diesen Reallaboren können auch darin beaufsichtigte Tests unter Realbedingungen durchgeführt werden.\n(6) Die zuständigen Behörden stellen innerhalb der KI-Reallabore gegebenenfalls Anleitung, Aufsicht und Unterstützung bereit, um Risiken, insbesondere im Hinblick auf Grundrechte, Gesundheit und Sicherheit, Tests und Risikominderungsmaßnahmen sowie deren Wirksamkeit hinsichtlich der Pflichten und Anforderungen dieser Verordnung und gegebenenfalls anderem Unionsrecht und nationalem Recht, deren Einhaltung innerhalb des Reallabors beaufsichtigt wird, zu ermitteln.\n(7) Die zuständigen Behörden stellen den Anbietern und zukünftigen Anbietern, die am KI-Reallabor teilnehmen, Leitfäden zu regulatorischen Erwartungen und zur Erfüllung der in dieser Verordnung festgelegten Anforderungen und Pflichten zur Verfügung.\nDie zuständige Behörde legt dem Anbieter oder zukünftigen Anbieter des KI-Systems auf dessen Anfrage einen schriftlichen Nachweis für die im Reallabor erfolgreich durchgeführten Tätigkeiten vor. Außerdem legt die zuständige Behörde einen Abschlussbericht vor, in dem sie die im Reallabor durchgeführten Tätigkeiten, deren Ergebnisse und die gewonnenen Erkenntnisse im Einzelnen darlegt. Die Anbieter können diese Unterlagen nutzen, um im Rahmen des Konformitätsbewertungsverfahrens oder einschlägiger Marktüberwachungstätigkeiten nachzuweisen, dass sie dieser Verordnung nachkommen. In diesem Zusammenhang werden die Abschlussberichte und die von der zuständigen nationalen Behörde vorgelegten schriftlichen Nachweise von den Marktüberwachungsbehörden und den notifizierten Stellen im Hinblick auf eine Beschleunigung der Konformitätsbewertungsverfahren in angemessenem Maße positiv gewertet.\n(8) Vorbehaltlich der in Artikel 78 enthaltenen Bestimmungen über die Vertraulichkeit und im Einvernehmen mit den Anbietern oder zukünftigen Anbietern, sind die Kommission und das KI-Gremium befugt, die Abschlussberichte einzusehen und tragen diesen gegebenenfalls bei der Wahrnehmung ihrer Aufgaben gemäß dieser Verordnung Rechnung. Wenn der Anbieter oder der zukünftige Anbieter und die zuständige nationale Behörde ihr ausdrückliches Einverständnis erklären, kann der Abschlussbericht über die in diesem Artikel genannte zentrale Informationsplattform veröffentlicht werden.\n(9) Die Einrichtung von KI-Reallaboren soll zu den folgenden Zielen beitragen:\na) Verbesserung der Rechtssicherheit, um für die Einhaltung der Regulierungsvorschriften dieser Verordnung oder, gegebenenfalls, anderem geltenden Unionsrecht und nationalem Recht zu sorgen;\nb) Förderung des Austauschs bewährter Verfahren durch Zusammenarbeit mit den am KI-Reallabor beteiligten Behörden;\nc) Förderung von Innovation und Wettbewerbsfähigkeit sowie Erleichterung der Entwicklung eines KI-Ökosystems;\nd) Leisten eines Beitrags zum evidenzbasierten regulatorischen Lernen;\ne) Erleichterung und Beschleunigung des Zugangs von KI-Systemen zum Unionsmarkt, insbesondere wenn sie von KMU — einschließlich Start-up-Unternehmen — angeboten werden.\n(10) Soweit die innovativen KI-Systeme personenbezogene Daten verarbeiten oder anderweitig der Aufsicht anderer nationaler Behörden oder zuständiger Behörden unterstehen, die den Zugang zu personenbezogenen Daten gewähren oder unterstützen, sorgen die zuständigen nationalen Behörden dafür, dass die nationalen Datenschutzbehörden oder diese anderen nationalen oder zuständigen Behörden in den Betrieb des KI-Reallabors sowie in die Überwachung dieser Aspekte im vollen Umfang ihrer entsprechenden Aufgaben und Befugnisse einbezogen werden.\n(11) Die KI-Reallabore lassen die Aufsichts- oder Abhilfebefugnisse der die Reallabore beaufsichtigenden zuständigen Behörden, einschließlich auf regionaler oder lokaler Ebene, unberührt. Alle erheblichen Risiken für die Gesundheit und Sicherheit und die Grundrechte, die bei der Entwicklung und Erprobung solcher KI-Systeme festgestellt werden, führen zur sofortigen und angemessenen Risikominderung. Die zuständigen nationalen Behörden sind befugt, das Testverfahren oder die Beteiligung am Reallabor vorübergehend oder dauerhaft auszusetzen, wenn keine wirksame Risikominderung möglich ist, und unterrichten das Büro für Künstliche Intelligenz über diese Entscheidung. Um Innovationen im Bereich KI in der Union zu fördern, üben die zuständigen nationalen Behörden ihre Aufsichtsbefugnisse im Rahmen des geltenden Rechts aus, indem sie bei der Anwendung der Rechtsvorschriften auf ein bestimmtes KI-Reallabor ihren Ermessensspielraum nutzen.\n(12) Die am KI-Reallabor beteiligten Anbieter und zukünftigen Anbieter bleiben nach geltendem Recht der Union und nationalem Haftungsrecht für Schäden haftbar, die Dritten infolge der Erprobung im Reallabor entstehen. Sofern die zukünftigen Anbieter den spezifischen Plan und die Bedingungen für ihre Beteiligung beachten und der Anleitung durch die zuständigen nationalen Behörden in gutem Glauben folgen, werden jedoch von den Behörden keine Geldbußen für Verstöße gegen diese Verordnung verhängt. In Fällen, in denen andere zuständige Behörden, die für anderes Unionsrecht und nationales Recht zuständig sind, aktiv an der Beaufsichtigung des KI-Systems im Reallabor beteiligt waren und Anleitung für die Einhaltung gegeben haben, werden im Hinblick auf dieses Recht keine Geldbußen verhängt.\n(13) Die KI-Reallabore sind so konzipiert und werden so umgesetzt, dass sie gegebenenfalls die grenzüberschreitende Zusammenarbeit zwischen zuständigen nationalen Behörden erleichtern.\n(14) Die zuständigen nationalen Behörden koordinieren ihre Tätigkeiten und arbeiten im Rahmen des KI-Gremiums zusammen.\n(15) Die zuständigen nationalen Behörden unterrichten das Büro für Künstliche Intelligenz und das KI-Gremium über die Einrichtung eines Reallabors und können sie um Unterstützung und Anleitung bitten. Das Büro für Künstliche Intelligenz veröffentlicht eine Liste der geplanten und bestehenden Reallabore und hält sie auf dem neuesten Stand, um eine stärkere Interaktion in den KI-Reallaboren und die grenzüberschreitende Zusammenarbeit zu fördern.\n(16) Die zuständigen nationalen Behörden übermitteln dem Büro für Künstliche Intelligenz und dem KI-Gremium jährliche Berichte, und zwar ab einem Jahr nach der Einrichtung des Reallabors und dann jedes Jahr bis zu dessen Beendigung, sowie einen Abschlussbericht. Diese Berichte informieren über den Fortschritt und die Ergebnisse der Umsetzung dieser Reallabore, einschließlich bewährter Verfahren, Vorfällen, gewonnener Erkenntnisse und Empfehlungen zu deren Aufbau, sowie gegebenenfalls über die Anwendung und mögliche Überarbeitung dieser Verordnung, einschließlich ihrer delegierten Rechtsakte und Durchführungsrechtsakte, sowie über die Anwendung anderen Unionsrechts, deren Einhaltung von den zuständigen Behörden innerhalb des Reallabors beaufsichtigt wird. Die zuständigen nationalen Behörden stellen diese jährlichen Berichte oder Zusammenfassungen davon der Öffentlichkeit online zur Verfügung. Die Kommission trägt den jährlichen Berichten gegebenenfalls bei der Wahrnehmung ihrer Aufgaben gemäß dieser Verordnung Rechnung.\n(17) Die Kommission richtet eine eigene Schnittstelle ein, die alle relevanten Informationen zu den KI-Reallaboren enthält, um es den Interessenträgern zu ermöglichen, mit den KI-Reallaboren zu interagieren und Anfragen an die zuständigen Behörden zu richten und unverbindliche Beratung zur Konformität von innovativen Produkten, Dienstleistungen und Geschäftsmodellen mit integrierter KI-Technologie im Einklang mit Artikel 62 Absatz 1 Buchstabe c einzuholen. Die Kommission stimmt sich gegebenenfalls proaktiv mit den zuständigen nationalen Behörden ab."
    },
    {
      "chunk_idx": 262,
      "id": "6ad1abec-e75e-499f-9eb0-9839e8ff38cc",
      "title": "Art 58",
      "relevantChunksIds": [
        "d5fd389b-f8c7-499f-a3a9-2283e1b0befd"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 58: Detailed arrangements for, and functioning of, AI regulatory sandboxes\n1. In order to avoid fragmentation across the Union, the Commission shall adopt implementing acts specifying the detailed arrangements for the establishment, development, implementation, operation and supervision of the AI regulatory sandboxes. The implementing acts shall include common principles on the following issues:\n(a) eligibility and selection criteria for participation in the AI regulatory sandbox;\n(b) procedures for the application, participation, monitoring, exiting from and termination of the AI regulatory sandbox, including the sandbox plan and the exit report;\n(c) the terms and conditions applicable to the participants.\nThose implementing acts shall be adopted in accordance with the examination procedure referred to in Article 98(2).\n2. The implementing acts referred to in paragraph 1 shall ensure:\n(a) that AI regulatory sandboxes are open to any applying provider or prospective provider of an AI system who fulfils eligibility and selection criteria, which shall be transparent and fair, and that national competent authorities inform applicants of their decision within three months of the application;\n(b) that AI regulatory sandboxes allow broad and equal access and keep up with demand for participation; providers and prospective providers may also submit applications in partnerships with deployers and other relevant third parties;\n(c) that the detailed arrangements for, and conditions concerning AI regulatory sandboxes support, to the best extent possible, flexibility for national competent authorities to establish and operate their AI regulatory sandboxes;\n(d) that access to the AI regulatory sandboxes is free of charge for SMEs, including start-ups, without prejudice to exceptional costs that national competent authorities may recover in a fair and proportionate manner;\n(e) that they facilitate providers and prospective providers, by means of the learning outcomes of the AI regulatory sandboxes, in complying with conformity assessment obligations under this Regulation and the voluntary application of the codes of conduct referred to in Article 95;\n(f) that AI regulatory sandboxes facilitate the involvement of other relevant actors within the AI ecosystem, such as notified bodies and standardisation organisations, SMEs, including start-ups, enterprises, innovators, testing and experimentation facilities, research and experimentation labs and European Digital Innovation Hubs, centres of excellence, individual researchers, in order to allow and facilitate cooperation with the public and private sectors;\n(g) that procedures, processes and administrative requirements for application, selection, participation and exiting the AI regulatory sandbox are simple, easily intelligible, and clearly communicated in order to facilitate the participation of SMEs, including start-ups, with limited legal and administrative capacities and are streamlined across the Union, in order to avoid fragmentation and that participation in an AI regulatory sandbox established by a Member State, or by the European Data Protection Supervisor is mutually and uniformly recognised and carries the same legal effects across the Union;\n(h) that participation in the AI regulatory sandbox is limited to a period that is appropriate to the complexity and scale of the project and that may be extended by the national competent authority;\n(i) that AI regulatory sandboxes facilitate the development of tools and infrastructure for testing, benchmarking, assessing and explaining dimensions of AI systems relevant for regulatory learning, such as accuracy, robustness and cybersecurity, as well as measures to mitigate risks to fundamental rights and society at large.\n3. Prospective providers in the AI regulatory sandboxes, in particular SMEs and start-ups, shall be directed, where relevant, to pre-deployment services such as guidance on the implementation of this Regulation, to other value-adding services such as help with standardisation documents and certification, testing and experimentation facilities, European Digital Innovation Hubs and centres of excellence.\n4. Where national competent authorities consider authorising testing in real world conditions supervised within the framework of an AI regulatory sandbox to be established under this Article, they shall specifically agree the terms and conditions of such testing and, in particular, the appropriate safeguards with the participants, with a view to protecting fundamental rights, health and safety. Where appropriate, they shall cooperate with other national competent authorities with a view to ensuring consistent practices across the Union.",
      "original_content": "### Artikel 58: Detaillierte Regelungen für KI-Reallabore und deren Funktionsweise\n(1) Um eine Zersplitterung in der Union zu vermeiden, erlässt die Kommission Durchführungsrechtsakte, in denen detaillierte Regelungen für die Einrichtung, Entwicklung, Umsetzung, den Betrieb und die Beaufsichtigung der KI-Reallabore enthalten sind. In den Durchführungsrechtsakten sind gemeinsame Grundsätze zu den folgenden Aspekten festgelegt:\na) Voraussetzungen und Auswahlkriterien für eine Beteiligung am KI-Reallabor;\nb) Verfahren für Antragstellung, Beteiligung, Überwachung, Ausstieg und Beendigung bezüglich des KI-Reallabors, einschließlich Plan und Abschlussbericht für das Reallabor;\nc) für Beteiligte geltende Anforderungen und Bedingungen.\nDiese Durchführungsrechtsakte werden gemäß dem in Artikel 98 Absatz 2 genannten Prüfverfahren erlassen.\n(2) Die in Absatz 1 genannten Durchführungsrechtsakte gewährleisten,\na) dass KI-Reallabore allen Anbietern oder zukünftigen Anbietern eines KI-Systems, die einen Antrag stellen und die Voraussetzungen und Auswahlkriterien erfüllen, offen stehen; diese Voraussetzungen und Kriterien sind transparent und fair und die zuständigen nationalen Behörden informieren die Antragsteller innerhalb von drei Monaten nach Antragstellung über ihre Entscheidung;\nb) dass die KI-Reallabore einen breiten und gleichberechtigten Zugang ermöglichen und mit der Nachfrage nach Beteiligung Schritt halten; die Anbieter und zukünftigen Anbieter auch Anträge zusammen mit Betreibern oder einschlägigen Dritten, die ihre Partner sind, stellen können;\nc) dass die detaillierten Regelungen und Bedingungen für KI-Reallabore so gut wie möglich die Flexibilität der zuständigen nationalen Behörden bei der Einrichtung und dem Betrieb ihrer KI-Reallabore unterstützen;\nd) dass der Zugang zu KI-Reallaboren für KMU, einschließlich Start-up-Unternehmen, kostenlos ist, unbeschadet außergewöhnlicher Kosten, die die zuständigen nationalen Behörden in einer fairen und verhältnismäßigen Weise einfordern können;\ne) dass den Anbietern und zukünftigen Anbietern die Einhaltung der Verpflichtungen zur Konformitätsbewertung nach dieser Verordnung oder die freiwillige Anwendung der in Artikel 95 genannten Verhaltenskodizes mittels der gewonnenen Erkenntnisse der KI-Reallabore erleichtert wird;\nf) dass KI-Reallabore die Einbeziehung anderer einschlägiger Akteure innerhalb des KI-Ökosystems, wie etwa notifizierte Stellen und Normungsorganisationen, KMU, einschließlich Start-up-Unternehmen, Unternehmen, Innovatoren, Test- und Versuchseinrichtungen, Forschungs- und Versuchslabore, europäische digitale Innovationszentren, Kompetenzzentren und einzelne Forscher begünstigen, um die Zusammenarbeit mit dem öffentlichen und dem privaten Sektor zu ermöglichen und zu erleichtern;\ng) dass die Verfahren, Prozesse und administrativen Anforderungen für die Antragstellung, die Auswahl, die Beteiligung und den Ausstieg aus dem KI-Reallabor einfach, leicht verständlich und klar kommuniziert sind, um die Beteiligung von KMU, einschließlich Start-up-Unternehmen, mit begrenzten rechtlichen und administrativen Kapazitäten zu erleichtern, sowie unionsweit gestrafft sind, um eine Zersplitterung zu vermeiden, und dass die Beteiligung an einem von einem Mitgliedstaat oder dem Europäischen Datenschutzbeauftragten eingerichteten KI-Reallabor gegenseitig und einheitlich anerkannt wird und in der gesamten Union die gleiche Rechtswirkung hat;\nh) dass die Beteiligung an dem KI-Reallabor auf einen der Komplexität und dem Umfang des Projekts entsprechenden Zeitraum beschränkt ist, der von der zuständigen nationalen Behörde verlängert werden kann;\ni) dass die KI-Reallabore die Entwicklung von Instrumenten und Infrastruktur für das Testen, das Benchmarking, die Bewertung und die Erklärung der Dimensionen von KI-Systemen erleichtern, die für das regulatorische Lernen Bedeutung sind, wie etwa Genauigkeit, Robustheit und Cybersicherheit, sowie Maßnahmen zur Risikominderung im Hinblick auf die Grundrechte und die Gesellschaft als Ganzes fördern.\n(3) Zukünftige Anbieter in den KI-Reallaboren, insbesondere KMU und Start-up-Unternehmen, werden gegebenenfalls vor der Einrichtung an Dienste verwiesen, die beispielsweise eine Anleitung zur Umsetzung dieser Verordnung oder andere Mehrwertdienste wie Hilfe bei Normungsdokumenten bereitstellen, sowie an Zertifizierungs-, Test- und Versuchseinrichtungen, europäische digitale Innovationszentren und Exzellenzzentren.\n(4) Wenn zuständige nationale Behörden in Betracht ziehen, Tests unter Realbedingungen zu genehmigen, die im Rahmen eines KI-Reallabors beaufsichtigt werden, welches nach diesem Artikel einzurichten ist, vereinbaren sie mit den Beteiligten ausdrücklich die Anforderungen und Bedingungen für diese Tests und insbesondere geeignete Schutzvorkehrungen für Grundrechte, Gesundheit und Sicherheit. Gegebenenfalls arbeiten sie mit anderen zuständigen nationalen Behörden zusammen, um für unionsweit einheitliche Verfahren zu sorgen."
    },
    {
      "chunk_idx": 263,
      "id": "4095ee13-aa11-4440-bbb9-b3bd17715a47",
      "title": "Art 59",
      "relevantChunksIds": [
        "b0f84d4d-7c65-4840-a9aa-aa32b68acc74"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 59: Further processing of personal data for developing certain AI systems in the public interest in the AI regulatory sandbox\n1. In the AI regulatory sandbox, personal data lawfully collected for other purposes may be processed solely for the purpose of developing, training and testing certain AI systems in the sandbox when all of the following conditions are met:\n(a) AI systems shall be developed for safeguarding substantial public interest by a public authority or another natural or legal person and in one or more of the following areas:\n(i) public safety and public health, including disease detection, diagnosis prevention, control and treatment and improvement of health care systems;\n(ii) a high level of protection and improvement of the quality of the environment, protection of biodiversity, protection against pollution, green transition measures, climate change mitigation and adaptation measures;\n(iii) energy sustainability;\n(iv) safety and resilience of transport systems and mobility, critical infrastructure and networks;\n(v) efficiency and quality of public administration and public services;\n(b) the data processed are necessary for complying with one or more of the requirements referred to in Chapter III, Section 2 where those requirements cannot effectively be fulfilled by processing anonymised, synthetic or other non-personal data;\n(c) there are effective monitoring mechanisms to identify if any high risks to the rights and freedoms of the data subjects, as referred to in Article 35 of Regulation (EU) 2016/679 and in Article 39 of Regulation (EU) 2018/1725, may arise during the sandbox experimentation, as well as response mechanisms to promptly mitigate those risks and, where necessary, stop the processing;\n(d) any personal data to be processed in the context of the sandbox are in a functionally separate, isolated and protected data processing environment under the control of the prospective provider and only authorised persons have access to those data;\n(e) providers can further share the originally collected data only in accordance with Union data protection law; any personal data created in the sandbox cannot be shared outside the sandbox;\n(f) any processing of personal data in the context of the sandbox neither leads to measures or decisions affecting the data subjects nor does it affect the application of their rights laid down in Union law on the protection of personal data;\n(g) any personal data processed in the context of the sandbox are protected by means of appropriate technical and organisational measures and deleted once the participation in the sandbox has terminated or the personal data has reached the end of its retention period;\n(h) the logs of the processing of personal data in the context of the sandbox are kept for the duration of the participation in the sandbox, unless provided otherwise by Union or national law;\n(i) a complete and detailed description of the process and rationale behind the training, testing and validation of the AI system is kept together with the testing results as part of the technical documentation referred to in Annex IV;\n(j) a short summary of the AI project developed in the sandbox, its objectives and expected results is published on the website of the competent authorities; this obligation shall not cover sensitive operational data in relation to the activities of law enforcement, border control, immigration or asylum authorities.\n",
      "original_content": "### Artikel 59: Weiterverarbeitung personenbezogener Daten zur Entwicklung bestimmter KI-Systeme im öffentlichen Interesse im KI-Reallabor\n(1) Rechtmäßig für andere Zwecke erhobene personenbezogene Daten dürfen im KI-Reallabor ausschließlich für die Zwecke der Entwicklung, des Trainings und des Testens bestimmter KI-Systeme im Reallabor verarbeitet werden, wenn alle der folgenden Bedingungen erfüllt sind:\na) Die KI-Systeme werden zur Wahrung eines erheblichen öffentlichen Interesses durch eine Behörde oder eine andere natürliche oder juristische Person und in einem oder mehreren der folgenden Bereiche entwickelt:\ni) öffentliche Sicherheit und öffentliche Gesundheit, einschließlich Erkennung, Diagnose, Verhütung, Bekämpfung und Behandlung von Krankheiten sowie Verbesserung von Gesundheitsversorgungssystemen;\nii) hohes Umweltschutzniveau und Verbesserung der Umweltqualität, Schutz der biologischen Vielfalt, Schutz gegen Umweltverschmutzung, Maßnahmen für den grünen Wandel sowie Klimaschutz und Anpassung an den Klimawandel;\niii) nachhaltige Energie;\niv) Sicherheit und Widerstandsfähigkeit von Verkehrssystemen und Mobilität, kritischen Infrastrukturen und Netzen;\nv) Effizienz und Qualität der öffentlichen Verwaltung und öffentlicher Dienste;\nb) die verarbeiteten Daten sind für die Erfüllung einer oder mehrerer der in Kapitel III Abschnitt 2 genannten Anforderungen erforderlich, sofern diese Anforderungen durch die Verarbeitung anonymisierter, synthetischer oder sonstiger nicht personenbezogener Daten nicht wirksam erfüllt werden können;\nc) es bestehen wirksame Überwachungsmechanismen, mit deren Hilfe festgestellt wird, ob während der Reallaborversuche hohe Risiken für die Rechte und Freiheiten betroffener Personen gemäß Artikel 35 der Verordnung (EU) 2016/679 und gemäß Artikel 39 der Verordnung (EU) 2018/1725 auftreten können, sowie Reaktionsmechanismen, mit deren Hilfe diese Risiken umgehend eingedämmt werden können und die Verarbeitung bei Bedarf beendet werden kann;\nd) personenbezogene Daten, die im Rahmen des Reallabors verarbeitet werden sollen, befinden sich in einer funktional getrennten, isolierten und geschützten Datenverarbeitungsumgebung unter der Kontrolle des zukünftigen Anbieters, und nur befugte Personen haben Zugriff auf diese Daten;\ne) Anbieter dürfen die ursprünglich erhobenen Daten nur im Einklang mit dem Datenschutzrecht der Union weitergeben; personenbezogene Daten, die im Reallabor erstellt wurden, dürfen nicht außerhalb des Reallabors weitergegeben werden;\nf) eine Verarbeitung personenbezogener Daten im Rahmen des Reallabors führt zu keinen Maßnahmen oder Entscheidungen, die Auswirkungen auf die betroffenen Personen haben, und berührt nicht die Anwendung ihrer Rechte, die in den Rechtsvorschriften der Union über den Schutz personenbezogener Daten festgelegt sind;\ng) im Rahmen des Reallabors verarbeitete personenbezogene Daten sind durch geeignete technische und organisatorische Maßnahmen geschützt und werden gelöscht, sobald die Beteiligung an dem Reallabor endet oder das Ende der Speicherfrist für die personenbezogenen Daten erreicht ist;\nh) die Protokolle der Verarbeitung personenbezogener Daten im Rahmen des Reallabors werden für die Dauer der Beteiligung am Reallabor aufbewahrt, es sei denn, im Unionsrecht oder nationalen Recht ist etwas anderes bestimmt;\ni) eine vollständige und detaillierte Beschreibung des Prozesses und der Gründe für das Trainieren, Testen und Validieren des KI-Systems wird zusammen mit den Testergebnissen als Teil der technischen Dokumentation gemäß Anhang IV aufbewahrt;\nj) eine kurze Zusammenfassung des im Reallabor entwickelten KI-Projekts, seiner Ziele und der erwarteten Ergebnisse wird auf der Website der zuständigen Behörden veröffentlicht; diese Pflicht erstreckt sich nicht auf sensible operative Daten zu den Tätigkeiten von Strafverfolgungs-, Grenzschutz-, Einwanderungs- oder Asylbehörden.\n(2) Für die Zwecke der Verhütung, Ermittlung, Aufdeckung oder Verfolgung von Straftaten oder der Strafvollstreckung — einschließlich des Schutzes vor und der Abwehr von Gefahren für die öffentliche Sicherheit — unter der Kontrolle und Verantwortung der Strafverfolgungsbehörden erfolgt die Verarbeitung personenbezogener Daten in KI-Reallaboren auf der Grundlage eines spezifischen Unionsrechts oder nationalen Rechts und unterliegt den kumulativen Bedingungen des Absatzes 1. (3) Das Unionsrecht oder nationale Recht, das die Verarbeitung personenbezogener Daten für andere Zwecke als die ausdrücklich in jenem Recht genannten ausschließt, sowie Unionsrecht oder nationales Recht, in dem die Grundlagen für eine für die Zwecke der Entwicklung, des Testens oder des Trainings innovativer KI-Systeme notwendige Verarbeitung personenbezogener Daten festgelegt sind, oder jegliche anderen dem Unionsrecht zum Schutz personenbezogener Daten entsprechenden Rechtsgrundlagen bleiben von Absatz 1 unberührt."
    },
    {
      "chunk_idx": 264,
      "id": "cbfbb51e-9675-4e61-b006-d153a588f1e1",
      "title": "Art 60",
      "relevantChunksIds": [
        "d9af965b-cc32-4306-b4f8-f4caeb8bfa48"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 60: Testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes\n1. Testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes may be conducted by providers or prospective providers of high-risk AI systems listed in Annex III, in accordance with this Article and the real-world testing plan referred to in this Article, without prejudice to the prohibitions under Article 5.\nThe Commission shall, by means of implementing acts, specify the detailed elements of the real-world testing plan. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 98(2).\nThis paragraph shall be without prejudice to Union or national law on the testing in real world conditions of high-risk AI systems related to products covered by Union harmonisation legislation listed in Annex I.\n2. Providers or prospective providers may conduct testing of high-risk AI systems referred to in Annex III in real world conditions at any time before the placing on the market or the putting into service of the AI system on their own or in partnership with one or more deployers or prospective deployers.\n3. The testing of high-risk AI systems in real world conditions under this Article shall be without prejudice to any ethical review that is required by Union or national law.\n4. Providers or prospective providers may conduct the testing in real world conditions only where all of the following conditions are met:\n(a) the provider or prospective provider has drawn up a real-world testing plan and submitted it to the market surveillance authority in the Member State where the testing in real world conditions is to be conducted;\n(b) the market surveillance authority in the Member State where the testing in real world conditions is to be conducted has approved the testing in real world conditions and the real-world testing plan; where the market surveillance authority has not provided an answer within 30 days, the testing in real world conditions and the real-world testing plan shall be understood to have been approved; where national law does not provide for a tacit approval, the testing in real world conditions shall remain subject to an authorisation;\n(c) the provider or prospective provider, with the exception of providers or prospective providers of high-risk AI systems referred to in points 1, 6 and 7 of Annex III in the areas of law enforcement, migration, asylum and border control management, and high-risk AI systems referred to in point 2 of Annex III has registered the testing in real world conditions in accordance with Article 71(4) with a Union-wide unique single identification number and with the information specified in Annex IX; the provider or prospective provider of high-risk AI systems referred to in points 1, 6 and 7 of Annex III in the areas of law enforcement, migration, asylum and border control management, has registered the testing in real-world conditions in the secure non-public section of the EU database according to Article 49(4), point (d), with a Union-wide unique single identification number and with the information specified therein; the provider or prospective provider of high-risk AI systems referred to in point 2 of Annex III has registered the testing in real-world conditions in accordance with Article 49(5);\n(d) the provider or prospective provider conducting the testing in real world conditions is established in the Union or has appointed a legal representative who is established in the Union;\n(e) data collected and processed for the purpose of the testing in real world conditions shall be transferred to third countries only provided that appropriate and applicable safeguards under Union law are implemented;\n(f) the testing in real world conditions does not last longer than necessary to achieve its objectives and in any case not longer than six months, which may be extended for an additional period of six months, subject to prior notification by the provider or prospective provider to the market surveillance authority, accompanied by an explanation of the need for such an extension;\n(g) the subjects of the testing in real world conditions who are persons belonging to vulnerable groups due to their age or disability, are appropriately protected;\n(h) where a provider or prospective provider organises the testing in real world conditions in cooperation with one or more deployers or prospective deployers, the latter have been informed of all aspects of the testing that are relevant to their decision to participate, and given the relevant instructions for use of the AI system referred to in Article 13; the provider or prospective provider and the deployer or prospective deployer shall conclude an agreement specifying their roles and responsibilities with a view to ensuring compliance with the provisions for testing in real world conditions under this Regulation and under other applicable Union and national law;\n(i) the subjects of the testing in real world conditions have given informed consent in accordance with Article 61, or in the case of law enforcement, where the seeking of informed consent would prevent the AI system from being tested, the testing itself and the outcome of the testing in the real world conditions shall not have any negative effect on the subjects, and their personal data shall be deleted after the test is performed;\n(j) the testing in real world conditions is effectively overseen by the provider or prospective provider, as well as by deployers or prospective deployers through persons who are suitably qualified in the relevant field and have the necessary capacity, training and authority to perform their tasks;\n(k) the predictions, recommendations or decisions of the AI system can be effectively reversed and disregarded.\n5. Any subjects of the testing in real world conditions, or their legally designated representative, as appropriate, may, without any resulting detriment and without having to provide any justification, withdraw from the testing at any time by revoking their informed consent and may request the immediate and permanent deletion of their personal data. The withdrawal of the informed consent shall not affect the activities already carried out.\n6. In accordance with Article 75, Member States shall confer on their market surveillance authorities the powers of requiring providers and prospective providers to provide information, of carrying out unannounced remote or on-site inspections, and of performing checks on the conduct of the testing in real world conditions and the related high-risk AI systems. Market surveillance authorities shall use those powers to ensure the safe development of testing in real world conditions.\n7. Any serious incident identified in the course of the testing in real world conditions shall be reported to the national market surveillance authority in accordance with Article 73. The provider or prospective provider shall adopt immediate mitigation measures or, failing that, shall suspend the testing in real world conditions until such mitigation takes place, or otherwise terminate it. The provider or prospective provider shall establish a procedure for the prompt recall of the AI system upon such termination of the testing in real world conditions.\n8. Providers or prospective providers shall notify the national market surveillance authority in the Member State where the testing in real world conditions is to be conducted of the suspension or termination of the testing in real world conditions and of the final outcomes.\n9. The provider or prospective provider shall be liable under applicable Union and national liability law for any damage caused in the course of their testing in real world conditions.",
      "original_content": "### Artikel 60: Tests von Hochrisiko-KI-Systemen unter Realbedingungen außerhalb von KI-Reallaboren\n(1) Tests von Hochrisiko-KI-Systemen unter Realbedingungen können von Anbietern oder zukünftigen Anbietern von in Anhang III aufgeführten Hochrisiko-KI-Systemen außerhalb von KI-Reallaboren gemäß diesem Artikel und — unbeschadet der Bestimmungen unter Artikel 5 — dem in diesem Artikel genannten Plan für einen Test unter Realbedingungen durchgeführt werden.\nDie Kommission legt die einzelnen Elemente des Plans für einen Test unter Realbedingungen im Wege von Durchführungsrechtsakten fest. Diese Durchführungsrechtsakte werden gemäß dem in Artikel 98 Absatz 2 genannten Prüfverfahren erlassen.\nDas Unionsrecht oder nationale Recht für das Testen von Hochrisiko-KI-Systemen unter Realbedingungen im Zusammenhang mit Produkten, die unter die in Anhang I aufgeführten Harmonisierungsrechtsvorschriften der Union fallen, bleibt von dieser Bestimmung unberührt.\n(2) Anbieter oder zukünftige Anbieter können in Anhang III genannte Hochrisiko-KI-Systeme vor deren Inverkehrbringen oder Inbetriebnahme jederzeit selbst oder in Partnerschaft mit einem oder mehreren Betreibern oder zukünftigen Betreibern unter Realbedingungen testen.\n(3) Tests von KI-Systemen unter Realbedingungen gemäß diesem Artikel lassen nach dem Unionsrecht oder dem nationalen Recht gegebenenfalls vorgeschriebene Ethikprüfungen unberührt.\n(4) Tests unter Realbedingungen dürfen von Anbietern oder zukünftigen Anbietern nur durchgeführt werden, wenn alle der folgenden Bedingungen erfüllt sind:\na) Der Anbieter oder der zukünftige Anbieter hat einen Plan für einen Test unter Realbedingungen erstellt und diesen bei der Marktüberwachungsbehörde in dem Mitgliedstaat eingereicht, in dem der Test unter Realbedingungen stattfinden soll;\nb) die Marktüberwachungsbehörde in dem Mitgliedstaat, in dem der Test unter Realbedingungen stattfinden soll, hat den Test unter Realbedingungen und den Plan für einen Test unter Realbedingungen genehmigt; hat die Marktüberwachungsbehörde innerhalb von 30 Tagen keine Antwort gegeben, so gelten der Test unter Realbedingungen und der Plan für einen Test unter Realbedingungen als genehmigt; ist im nationalen Recht keine stillschweigende Genehmigung vorgesehen, so bleibt der Test unter Realbedingungen genehmigungspflichtig;\nc) der Anbieter oder zukünftige Anbieter, mit Ausnahme der in Anhang III Nummern 1, 6 und 7 genannten Anbieter oder zukünftigen Anbieter von Hochrisiko-KI-Systemen in den Bereichen Strafverfolgung, Migration, Asyl und Grenzkontrolle und der in Anhang III Nummer 2 genannten Hochrisiko-KI-Systeme, hat den Test unter Realbedingungen unter Angabe einer unionsweit einmaligen Identifizierungsnummer und der in Anhang IX festgelegten Informationen gemäß Artikel 71 Absatz 4 registriert; der Anbieter oder zukünftige Anbieter von Hochrisiko-KI-Systemen gemäß Anhang III Nummern 1, 6 und 7 in den Bereichen Strafverfolgung, Migration, Asyl und Grenzkontrollmanagement hat die Tests unter Realbedingungen im sicheren nicht öffentlichen Teil der EU-Datenbank gemäß Artikel 49 Absatz 4 Buchstabe d mit einer unionsweit einmaligen Identifizierungsnummer und den darin festgelegten Informationen registriert; der Anbieter oder zukünftige Anbieter von Hochrisiko-KI-Systemen gemäß Anhang III Nummer 2 hat die Tests unter Realbedingungen gemäß Artikel 49 Absatz 5 registriert;\nd) der Anbieter oder der zukünftige Anbieter, der den Test unter Realbedingungen durchführt, ist in der Union niedergelassen oder hat einen in der Union niedergelassenen gesetzlichen Vertreter bestellt;\ne) die für die Zwecke des Tests unter Realbedingungen erhobenen und verarbeiteten Daten werden nur dann an Drittländer übermittelt, wenn gemäß Unionsrecht geeignete und anwendbare Schutzvorkehrungen greifen;\nf) der Test unter Realbedingungen dauert nicht länger als zur Erfüllung seiner Zielsetzungen nötig und in keinem Fall länger als sechs Monate; dieser Zeitraum kann um weitere sechs Monate verlängert werden, sofern der Anbieter oder der zukünftige Anbieter die Marktüberwachungsbehörde davon vorab in Kenntnis setzt und erläutert, warum eine solche Verlängerung erforderlich ist;\ng) Testteilnehmer im Rahmen von Tests unter Realbedingungen, die aufgrund ihres Alters oder einer Behinderung schutzbedürftigen Gruppen angehören, sind angemessen geschützt;\nh) wenn ein Anbieter oder zukünftiger Anbieter den Test unter Realbedingungen in Zusammenarbeit mit einem oder mehreren Betreibern oder zukünftigen Betreibern organisiert, werden Letztere vorab über alle für ihre Teilnahmeentscheidung relevanten Aspekte des Tests informiert und erhalten die einschlägigen in Artikel 13 genannten Betriebsanleitungen für das KI-System; der Anbieter oder zukünftige Anbieter und der Betreiber oder zukünftige Betreiber schließen eine Vereinbarung, in der ihre Aufgaben und Zuständigkeiten festgelegt sind, um für die Einhaltung der nach dieser Verordnung und anderem Unionsrecht und nationalem Recht für Tests unter Realbedingungen geltenden Bestimmungen zu sorgen;\ni) die Testteilnehmer im Rahmen von Tests unter Realbedingungen erteilen ihre informierte Einwilligung gemäß Artikel 61, oder, wenn im Fall der Strafverfolgung die Einholung einer informierten Einwilligung den Test des KI-Systems verhindern würde, dürfen sich der Test und die Ergebnisse des Tests unter Realbedingungen nicht negativ auf die Testteilnehmer auswirken und ihre personenbezogenen Daten werden nach Durchführung des Tests gelöscht;\nj) der Anbieter oder zukünftige Anbieter und die Betreiber und zukünftigen Betreiber lassen den Test unter Realbedingungen von Personen wirksam überwachen, die auf dem betreffenden Gebiet angemessen qualifiziert sind und über die Fähigkeit, Ausbildung und Befugnis verfügen, die für die Wahrnehmung ihrer Aufgaben erforderlich sind;\nk) die Vorhersagen, Empfehlungen oder Entscheidungen des KI-Systems können effektiv rückgängig gemacht und außer Acht gelassen werden.\n(5) Jeder Testteilnehmer bezüglich des Tests unter Realbedingungen oder gegebenenfalls dessen gesetzlicher Vertreter kann seine Teilnahme an dem Test jederzeit durch Widerruf seiner informierten Einwilligung beenden und die unverzügliche und dauerhafte Löschung seiner personenbezogenen Daten verlangen, ohne dass ihm daraus Nachteile entstehen und er dies in irgendeiner Weise begründen müsste. Der Widerruf der informierten Einwilligung wirkt sich nicht auf bereits durchgeführte Tätigkeiten aus.\n(6) Im Einklang mit Artikel 75 übertragen die Mitgliedstaaten ihren Marktüberwachungsbehörden die Befugnis, Anbieter und zukünftige Anbieter zur Bereitstellung von Informationen zu verpflichten, unangekündigte Ferninspektionen oder Vor-Ort-Inspektionen durchzuführen und die Durchführung der Tests unter Realbedingungen und damit zusammenhängende Hochrisiko-KI-Systeme zu prüfen. Die Marktüberwachungsbehörden nutzen diese Befugnisse, um für die sichere Entwicklung von Tests unter Realbedingungen zu sorgen.\n(7) Jegliche schwerwiegenden Vorfälle im Verlauf des Tests unter Realbedingungen sind den nationalen Marktüberwachungsbehörden gemäß Artikel 73 zu melden. Der Anbieter oder zukünftige Anbieter trifft Sofortmaßnahmen zur Schadensbegrenzung; andernfalls setzt er den Test unter Realbedingungen so lange aus, bis eine Schadensbegrenzung stattgefunden hat, oder bricht ihn ab. Im Fall eines solchen Abbruchs des Tests unter Realbedingungen richtet der Anbieter oder zukünftige Anbieter ein Verfahren für den sofortigen Rückruf des KI-Systems ein.\n(8) Anbieter oder zukünftige Anbieter setzen die nationalen Marktüberwachungsbehörde in dem Mitgliedstaat, in dem der Test unter Realbedingungen stattfindet, über die Aussetzung oder den Abbruch des Tests unter Realbedingungen und die Endergebnisse in Kenntnis.\n(9) Anbieter oder zukünftige Anbieter sind nach geltendem Recht der Union und geltendem nationalen Recht für Schäden haftbar, die während ihrer Tests unter Realbedingungen entstehen."
    },
    {
      "chunk_idx": 265,
      "id": "9c639c94-6184-4e06-bc69-abb1f3fffbfe",
      "title": "Art 61",
      "relevantChunksIds": [
        "d9af965b-cc32-4306-b4f8-f4caeb8bfa48"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 61: Informed consent to participate in testing in real world conditions outside AI regulatory sandboxes\n1. For the purpose of testing in real world conditions under Article 60, freely-given informed consent shall be obtained from the subjects of testing prior to their participation in such testing and after their having been duly informed with concise, clear, relevant, and understandable information regarding:\n(a) the nature and objectives of the testing in real world conditions and the possible inconvenience that may be linked to their participation;\n(b) the conditions under which the testing in real world conditions is to be conducted, including the expected duration of the subject or subjects’ participation;\n(c) their rights, and the guarantees regarding their participation, in particular their right to refuse to participate in, and the right to withdraw from, testing in real world conditions at any time without any resulting detriment and without having to provide any justification;\n(d) the arrangements for requesting the reversal or the disregarding of the predictions, recommendations or decisions of the AI system;\n(e) the Union-wide unique single identification number of the testing in real world conditions in accordance with Article 60(4) point (c), and the contact details of the provider or its legal representative from whom further information can be obtained.\n2. The informed consent shall be dated and documented and a copy shall be given to the subjects of testing or their legal representative.",
      "original_content": "### Artikel 61: Informierte Einwilligung zur Teilnahme an einem Test unter Realbedingungen außerhalb von KI-Reallaboren\n(1) Für die Zwecke von Tests unter Realbedingungen gemäß Artikel 60 ist von den Testteilnehmern eine freiwillig erteilte informierte Einwilligung einzuholen, bevor sie an dem Test teilnehmen und nachdem sie mit präzisen, klaren, relevanten und verständlichen Informationen über Folgendes ordnungsgemäß informiert wurden:\na) die Art und die Zielsetzungen des Tests unter Realbedingungen und etwaige mit ihrer Teilnahme verbundene Unannehmlichkeiten;\nb) die Bedingungen, unter denen der Test unter Realbedingungen erfolgen soll, einschließlich der voraussichtlichen Dauer der Teilnahme des Testteilnehmers oder der Testteilnehmer;\nc) ihre Rechte und Garantien, die ihnen bezüglich ihrer Teilnahme zustehen, insbesondere ihr Recht, die Teilnahme an dem Test unter Realbedingungen zu verweigern oder diese Teilnahme jederzeit zu beenden, ohne dass ihnen daraus Nachteile entstehen und sie dies in irgendeiner Weise begründen müssten;\nd) die Regelungen, unter denen die Rückgängigmachung oder Außerachtlassung der Vorhersagen, Empfehlungen oder Entscheidungen des KI-Systems beantragt werden kann;\ne) die unionsweit einmalige Identifizierungsnummer des Tests unter Realbedingungen gemäß Artikel 60 Absatz 4 Buchstabe c und die Kontaktdaten des Anbieters oder seines gesetzlichen Vertreters, bei dem weitere Informationen eingeholt werden können.\n(2) Die informierte Einwilligung ist zu datieren und zu dokumentieren, und eine Kopie wird den Testteilnehmern oder ihren gesetzlichen Vertretern ausgehändigt."
    },
    {
      "chunk_idx": 266,
      "id": "576c7db3-5572-4780-85cc-23bdb6f5f13b",
      "title": "Art 62",
      "relevantChunksIds": [
        "984548c3-63d4-4d21-99dc-c4f542c22ab3",
        "c87d7613-0407-4eb6-8bb8-5dad9dd7e48d"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 62: Measures for providers and deployers, in particular SMEs, including start-ups\n1. Member States shall undertake the following actions:\n(a) provide SMEs, including start-ups, having a registered office or a branch in the Union, with priority access to the AI regulatory sandboxes, to the extent that they fulfil the eligibility conditions and selection criteria; the priority access shall not preclude other SMEs, including start-ups, other than those referred to in this paragraph from access to the AI regulatory sandbox, provided that they also fulfil the eligibility conditions and selection criteria;\n(b) organise specific awareness raising and training activities on the application of this Regulation tailored to the needs of SMEs including start-ups, deployers and, as appropriate, local public authorities;\n(c) utilise existing dedicated channels and where appropriate, establish new ones for communication with SMEs including start-ups, deployers, other innovators and, as appropriate, local public authorities to provide advice and respond to queries about the implementation of this Regulation, including as regards participation in AI regulatory sandboxes;\n(d) facilitate the participation of SMEs and other relevant stakeholders in the standardisation development process.\n2. The specific interests and needs of the SME providers, including start-ups, shall be taken into account when setting the fees for conformity assessment under Article 43, reducing those fees proportionately to their size, market size and other relevant indicators.\n3. The AI Office shall undertake the following actions:\n(a) provide standardised templates for areas covered by this Regulation, as specified by the Board in its request;\n(b) develop and maintain a single information platform providing easy to use information in relation to this Regulation for all operators across the Union;\n(c) organise appropriate communication campaigns to raise awareness about the obligations arising from this Regulation;\n(d) evaluate and promote the convergence of best practices in public procurement procedures in relation to AI systems.",
      "original_content": "### Artikel 62: Maßnahmen für Anbieter und Betreiber, insbesondere KMU, einschließlich Start-up-Unternehmen\n(1) Die Mitgliedstaaten ergreifen die folgenden Maßnahmen:\na) Sie gewähren KMU — einschließlich Start-up-Unternehmen —, die ihren Sitz oder eine Zweigniederlassung in der Union haben, soweit sie die Voraussetzungen und Auswahlkriterien erfüllen, vorrangigen Zugang zu den KI-Reallaboren; der vorrangige Zugang schließt nicht aus, dass andere als die in diesem Absatz genannten KMU, einschließlich Start-up-Unternehmen, Zugang zum KI-Reallabor erhalten, sofern sie ebenfalls die Zulassungsvoraussetzungen und Auswahlkriterien erfüllen;\nb) sie führen besondere Sensibilisierungs- und Schulungsmaßnahmen für die Anwendung dieser Verordnung durch, die auf die Bedürfnisse von KMU, einschließlich Start-up-Unternehmen, Betreibern sowie gegebenenfalls lokalen Behörden ausgerichtet sind;\nc) sie nutzen entsprechende bestehende Kanäle und richten gegebenenfalls neue Kanäle für die Kommunikation mit KMU, einschließlich Start-up-Unternehmen, Betreibern, anderen Innovatoren sowie gegebenenfalls lokalen Behörden ein, um Ratschläge zu geben und Fragen zur Durchführung dieser Verordnung, auch bezüglich der Beteiligung an KI-Reallaboren, zu beantworten;\nd) sie fördern die Beteiligung von KMU und anderen einschlägigen Interessenträgern an der Entwicklung von Normen.\n(2) Bei der Festsetzung der Gebühren für die Konformitätsbewertung gemäß Artikel 43 werden die besonderen Interessen und Bedürfnisse von KMU, einschließlich Start-up-Unternehmen, berücksichtigt, indem diese Gebühren proportional zur Größe der Unternehmen, der Größe ihres Marktes und anderen einschlägigen Kennzahlen gesenkt werden.\n(3) Das Büro für Künstliche Intelligenz ergreift die folgenden Maßnahmen:\na) es stellt standardisierte Muster für die unter diese Verordnung fallenden Bereiche bereit, wie vom KI-Gremium in seinem Antrag festgelegt;\nb) es entwickelt und führt eine zentrale Informationsplattform, über die allen Akteuren in der Union leicht nutzbare Informationen zu dieser Verordnung bereitgestellt werden;\nc) es führt geeignete Informationskampagnen durch, um für die aus dieser Verordnung erwachsenden Pflichten zu sensibilisieren;\nd) es bewertet und fördert die Zusammenführung bewährter Verfahren im Bereich der mit KI-Systemen verbundenen Vergabeverfahren."
    },
    {
      "chunk_idx": 267,
      "id": "0c488757-5b64-4a4f-ac00-98ce42a3968a",
      "title": "Art 63",
      "relevantChunksIds": [
        "c9e5148b-7510-4514-9f53-91eb5588c91e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 63: Derogations for specific operators\n1. Microenterprises within the meaning of Recommendation 2003/361/EC may comply with certain elements of the quality management system required by Article 17 of this Regulation in a simplified manner, provided that they do not have partner enterprises or linked enterprises within the meaning of that Recommendation. For that purpose, the Commission shall develop guidelines on the elements of the quality management system which may be complied with in a simplified manner considering the needs of microenterprises, without affecting the level of protection or the need for compliance with the requirements in respect of high-risk AI systems.\n2. Paragraph 1 of this Article shall not be interpreted as exempting those operators from fulfilling any other requirements or obligations laid down in this Regulation, including those established in Articles 9, 10, 11, 12, 13, 14, 15, 72 and 73.",
      "original_content": "### Artikel 63: Ausnahmen für bestimmte Akteure\n(1) Kleinstunternehmen im Sinne der Empfehlung 2003/361/EG können bestimmte Elemente des in Artikel 17 dieser Verordnung vorgeschriebenen Qualitätsmanagementsystems in vereinfachter Weise einhalten, sofern sie keine Partnerunternehmen oder verbundenen Unternehmen im Sinne dieser Empfehlung haben. Zu diesem Zweck arbeitet die Kommission Leitlinien zu den Elementen des Qualitätsmanagementsystems aus, die unter Berücksichtigung der Bedürfnisse von Kleinstunternehmen in vereinfachter Weise eingehalten werden können, ohne das Schutzniveau oder die Notwendigkeit zur Einhaltung der Anforderungen in Bezug auf Hochrisiko-KI-Systeme zu beeinträchtigen.\n(2) Absatz 1 dieses Artikels ist nicht dahin gehend auszulegen, dass diese Akteure auch von anderen in dieser Verordnung festgelegten Anforderungen oder Pflichten, einschließlich der nach den Artikeln 9, 10, 11, 12, 13, 14, 15, 72 und 73 geltenden, befreit sind."
    },
    {
      "chunk_idx": 268,
      "id": "63ae5b47-b44b-4b4b-82ac-5fa868177399",
      "title": "Art 64",
      "relevantChunksIds": [
        "e0492e6c-e987-41c3-b664-775286020859"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "# CHAPTER VII: GOVERNANCE\n## SECTION 1: Governance at Union level\n### Article 64: AI Office\n1. The Commission shall develop Union expertise and capabilities in the field of AI through the AI Office.\n2. Member States shall facilitate the tasks entrusted to the AI Office, as reflected in this Regulation.",
      "original_content": "# KAPITEL VII: GOVERNANCE\n## ABSCHNITT 1: Governance auf Unionsebene\n### Artikel 64: Büro für Künstliche Intelligenz\n(1) Die Kommission entwickelt über das Büro für Künstliche Intelligenz die Sachkenntnis und Fähigkeiten der Union auf dem Gebiet der KI.\n(2) Die Mitgliedstaaten erleichtern dem Büro für Künstliche Intelligenz die ihm gemäß dieser Verordnung übertragenen Aufgaben."
    },
    {
      "chunk_idx": 269,
      "id": "e684a05a-dce1-41e7-9743-fd5b1b778be8",
      "title": "Art 65",
      "relevantChunksIds": [
        "bf998658-e660-43ca-8849-a1cc9c46930c"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 65: Establishment and structure of the European Artificial Intelligence Board\n1. A European Artificial Intelligence Board (the ‘Board’) is hereby established.\n2. The Board shall be composed of one representative per Member State. The European Data Protection Supervisor shall participate as observer. The AI Office shall also attend the Board’s meetings, without taking part in the votes. Other national and Union authorities, bodies or experts may be invited to the meetings by the Board on a case by case basis, where the issues discussed are of relevance for them.\n3. Each representative shall be designated by their Member State for a period of three years, renewable once.\n4. Member States shall ensure that their representatives on the Board:\n(a) have the relevant competences and powers in their Member State so as to contribute actively to the achievement of the Board’s tasks referred to in Article 66;\n(b) are designated as a single contact point vis-à-vis the Board and, where appropriate, taking into account Member States’ needs, as a single contact point for stakeholders;\n(c) are empowered to facilitate consistency and coordination between national competent authorities in their Member State as regards the implementation of this Regulation, including through the collection of relevant data and information for the purpose of fulfilling their tasks on the Board.\n5. The designated representatives of the Member States shall adopt the Board’s rules of procedure by a two-thirds majority. The rules of procedure shall, in particular, lay down procedures for the selection process, the duration of the mandate of, and specifications of the tasks of, the Chair, detailed arrangements for voting, and the organisation of the Board’s activities and those of its sub-groups.\n6. The Board shall establish two standing sub-groups to provide a platform for cooperation and exchange among market surveillance authorities and notifying authorities about issues related to market surveillance and notified bodies respectively.\nThe standing sub-group for market surveillance should act as the administrative cooperation group (ADCO) for this Regulation within the meaning of Article 30 of Regulation (EU) 2019/1020.\nThe Board may establish other standing or temporary sub-groups as appropriate for the purpose of examining specific issues. Where appropriate, representatives of the advisory forum referred to in Article 67 may be invited to such sub-groups or to specific meetings of those subgroups as observers.\n7. The Board shall be organised and operated so as to safeguard the objectivity and impartiality of its activities.\n8. The Board shall be chaired by one of the representatives of the Member States. The AI Office shall provide the secretariat for the Board, convene the meetings upon request of the Chair, and prepare the agenda in accordance with the tasks of the Board pursuant to this Regulation and its rules of procedure.",
      "original_content": "### Artikel 65: Einrichtung und Struktur des Europäischen Gremiums für Künstliche Intelligenz\n(1) Ein Europäisches Gremium für Künstliche Intelligenz (im Folgenden „KI-Gremium“) wird hiermit eingerichtet.\n(2) Das KI-Gremium setzt sich aus einem Vertreter je Mitgliedstaat zusammen. Der Europäische Datenschutzbeauftragte nimmt als Beobachter teil. Das Büro für Künstliche Intelligenz nimmt ebenfalls an den Sitzungen des KI-Gremiums teil, ohne sich jedoch an den Abstimmungen zu beteiligen. Andere Behörden oder Stellen der Mitgliedstaaten und der Union oder Sachverständige können im Einzelfall zu den Sitzungen des KI-Gremiums eingeladen werden, wenn die erörterten Fragen für sie von Belang sind.\n(3) Die Vertreter werden von ihren Mitgliedstaaten für einen Zeitraum von drei Jahren benannt, der einmal verlängert werden kann.\n(4) Die Mitgliedstaaten sorgen dafür, dass ihre Vertreter im KI-Gremium\na) in ihrem Mitgliedstaat über die einschlägigen Kompetenzen und Befugnisse verfügen, sodass sie aktiv zur Bewältigung der in Artikel 66 genannten Aufgaben des KI-Gremiums beitragen können;\nb) gegenüber dem KI-Gremium sowie gegebenenfalls, unter Berücksichtigung der Erfordernisse der Mitgliedstaaten, gegenüber Interessenträgern als zentrale Ansprechpartner fungieren;\nc) ermächtigt sind, auf die Kohärenz und die Abstimmung zwischen den zuständigen nationalen Behörden in ihrem Mitgliedstaat bei der Durchführung dieser Verordnung hinzuwirken, auch durch Erhebung einschlägiger Daten und Informationen für die Zwecke der Erfüllung ihrer Aufgaben im KI-Gremium.\n(5) Die benannten Vertreter der Mitgliedstaaten nehmen die Geschäftsordnung des KI-Gremiums mit einer Zweidrittelmehrheit an. In der Geschäftsordnung sind insbesondere die Vorgehensweise für das Auswahlverfahren, die Dauer des Mandats und die genauen Aufgaben des Vorsitzes, die Abstimmungsregelungen und die Organisation der Tätigkeiten des KI-Gremiums und seiner Untergruppen festgelegt.\n(6) Das KI-Gremium richtet zwei ständige Untergruppen ein, um Marktüberwachungsbehörden eine Plattform für die Zusammenarbeit und den Austausch zu bieten und Behörden über Angelegenheiten, die jeweils die Marktüberwachung und notifizierte Stellen betreffen, zu unterrichten.\nDie ständige Untergruppe für Marktüberwachung sollte für diese Verordnung als Gruppe für die Verwaltungszusammenarbeit (ADCO-Gruppe) im Sinne des Artikels 30 der Verordnung (EU) 2019/1020 fungieren.\nDas KI-Gremium kann weitere ständige oder nichtständige Untergruppen einrichten, falls das für die Prüfung bestimmter Fragen zweckmäßig sein sollte. Gegebenenfalls können Vertreter des in Artikel 67 genannten Beratungsforums als Beobachter zu diesen Untergruppen oder zu bestimmten Sitzungen dieser Untergruppen eingeladen werden.\n(7) Das KI-Gremium wird so organisiert und geführt, dass bei seinen Tätigkeiten Objektivität und Unparteilichkeit gewahrt sind.\n(8) Den Vorsitz im KI-Gremium führt einer der Vertreter der Mitgliedstaaten. Die Sekretariatsgeschäfte des KI-Gremiums werden vom Büro für Künstliche Intelligenz geführt; dieses beruft auf Anfrage des Vorsitzes die Sitzungen ein und erstellt die Tagesordnung im Einklang mit den Aufgaben des KI-Gremiums gemäß dieser Verordnung und seiner Geschäftsordnung."
    },
    {
      "chunk_idx": 270,
      "id": "4035dbfb-54eb-4161-9b2d-4974bd7440ba",
      "title": "Art 66",
      "relevantChunksIds": [
        "bf998658-e660-43ca-8849-a1cc9c46930c"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 66: Tasks of the Board\nThe Board shall advise and assist the Commission and the Member States in order to facilitate the consistent and effective application of this Regulation. To that end, the Board may in particular:\n(a) contribute to the coordination among national competent authorities responsible for the application of this Regulation and, in cooperation with and subject to the agreement of the market surveillance authorities concerned, support joint activities of market surveillance authorities referred to in Article 74(11);\n(b) collect and share technical and regulatory expertise and best practices among Member States;\n(c) provide advice on the implementation of this Regulation, in particular as regards the enforcement of rules on general-purpose AI models;\n(d) contribute to the harmonisation of administrative practices in the Member States, including in relation to the derogation from the conformity assessment procedures referred to in Article 46, the functioning of AI regulatory sandboxes, and testing in real world conditions referred to in Articles 57, 59 and 60;\n(e) at the request of the Commission or on its own initiative, issue recommendations and written opinions on any relevant matters related to the implementation of this Regulation and to its consistent and effective application, including:\n(i) on the development and application of codes of conduct and codes of practice pursuant to this Regulation, as well as of the Commission’s guidelines;\n(ii) the evaluation and review of this Regulation pursuant to Article 112, including as regards the serious incident reports referred to in Article 73, and the functioning of the EU database referred to in Article 71, the preparation of the delegated or implementing acts, and as regards possible alignments of this Regulation with the Union harmonisation legislation listed in Annex I;\n(iii) on technical specifications or existing standards regarding the requirements set out in Chapter III, Section 2;\n(iv) on the use of harmonised standards or common specifications referred to in Articles 40 and 41;\n(v) trends, such as European global competitiveness in AI, the uptake of AI in the Union, and the development of digital skills;\n(vi) trends on the evolving typology of AI value chains, in particular on the resulting implications in terms of accountability;\n(vii) on the potential need for amendment to Annex III in accordance with Article 7, and on the potential need for possible revision of Article 5 pursuant to Article 112, taking into account relevant available evidence and the latest developments in technology;\n(f) support the Commission in promoting AI literacy, public awareness and understanding of the benefits, risks, safeguards and rights and obligations in relation to the use of AI systems;\n(g) facilitate the development of common criteria and a shared understanding among market operators and competent authorities of the relevant concepts provided for in this Regulation, including by contributing to the development of benchmarks;\n(h) cooperate, as appropriate, with other Union institutions, bodies, offices and agencies, as well as relevant Union expert groups and networks, in particular in the fields of product safety, cybersecurity, competition, digital and media services, financial services, consumer protection, data and fundamental rights protection;\n(i) contribute to effective cooperation with the competent authorities of third countries and with international organisations;\n(j) assist national competent authorities and the Commission in developing the organisational and technical expertise required for the implementation of this Regulation, including by contributing to the assessment of training needs for staff of Member States involved in implementing this Regulation;\n(k) assist the AI Office in supporting national competent authorities in the establishment and development of AI regulatory sandboxes, and facilitate cooperation and information-sharing among AI regulatory sandboxes;\n(l) contribute to, and provide relevant advice on, the development of guidance documents;\n(m) advise the Commission in relation to international matters on AI;\n(n) provide opinions to the Commission on the qualified alerts regarding general-purpose AI models;\n(o) receive opinions by the Member States on qualified alerts regarding general-purpose AI models, and on national experiences and practices on the monitoring and enforcement of AI systems, in particular systems integrating the general-purpose AI models.",
      "original_content": "### Artikel 66: Aufgaben des KI-Gremiums\nDas KI-Gremium berät und unterstützt die Kommission und die Mitgliedstaaten, um die einheitliche und wirksame Anwendung dieser Verordnung zu erleichtern. Für diese Zwecke kann das KI-Gremium insbesondere\na) zur Koordinierung zwischen den für die Anwendung dieser Verordnung zuständigen nationalen Behörden beitragen und in Zusammenarbeit mit den betreffenden Marktüberwachungsbehörden und vorbehaltlich ihrer Zustimmung gemeinsame Tätigkeiten der Marktüberwachungsbehörden gemäß Artikel 74 Absatz 11 unterstützen;\nb) technisches und regulatorisches Fachwissen und bewährte Verfahren zusammentragen und unter den Mitgliedstaaten verbreiten;\nc) zur Durchführung dieser Verordnung Beratung anbieten, insbesondere im Hinblick auf die Durchsetzung der Vorschriften zu KI-Modellen mit allgemeinem Verwendungszweck;\nd) zur Harmonisierung der Verwaltungspraxis in den Mitgliedstaaten beitragen, auch bezüglich der Ausnahme vom Konformitätsbewertungsverfahren gemäß Artikel 46 und der Funktionsweise von KI-Reallaboren und Tests unter Realbedingungen gemäß den Artikeln 57, 59 und 60;\ne) auf Anfrage der Kommission oder in Eigeninitiative Empfehlungen und schriftliche Stellungnahmen zu einschlägigen Fragen der Durchführung dieser Verordnung und ihrer einheitlichen und wirksamen Anwendung abgeben, einschließlich\ni) zur Entwicklung und Anwendung von Verhaltenskodizes und Praxisleitfäden gemäß dieser Verordnung sowie der Leitlinien der Kommission;\nii) zur Bewertung und Überprüfung dieser Verordnung gemäß Artikel 112, auch in Bezug auf die Meldung schwerwiegender Vorfälle gemäß Artikel 73 und das Funktionieren der EU-Datenbank gemäß Artikel 71, die Ausarbeitung der delegierten Rechtsakte oder Durchführungsrechtsakte sowie im Hinblick auf mögliche Anpassungen dieser Verordnung an die in Anhang I aufgeführten Harmonisierungsrechtsvorschriften der Union;\niii) zu technischen Spezifikationen oder geltenden Normen in Bezug auf die in Kapitel III Abschnitt 2 festgelegten Anforderungen;\niv) zur Anwendung der in den Artikeln 40 und 41 genannten harmonisierten Normen oder gemeinsamen Spezifikationen;\nv) zu Tendenzen, etwa im Bereich der globalen Wettbewerbsfähigkeit Europas auf dem Gebiet der KI, bei der Verbreitung von KI in der Union und bei der Entwicklung digitaler Fähigkeiten;\nvi) zu Tendenzen im Bereich der sich ständig weiterentwickelnden Typologie der KI-Wertschöpfungsketten insbesondere hinsichtlich der sich daraus ergebenden Auswirkungen auf die Rechenschaftspflicht;\nvii) zur möglicherweise notwendigen Änderung des Anhangs III im Einklang mit Artikel 7 und zur möglicherweise notwendigen Überarbeitung des Artikels 5 gemäß Artikel 112 unter Berücksichtigung der einschlägigen verfügbaren Erkenntnisse und der neuesten technologischen Entwicklungen;\nf) die Kommission bei der Förderung der KI-Kompetenz, der Sensibilisierung und Aufklärung der Öffentlichkeit in Bezug auf die Vorteile, Risiken, Schutzmaßnahmen, Rechte und Pflichten im Zusammenhang mit der Nutzung von KI-Systemen unterstützen;\ng) die Entwicklung gemeinsamer Kriterien und eines gemeinsamen Verständnisses der Marktteilnehmer und der zuständigen Behörden in Bezug auf die in dieser Verordnung vorgesehenen einschlägigen Konzepte erleichtern, auch durch einen Beitrag zur Entwicklung von Benchmarks;\nh) gegebenenfalls mit anderen Organen, Einrichtungen und sonstigen Stellen der EU, einschlägigen Sachverständigengruppen und Netzwerken der EU insbesondere in den Bereichen Produktsicherheit, Cybersicherheit, Wettbewerb, digitale und Mediendienste, Finanzdienstleistungen, Verbraucherschutz, Datenschutz und Schutz der Grundrechte zusammenarbeiten;\ni) zur wirksamen Zusammenarbeit mit den zuständigen Behörden von Drittstaaten und mit internationalen Organisationen beitragen;\nj) die zuständigen nationalen Behörden und die Kommission beim Aufbau des für die Durchführung dieser Verordnung erforderlichen organisatorischen und technischen Fachwissens beraten, unter anderem durch einen Beitrag zur Einschätzung des Schulungsbedarfs des Personals der Mitgliedstaaten, das an der Durchführung dieser Verordnung beteiligt ist;\nk) dem Büro für Künstliche Intelligenz helfen, die zuständigen nationalen Behörden bei der Einrichtung und Entwicklung von KI-Reallaboren zu unterstützen, und die Zusammenarbeit und den Informationsaustausch zwischen KI-Reallaboren erleichtern;\nl) zur Entwicklung von Leitfäden beitragen und diesbezüglich entsprechend beraten;\nm) die Kommission zu internationalen Angelegenheiten im Bereich der KI beraten;\nn) der Kommission Stellungnahmen zu qualifizierten Warnungen in Bezug auf KI-Modelle mit allgemeinem Verwendungszweck vorlegen;\no) Stellungnahmen der Mitgliedstaaten zu qualifizierten Warnungen in Bezug auf KI-Modelle mit allgemeinem Verwendungszweck entgegennehmen sowie zu nationalen Erfahrungen und Praktiken bei der Überwachung und Durchsetzung von KI-Systemen, insbesondere von Systemen, die KI-Modelle mit allgemeinem Verwendungszweck integrieren."
    },
    {
      "chunk_idx": 271,
      "id": "957b417e-f553-4553-aa90-daea77a9a036",
      "title": "Art 67",
      "relevantChunksIds": [
        "199c6e64-698e-4e3f-a806-9ff9d364cc85"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 67: Advisory forum\n1. An advisory forum shall be established to provide technical expertise and advise the Board and the Commission, and to contribute to their tasks under this Regulation.\n2. The membership of the advisory forum shall represent a balanced selection of stakeholders, including industry, start-ups, SMEs, civil society and academia. The membership of the advisory forum shall be balanced with regard to commercial and non-commercial interests and, within the category of commercial interests, with regard to SMEs and other undertakings.\n3. The Commission shall appoint the members of the advisory forum, in accordance with the criteria set out in paragraph 2, from amongst stakeholders with recognised expertise in the field of AI.\n4. The term of office of the members of the advisory forum shall be two years, which may be extended by up to no more than four years.\n5. The Fundamental Rights Agency, ENISA, the European Committee for Standardization (CEN), the European Committee for Electrotechnical Standardization (CENELEC), and the European Telecommunications Standards Institute (ETSI) shall be permanent members of the advisory forum.\n6. The advisory forum shall draw up its rules of procedure. It shall elect two co-chairs from among its members, in accordance with criteria set out in paragraph 2. The term of office of the co-chairs shall be two years, renewable once.\n7. The advisory forum shall hold meetings at least twice a year. The advisory forum may invite experts and other stakeholders to its meetings.\n8. The advisory forum may prepare opinions, recommendations and written contributions at the request of the Board or the Commission.\n9. The advisory forum may establish standing or temporary sub-groups as appropriate for the purpose of examining specific questions related to the objectives of this Regulation.\n10. The advisory forum shall prepare an annual report on its activities. That report shall be made publicly available.",
      "original_content": "### Artikel 67: Beratungsforum\n(1) Es wird ein Beratungsforum eingerichtet, das technisches Fachwissen bereitstellt, das KI-Gremium und die Kommission berät und zu deren Aufgaben im Rahmen dieser Verordnung beiträgt.\n(2) Die Mitglieder des Beratungsforums vertreten eine ausgewogene Auswahl von Interessenträgern, darunter die Industrie, Start-up-Unternehmen, KMU, die Zivilgesellschaft und die Wissenschaft. Bei der Zusammensetzung des Beratungsforums wird auf ein ausgewogenes Verhältnis zwischen wirtschaftlichen und nicht-wirtschaftlichen Interessen und innerhalb der Kategorie der wirtschaftlichen Interessen zwischen KMU und anderen Unternehmen geachtet.\n(3) Die Kommission ernennt die Mitglieder des Beratungsforums gemäß den in Absatz 2 genannten Kriterien aus dem Kreis der Interessenträger mit anerkanntem Fachwissen auf dem Gebiet der KI.\n(4) Die Amtszeit der Mitglieder des Beratungsforums beträgt zwei Jahre; sie kann bis zu höchstens vier Jahre verlängert werden.\n(5) Die Agentur der Europäischen Union für Grundrechte, ENISA, das Europäische Komitee für Normung (CEN), das Europäische Komitee für elektrotechnische Normung (CENELEC) und das Europäische Institut für Telekommunikationsnormen (ETSI) sind ständige Mitglieder des Beratungsforums.\n(6) Das Beratungsforum gibt sich eine Geschäftsordnung. Es wählt gemäß den in Absatz 2 festgelegten Kriterien zwei Ko-Vorsitzende unter seinen Mitgliedern. Die Amtszeit der Ko-Vorsitzenden beträgt zwei Jahre und kann einmal verlängert werden.\n(7) Das Beratungsforum hält mindestens zweimal pro Jahr Sitzungen ab. Das Beratungsforum kann Sachverständige und andere Interessenträger zu seinen Sitzungen einladen.\n(8) Das Beratungsforum kann auf Ersuchen des KI-Gremiums oder der Kommission Stellungnahmen, Empfehlungen und schriftliche Beiträge ausarbeiten.\n(9) Das Beratungsforum kann gegebenenfalls ständige oder zeitweilige Untergruppen einsetzen, um spezifische Fragen im Zusammenhang mit den Zielen dieser Verordnung zu prüfen.\n(10) Das Beratungsforum erstellt jährlich einen Bericht über seine Tätigkeit. Dieser Bericht wird veröffentlicht."
    },
    {
      "chunk_idx": 272,
      "id": "6efc5cc4-f5b5-4581-bc5d-10d2aeca4486",
      "title": "Art 68",
      "relevantChunksIds": [
        "47710b63-0230-41a5-80af-2581299ac9d6",
        "af44a0ad-8425-4b78-8b03-21093d6fc548"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 68: Scientific panel of independent experts\n1. The Commission shall, by means of an implementing act, make provisions on the establishment of a scientific panel of independent experts (the ‘scientific panel’) intended to support the enforcement activities under this Regulation. That implementing act shall be adopted in accordance with the examination procedure referred to in Article 98(2).\n2. The scientific panel shall consist of experts selected by the Commission on the basis of up-to-date scientific or technical expertise in the field of AI necessary for the tasks set out in paragraph 3, and shall be able to demonstrate meeting all of the following conditions:\n(a) having particular expertise and competence and scientific or technical expertise in the field of AI;\n(b) independence from any provider of AI systems or general-purpose AI models;\n(c) an ability to carry out activities diligently, accurately and objectively.\nThe Commission, in consultation with the Board, shall determine the number of experts on the panel in accordance with the required needs and shall ensure fair gender and geographical representation.\n3. The scientific panel shall advise and support the AI Office, in particular with regard to the following tasks:\n(a) supporting the implementation and enforcement of this Regulation as regards general-purpose AI models and systems, in particular by:\n(i) alerting the AI Office of possible systemic risks at Union level of general-purpose AI models, in accordance with Article 90;\n(ii) contributing to the development of tools and methodologies for evaluating capabilities of general-purpose AI models and systems, including through benchmarks;\n(iii) providing advice on the classification of general-purpose AI models with systemic risk;\n(iv) providing advice on the classification of various general-purpose AI models and systems;\n(v) contributing to the development of tools and templates;\n(b) supporting the work of market surveillance authorities, at their request;\n(c) supporting cross-border market surveillance activities as referred to in Article 74(11), without prejudice to the powers of market surveillance authorities;\n\n5. The implementing act referred to in paragraph 1 shall include provisions on the conditions, procedures and detailed arrangements for the scientific panel and its members to issue alerts, and to request the assistance of the AI Office for the performance of the tasks of the scientific panel.",
      "original_content": "### Artikel 68: Wissenschaftliches Gremium unabhängiger Sachverständiger\n(1) Die Kommission erlässt im Wege eines Durchführungsrechtsakts Bestimmungen über die Einrichtung eines wissenschaftlichen Gremiums unabhängiger Sachverständiger („wissenschaftliches Gremium“), das die Durchsetzungstätigkeiten im Rahmen dieser Verordnung unterstützen soll. Dieser Durchführungsrechtsakt wird gemäß dem in Artikel 98 Absatz 2 genannten Prüfverfahren erlassen.\n(2) Das wissenschaftliche Gremium setzt sich aus Sachverständigen zusammen, die von der Kommission auf der Grundlage aktueller wissenschaftlicher oder technischer Fachkenntnisse auf dem Gebiet der KI, die zur Erfüllung der in Absatz 3 genannten Aufgaben erforderlich sind, ausgewählt werden, und muss nachweisen können, dass es alle folgenden Bedingungen erfüllt:\na) es verfügt über besondere Fachkenntnisse und Kompetenzen sowie über wissenschaftliches oder technisches Fachwissen auf dem Gebiet der KI;\nb) es ist von Anbietern von KI-Systemen oder KI-Modellen mit allgemeinem Verwendungszweck unabhängig;\nc) es ist in der Lage, Tätigkeiten sorgfältig, präzise und objektiv auszuführen.\nDie Kommission legt in Absprache mit dem KI-Gremium die Anzahl der Sachverständigen des Gremiums nach Maßgabe der jeweiligen Erfordernisse fest und sorgt für eine ausgewogene Vertretung der Geschlechter und eine gerechte geografische Verteilung.\n(3) Das wissenschaftliche Gremium berät und unterstützt das Büro für Künstliche Intelligenz, insbesondere in Bezug auf folgende Aufgaben:\na) Unterstützung bei der Durchführung und Durchsetzung dieser Verordnung in Bezug auf KI-Modelle und -Systeme mit allgemeinem Verwendungszweck, insbesondere indem es\ni) das Büro für Künstliche Intelligenz im Einklang mit Artikel 90 vor möglichen systemischen Risiken von KI-Modellen mit allgemeinem Verwendungszweck auf Unionsebene warnt;\nii) einen Beitrag zur Entwicklung von Instrumenten und Methoden für die Bewertung der Fähigkeiten von KI-Modellen und -Systemen mit allgemeinem Verwendungszweck, auch durch Benchmarks, leistet;\niii) Beratung über die Einstufung von KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko anbietet;\niv) Beratung über die Einstufung verschiedener KI-Modelle und -Systeme mit allgemeinem Verwendungszweck anbietet;\nv) einen Beitrag zur Entwicklung von Instrumenten und Mustern leistet;\nb) Unterstützung der Arbeit der Marktüberwachungsbehörden auf deren Ersuchen;\nc) Unterstützung grenzüberschreitender Marktüberwachungstätigkeiten gemäß Artikel 74 Absatz 11, ohne dass die Befugnisse der Marktüberwachungsbehörden berührt werden;\nd) Unterstützung des Büros für Künstliche Intelligenz bei der Wahrnehmung seiner Aufgaben im Rahmen des Schutzklauselverfahrens der Union gemäß Artikel 81. (4) Die Sachverständigen des wissenschaftlichen Gremiums führen ihre Aufgaben nach den Grundsätzen der Unparteilichkeit und der Objektivität aus und gewährleisten die Vertraulichkeit der Informationen und Daten, in deren Besitz sie bei der Ausführung ihrer Aufgaben und Tätigkeiten gelangen. Sie dürfen bei der Wahrnehmung ihrer Aufgaben nach Absatz 3 weder Weisungen anfordern noch entgegennehmen. Jeder Sachverständige gibt eine Interessenerklärung ab, die öffentlich zugänglich gemacht wird. Das Büro für Künstliche Intelligenz richtet Systeme und Verfahren ein, mit denen mögliche Interessenkonflikte aktiv bewältigt und verhindert werden können.\n(5) Der in Absatz 1 genannte Durchführungsrechtsakt enthält Bestimmungen über die Bedingungen, Verfahren und detaillierten Regelungen nach denen das wissenschaftliche Gremium und seine Mitglieder Warnungen ausgeben und das Büro für Künstliche Intelligenz um Unterstützung bei der Wahrnehmung der Aufgaben des wissenschaftlichen Gremiums ersuchen können."
    },
    {
      "chunk_idx": 273,
      "id": "3810e28c-9fab-460b-ac8f-f4dd846665f9",
      "title": "Art 69",
      "relevantChunksIds": [
        "47710b63-0230-41a5-80af-2581299ac9d6"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 69: Access to the pool of experts by the Member States\n1. Member States may call upon experts of the scientific panel to support their enforcement activities under this Regulation.\n2. The Member States may be required to pay fees for the advice and support provided by the experts. The structure and the level of fees as well as the scale and structure of recoverable costs shall be set out in the implementing act referred to in Article 68(1), taking into account the objectives of the adequate implementation of this Regulation, cost-effectiveness and the necessity of ensuring effective access to experts for all Member States.\n3. The Commission shall facilitate timely access to the experts by the Member States, as needed, and ensure that the combination of support activities carried out by Union AI testing support pursuant to Article 84 and experts pursuant to this Article is efficiently organised and provides the best possible added value.",
      "original_content": "### Artikel 69: Zugang zum Pool von Sachverständigen durch die Mitgliedstaaten\n(1) Die Mitgliedstaaten können Sachverständige des wissenschaftlichen Gremiums hinzuziehen, um ihre Durchsetzungstätigkeiten im Rahmen dieser Verordnung zu unterstützen.\n(2) Die Mitgliedstaaten können verpflichtet werden, für die Beratung und Unterstützung durch die Sachverständigen Gebühren zu entrichten. Struktur und Höhe der Gebühren sowie Umfang und Struktur erstattungsfähiger Kosten werden in dem in Artikel 68 Absatz 1 genannten Durchführungsrechtsakt festgelegt, wobei die Zielsetzung berücksichtigt wird, für die angemessene Durchführung dieser Verordnung, für Kosteneffizienz sowie dafür zu sorgen, dass alle Mitgliedstaaten effektiven Zugang zu Sachverständigen haben müssen.\n(3) Die Kommission ermöglicht den Mitgliedstaaten bei Bedarf einen rechtzeitigen Zugang zu den Sachverständigen und sorgt dafür, dass die Kombination aus unterstützenden Tätigkeiten durch die Union zur Prüfung von KI gemäß Artikel 84 und durch die Sachverständigen gemäß dem vorliegenden Artikel effizient organisiert ist und den bestmöglichen zusätzlichen Nutzen bringt."
    },
    {
      "chunk_idx": 274,
      "id": "2d1207bb-4da9-4b6c-abee-bdd5de6d80e7",
      "title": "Art 70",
      "relevantChunksIds": [
        "577d5a3c-36b6-48b6-a3fd-27b111efe5fd",
        "0d1ee002-929c-4cd0-99ba-0e8f530307b5",
        "491d84d6-b754-4408-8bbd-21c789451f0f",
        "ecc6bef2-71ac-41e5-a335-c79864647312"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "## SECTION 2: National competent authorities\n### Article 70: Designation of national competent authorities and single points of contact\n1. Each Member State shall establish or designate as national competent authorities at least one notifying authority and at least one market surveillance authority for the purposes of this Regulation. Those national competent authorities shall exercise their powers independently, impartially and without bias so as to safeguard the objectivity of their activities and tasks, and to ensure the application and implementation of this Regulation. The members of those authorities shall refrain from any action incompatible with their duties. Provided that those principles are observed, such activities and tasks may be performed by one or more designated authorities, in accordance with the organisational needs of the Member State.\n2. Member States shall communicate to the Commission the identity of the notifying authorities and the market surveillance authorities and the tasks of those authorities, as well as any subsequent changes thereto. Member States shall make publicly available information on how competent authorities and single points of contact can be contacted, through electronic communication means by 2 August 2025. Member States shall designate a market surveillance authority to act as the single point of contact for this Regulation, and shall notify the Commission of the identity of the single point of contact. The Commission shall make a list of the single points of contact publicly available.\n3. Member States shall ensure that their national competent authorities are provided with adequate technical, financial and human resources, and with infrastructure to fulfil their tasks effectively under this Regulation. In particular, the national competent authorities shall have a sufficient number of personnel permanently available whose competences and expertise shall include an in-depth understanding of AI technologies, data and data computing, personal data protection, cybersecurity, fundamental rights, health and safety risks and knowledge of existing standards and legal requirements. Member States shall assess and, if necessary, update competence and resource requirements referred to in this paragraph on an annual basis.\n4. National competent authorities shall take appropriate measures to ensure an adequate level of cybersecurity.\n5. When performing their tasks, the national competent authorities shall act in accordance with the confidentiality obligations set out in Article 78.\n6. By 2 August 2025, and once every two years thereafter, Member States shall report to the Commission on the status of the financial and human resources of the national competent authorities, with an assessment of their adequacy. The Commission shall transmit that information to the Board for discussion and possible recommendations.\n7. The Commission shall facilitate the exchange of experience between national competent authorities.\n8. National competent authorities may provide guidance and advice on the implementation of this Regulation, in particular to SMEs including start-ups, taking into account the guidance and advice of the Board and the Commission, as appropriate. Whenever national competent authorities intend to provide guidance and advice with regard to an AI system in areas covered by other Union law, the national competent authorities under that Union law shall be consulted, as appropriate.\n9. Where Union institutions, bodies, offices or agencies fall within the scope of this Regulation, the European Data Protection Supervisor shall act as the competent authority for their supervision.",
      "original_content": "## ABSCHNITT 2: Zuständige nationale Behörden\n### Artikel 70: Benennung von zuständigen nationalen Behörden und zentrale Anlaufstelle\n(1) Jeder Mitgliedstaat muss für die Zwecke dieser Verordnung mindestens eine notifizierende Behörde und mindestens eine Marktüberwachungsbehörde als zuständige nationale Behörden einrichten oder benennen. Diese zuständigen nationalen Behörden üben ihre Befugnisse unabhängig, unparteiisch und unvoreingenommen aus, um die Objektivität ihrer Tätigkeiten und Aufgaben zu gewährleisten und die Anwendung und Durchführung dieser Verordnung sicherzustellen. Die Mitglieder dieser Behörden haben jede Handlung zu unterlassen, die mit ihren Aufgaben unvereinbar ist. Sofern diese Grundsätze gewahrt werden, können die betreffenden Tätigkeiten und Aufgaben gemäß den organisatorischen Erfordernissen des Mitgliedstaats von einer oder mehreren benannten Behörden wahrgenommen werden.\n(2) Die Mitgliedstaaten teilen der Kommission die Namen der notifizierenden Behörden und der Marktüberwachungsbehörden und die Aufgaben dieser Behörden sowie alle späteren Änderungen mit. Die Mitgliedstaaten machen Informationen darüber, wie die zuständigen Behörden und zentralen Anlaufstellen bis zum 2. August 2025 auf elektronischem Wege kontaktiert werden können, öffentlich zugänglich. Die Mitgliedstaaten benennen eine Marktüberwachungsbehörde, die als zentrale Anlaufstelle für diese Verordnung fungiert, und teilen der Kommission den Namen der zentralen Anlaufstelle mit. Die Kommission erstellt eine öffentlich verfügbare Liste der zentralen Anlaufstellen.\n(3) Die Mitgliedstaaten sorgen dafür, dass ihre zuständigen nationalen Behörden mit angemessenen technischen und finanziellen Mitteln sowie geeignetem Personal und Infrastrukturen ausgestattet werden, damit sie ihre Aufgaben im Rahmen dieser Verordnung wirksam erfüllen können. Insbesondere müssen die zuständigen nationalen Behörden zu jeder Zeit über eine ausreichende Zahl von Mitarbeitern verfügen, zu deren Kompetenzen und Fachwissen ein tiefes Verständnis der KI-Technologien, der Daten und Datenverarbeitung, des Schutzes personenbezogener Daten, der Cybersicherheit, der Grundrechte, der Gesundheits- und Sicherheitsrisiken sowie Kenntnis der bestehenden Normen und rechtlichen Anforderungen gehört. Die Mitgliedstaaten bewerten und aktualisieren, falls erforderlich, jährlich die in diesem Absatz genannten Erfordernisse bezüglich Kompetenzen und Ressourcen.\n(4) Die zuständigen nationalen Behörden ergreifen geeignete Maßnahmen zur Sicherstellung eines angemessenen Maßes an Cybersicherheit.\n(5) Bei der Erfüllung ihrer Aufgaben halten sich die zuständigen nationalen Behörden an die in Artikel 78 festgelegten Vertraulichkeitspflichten.\n(6) Bis zum 2. August 2025 und anschließend alle zwei Jahre erstatten die Mitgliedstaaten der Kommission Bericht über den Sachstand bezüglich der finanziellen Mittel und des Personals der zuständigen nationalen Behörden und geben eine Einschätzung über deren Angemessenheit ab. Die Kommission leitet diese Informationen zur Erörterung und etwaigen Abgabe von Empfehlungen an das KI-Gremium weiter.\n(7) Die Kommission fördert den Erfahrungsaustausch zwischen den zuständigen nationalen Behörden.\n(8) Die zuständigen nationalen Behörden können gegebenenfalls insbesondere KMU, einschließlich Start-up-Unternehmen, unter Berücksichtigung der Anleitung und Beratung durch das KI-Gremium oder der Kommission mit Anleitung und Beratung bei der Durchführung dieser Verordnung zur Seite stehen. Wenn zuständige nationale Behörden beabsichtigen, Anleitung und Beratung in Bezug auf ein KI-System in Bereichen anzubieten, die unter das Unionrecht fallen, so sind gegebenenfalls die nach jenen Unionsrecht zuständigen nationalen Behörden zu konsultieren.\n(9) Soweit Organe, Einrichtungen und sonstige Stellen der Union in den Anwendungsbereich dieser Verordnung fallen, übernimmt der Europäische Datenschutzbeauftragte die Funktion der für ihre Beaufsichtigung zuständigen Behörde."
    },
    {
      "chunk_idx": 275,
      "id": "e85b8add-b4ef-4422-bafa-b843f3f02662",
      "title": "Art 71",
      "relevantChunksIds": [
        "90893d61-25fd-4ae1-8232-5e4cdb5675e9"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "# CHAPTER VIII: EU DATABASE FOR HIGH-RISK AI SYSTEMS\n### Article 71: EU database for high-risk AI systems listed in Annex III\n1. The Commission shall, in collaboration with the Member States, set up and maintain an EU database containing information referred to in paragraphs 2 and 3 of this Article concerning high-risk AI systems referred to in Article 6(2) which are registered in accordance with Articles 49 and 60 and AI systems that are not considered as high-risk pursuant to Article 6(3) and which are registered in accordance with Article 6(4) and Article 49. When setting the functional specifications of such database, the Commission shall consult the relevant experts, and when updating the functional specifications of such database, the Commission shall consult the Board.\n2. The data listed in Sections A and B of Annex VIII shall be entered into the EU database by the provider or, where applicable, by the authorised representative.\n3. The data listed in Section C of Annex VIII shall be entered into the EU database by the deployer who is, or who acts on behalf of, a public authority, agency or body, in accordance with Article 49(3) and (4).\n4. With the exception of the section referred to in Article 49(4) and Article 60(4), point (c), the information contained in the EU database registered in accordance with Article 49 shall be accessible and publicly available in a user-friendly manner. The information should be easily navigable and machine-readable. The information registered in accordance with Article 60 shall be accessible only to market surveillance authorities and the Commission, unless the prospective provider or provider has given consent for also making the information accessible the public.\n5. The EU database shall contain personal data only in so far as necessary for collecting and processing information in accordance with this Regulation. That information shall include the names and contact details of natural persons who are responsible for registering the system and have the legal authority to represent the provider or the deployer, as applicable.\n6. The Commission shall be the controller of the EU database. It shall make available to providers, prospective providers and deployers adequate technical and administrative support. The EU database shall comply with the applicable accessibility requirements.",
      "original_content": "# KAPITEL VIII: EU-DATENBANK FÜR HOCHRISIKO-KI-SYSTEME\n### Artikel 71: EU-Datenbank für die in Anhang III aufgeführten Hochrisiko-KI-Systeme\n(1) Die Kommission errichtet und führt in Zusammenarbeit mit den Mitgliedstaaten eine EU-Datenbank mit den in den Absätzen 2 und 3 dieses Artikels genannten Informationen über Hochrisiko-KI-Systeme nach Artikel 6 Absatz 2, die gemäß den Artikeln 49 und 60 registriert werden und über KI-Systeme, die nicht als Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 3 gelten und gemäß Artikel 6 Absatz 4 und Artikel 49 registriert werden. Bei der Festlegung der Funktionsspezifikationen dieser Datenbank konsultiert die Kommission die einschlägigen Sachverständigen und bei der Aktualisierung der Funktionsspezifikationen dieser Datenbank konsultiert sie das KI-Gremium.\n(2) Die in Anhang VIII Abschnitte A und B aufgeführten Daten werden vom Anbieter oder gegebenenfalls vom Bevollmächtigten in die EU-Datenbank eingegeben.\n(3) Die in Anhang VIII Abschnitt C aufgeführten Daten werden vom Betreiber, der eine Behörde, Einrichtung oder sonstige Stelle ist oder in deren Namen handelt, gemäß Artikel 49 Absätze 3 und 4 in die EU-Datenbank eingegeben.\n(4) Mit Ausnahme des in Artikel 49 Absatz 4 und Artikel 60 Absatz 4 Buchstabe c genannten Abschnitts müssen die gemäß Artikel 49 in der Datenbank registrierten und dort enthaltenen Informationen auf benutzerfreundliche Weise zugänglich und öffentlich verfügbar sein. Die Informationen sollten leicht handhabbar und maschinenlesbar sein. Auf die gemäß Artikel 60 registrierten Informationen können nur Marktüberwachungsbehörden und die Kommission zugreifen, es sei denn, der zukünftige Anbieter oder der Anbieter hat seine Zustimmung dafür erteilt, dass die Informationen auch öffentlich zugänglich sind.\n(5) Die EU-Datenbank enthält personenbezogene Daten nur, soweit dies für die Erfassung und Verarbeitung von Informationen gemäß dieser Verordnung erforderlich ist. Zu diesen Informationen gehören die Namen und Kontaktdaten der natürlichen Personen, die für die Registrierung des Systems verantwortlich sind und die rechtlich befugt sind, den Anbieter oder gegebenenfalls den Betreiber zu vertreten.\n(6) Die Kommission gilt als für die EU-Datenbank Verantwortlicher. Sie stellt Anbietern, zukünftigen Anbietern und Betreibern angemessene technische und administrative Unterstützung bereit. Die EU-Datenbank muss den geltenden Barrierefreiheitsanforderungen entsprechen."
    },
    {
      "chunk_idx": 276,
      "id": "5b7ccaab-a361-4b2e-bf6c-67a90fd684eb",
      "title": "Art 72",
      "relevantChunksIds": [
        "3ccde775-96a9-422b-87da-43e4cd707e7f",
        "d685b8fd-f03f-4563-8a9a-41aaf6ee30ad"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "# CHAPTER IX: POST-MARKET MONITORING, INFORMATION SHARING AND MARKET SURVEILLANCE\n## SECTION 1: Post-market monitoring\n### Article 72: Post-market monitoring by providers and post-market monitoring plan for high-risk AI systems\n1. Providers shall establish and document a post-market monitoring system in a manner that is proportionate to the nature of the AI technologies and the risks of the high-risk AI system.\n2. The post-market monitoring system shall actively and systematically collect, document and analyse relevant data which may be provided by deployers or which may be collected through other sources on the performance of high-risk AI systems throughout their lifetime, and which allow the provider to evaluate the continuous compliance of AI systems with the requirements set out in Chapter III, Section 2. Where relevant, post-market monitoring shall include an analysis of the interaction with other AI systems. This obligation shall not cover sensitive operational data of deployers which are law-enforcement authorities.\n3. The post-market monitoring system shall be based on a post-market monitoring plan. The post-market monitoring plan shall be part of the technical documentation referred to in Annex IV. The Commission shall adopt an implementing act laying down detailed provisions establishing a template for the post-market monitoring plan and the list of elements to be included in the plan by 2 February 2026. That implementing act shall be adopted in accordance with the examination procedure referred to in Article 98(2).\n4. For high-risk AI systems covered by the Union harmonisation legislation listed in Section A of Annex I, where a post-market monitoring system and plan are already established under that legislation, in order to ensure consistency, avoid duplications and minimise additional burdens, providers shall have a choice of integrating, as appropriate, the necessary elements described in paragraphs 1, 2 and 3 using the template referred in paragraph 3 into systems and plans already existing under that legislation, provided that it achieves an equivalent level of protection.\nThe first subparagraph of this paragraph shall also apply to high-risk AI systems referred to in point 5 of Annex III placed on the market or put into service by financial institutions that are subject to requirements under Union financial services law regarding their internal governance, arrangements or processes.",
      "original_content": "# KAPITEL IX: BEOBACHTUNG NACH DEM INVERKEHRBRINGEN, INFORMATIONSAUSTAUSCH UND MARKTÜBERWACHUNG\n## ABSCHNITT 1: Beobachtung nach dem Inverkehrbringen\n### Artikel 72: Beobachtung nach dem Inverkehrbringen durch die Anbieter und Plan für die Beobachtung nach dem Inverkehrbringen für Hochrisiko-KI-Systeme\n(1) Anbieter müssen ein System zur Beobachtung nach dem Inverkehrbringen, das im Verhältnis zur Art der KI-Technik und zu den Risiken des Hochrisiko-KI-Systems steht, einrichten und dokumentieren.\n(2) Mit dem System zur Beobachtung nach dem Inverkehrbringen müssen sich die einschlägigen Daten zur Leistung der Hochrisiko-KI-Systeme, die von den Anbietern oder den Betreibern bereitgestellt oder aus anderen Quellen erhoben werden können, über ihre gesamte Lebensdauer hinweg aktiv und systematisch erheben, dokumentieren und analysieren lassen, und der Anbieter muss damit die fortdauernde Einhaltung der in Kapitel III Abschnitt 2 genannten Anforderungen an die KI-Systeme bewerten können. Gegebenenfalls umfasst die Beobachtung nach dem Inverkehrbringen eine Analyse der Interaktion mit anderen KI-Systemen. Diese Pflicht gilt nicht für sensible operative Daten von Betreibern, die Strafverfolgungsbehörden sind.\n(3) Das System zur Beobachtung nach dem Inverkehrbringen muss auf einem Plan für die Beobachtung nach dem Inverkehrbringen beruhen. Der Plan für die Beobachtung nach dem Inverkehrbringen ist Teil der in Anhang IV genannten technischen Dokumentation. Die Kommission erlässt einen Durchführungsrechtsakt, in dem sie detaillierte Bestimmungen für die Erstellung eines Musters des Plans für die Beobachtung nach dem Inverkehrbringen sowie die Liste der in den Plan aufzunehmenden Elemente bis zum 2. Februar 2026 detailliert festlegt. Dieser Durchführungsrechtsakt wird gemäß dem in Artikel 98 Absatz 2 genannten Prüfverfahren erlassen.\n(4) Bei Hochrisiko-KI-Systemen, die unter die in Anhang I Abschnitt A aufgeführten Harmonisierungsrechtsvorschriften der Union fallen, und für die auf der Grundlage dieser Rechtvorschriften bereits ein System zur Beobachtung nach dem Inverkehrbringen sowie ein entsprechender Plan festgelegt wurden, haben die Anbieter zur Gewährleistung der Kohärenz, zur Vermeidung von Doppelarbeit und zur Minimierung zusätzlicher Belastungen die Möglichkeit, unter Verwendung des Musters nach Absatz 3, gegebenenfalls die in den Absätzen 1, 2 und 3 genannten erforderlichen Elemente in die im Rahmen dieser Vorschriften bereits vorhandenen Systeme und Pläne zu integrieren, sofern ein gleichwertiges Schutzniveau erreicht wird.\nUnterabsatz 1 dieses Absatzes gilt auch für in Anhang III Nummer 5 genannte Hochrisiko-KI-Systeme, die von Finanzinstituten in Verkehr gebracht oder in Betrieb genommen wurden, die bezüglich ihrer internen Unternehmensführung, Regelungen oder Verfahren Anforderungen gemäß den Rechtsvorschriften der Union über Finanzdienstleistungen unterliegen."
    },
    {
      "chunk_idx": 277,
      "id": "fbe4d324-9497-4865-b73a-72e761e651b7",
      "title": "Art 73",
      "relevantChunksIds": [
        "3ccde775-96a9-422b-87da-43e4cd707e7f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "## SECTION 2: Sharing of information on serious incidents\n### Article 73: Reporting of serious incidents\n1. Providers of high-risk AI systems placed on the Union market shall report any serious incident to the market surveillance authorities of the Member States where that incident occurred.\n2. The report referred to in paragraph 1 shall be made immediately after the provider has established a causal link between the AI system and the serious incident or the reasonable likelihood of such a link, and, in any event, not later than 15 days after the provider or, where applicable, the deployer, becomes aware of the serious incident.\nThe period for the reporting referred to in the first subparagraph shall take account of the severity of the serious incident.\n3. Notwithstanding paragraph 2 of this Article, in the event of a widespread infringement or a serious incident as defined in Article 3, point (49)(b), the report referred to in paragraph 1 of this Article shall be provided immediately, and not later than two days after the provider or, where applicable, the deployer becomes aware of that incident.\n4. Notwithstanding paragraph 2, in the event of the death of a person, the report shall be provided immediately after the provider or the deployer has established, or as soon as it suspects, a causal relationship between the high-risk AI system and the serious incident, but not later than 10 days after the date on which the provider or, where applicable, the deployer becomes aware of the serious incident.\n5. Where necessary to ensure timely reporting, the provider or, where applicable, the deployer, may submit an initial report that is incomplete, followed by a complete report.\n6. Following the reporting of a serious incident pursuant to paragraph 1, the provider shall, without delay, perform the necessary investigations in relation to the serious incident and the AI system concerned. This shall include a risk assessment of the incident, and corrective action.\nThe provider shall cooperate with the competent authorities, and where relevant with the notified body concerned, during the investigations referred to in the first subparagraph, and shall not perform any investigation which involves altering the AI system concerned in a way which may affect any subsequent evaluation of the causes of the incident, prior to informing the competent authorities of such action.\n7. Upon receiving a notification related to a serious incident referred to in Article 3, point (49)(c), the relevant market surveillance authority shall inform the national public authorities or bodies referred to in Article 77(1). The Commission shall develop dedicated guidance to facilitate compliance with the obligations set out in paragraph 1 of this Article. That guidance shall be issued by 2 August 2025, and shall be assessed regularly.\n8. The market surveillance authority shall take appropriate measures, as provided for in Article 19 of Regulation (EU) 2019/1020, within seven days from the date it received the notification referred to in paragraph 1 of this Article, and shall follow the notification procedures as provided in that Regulation.\n9. For high-risk AI systems referred to in Annex III that are placed on the market or put into service by providers that are subject to Union legislative instruments laying down reporting obligations equivalent to those set out in this Regulation, the notification of serious incidents shall be limited to those referred to in Article 3, point (49)(c).\n10. For high-risk AI systems which are safety components of devices, or are themselves devices, covered by Regulations (EU) 2017/745 and (EU) 2017/746, the notification of serious incidents shall be limited to those referred to in Article 3, point (49)(c) of this Regulation, and shall be made to the national competent authority chosen for that purpose by the Member States where the incident occurred.\n11. National competent authorities shall immediately notify the Commission of any serious incident, whether or not they have taken action on it, in accordance with Article 20 of Regulation (EU) 2019/1020.",
      "original_content": "## ABSCHNITT 2: Austausch von Informationen über schwerwiegende Vorfälle\n### Artikel 73: Meldung schwerwiegender Vorfälle\n(1) Anbieter von in der Union in Verkehr gebrachten Hochrisiko-KI-Systemen melden schwerwiegende Vorfälle den Marktüberwachungsbehörden der Mitgliedstaaten, in denen der Vorfall stattgefunden hat.\n(2) Die Meldung nach Absatz 1 erfolgt unmittelbar, nachdem der Anbieter den kausalen Zusammenhang zwischen dem KI-System und dem schwerwiegenden Vorfall oder die naheliegende Wahrscheinlichkeit eines solchen Zusammenhangs festgestellt hat und in jedem Fall spätestens 15 Tage, nachdem der Anbieter oder gegebenenfalls der Betreiber Kenntnis von diesem schwerwiegenden Vorfall erlangt hat.\nBezüglich des in Unterabsatz 1 genannten Meldezeitraums wird der Schwere des schwerwiegenden Vorfalls Rechnung getragen.\n(3) Ungeachtet des Absatzes 2 dieses Artikels erfolgt die in Absatz 1 dieses Artikels genannte Meldung im Falle eines weitverbreiteten Verstoßes oder eines schwerwiegenden Vorfalls im Sinne des Artikels 3 Nummer 49 Buchstabe b unverzüglich, spätestens jedoch zwei Tage nachdem der Anbieter oder gegebenenfalls der Betreiber von diesem Vorfall Kenntnis erlangt hat.\n(4) Ungeachtet des Absatzes 2 erfolgt die Meldung im Falle des Todes einer Person unverzüglich nachdem der Anbieter oder der Betreiber einen kausalen Zusammenhang zwischen dem Hochrisiko-KI-System und dem schwerwiegenden Vorfall festgestellt hat, oder einen solchen vermutet, spätestens jedoch zehn Tage nach dem Datum, an dem der Anbieter oder gegebenenfalls der Betreiber von dem schwerwiegenden Vorfall Kenntnis erlangt hat.\n(5) Wenn es zur Gewährleistung der rechtzeitigen Meldung erforderlich ist, kann der Anbieter oder gegebenenfalls der Betreiber einen unvollständigen Erstbericht vorlegen, dem ein vollständiger Bericht folgt.\n(6) Im Anschluss an die Meldung eines schwerwiegenden Vorfalls gemäß Absatz 1 führt der Anbieter unverzüglich die erforderlichen Untersuchungen im Zusammenhang mit dem schwerwiegenden Vorfall und dem betroffenen KI-System durch. Dies umfasst eine Risikobewertung des Vorfalls sowie Korrekturmaßnahmen.\nDer Anbieter arbeitet bei den Untersuchungen gemäß Unterabsatz 1 mit den zuständigen Behörden und gegebenenfalls mit der betroffenen notifizierten Stelle zusammen und nimmt keine Untersuchung vor, die zu einer Veränderung des betroffenen KI-Systems in einer Weise führt, die möglicherweise Auswirkungen auf eine spätere Bewertung der Ursachen des Vorfalls hat, bevor er die zuständigen Behörden über eine solche Maßnahme nicht unterrichtet hat.\n(7) Sobald die zuständige Marktüberwachungsbehörde eine Meldung über einen in Artikel 3 Nummer 49 Buchstabe c genannten schwerwiegenden Vorfall erhält, informiert sie die in Artikel 77 Absatz 1 genannten nationalen Behörden oder öffentlichen Stellen. Zur leichteren Einhaltung der Pflichten nach Absatz 1 dieses Artikels arbeitet die Kommission entsprechende Leitlinien aus. Diese Leitlinien werden bis zum 2. August 2025 veröffentlicht und regelmäßig bewertet.\n(8) Die Marktüberwachungsbehörde ergreift innerhalb von sieben Tagen nach Eingang der in Absatz 1 dieses Artikels genannten Meldung geeignete Maßnahmen gemäß Artikel 19 der Verordnung (EU) 2019/1020 und befolgt die in der genannten Verordnung vorgesehenen Meldeverfahren.\n(9) Bei Hochrisiko-KI-Systemen nach Anhang III, die von Anbietern in Verkehr gebracht oder in Betrieb genommen wurden, die Rechtsinstrumenten der Union mit gleichwertigen Meldepflichten wie jenen in dieser Verordnung festgesetzten unterliegen, müssen nur jene schwerwiegenden Vorfälle gemeldet werden, die in Artikel 3 Nummer 49 Buchstabe c genannt werden.\n(10) Bei Hochrisiko-KI-Systemen, bei denen es sich um Sicherheitsbauteile von Produkten handelt, die unter die Verordnungen (EU) 2017/745 und (EU) 2017/746 fallen, oder die selbst solche Produkte sind, müssen nur die in Artikel 3 Nummer 49 Buchstabe c dieser Verordnung genannten schwerwiegenden Vorfälle gemeldet werden, und zwar der zuständigen nationalen Behörde, die für diesen Zweck von den Mitgliedstaaten, in denen der Vorfall stattgefunden hat, ausgewählt wurde.\n(11) Die zuständigen nationalen Behörden melden der Kommission unverzüglich jeden schwerwiegenden Vorfall gemäß Artikel 20 der Verordnung (EU) 2019/1020, unabhängig davon, ob sie diesbezüglich Maßnahmen ergriffen haben."
    },
    {
      "chunk_idx": 278,
      "id": "a7a05ae9-8c7c-44c1-8221-328bc0dcc4a5",
      "title": "Art 74",
      "relevantChunksIds": [
        "491d84d6-b754-4408-8bbd-21c789451f0f",
        "ecc6bef2-71ac-41e5-a335-c79864647312",
        "d685b8fd-f03f-4563-8a9a-41aaf6ee30ad",
        "af6b62de-8c37-48f7-b7f7-5bcdb2b0bf39",
        "1c919b24-516b-4255-979e-f4cbc0da447e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "## SECTION 3: Enforcement\n### Article 74: Market surveillance and control of AI systems in the Union market\n1. Regulation (EU) 2019/1020 shall apply to AI systems covered by this Regulation. For the purposes of the effective enforcement of this Regulation:\n(a) any reference to an economic operator under Regulation (EU) 2019/1020 shall be understood as including all operators identified in Article 2(1) of this Regulation;\n(b) any reference to a product under Regulation (EU) 2019/1020 shall be understood as including all AI systems falling within the scope of this Regulation.\n2. As part of their reporting obligations under Article 34(4) of Regulation (EU) 2019/1020, the market surveillance authorities shall report annually to the Commission and relevant national competition authorities any information identified in the course of market surveillance activities that may be of potential interest for the application of Union law on competition rules. They shall also annually report to the Commission about the use of prohibited practices that occurred during that year and about the measures taken.\n3. For high-risk AI systems related to products covered by the Union harmonisation legislation listed in Section A of Annex I, the market surveillance authority for the purposes of this Regulation shall be the authority responsible for market surveillance activities designated under those legal acts.\nBy derogation from the first subparagraph, and in appropriate circumstances, Member States may designate another relevant authority to act as a market surveillance authority, provided they ensure coordination with the relevant sectoral market surveillance authorities responsible for the enforcement of the Union harmonisation legislation listed in Annex I.\n4. The procedures referred to in Articles 79 to 83 of this Regulation shall not apply to AI systems related to products covered by the Union harmonisation legislation listed in section A of Annex I, where such legal acts already provide for procedures ensuring an equivalent level of protection and having the same objective. In such cases, the relevant sectoral procedures shall apply instead.\n5. Without prejudice to the powers of market surveillance authorities under Article 14 of Regulation (EU) 2019/1020, for the purpose of ensuring the effective enforcement of this Regulation, market surveillance authorities may exercise the powers referred to in Article 14(4), points (d) and (j), of that Regulation remotely, as appropriate.\n6. For high-risk AI systems placed on the market, put into service, or used by financial institutions regulated by Union financial services law, the market surveillance authority for the purposes of this Regulation shall be the relevant national authority responsible for the financial supervision of those institutions under that legislation in so far as the placing on the market, putting into service, or the use of the AI system is in direct connection with the provision of those financial services.\n7. By way of derogation from paragraph 6, in appropriate circumstances, and provided that coordination is ensured, another relevant authority may be identified by the Member State as market surveillance authority for the purposes of this Regulation.\nNational market surveillance authorities supervising regulated credit institutions regulated under Directive 2013/36/EU, which are participating in the Single Supervisory Mechanism established by Regulation (EU) No 1024/2013, should report, without delay, to the European Central Bank any information identified in the course of their market surveillance activities that may be of potential interest for the prudential supervisory tasks of the European Central Bank specified in that Regulation.\n8. For high-risk AI systems listed in point 1 of Annex III to this Regulation, in so far as the systems are used for law enforcement purposes, border management and justice and democracy, and for high-risk AI systems listed in points 6, 7 and 8 of Annex III to this Regulation, Member States shall designate as market surveillance authorities for the purposes of this Regulation either the competent data protection supervisory authorities under Regulation (EU) 2016/679 or Directive (EU) 2016/680, or any other authority designated pursuant to the same conditions laid down in Articles 41 to 44 of Directive (EU) 2016/680. Market surveillance activities shall in no way affect the independence of judicial authorities, or otherwise interfere with their activities when acting in their judicial capacity.\n9. Where Union institutions, bodies, offices or agencies fall within the scope of this Regulation, the European Data Protection Supervisor shall act as their market surveillance authority, except in relation to the Court of Justice of the European Union acting in its judicial capacity.\n10. Member States shall facilitate coordination between market surveillance authorities designated under this Regulation and other relevant national authorities or bodies which supervise the application of Union harmonisation legislation listed in Annex I, or in other Union law, that might be relevant for the high-risk AI systems referred to in Annex III.\n11. Market surveillance authorities and the Commission shall be able to propose joint activities, including joint investigations, to be conducted by either market surveillance authorities or market surveillance authorities jointly with the Commission, that have the aim of promoting compliance, identifying non-compliance, raising awareness or providing guidance in relation to this Regulation with respect to specific categories of high-risk AI systems that are found to present a serious risk across two or more Member States in accordance with Article 9 of Regulation (EU) 2019/1020. The AI Office shall provide coordination support for joint investigations.\n12. Without prejudice to the powers provided for under Regulation (EU) 2019/1020, and where relevant and limited to what is necessary to fulfil their tasks, the market surveillance authorities shall be granted full access by providers to the documentation as well as the training, validation and testing data sets used for the development of high-risk AI systems, including, where appropriate and subject to security safeguards, through application programming interfaces (API) or other relevant technical means and tools enabling remote access.\n13. Market surveillance authorities shall be granted access to the source code of the high-risk AI system upon a reasoned request and only when both of the following conditions are fulfilled:\n(a) access to source code is necessary to assess the conformity of a high-risk AI system with the requirements set out in Chapter III, Section 2; and\n(b) testing or auditing procedures and verifications based on the data and documentation provided by the provider have been exhausted or proved insufficient.\n14. Any information or documentation obtained by market surveillance authorities shall be treated in accordance with the confidentiality obligations set out in Article 78.",
      "original_content": "## ABSCHNITT 3: Durchsetzung\n### Artikel 74: Marktüberwachung und Kontrolle von KI-Systemen auf dem Unionsmarkt\n(1) Die Verordnung (EU) 2019/1020 gilt für KI-Systeme, die unter die vorliegende Verordnung fallen. Für die Zwecke einer wirksamen Durchsetzung der vorliegenden Verordnung gilt Folgendes:\na) Jede Bezugnahme auf einen Wirtschaftsakteur nach der Verordnung (EU) 2019/1020 gilt auch als Bezugnahme auf alle Akteure, die in Artikel 2 Absatz 1 der vorliegenden Verordnung genannt werden;\nb) jede Bezugnahme auf ein Produkt nach der Verordnung (EU) 2019/1020 gilt auch als Bezugnahme auf alle KI-Systeme, die in den Anwendungsbereich der vorliegenden Verordnung fallen.\n(2) Im Rahmen ihrer Berichtspflichten gemäß Artikel 34 Absatz 4 der Verordnung (EU) 2019/1020 melden die Marktüberwachungsbehörden der Kommission und den einschlägigen nationalen Wettbewerbsbehörden jährlich alle Informationen, die sie im Verlauf ihrer Marktüberwachungstätigkeiten erlangt haben und die für die Anwendung von Unionsrecht im Bereich der Wettbewerbsregeln von Interesse sein könnten. Ferner erstatten sie der Kommission jährlich Bericht über die Anwendung verbotener Praktiken in dem betreffenden Jahr und über die ergriffenen Maßnahmen.\n(3) Bei Hochrisiko-KI-Systemen und damit in Zusammenhang stehenden Produkten, auf die die in Anhang I Abschnitt A aufgeführten Harmonisierungsrechtsvorschriften der Union Anwendung finden, gilt als Marktüberwachungsbehörde für die Zwecke dieser Verordnung die in jenen Rechtsakten für die Marktüberwachung benannte Behörde.\nAbweichend von Unterabsatz 1 und unter geeigneten Umständen können die Mitgliedstaaten eine andere einschlägige Behörde benennen, die die Funktion der Marktüberwachungsbehörde übernimmt, sofern sie die Koordinierung mit den einschlägigen sektorspezifischen Marktüberwachungsbehörden, die für die Durchsetzung der in Anhang I aufgeführten Harmonisierungsrechtsvorschriften der Union zuständig sind, sicherstellen.\n(4) Die Verfahren gemäß den Artikeln 79 bis 83 der vorliegenden Verordnung gelten nicht für KI-Systeme, die im Zusammenhang mit Produkten stehen, auf die die in Anhang I Abschnitt A aufgeführten Harmonisierungsrechtsvorschriften der Union Anwendung finden, wenn in diesen Rechtsakten bereits Verfahren, die ein gleichwertiges Schutzniveau sicherstellen und dasselbe Ziel haben, vorgesehen sind. In diesen Fällen kommen stattdessen die einschlägigen sektorspezifischen Verfahren zur Anwendung.\n(5) Unbeschadet der Befugnisse der Marktüberwachungsbehörden gemäß Artikel 14 der Verordnung (EU) 2019/1020 können die Marktüberwachungsbehörden für die Zwecke der Sicherstellung der wirksamen Durchsetzung der vorliegenden Verordnung die in Artikel 14 Absatz 4 Buchstaben d und j der genannten Verordnung genannten Befugnisse gegebenenfalls aus der Ferne ausüben.\n(6) Bei Hochrisiko-KI-Systemen, die von auf der Grundlage des Unionsrechts im Bereich der Finanzdienstleistungen regulierten Finanzinstituten in Verkehr gebracht, in Betrieb genommen oder verwendet werden, gilt die in jenen Rechtsvorschriften für die Finanzaufsicht über diese Institute benannte nationale Behörde als Marktüberwachungsbehörde für die Zwecke dieser Verordnung, sofern das Inverkehrbringen, die Inbetriebnahme oder die Verwendung des KI-Systems mit der Erbringung dieser Finanzdienstleistungen in direktem Zusammenhang steht.\n(7) Abweichend von Absatz 6 kann der Mitgliedstaat — unter geeigneten Umständen und wenn für Abstimmung gesorgt ist — eine andere einschlägige Behörde als Marktüberwachungsbehörde für die Zwecke dieser Verordnung benennen.\nNationale Marktüberwachungsbehörden, die unter die Richtlinie 2013/36/EU fallende Kreditinstitute, welche an dem mit der Verordnung (EU) Nr. 1024/2013 eingerichteten einheitlichen Aufsichtsmechanismus teilnehmen, beaufsichtigen, sollten der Europäischen Zentralbank unverzüglich alle im Zuge ihrer Marktüberwachungstätigkeiten ermittelten Informationen übermitteln, die für die in der genannten Verordnung festgelegten Aufsichtsaufgaben der Europäischen Zentralbank von Belang sein könnten.\n(8) Für die in Anhang III Nummer 1 der vorliegenden Verordnung genannten Hochrisiko-KI-Systeme, sofern diese Systeme für Strafverfolgungszwecke, Grenzmanagement und Justiz und Demokratie eingesetzt werden, und für die in Anhang III Nummern 6, 7 und 8 genannten Hochrisiko-KI-Systeme benennen die Mitgliedstaaten für die Zwecke dieser Verordnung als Marktüberwachungsbehörden entweder die nach der Verordnung (EU) 2016/679 oder der Richtlinie (EU) 2016/680 für den Datenschutz zuständigen Aufsichtsbehörden oder jede andere gemäß denselben Bedingungen wie den in den Artikeln 41 bis 44 der Richtlinie (EU) 2016/680 festgelegten benannte Behörde. Marktüberwachungstätigkeiten dürfen in keiner Weise die Unabhängigkeit von Justizbehörden beeinträchtigen oder deren Handlungen im Rahmen ihrer justiziellen Tätigkeit anderweitig beeinflussen.\n(9) Soweit Organe, Einrichtungen und sonstige Stellen der Union in den Anwendungsbereich dieser Verordnung fallen, übernimmt der Europäische Datenschutzbeauftragte die Funktion der für sie zuständigen Marktüberwachungsbehörde — ausgenommen für den Gerichtshof der Europäischen Union im Rahmen seiner Rechtsprechungstätigkeit.\n(10) Die Mitgliedstaaten erleichtern die Koordinierung zwischen den auf der Grundlage dieser Verordnung benannten Marktüberwachungsbehörden und anderen einschlägigen nationalen Behörden oder Stellen, die die Anwendung der in Anhang I aufgeführten Harmonisierungsrechtsvorschriften der Union oder sonstigen Unionsrechts überwachen, das für die in Anhang III genannten Hochrisiko-KI-Systeme relevant sein könnte.\n(11) Die Marktüberwachungsbehörden und die Kommission können gemeinsame Tätigkeiten, einschließlich gemeinsamer Untersuchungen, vorschlagen, die von den Marktüberwachungsbehörden oder von den Marktüberwachungsbehörden gemeinsam mit der Kommission durchgeführt werden, um Konformität zu fördern, Nichtkonformität festzustellen, zu sensibilisieren oder Orientierung zu dieser Verordnung und bestimmten Kategorien von Hochrisiko-KI-Systemen, bei denen festgestellt wird, dass sie in zwei oder mehr Mitgliedstaaten gemäß Artikel 9 der Verordnung (EU) 2019/1020 ein ernstes Risiko darstellen, zu geben. Das Büro für Künstliche Intelligenz unterstützt die Koordinierung der gemeinsamen Untersuchungen.\n(12) Die Anbieter gewähren den Marktüberwachungsbehörden unbeschadet der Befugnisübertragung gemäß der Verordnung (EU) 2019/1020 — sofern dies relevant ist und beschränkt auf das zur Wahrnehmung der Aufgaben dieser Behörden erforderliche Maß — uneingeschränkten Zugang zur Dokumentation sowie zu den für die Entwicklung von Hochrisiko-KI-Systemen verwendeten Trainings-, Validierungs- und Testdatensätzen, gegebenenfalls und unter Einhaltung von Sicherheitsvorkehrungen auch über die Anwendungsprogrammierschnittstellen (im Folgenden „API“) oder andere einschlägige technische Mittel und Instrumente, die den Fernzugriff ermöglichen.\n(13) Zum Quellcode des Hochrisiko-KI-Systems erhalten Marktüberwachungsbehörden auf begründete Anfrage und nur dann Zugang, wenn die beiden folgenden Bedingungen erfüllt sind:\na) Der Zugang zum Quellcode ist zur Bewertung der Konformität eines Hochrisiko-KI-Systems mit den in Kapitel III Abschnitt 2 festgelegten Anforderungen notwendig und\nb) die Test- oder Prüfverfahren und Überprüfungen aufgrund der vom Anbieter bereitgestellten Daten und Dokumentation wurden ausgeschöpft oder haben sich als unzureichend erwiesen.\n(14) Jegliche Informationen oder Dokumentation, in deren Besitz die Marktüberwachungsbehörden gelangen, werden im Einklang mit den in Artikel 78 festgelegten Vertraulichkeitspflichten behandelt."
    },
    {
      "chunk_idx": 279,
      "id": "41caa9c8-36d6-4917-badd-540c9137f2cf",
      "title": "Art 75",
      "relevantChunksIds": [
        "e83ab1ea-4c87-4599-978e-5912e4a9bda3",
        "7ce8047c-1e69-42c0-aba4-40614e1ff6b8",
        "af44a0ad-8425-4b78-8b03-21093d6fc548",
        "a0c2ce4c-d906-4d3e-adb7-96e4be45e9bf"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 75: Mutual assistance, market surveillance and control of general-purpose AI systems\n1. Where an AI system is based on a general-purpose AI model, and the model and the system are developed by the same provider, the AI Office shall have powers to monitor and supervise compliance of that AI system with obligations under this Regulation. To carry out its monitoring and supervision tasks, the AI Office shall have all the powers of a market surveillance authority provided for in this Section and Regulation (EU) 2019/1020.\n2. Where the relevant market surveillance authorities have sufficient reason to consider general-purpose AI systems that can be used directly by deployers for at least one purpose that is classified as high-risk pursuant to this Regulation to be non-compliant with the requirements laid down in this Regulation, they shall cooperate with the AI Office to carry out compliance evaluations, and shall inform the Board and other market surveillance authorities accordingly.\n3. Where a market surveillance authority is unable to conclude its investigation of the high-risk AI system because of its inability to access certain information related to the general-purpose AI model despite having made all appropriate efforts to obtain that information, it may submit a reasoned request to the AI Office, by which access to that information shall be enforced. In that case, the AI Office shall supply to the applicant authority without delay, and in any event within 30 days, any information that the AI Office considers to be relevant in order to establish whether a high-risk AI system is non-compliant. Market surveillance authorities shall safeguard the confidentiality of the information that they obtain in accordance with Article 78 of this Regulation. The procedure provided for in Chapter VI of Regulation (EU) 2019/1020 shall apply mutatis mutandis.",
      "original_content": "### Artikel 75: Amtshilfe, Marktüberwachung und Kontrolle von KI-Systemen mit allgemeinem Verwendungszweck\n(1) Beruht ein KI-System auf einem KI-Modell mit allgemeinem Verwendungszweck und werden das Modell und das System vom selben Anbieter entwickelt, so ist das Büro für Künstliche Intelligenz befugt, die Konformität des KI-Systems mit den Pflichten aus dieser Verordnung zu überwachen und beaufsichtigen. Zur Wahrnehmung seiner Beobachtungs- und Überwachungsaufgaben hat das Büro für Künstliche Intelligenz alle in diesem Abschnitt und in der Verordnung (EU) 2019/1020 vorgesehenen Befugnisse einer Marktüberwachungsbehörde.\n(2) Haben die zuständigen Marktüberwachungsbehörden hinreichenden Grund für die Auffassung, dass KI-Systeme mit allgemeinem Verwendungszweck, die von Betreibern direkt für mindestens einen Zweck, der gemäß dieser Verordnung als hochriskant eingestuft ist, verwendet werden können, nicht mit den in dieser Verordnung festgelegten Anforderungen konform sind, so arbeiten sie bei der Durchführung von Konformitätsbewertungen mit dem Büro für Künstliche Intelligenz zusammen und unterrichten das KI-Gremium und andere Marktüberwachungsbehörden entsprechend.\n(3) Ist eine Marktüberwachungsbehörde wegen der Unzugänglichkeit bestimmter Informationen im Zusammenhang mit dem KI-Modell mit allgemeinem Verwendungszweck nicht in der Lage, ihre Ermittlungen zu dem Hochrisiko-KI-System abzuschließen, obwohl sie alle angemessenen Anstrengungen unternommen hat, diese Informationen zu erhalten, kann sie ein begründetes Ersuchen an das Büro für Künstliche Intelligenz richten, durch das der Zugang zu den Informationen durchgesetzt werden kann. In diesem Fall übermittelt das Büro für Künstliche Intelligenz der ersuchenden Behörde unverzüglich, spätestens aber innerhalb von 30 Tagen, alle Informationen, die das Büro für Künstliche Intelligenz für die Feststellung, ob ein Hochrisiko-KI-System nicht konform ist, für erforderlich erachtet. Die Marktüberwachungsbehörden gewährleisten gemäß Artikel 78 der vorliegenden Verordnung die Vertraulichkeit der von ihnen erlangten Informationen. Das Verfahren nach Kapitel VI der Verordnung (EU) 2019/1020 gilt entsprechend."
    },
    {
      "chunk_idx": 280,
      "id": "8c8b84b1-d7b3-4ed3-a2ae-fb679465dbe9",
      "title": "Art 76",
      "relevantChunksIds": [
        "ca5614f2-4cf4-460e-addd-b5609143a0cd",
        "d9af965b-cc32-4306-b4f8-f4caeb8bfa48"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 76: Supervision of testing in real world conditions by market surveillance authorities\n1. Market surveillance authorities shall have competences and powers to ensure that testing in real world conditions is in accordance with this Regulation.\n2. Where testing in real world conditions is conducted for AI systems that are supervised within an AI regulatory sandbox under Article 58, the market surveillance authorities shall verify the compliance with Article 60 as part of their supervisory role for the AI regulatory sandbox. Those authorities may, as appropriate, allow the testing in real world conditions to be conducted by the provider or prospective provider, in derogation from the conditions set out in Article 60(4), points (f) and (g).\n3. Where a market surveillance authority has been informed by the prospective provider, the provider or any third party of a serious incident or has other grounds for considering that the conditions set out in Articles 60 and 61 are not met, it may take either of the following decisions on its territory, as appropriate:\n(a) to suspend or terminate the testing in real world conditions;\n(b) to require the provider or prospective provider and the deployer or prospective deployer to modify any aspect of the testing in real world conditions.\n4. Where a market surveillance authority has taken a decision referred to in paragraph 3 of this Article, or has issued an objection within the meaning of Article 60(4), point (b), the decision or the objection shall indicate the grounds therefor and how the provider or prospective provider can challenge the decision or objection.\n5. Where applicable, where a market surveillance authority has taken a decision referred to in paragraph 3, it shall communicate the grounds therefor to the market surveillance authorities of other Member States in which the AI system has been tested in accordance with the testing plan.",
      "original_content": "### Artikel 76: Beaufsichtigung von Tests unter Realbedingungen durch Marktüberwachungsbehörden\n(1) Marktüberwachungsbehörden müssen über die Kompetenzen und Befugnisse verfügen, um sicherzustellen, dass Tests unter Realbedingungen gemäß dieser Verordnung erfolgen.\n(2) Wenn ein Test unter Realbedingungen für KI-Systeme durchgeführt wird, die in einem KI-Reallabor gemäß Artikel 58 beaufsichtigt werden, überprüfen die Marktüberwachungsbehörden im Rahmen ihrer Aufsichtsaufgaben für das KI-Reallabor die Einhaltung des Artikels 60. Die Behörden können gegebenenfalls gestatten, dass der Anbieter oder zukünftige Anbieter den Test unter Realbedingungen in Abweichung von den in Artikel 60 Absatz 4 Buchstaben f und g festgelegten Bedingungen durchführt.\n(3) Wenn eine Marktüberwachungsbehörde vom zukünftigen Anbieter, vom Anbieter oder von einem Dritten über einen schwerwiegenden Vorfall informiert wurde oder Grund zu der Annahme hat, dass die in den Artikeln 60 und 61 festgelegten Bedingungen nicht erfüllt sind, kann sie — je nachdem, was angemessen ist — in ihrem Hoheitsgebiet gegebenenfalls entscheiden, entweder\na) den Test unter Realbedingungen auszusetzen oder abzubrechen oder\nb) den Anbieter oder zukünftigen Anbieter und die Betreiber oder zukünftigen Betreiber zur Änderung eines beliebigen Aspekts des Tests unter Realbedingungen zu verpflichten.\n(4) Wenn eine Marktüberwachungsbehörde eine Entscheidung nach Absatz 3 des vorliegenden Artikels getroffen oder Einwände im Sinne des Artikels 60 Absatz 4 Buchstabe b erhoben hat, sind im Rahmen der Entscheidung oder der Einwände die Gründe dafür zu nennen sowie anzugeben, wie der Anbieter oder zukünftige Anbieter die Entscheidung oder die Einwände anfechten kann.\n(5) Wenn eine Marktüberwachungsbehörde eine Entscheidung nach Absatz 3 getroffen hat, teilt sie ihre Gründe dafür gegebenenfalls den Marktüberwachungsbehörden anderer Mitgliedstaaten mit, in denen das KI-System gemäß dem Plan für den Test getestet wurde."
    },
    {
      "chunk_idx": 281,
      "id": "b4031ce5-9908-41f5-b9a4-6bc0efc0d784",
      "title": "Art 77",
      "relevantChunksIds": [
        "0d1ee002-929c-4cd0-99ba-0e8f530307b5",
        "ecc6bef2-71ac-41e5-a335-c79864647312"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 77: Powers of authorities protecting fundamental rights\n1. National public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights, including the right to non-discrimination, in relation to the use of high-risk AI systems referred to in Annex III shall have the power to request and access any documentation created or maintained under this Regulation in accessible language and format when access to that documentation is necessary for effectively fulfilling their mandates within the limits of their jurisdiction. The relevant public authority or body shall inform the market surveillance authority of the Member State concerned of any such request.\n2. By 2 November 2024, each Member State shall identify the public authorities or bodies referred to in paragraph 1 and make a list of them publicly available. Member States shall notify the list to the Commission and to the other Member States, and shall keep the list up to date.\n3. Where the documentation referred to in paragraph 1 is insufficient to ascertain whether an infringement of obligations under Union law protecting fundamental rights has occurred, the public authority or body referred to in paragraph 1 may make a reasoned request to the market surveillance authority, to organise testing of the high-risk AI system through technical means. The market surveillance authority shall organise the testing with the close involvement of the requesting public authority or body within a reasonable time following the request.\n4. Any information or documentation obtained by the national public authorities or bodies referred to in paragraph 1 of this Article pursuant to this Article shall be treated in accordance with the confidentiality obligations set out in Article 78.",
      "original_content": "### Artikel 77: Befugnisse der für den Schutz der Grundrechte zuständigen Behörden\n(1) Nationale Behörden oder öffentliche Stellen, die die Einhaltung des Unionsrechts zum Schutz der Grundrechte, einschließlich des Rechts auf Nichtdiskriminierung, in Bezug auf die Verwendung der in Anhang III genannten Hochrisiko-KI-Systeme beaufsichtigen oder durchsetzen, sind befugt, sämtliche auf der Grundlage dieser Verordnung in zugänglicher Sprache und Format erstellte oder geführte Dokumentation anzufordern und einzusehen, sofern der Zugang zu dieser Dokumentation für die wirksame Ausübung ihrer Aufträge im Rahmen ihrer Befugnisse innerhalb der Grenzen ihrer Hoheitsgewalt notwendig ist. Die jeweilige Behörde oder öffentliche Stelle informiert die Marktüberwachungsbehörde des betreffenden Mitgliedstaats von jeder diesbezüglichen Anfrage.\n(2) Bis 2. November 2024 muss jeder Mitgliedstaat die in Absatz 1 genannten Behörden oder öffentlichen Stellen benennen und in einer öffentlichen Liste verfügbar machen. Die Mitgliedstaaten übermitteln die Liste der Kommission und den anderen Mitgliedstaaten und halten die Liste auf dem neuesten Stand.\n(3) Sollte die in Absatz 1 genannte Dokumentation nicht ausreichen, um feststellen zu können, ob ein Verstoß gegen das Unionsrecht zum Schutz der Grundrechte vorliegt, so kann die in Absatz 1 genannte Behörde oder öffentliche Stelle bei der Marktüberwachungsbehörde einen begründeten Antrag auf Durchführung eines technischen Tests des Hochrisiko-KI-Systems stellen. Die Marktüberwachungsbehörde führt den Test unter enger Einbeziehung der beantragenden Behörde oder öffentlichen Stelle innerhalb eines angemessenen Zeitraums nach Eingang des Antrags durch.\n(4) Jegliche Informationen oder Dokumentation, in deren Besitz die in Absatz 1 des vorliegenden Artikels genannten nationalen Behörden oder öffentlichen Stellen auf der Grundlage des vorliegenden Artikels gelangen, werden im Einklang mit den in Artikel 78 festgelegten Vertraulichkeitspflichten behandelt."
    },
    {
      "chunk_idx": 282,
      "id": "bd393311-1589-4e5f-8a95-ca6421fe8583",
      "title": "Art 78",
      "relevantChunksIds": [
        "b1c54e3f-0856-463c-a767-339866de57a1"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 78: Confidentiality\n1. The Commission, market surveillance authorities and notified bodies and any other natural or legal person involved in the application of this Regulation shall, in accordance with Union or national law, respect the confidentiality of information and data obtained in carrying out their tasks and activities in such a manner as to protect, in particular:\n(a) the intellectual property rights and confidential business information or trade secrets of a natural or legal person, including source code, except in the cases referred to in Article 5 of Directive (EU) 2016/943 of the European Parliament and of the Council (57);\n(b) the effective implementation of this Regulation, in particular for the purposes of inspections, investigations or audits;\n(c) public and national security interests;\n(d) the conduct of criminal or administrative proceedings;\n(e) information classified pursuant to Union or national law.\n2. The authorities involved in the application of this Regulation pursuant to paragraph 1 shall request only data that is strictly necessary for the assessment of the risk posed by AI systems and for the exercise of their powers in accordance with this Regulation and with Regulation (EU) 2019/1020. They shall put in place adequate and effective cybersecurity measures to protect the security and confidentiality of the information and data obtained, and shall delete the data collected as soon as it is no longer needed for the purpose for which it was obtained, in accordance with applicable Union or national law.\n3. Without prejudice to paragraphs 1 and 2, information exchanged on a confidential basis between the national competent authorities or between national competent authorities and the Commission shall not be disclosed without prior consultation of the originating national competent authority and the deployer when high-risk AI systems referred to in point 1, 6 or 7 of Annex III are used by law enforcement, border control, immigration or asylum authorities and when such disclosure would jeopardise public and national security interests. This exchange of information shall not cover sensitive operational data in relation to the activities of law enforcement, border control, immigration or asylum authorities.\nWhen the law enforcement, immigration or asylum authorities are providers of high-risk AI systems referred to in point 1, 6 or 7 of Annex III, the technical documentation referred to in Annex IV shall remain within the premises of those authorities. Those authorities shall ensure that the market surveillance authorities referred to in Article 74(8) and (9), as applicable, can, upon request, immediately access the documentation or obtain a copy thereof. Only staff of the market surveillance authority holding the appropriate level of security clearance shall be allowed to access that documentation or any copy thereof.\n4. Paragraphs 1, 2 and 3 shall not affect the rights or obligations of the Commission, Member States and their relevant authorities, as well as those of notified bodies, with regard to the exchange of information and the dissemination of warnings, including in the context of cross-border cooperation, nor shall they affect the obligations of the parties concerned to provide information under criminal law of the Member States.\n5. The Commission and Member States may exchange, where necessary and in accordance with relevant provisions of international and trade agreements, confidential information with regulatory authorities of third countries with which they have concluded bilateral or multilateral confidentiality arrangements guaranteeing an adequate level of confidentiality.",
      "original_content": "### Artikel 78: Vertraulichkeit\n(1) Die Kommission, die Marktüberwachungsbehörden und die notifizierten Stellen sowie alle anderen natürlichen oder juristischen Personen, die an der Anwendung dieser Verordnung beteiligt sind, wahren gemäß dem Unionsrecht oder dem nationalen Recht die Vertraulichkeit der Informationen und Daten, in deren Besitz sie bei der Ausführung ihrer Aufgaben und Tätigkeiten gelangen, sodass insbesondere Folgendes geschützt ist:\na) die Rechte des geistigen Eigentums sowie vertrauliche Geschäftsinformationen oder Geschäftsgeheimnisse natürlicher oder juristischer Personen, einschließlich Quellcodes, mit Ausnahme der in Artikel 5 der Richtlinie (EU) 2016/943 des Europäischen Parlaments und des Rates (Fußnote 57); Fußnote 57: Richtlinie (EU) 2016/943 des Europäischen Parlaments und des Rates vom 8. Juni 2016 über den Schutz vertraulichen Know-hows und vertraulicher Geschäftsinformationen (Geschäftsgeheimnisse) vor rechtswidrigem Erwerb sowie rechtswidriger Nutzung und Offenlegung (ABl. L 157 vom 15.6.2016, S. 1).\nb) die wirksame Durchführung dieser Verordnung, insbesondere für die Zwecke von Inspektionen, Untersuchungen oder Audits;\nc) öffentliche und nationale Sicherheitsinteressen;\nd) die Durchführung von Straf- oder Verwaltungsverfahren;\ne) gemäß dem Unionsrecht oder dem nationalen Recht als Verschlusssache eingestufte Informationen.\n(2) Die gemäß Absatz 1 an der Anwendung dieser Verordnung beteiligten Behörden fragen nur Daten an, die für die Bewertung des von KI-Systemen ausgehenden Risikos und für die Ausübung ihrer Befugnisse in Übereinstimmung mit dieser Verordnung und mit der Verordnung (EU) 2019/1020 unbedingt erforderlich sind. Sie ergreifen angemessene und wirksame Cybersicherheitsmaßnahmen zum Schutz der Sicherheit und Vertraulichkeit der erlangten Informationen und Daten und löschen im Einklang mit dem geltenden Unionsrecht oder nationalen Recht die erhobenen Daten, sobald sie für den Zweck, für den sie erlangt wurden, nicht mehr benötigt werden.\n(3) Unbeschadet der Absätze 1 und 2 darf der Austausch vertraulicher Informationen zwischen den zuständigen nationalen Behörden untereinander oder zwischen den zuständigen nationalen Behörden und der Kommission nicht ohne vorherige Rücksprache mit der zuständigen nationalen Behörde, von der die Informationen stammen, und dem Betreiber offengelegt werden, sofern die in Anhang III Nummer 1, 6 oder 7 genannten Hochrisiko-KI-Systeme von Strafverfolgungs-, Grenzschutz-, Einwanderungs- oder Asylbehörden verwendet werden und eine solche Offenlegung die öffentlichen und nationalen Sicherheitsinteressen gefährden könnte. Dieser Informationsaustausch erstreckt sich nicht auf sensible operative Daten zu den Tätigkeiten von Strafverfolgungs-, Grenzschutz-, Einwanderungs- oder Asylbehörden.\nHandeln Strafverfolgungs-, Einwanderungs- oder Asylbehörden als Anbieter von in Anhang III Nummer 1, 6 oder 7 genannten Hochrisiko-KI-Systemen, so verbleibt die technische Dokumentation nach Anhang IV in den Räumlichkeiten dieser Behörden. Diese Behörden sorgen dafür, dass die in Artikel 74 Absätze 8 und 9 genannten Marktüberwachungsbehörden auf Anfrage unverzüglich Zugang zu dieser Dokumentation oder eine Kopie davon erhalten. Zugang zu dieser Dokumentation oder zu einer Kopie davon darf nur das Personal der Marktüberwachungsbehörde erhalten, das über eine entsprechende Sicherheitsfreigabe verfügt.\n(4) Die Absätze 1, 2 und 3 dürfen sich weder auf die Rechte oder Pflichten der Kommission, der Mitgliedstaaten und ihrer einschlägigen Behörden sowie der notifizierten Stellen in Bezug auf den Informationsaustausch und die Weitergabe von Warnungen, einschließlich im Rahmen der grenzüberschreitenden Zusammenarbeit, noch auf die Pflichten der betreffenden Parteien auswirken, Informationen auf der Grundlage des Strafrechts der Mitgliedstaaten bereitzustellen.\n(5) Die Kommission und die Mitgliedstaaten können erforderlichenfalls und im Einklang mit den einschlägigen Bestimmungen internationaler Übereinkommen und Handelsabkommen mit Regulierungsbehörden von Drittstaaten, mit denen sie bilaterale oder multilaterale Vertraulichkeitsvereinbarungen getroffen haben und die ein angemessenes Niveau an Vertraulichkeit gewährleisten, vertrauliche Informationen austauschen."
    },
    {
      "chunk_idx": 283,
      "id": "069d1fdf-6329-4927-865f-862b02fbc7c1",
      "title": "Art 79",
      "relevantChunksIds": [
        "577d5a3c-36b6-48b6-a3fd-27b111efe5fd",
        "491d84d6-b754-4408-8bbd-21c789451f0f",
        "ecc6bef2-71ac-41e5-a335-c79864647312",
        "1c919b24-516b-4255-979e-f4cbc0da447e",
        "90ac7181-45af-41f1-817b-5a51223d7825"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 79: Procedure at national level for dealing with AI systems presenting a risk\n1. AI systems presenting a risk shall be understood as a ‘product presenting a risk’ as defined in Article 3, point 19 of Regulation (EU) 2019/1020, in so far as they present risks to the health or safety, or to fundamental rights, of persons.\n2. Where the market surveillance authority of a Member State has sufficient reason to consider an AI system to present a risk as referred to in paragraph 1 of this Article, it shall carry out an evaluation of the AI system concerned in respect of its compliance with all the requirements and obligations laid down in this Regulation. Particular attention shall be given to AI systems presenting a risk to vulnerable groups. Where risks to fundamental rights are identified, the market surveillance authority shall also inform and fully cooperate with the relevant national public authorities or bodies referred to in Article 77(1). The relevant operators shall cooperate as necessary with the market surveillance authority and with the other national public authorities or bodies referred to in Article 77(1).\nWhere, in the course of that evaluation, the market surveillance authority or, where applicable the market surveillance authority in cooperation with the national public authority referred to in Article 77(1), finds that the AI system does not comply with the requirements and obligations laid down in this Regulation, it shall without undue delay require the relevant operator to take all appropriate corrective actions to bring the AI system into compliance, to withdraw the AI system from the market, or to recall it within a period the market surveillance authority may prescribe, and in any event within the shorter of 15 working days, or as provided for in the relevant Union harmonisation legislation.\nThe market surveillance authority shall inform the relevant notified body accordingly. Article 18 of Regulation (EU) 2019/1020 shall apply to the measures referred to in the second subparagraph of this paragraph.\n3. Where the market surveillance authority considers that the non-compliance is not restricted to its national territory, it shall inform the Commission and the other Member States without undue delay of the results of the evaluation and of the actions which it has required the operator to take.\n4. The operator shall ensure that all appropriate corrective action is taken in respect of all the AI systems concerned that it has made available on the Union market.\n5. Where the operator of an AI system does not take adequate corrective action within the period referred to in paragraph 2, the market surveillance authority shall take all appropriate provisional measures to prohibit or restrict the AI system’s being made available on its national market or put into service, to withdraw the product or the standalone AI system from that market or to recall it. That authority shall without undue delay notify the Commission and the other Member States of those measures.\n6. The notification referred to in paragraph 5 shall include all available details, in particular the information necessary for the identification of the non-compliant AI system, the origin of the AI system and the supply chain, the nature of the non-compliance alleged and the risk involved, the nature and duration of the national measures taken and the arguments put forward by the relevant operator. In particular, the market surveillance authorities shall indicate whether the non-compliance is due to one or more of the following:\n(a) non-compliance with the prohibition of the AI practices referred to in Article 5;\n(b) a failure of a high-risk AI system to meet requirements set out in Chapter III, Section 2;\n(c) shortcomings in the harmonised standards or common specifications referred to in Articles 40 and 41 conferring a presumption of conformity;\n\n8. Where, within three months of receipt of the notification referred to in paragraph 5 of this Article, no objection has been raised by either a market surveillance authority of a Member State or by the Commission in respect of a provisional measure taken by a market surveillance authority of another Member State, that measure shall be deemed justified. This shall be without prejudice to the procedural rights of the concerned operator in accordance with Article 18 of Regulation (EU) 2019/1020. The three-month period referred to in this paragraph shall be reduced to 30 days in the event of non-compliance with the prohibition of the AI practices referred to in Article 5 of this Regulation.\n9. The market surveillance authorities shall ensure that appropriate restrictive measures are taken in respect of the product or the AI system concerned, such as withdrawal of the product or the AI system from their market, without undue delay.",
      "original_content": "### Artikel 79: Verfahren auf nationaler Ebene für den Umgang mit KI-Systemen, die ein Risiko bergen\n(1) Als KI-Systeme, die ein Risiko bergen, gelten „Produkte, mit denen ein Risiko verbunden ist“ im Sinne des Artikels 3 Nummer 19 der Verordnung (EU) 2019/1020, sofern sie Risiken für die Gesundheit oder Sicherheit oder Grundrechte von Personen bergen.\n(2) Hat die Marktüberwachungsbehörde eines Mitgliedstaats hinreichend Grund zu der Annahme, dass ein KI-System ein Risiko nach Absatz 1 des vorliegenden Artikels birgt, so prüft sie das betreffende KI-System im Hinblick auf die Erfüllung aller in der vorliegenden Verordnung festgelegten Anforderungen und Pflichten. Besondere Aufmerksamkeit gilt KI-Systemen, die für schutzbedürftige Gruppen ein Risiko bergen. Wenn Risiken für die Grundrechte festgestellt werden, informiert die Marktüberwachungsbehörde auch die in Artikel 77 Absatz 1 genannten einschlägigen nationalen Behörden oder öffentlichen Stellen und arbeitet uneingeschränkt mit ihnen zusammen. Die betreffenden Akteure arbeiten erforderlichenfalls mit der Marktüberwachungsbehörde und den in Artikel 77 Absatz 1 genannten anderen Behörden oder öffentlichen Stellen zusammen.\nStellt die Marktüberwachungsbehörde oder gegebenenfalls die Marktüberwachungsbehörde in Zusammenarbeit mit der in Artikel 77 Absatz 1 genannten nationalen Behörde im Verlauf dieser Prüfung fest, dass das KI-System die in dieser Verordnung festgelegten Anforderungen und Pflichten nicht erfüllt, fordert sie den jeweiligen Akteur unverzüglich auf, alle Korrekturmaßnahmen zu ergreifen, die geeignet sind, die Konformität des KI-Systems herzustellen, das KI-System vom Markt zu nehmen oder es innerhalb einer Frist, die die Marktüberwachungsbehörde vorgeben kann, in jedem Fall innerhalb von weniger als 15 Arbeitstagen, oder gemäß den einschlägigen Harmonisierungsrechtsvorschriften der Union zurückzurufen.\nDie Marktüberwachungsbehörde informiert die betreffende notifizierte Stelle entsprechend. Artikel 18 der Verordnung (EU) 2019/1020 gilt für die in Unterabsatz 2 des vorliegenden Absatzes genannten Maßnahmen.\n(3) Gelangt die Marktüberwachungsbehörde zu der Auffassung, dass die Nichtkonformität nicht auf ihr nationales Hoheitsgebiet beschränkt ist, so informiert sie die Kommission und die anderen Mitgliedstaaten unverzüglich über die Ergebnisse der Prüfung und über die Maßnahmen, zu denen sie den Akteur aufgefordert hat.\n(4) Der Akteur sorgt dafür, dass alle geeigneten Korrekturmaßnahmen in Bezug auf alle betreffenden KI-Systeme, die er auf dem Unionsmarkt bereitgestellt hat, getroffen werden.\n(5) Ergreift der Akteur in Bezug auf sein KI-System keine geeigneten Korrekturmaßnahmen innerhalb der in Absatz 2 genannten Frist, trifft die Marktüberwachungsbehörde alle geeigneten vorläufigen Maßnahmen, um die Bereitstellung oder Inbetriebnahme des KI-Systems auf ihrem nationalen Markt zu verbieten oder einzuschränken, das Produkt oder das eigenständige KI-System von diesem Markt zu nehmen oder es zurückzurufen. Diese Behörde notifiziert unverzüglich die Kommission und die anderen Mitgliedstaaten über diese Maßnahmen.\n(6) Die Notifizierung nach Absatz 5 enthält alle vorliegenden Angaben, insbesondere die für die Identifizierung des nicht konformen Systems notwendigen Informationen, den Ursprung des KI-Systems und die Lieferkette, die Art der vermuteten Nichtkonformität und das sich daraus ergebende Risiko, die Art und Dauer der ergriffenen nationalen Maßnahmen und die von dem betreffenden Akteur vorgebrachten Argumente. Die Marktüberwachungsbehörden geben insbesondere an, ob die Nichtkonformität eine oder mehrere der folgenden Ursachen hat:\na) Missachtung des Verbots der in Artikel 5 genannten KI-Praktiken;\nb) Nichterfüllung der in Kapitel III Abschnitt 2 festgelegten Anforderungen durch ein Hochrisiko-KI-System;\nc) Mängel in den in den Artikeln 40 und 41 genannten harmonisierten Normen oder gemeinsamen Spezifikationen, die eine Konformitätsvermutung begründen;\nd) Nichteinhaltung des Artikels 50. (7) Die anderen Marktüberwachungsbehörden — mit Ausnahme der Marktüberwachungsbehörde des Mitgliedstaats, der das Verfahren eingeleitet hat — informieren unverzüglich die Kommission und die anderen Mitgliedstaaten über jegliche Maßnahmen und ihnen vorliegende zusätzlichen Informationen zur Nichtkonformität des betreffenden KI-Systems sowie — falls sie die ihnen mitgeteilte nationale Maßnahme ablehnen — über ihre Einwände.\n(8) Erhebt weder eine Marktüberwachungsbehörde eines Mitgliedstaats noch die Kommission innerhalb von drei Monaten nach Eingang der in Absatz 5 des vorliegenden Artikels genannten Notifizierung Einwände gegen eine von einer Marktüberwachungsbehörde eines anderen Mitgliedstaats erlassene vorläufige Maßnahme, so gilt diese Maßnahme als gerechtfertigt. Die Verfahrensrechte des betreffenden Akteurs nach Artikel 18 der Verordnung (EU) 2019/1020 bleiben hiervon unberührt. Die Frist von drei Monaten gemäß dem vorliegenden Absatz wird bei Nichteinhaltung des Verbots der in Artikel 5 der vorliegenden Verordnung genannten KI-Praktiken auf 30 Tage verkürzt.\n(9) Die Marktüberwachungsbehörden tragen dafür Sorge, dass geeignete einschränkende Maßnahmen in Bezug auf das betreffende Produkt oder KI-System ergriffen werden, beispielsweise die unverzügliche Rücknahme des Produkts oder KI-Systems von ihrem Markt."
    },
    {
      "chunk_idx": 284,
      "id": "88bfe2ee-ca8c-4bc7-a273-1cbab6e941ed",
      "title": "Art 80",
      "relevantChunksIds": [
        "577d5a3c-36b6-48b6-a3fd-27b111efe5fd",
        "491d84d6-b754-4408-8bbd-21c789451f0f",
        "ecc6bef2-71ac-41e5-a335-c79864647312",
        "1c919b24-516b-4255-979e-f4cbc0da447e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 80: Procedure for dealing with AI systems classified by the provider as non-high-risk in application of Annex III\n1. Where a market surveillance authority has sufficient reason to consider that an AI system classified by the provider as non-high-risk pursuant to Article 6(3) is indeed high-risk, the market surveillance authority shall carry out an evaluation of the AI system concerned in respect of its classification as a high-risk AI system based on the conditions set out in Article 6(3) and the Commission guidelines.\n2. Where, in the course of that evaluation, the market surveillance authority finds that the AI system concerned is high-risk, it shall without undue delay require the relevant provider to take all necessary actions to bring the AI system into compliance with the requirements and obligations laid down in this Regulation, as well as take appropriate corrective action within a period the market surveillance authority may prescribe.\n3. Where the market surveillance authority considers that the use of the AI system concerned is not restricted to its national territory, it shall inform the Commission and the other Member States without undue delay of the results of the evaluation and of the actions which it has required the provider to take.\n4. The provider shall ensure that all necessary action is taken to bring the AI system into compliance with the requirements and obligations laid down in this Regulation. Where the provider of an AI system concerned does not bring the AI system into compliance with those requirements and obligations within the period referred to in paragraph 2 of this Article, the provider shall be subject to fines in accordance with Article 99.\n5. The provider shall ensure that all appropriate corrective action is taken in respect of all the AI systems concerned that it has made available on the Union market.\n6. Where the provider of the AI system concerned does not take adequate corrective action within the period referred to in paragraph 2 of this Article, Article 79(5) to (9) shall apply.\n7. Where, in the course of the evaluation pursuant to paragraph 1 of this Article, the market surveillance authority establishes that the AI system was misclassified by the provider as non-high-risk in order to circumvent the application of requirements in Chapter III, Section 2, the provider shall be subject to fines in accordance with Article 99.\n8. In exercising their power to monitor the application of this Article, and in accordance with Article 11 of Regulation (EU) 2019/1020, market surveillance authorities may perform appropriate checks, taking into account in particular information stored in the EU database referred to in Article 71 of this Regulation.",
      "original_content": "### Artikel 80: Verfahren für den Umgang mit KI-Systemen, die vom Anbieter gemäß Anhang III als nicht hochriskant eingestuft werden\n(1) Hat eine Marktüberwachungsbehörde hinreichend Grund zu der Annahme, dass ein vom Anbieter als nicht hochriskant gemäß Artikel 6 Absatz 3 eingestuftes KI-System tatsächlich hochriskant ist, so prüft die Marktüberwachungsbehörde das betreffende KI-System im Hinblick auf seine Einstufung als Hochrisiko-KI-System auf der Grundlage der in Artikel 6 Absatz 3 festgelegten Bedingungen und den Leitlinien der Kommission.\n(2) Stellt die Marktüberwachungsbehörde im Verlauf dieser Prüfung fest, dass das betreffende KI-System hochriskant ist, fordert sie den jeweiligen Anbieter unverzüglich auf, alle erforderlichen Maßnahmen zu ergreifen, um die Konformität des KI-Systems mit den in dieser Verordnung festgelegten Anforderungen und Pflichten herzustellen, sowie innerhalb einer Frist, die die Marktüberwachungsbehörde vorgeben kann, geeignete Korrekturmaßnahmen zu ergreifen.\n(3) Gelangt die Marktüberwachungsbehörde zu der Auffassung, dass die Verwendung des betreffenden KI-Systems nicht auf ihr nationales Hoheitsgebiet beschränkt ist, so informiert sie die Kommission und die anderen Mitgliedstaaten unverzüglich über die Ergebnisse der Prüfung und über die Maßnahmen, zu denen sie den Anbieter aufgefordert hat.\n(4) Der Anbieter sorgt dafür, dass alle erforderlichen Maßnahmen ergriffen werden, um die Konformität des KI-Systems mit den in dieser Verordnung festgelegten Anforderungen und Pflichten herzustellen. Stellt der Anbieter eines betroffenen KI-Systems die Konformität des KI-Systems mit diesen Anforderungen und Pflichten nicht innerhalb der in Absatz 2 des vorliegenden Artikels genannten Frist her, so werden gegen den Anbieter Geldbußen gemäß Artikel 99 verhängt.\n(5) Der Anbieter sorgt dafür, dass alle geeigneten Korrekturmaßnahmen in Bezug auf alle betreffenden KI-Systeme, die er auf dem Unionsmarkt bereitgestellt hat, getroffen werden.\n(6) Ergreift der Anbieter des betreffenden KI-Systems innerhalb der in Absatz 2 des vorliegenden Artikels genannten Frist keine angemessenen Korrekturmaßnahmen, so findet Artikel 79 Absätze 5 bis 9 Anwendung.\n(7) Stellt die Marktüberwachungsbehörde im Verlauf der Prüfung gemäß Absatz 1 des vorliegenden Artikels fest, dass das KI-System vom Anbieter fälschlich als nicht hochriskant eingestuft wurde, um die Geltung der Anforderungen von Kapitel III Abschnitt 2 zu umgehen, so werden gegen den Anbieter Geldbußen gemäß Artikel 99 verhängt.\n(8) Bei der Ausübung ihrer Befugnis zur Überwachung der Anwendung dieses Artikels können die Marktüberwachungsbehörden im Einklang mit Artikel 11 der Verordnung (EU) 2019/1020 geeignete Überprüfungen durchführen, wobei sie insbesondere Informationen berücksichtigen, die in der EU-Datenbank gemäß Artikel 71 der vorliegenden Verordnung gespeichert sind."
    },
    {
      "chunk_idx": 285,
      "id": "d4f161e6-4fb1-4772-9a7c-d518161ea674",
      "title": "Art 81",
      "relevantChunksIds": [
        "577d5a3c-36b6-48b6-a3fd-27b111efe5fd",
        "491d84d6-b754-4408-8bbd-21c789451f0f",
        "ecc6bef2-71ac-41e5-a335-c79864647312"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 81: Union safeguard procedure\n1. Where, within three months of receipt of the notification referred to in Article 79(5), or within 30 days in the case of non-compliance with the prohibition of the AI practices referred to in Article 5, objections are raised by the market surveillance authority of a Member State to a measure taken by another market surveillance authority, or where the Commission considers the measure to be contrary to Union law, the Commission shall without undue delay enter into consultation with the market surveillance authority of the relevant Member State and the operator or operators, and shall evaluate the national measure. On the basis of the results of that evaluation, the Commission shall, within six months, or within 60 days in the case of non-compliance with the prohibition of the AI practices referred to in Article 5, starting from the notification referred to in Article 79(5), decide whether the national measure is justified and shall notify its decision to the market surveillance authority of the Member State concerned. The Commission shall also inform all other market surveillance authorities of its decision.\n2. Where the Commission considers the measure taken by the relevant Member State to be justified, all Member States shall ensure that they take appropriate restrictive measures in respect of the AI system concerned, such as requiring the withdrawal of the AI system from their market without undue delay, and shall inform the Commission accordingly. Where the Commission considers the national measure to be unjustified, the Member State concerned shall withdraw the measure and shall inform the Commission accordingly.\n3. Where the national measure is considered justified and the non-compliance of the AI system is attributed to shortcomings in the harmonised standards or common specifications referred to in Articles 40 and 41 of this Regulation, the Commission shall apply the procedure provided for in Article 11 of Regulation (EU) No 1025/2012.",
      "original_content": "### Artikel 81: Schutzklauselverfahren der Union\n(1) Erhebt eine Marktüberwachungsbehörde eines Mitgliedstaats innerhalb von drei Monaten nach Eingang der in Artikel 79 Absatz 5 genannten Notifizierung — oder bei Nichteinhaltung des Verbots der in Artikel 5 genannten KI-Praktiken innerhalb von 30 Tagen — Einwände gegen eine von der Marktüberwachungsbehörde eines anderen Mitgliedstaats getroffene Maßnahme oder ist die Kommission der Ansicht, dass die Maßnahme mit dem Unionsrecht unvereinbar ist, so nimmt die Kommission unverzüglich Konsultationen mit der Marktüberwachungsbehörde des betreffenden Mitgliedstaats und dem Akteur bzw. den Akteuren auf und prüft die nationale Maßnahme. Anhand der Ergebnisse dieser Prüfung entscheidet die Kommission innerhalb von sechs Monaten — oder bei Nichteinhaltung des Verbots der in Artikel 5 genannten KI-Praktiken innerhalb von 60 Tagen — ab dem Eingang der in Artikel 79 Absatz 5 genannten Notifizierung, ob die nationale Maßnahme gerechtfertigt ist, und teilt der Marktüberwachungsbehörde des betreffenden Mitgliedstaats ihre Entscheidung mit. Die Kommission unterrichtet auch alle übrigen Marktüberwachungsbehörden über ihre Entscheidung.\n(2) Ist die Kommission der Ansicht, dass die von dem betreffenden Mitgliedstaat ergriffene Maßnahme gerechtfertigt ist, so tragen alle Mitgliedstaaten dafür Sorge, dass sie geeignete einschränkende Maßnahmen in Bezug auf das betreffende KI-System ergreifen, etwa die Anordnung der unverzüglichen Rücknahme des KI-Systems von ihrem Markt, und informiert die Kommission darüber. Erachtet die Kommission die nationale Maßnahme als nicht gerechtfertigt, nimmt der betreffende Mitgliedstaat die Maßnahme zurück und informiert die Kommission darüber.\n(3) Gilt die nationale Maßnahme als gerechtfertigt und wird die Nichtkonformität des KI-Systems auf Mängel in den in den Artikeln 40 und 41 dieser Verordnung genannten harmonisierten Normen oder gemeinsamen Spezifikationen zurückgeführt, so leitet die Kommission das in Artikel 11 der Verordnung (EU) Nr. 1025/2012 festgelegte Verfahren ein."
    },
    {
      "chunk_idx": 286,
      "id": "0670f89c-c888-4331-9761-59f1d80cbddd",
      "title": "Art 82",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 82: Compliant AI systems which present a risk\n1. Where, having performed an evaluation under Article 79, after consulting the relevant national public authority referred to in Article 77(1), the market surveillance authority of a Member State finds that although a high-risk AI system complies with this Regulation, it nevertheless presents a risk to the health or safety of persons, to fundamental rights, or to other aspects of public interest protection, it shall require the relevant operator to take all appropriate measures to ensure that the AI system concerned, when placed on the market or put into service, no longer presents that risk without undue delay, within a period it may prescribe.\n2. The provider or other relevant operator shall ensure that corrective action is taken in respect of all the AI systems concerned that it has made available on the Union market within the timeline prescribed by the market surveillance authority of the Member State referred to in paragraph 1.\n3. The Member States shall immediately inform the Commission and the other Member States of a finding under paragraph 1. That information shall include all available details, in particular the data necessary for the identification of the AI system concerned, the origin and the supply chain of the AI system, the nature of the risk involved and the nature and duration of the national measures taken.\n4. The Commission shall without undue delay enter into consultation with the Member States concerned and the relevant operators, and shall evaluate the national measures taken. On the basis of the results of that evaluation, the Commission shall decide whether the measure is justified and, where necessary, propose other appropriate measures.\n5. The Commission shall immediately communicate its decision to the Member States concerned and to the relevant operators. It shall also inform the other Member States.",
      "original_content": "### Artikel 82: Konforme KI-Systeme, die ein Risiko bergen\n(1) Stellt die Marktüberwachungsbehörde eines Mitgliedstaats — nach einer Konsultation der in Artikel 77 Absatz 1 genannten betreffenden nationalen Behörde — nach der gemäß Artikel 79 durchgeführten Prüfung fest, dass ein Hochrisiko-KI-System zwar dieser Verordnung entspricht, aber dennoch ein Risiko für die Gesundheit oder Sicherheit von Personen, für die Grundrechte oder für andere Aspekte des Schutzes öffentlicher Interessen darstellt, so fordert sie unverzüglich den betreffenden Akteur auf, alle geeigneten Maßnahmen zu treffen, damit das betreffende KI-System zum Zeitpunkt des Inverkehrbringens oder der Inbetriebnahme dieses Risiko nicht mehr birgt, und zwar innerhalb einer Frist, die sie vorgeben kann.\n(2) Der Anbieter oder der andere einschlägige Akteur sorgt dafür, dass in Bezug auf alle betroffenen KI-Systeme, die er auf dem Unionsmarkt bereitgestellt hat, innerhalb der Frist, die von der in Absatz 1 genannten Marktüberwachungsbehörde des Mitgliedstaats vorgegeben wurde, Korrekturmaßnahmen ergriffen werden.\n(3) Die Mitgliedstaaten unterrichten unverzüglich die Kommission und die anderen Mitgliedstaaten über Feststellungen gemäß Absatz 1. Diese Unterrichtung enthält alle vorliegenden Angaben, insbesondere die für die Identifizierung des betreffenden KI-Systems notwendigen Daten, den Ursprung und die Lieferkette des KI-Systems, die Art des sich daraus ergebenden Risikos sowie die Art und Dauer der ergriffenen nationalen Maßnahmen.\n(4) Die Kommission nimmt unverzüglich mit den betreffenden Mitgliedstaaten und den jeweiligen Akteuren Konsultationen auf und prüft die ergriffenen nationalen Maßnahmen. Anhand der Ergebnisse dieser Prüfung entscheidet die Kommission, ob die Maßnahme gerechtfertigt ist, und schlägt, falls erforderlich, weitere geeignete Maßnahmen vor.\n(5) Die Kommission teilt ihren Beschluss unverzüglich den betroffenen Mitgliedstaaten und den jeweiligen Akteuren mit. Sie unterrichtet auch die übrigen Mitgliedstaaten."
    },
    {
      "chunk_idx": 287,
      "id": "6b4c8e97-479a-4850-9913-5cf4246fb403",
      "title": "Art 83",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 83: Formal non-compliance\n1. Where the market surveillance authority of a Member State makes one of the following findings, it shall require the relevant provider to put an end to the non-compliance concerned, within a period it may prescribe:\n(a) the CE marking has been affixed in violation of Article 48;\n(b) the CE marking has not been affixed;\n(c) the EU declaration of conformity referred to in Article 47 has not been drawn up;\n(d) the EU declaration of conformity referred to in Article 47 has not been drawn up correctly;\n(e) the registration in the EU database referred to in Article 71 has not been carried out;\n(f) where applicable, no authorised representative has been appointed;\n(g) technical documentation is not available.\n2. Where the non-compliance referred to in paragraph 1 persists, the market surveillance authority of the Member State concerned shall take appropriate and proportionate measures to restrict or prohibit the high-risk AI system being made available on the market or to ensure that it is recalled or withdrawn from the market without delay.",
      "original_content": "### Artikel 83: Formale Nichtkonformität\n(1) Wenn die Marktüberwachungsbehörde eines Mitgliedstaats eine der folgenden Nichtkonformitäten feststellt, fordert sie den jeweiligen Anbieter auf, diese binnen einer Frist, die sie vorgeben kann, zu beheben:\na) die CE-Kennzeichnung wurde unter Verstoß gegen Artikel 48 angebracht;\nb) es wurde keine CE-Kennzeichnung angebracht;\nc) es wurde keine EU-Konformitätserklärung gemäß Artikel 47 ausgestellt;\nd) es wurde keine EU-Konformitätserklärung gemäß Artikel 47 ordnungsgemäß ausgestellt;\ne) es wurde keine Registrierung in der EU-Datenbank gemäß Artikel 71 vorgenommen;\nf) es wurde kein Bevollmächtigter — sofern erforderlich — ernannt;\ng) es ist keine technische Dokumentation verfügbar.\n(2) Besteht die Nichtkonformität nach Absatz 1 weiter, so ergreift die Marktüberwachungsbehörde des betreffenden Mitgliedstaats geeignete und verhältnismäßige Maßnahmen, um die Bereitstellung des Hochrisiko-KI-Systems auf dem Markt zu beschränken oder zu verbieten oder um dafür zu sorgen, dass es unverzüglich zurückgerufen oder vom Markt genommen wird."
    },
    {
      "chunk_idx": 288,
      "id": "b4be5a09-fcbd-4180-bf6e-d7ece815b4e5",
      "title": "Art 84",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 84: Union AI testing support structures\n1. The Commission shall designate one or more Union AI testing support structures to perform the tasks listed under Article 21(6) of Regulation (EU) 2019/1020 in the area of AI.\n2. Without prejudice to the tasks referred to in paragraph 1, Union AI testing support structures shall also provide independent technical or scientific advice at the request of the Board, the Commission, or of market surveillance authorities.",
      "original_content": "### Artikel 84: Unionsstrukturen zur Unterstützung der Prüfung von KI\n(1) Die Kommission benennt eine oder mehrere Unionsstrukturen zur Unterstützung der Prüfung von KI, die die Aufgaben gemäß Artikel 21 Absatz 6 der Verordnung (EU) 2019/1020 im KI-Bereich wahrnehmen.\n(2) Unbeschadet der in Absatz 1 genannten Aufgaben leisten die Unionsstrukturen zur Unterstützung der Prüfung von KI auf Anfrage des KI-Gremiums, der Kommission oder der Marktüberwachungsbehörden auch unabhängige technische oder wissenschaftliche Beratung."
    },
    {
      "chunk_idx": 289,
      "id": "43ec355f-40dd-4ebb-be43-6f6618d28b69",
      "title": "Art 85",
      "relevantChunksIds": [
        "af6b62de-8c37-48f7-b7f7-5bcdb2b0bf39",
        "1c919b24-516b-4255-979e-f4cbc0da447e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "## SECTION 4: Remedies\n### Article 85: Right to lodge a complaint with a market surveillance authority\nWithout prejudice to other administrative or judicial remedies, any natural or legal person having grounds to consider that there has been an infringement of the provisions of this Regulation may submit complaints to the relevant market surveillance authority.\nIn accordance with Regulation (EU) 2019/1020, such complaints shall be taken into account for the purpose of conducting market surveillance activities, and shall be handled in line with the dedicated procedures established therefor by the market surveillance authorities.",
      "original_content": "## ABSCHNITT 4: Rechtsbehelfe\n### Artikel 85: Recht auf Beschwerde bei einer Marktüberwachungsbehörde\nUnbeschadet anderer verwaltungsrechtlicher oder gerichtlicher Rechtsbehelfe kann jede natürliche oder juristische Person, die Grund zu der Annahme hat, dass gegen die Bestimmungen dieser Verordnung verstoßen wurde, bei der betreffenden Marktüberwachungsbehörde Beschwerden einreichen.\nGemäß der Verordnung (EU) 2019/1020 werden solche Beschwerden für die Zwecke der Durchführung von Marktüberwachungstätigkeiten berücksichtigt und nach den einschlägigen von den Marktüberwachungsbehörden dafür eingerichteten Verfahren behandelt."
    },
    {
      "chunk_idx": 290,
      "id": "cb445557-7bf2-48a2-bb97-bcaf072934fd",
      "title": "Art 86",
      "relevantChunksIds": [
        "af6b62de-8c37-48f7-b7f7-5bcdb2b0bf39",
        "1c919b24-516b-4255-979e-f4cbc0da447e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 86: Right to explanation of individual decision-making\n1. Any affected person subject to a decision which is taken by the deployer on the basis of the output from a high-risk AI system listed in Annex III, with the exception of systems listed under point 2 thereof, and which produces legal effects or similarly significantly affects that person in a way that they consider to have an adverse impact on their health, safety or fundamental rights shall have the right to obtain from the deployer clear and meaningful explanations of the role of the AI system in the decision-making procedure and the main elements of the decision taken.\n2. Paragraph 1 shall not apply to the use of AI systems for which exceptions from, or restrictions to, the obligation under that paragraph follow from Union or national law in compliance with Union law.\n3. This Article shall apply only to the extent that the right referred to in paragraph 1 is not otherwise provided for under Union law.",
      "original_content": "### Artikel 86: Recht auf Erläuterung der Entscheidungsfindung im Einzelfall\n(1) Personen, die von einer Entscheidung betroffen sind, die der Betreiber auf der Grundlage der Ausgaben eines in Anhang III aufgeführten Hochrisiko-KI-Systems, mit Ausnahme der in Nummer 2 des genannten Anhangs aufgeführten Systeme, getroffen hat und die rechtliche Auswirkungen hat oder sie in ähnlicher Art erheblich auf eine Weise beeinträchtigt, die ihrer Ansicht nach ihre Gesundheit, ihre Sicherheit oder ihre Grundrechte beeinträchtigt, haben das Recht, vom Betreiber eine klare und aussagekräftige Erläuterung zur Rolle des KI-Systems im Entscheidungsprozess und zu den wichtigsten Elementen der getroffenen Entscheidung zu erhalten.\n(2) Absatz 1 gilt nicht für die Verwendung von KI-Systemen, bei denen sich Ausnahmen von oder Beschränkungen der Pflicht nach dem genannten Absatz aus dem Unionsrecht oder dem nationalen Recht im Einklang mit dem Unionsrecht ergeben.\n(3) Dieser Artikel gilt nur insoweit, als das Recht gemäß Absatz 1 nicht anderweitig im Unionsrecht festgelegt ist."
    },
    {
      "chunk_idx": 291,
      "id": "95b7f020-8180-4dc6-80ab-b1c0cb455293",
      "title": "Art 87",
      "relevantChunksIds": [
        "af6b62de-8c37-48f7-b7f7-5bcdb2b0bf39",
        "68a7aa5c-288c-4dce-9b64-cffa47db7506"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 87: Reporting of infringements and protection of reporting persons\nDirective (EU) 2019/1937 shall apply to the reporting of infringements of this Regulation and the protection of persons reporting such infringements.",
      "original_content": "### Artikel 87: Meldung von Verstößen und Schutz von Hinweisgebern\nFür die Meldung von Verstößen gegen diese Verordnung und den Schutz von Personen, die solche Verstöße melden, gilt die Richtlinie (EU) 2019/1937."
    },
    {
      "chunk_idx": 292,
      "id": "7697d697-974a-4f6e-b011-4f0b2b838f1c",
      "title": "Art 88",
      "relevantChunksIds": [
        "e83ab1ea-4c87-4599-978e-5912e4a9bda3",
        "7ce8047c-1e69-42c0-aba4-40614e1ff6b8",
        "af44a0ad-8425-4b78-8b03-21093d6fc548"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "## SECTION 5: Supervision, investigation, enforcement and monitoring in respect of providers of general-purpose AI models\n### Article 88: Enforcement of the obligations of providers of general-purpose AI models\n1. The Commission shall have exclusive powers to supervise and enforce Chapter V, taking into account the procedural guarantees under Article 94. The Commission shall entrust the implementation of these tasks to the AI Office, without prejudice to the powers of organisation of the Commission and the division of competences between Member States and the Union based on the Treaties.\n2. Without prejudice to Article 75(3), market surveillance authorities may request the Commission to exercise the powers laid down in this Section, where that is necessary and proportionate to assist with the fulfilment of their tasks under this Regulation.",
      "original_content": "## ABSCHNITT 5: Aufsicht, Ermittlung, Durchsetzung und Überwachung in Bezug auf Anbieter von KI-Modellen mit allgemeinem Verwendungszweck\n### Artikel 88: Durchsetzung der Pflichten der Anbieter von KI-Modellen mit allgemeinem Verwendungszweck\n(1) Die Kommission verfügt unter Berücksichtigung der Verfahrensgarantien nach Artikel 94 über ausschließliche Befugnisse zur Beaufsichtigung und Durchsetzung von Kapitel V. Unbeschadet der Organisationsbefugnisse der Kommission und der Aufteilung der Zuständigkeiten zwischen den Mitgliedstaaten und der Union auf der Grundlage der Verträge überträgt die Kommission dem Büro für Künstliche Intelligenz die Durchführung dieser Aufgaben.\n(2) Unbeschadet des Artikels 75 Absatz 3 können die Marktüberwachungsbehörden die Kommission ersuchen, die in diesem Abschnitt festgelegten Befugnisse auszuüben, wenn es erforderlich und verhältnismäßig ist, um die Wahrnehmung ihrer Aufgaben gemäß dieser Verordnung zu unterstützen."
    },
    {
      "chunk_idx": 293,
      "id": "1bca51b9-d068-497e-a46b-69fe66329d83",
      "title": "Art 89",
      "relevantChunksIds": [
        "e83ab1ea-4c87-4599-978e-5912e4a9bda3",
        "7ce8047c-1e69-42c0-aba4-40614e1ff6b8"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 89: Monitoring actions\n1. For the purpose of carrying out the tasks assigned to it under this Section, the AI Office may take the necessary actions to monitor the effective implementation and compliance with this Regulation by providers of general-purpose AI models, including their adherence to approved codes of practice.\n2. Downstream providers shall have the right to lodge a complaint alleging an infringement of this Regulation. A complaint shall be duly reasoned and indicate at least:\n(a) the point of contact of the provider of the general-purpose AI model concerned;\n(b) a description of the relevant facts, the provisions of this Regulation concerned, and the reason why the downstream provider considers that the provider of the general-purpose AI model concerned infringed this Regulation;\n(c) any other information that the downstream provider that sent the request considers relevant, including, where appropriate, information gathered on its own initiative.",
      "original_content": "### Artikel 89: Überwachungsmaßnahmen\n(1) Zur Wahrnehmung der ihr in diesem Abschnitt übertragenen Aufgaben kann das Büro für Künstliche Intelligenz die erforderlichen Maßnahmen ergreifen, um die wirksame Umsetzung und Einhaltung dieser Verordnung durch Anbieter von KI-Modellen mit allgemeinem Verwendungszweck, einschließlich der Einhaltung genehmigter Praxisleitfäden, zu überwachen.\n(2) Nachgelagerte Anbieter haben das Recht, eine Beschwerde wegen eines Verstoßes gegen diese Verordnung einzureichen. Eine Beschwerde ist hinreichend zu begründen und enthält mindestens Folgendes:\na) die Kontaktstelle des Anbieters des betreffenden KI-Modells mit allgemeinem Verwendungszweck;\nb) eine Beschreibung der einschlägigen Fakten, die betreffenden Bestimmungen dieser Verordnung und die Begründung, warum der nachgelagerte Anbieter der Auffassung ist, dass der Anbieter des KI-Modells mit allgemeinem Verwendungszweck gegen diese Verordnung verstoßen hat;\nc) alle sonstigen Informationen, die der nachgelagerte Anbieter, der die Anfrage übermittelt hat, für relevant hält, gegebenenfalls einschließlich Informationen, die er auf eigene Initiative hin zusammengetragen hat."
    },
    {
      "chunk_idx": 294,
      "id": "2d6ab2fa-54df-417d-8d80-b1cad8c8e2db",
      "title": "Art 90",
      "relevantChunksIds": [
        "47710b63-0230-41a5-80af-2581299ac9d6",
        "af44a0ad-8425-4b78-8b03-21093d6fc548"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 90: Alerts of systemic risks by the scientific panel\n1. The scientific panel may provide a qualified alert to the AI Office where it has reason to suspect that:\n(a) a general-purpose AI model poses concrete identifiable risk at Union level; or\n(b) a general-purpose AI model meets the conditions referred to in Article 51.\n\n(a) the point of contact of the provider of the general-purpose AI model with systemic risk concerned;\n(b) a description of the relevant facts and the reasons for the alert by the scientific panel;\n(c) any other information that the scientific panel considers to be relevant, including, where appropriate, information gathered on its own initiative.",
      "original_content": "### Artikel 90: Warnungen des wissenschaftlichen Gremiums vor systemischen Risiken\n(1) Das wissenschaftliche Gremium kann dem Büro für Künstliche Intelligenz eine qualifizierte Warnung übermitteln, wenn es Grund zu der Annahme hat, dass\na) ein KI-Modell mit allgemeinem Verwendungszweck ein konkretes, identifizierbares Risiko auf Unionsebene birgt oder\nb) ein KI-Modell mit allgemeinem Verwendungszweck die Bedingungen gemäß Artikel 51 erfüllt.\n(2) Aufgrund einer solchen qualifizierten Warnung kann die Kommission über das Büro für Künstliche Intelligenz und nach Unterrichtung des KI-Gremiums die in diesem Abschnitt festgelegten Befugnisse zur Beurteilung der Angelegenheit ausüben. Das Büro für Künstliche Intelligenz unterrichtet das KI-Gremium über jede Maßnahme gemäß den Artikeln 91 bis 94. (3) Eine qualifizierte Warnung ist hinreichend zu begründen und enthält mindestens Folgendes:\na) die Kontaktstelle des Anbieters des betreffenden KI-Modells mit allgemeinem Verwendungszweck mit systemischem Risiko;\nb) eine Beschreibung der einschlägigen Fakten und der Gründe für die Warnung durch das wissenschaftliche Gremium;\nc) alle sonstigen Informationen, die das wissenschaftliche Gremium für relevant hält, gegebenenfalls einschließlich Informationen, die es auf eigene Initiative hin zusammengetragen hat."
    },
    {
      "chunk_idx": 295,
      "id": "4f1989fc-d70f-4469-b223-686f0035ddc7",
      "title": "Art 91",
      "relevantChunksIds": [
        "e83ab1ea-4c87-4599-978e-5912e4a9bda3",
        "7ce8047c-1e69-42c0-aba4-40614e1ff6b8"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 91: Power to request documentation and information\n1. The Commission may request the provider of the general-purpose AI model concerned to provide the documentation drawn up by the provider in accordance with Articles 53 and 55, or any additional information that is necessary for the purpose of assessing compliance of the provider with this Regulation.\n2. Before sending the request for information, the AI Office may initiate a structured dialogue with the provider of the general-purpose AI model.\n3. Upon a duly substantiated request from the scientific panel, the Commission may issue a request for information to a provider of a general-purpose AI model, where the access to information is necessary and proportionate for the fulfilment of the tasks of the scientific panel under Article 68(2).\n4. The request for information shall state the legal basis and the purpose of the request, specify what information is required, set a period within which the information is to be provided, and indicate the fines provided for in Article 101 for supplying incorrect, incomplete or misleading information.\n5. The provider of the general-purpose AI model concerned, or its representative shall supply the information requested. In the case of legal persons, companies or firms, or where the provider has no legal personality, the persons authorised to represent them by law or by their statutes, shall supply the information requested on behalf of the provider of the general-purpose AI model concerned. Lawyers duly authorised to act may supply information on behalf of their clients. The clients shall nevertheless remain fully responsible if the information supplied is incomplete, incorrect or misleading.",
      "original_content": "### Artikel 91: Befugnis zur Anforderung von Dokumentation und Informationen\n(1) Die Kommission kann den Anbieter des betreffenden KI-Modells mit allgemeinem Verwendungszweck auffordern, die vom Anbieter gemäß den Artikeln 53 und 55 erstellte Dokumentation oder alle zusätzlichen Informationen vorzulegen, die erforderlich sind, um die Einhaltung dieser Verordnung durch den Anbieter zu beurteilen.\n(2) Vor der Übermittlung des Informationsersuchens kann das Büro für Künstliche Intelligenz einen strukturierten Dialog mit dem Anbieter des KI-Modells mit allgemeinem Verwendungszweck einleiten.\n(3) Auf hinreichend begründeten Antrag des wissenschaftlichen Gremiums kann die Kommission ein Informationsersuchen an einen Anbieter eines KI-Modells mit allgemeinem Verwendungszweck richten, wenn der Zugang zu Informationen für die Wahrnehmung der Aufgaben des wissenschaftlichen Gremiums gemäß Artikel 68 Absatz 2 erforderlich und verhältnismäßig ist.\n(4) In dem Auskunftsersuchen sind die Rechtsgrundlage und der Zweck des Ersuchens zu nennen, anzugeben, welche Informationen benötigt werden, eine Frist für die Übermittlung der Informationen zu setzen, und die Geldbußen für die Erteilung unrichtiger, unvollständiger oder irreführender Informationen gemäß Artikel 101 anzugeben.\n(5) Der Anbieter des betreffenden KI-Modells mit allgemeinem Verwendungszweck oder sein Vertreter stellt die angeforderten Informationen bereit. Im Falle juristischer Personen, Gesellschaften oder — wenn der Anbieter keine Rechtspersönlichkeit besitzt — die Personen, die nach Gesetz oder Satzung zur Vertretung dieser Personen befugt sind, stellen die angeforderten Informationen im Namen des Anbieters des betreffenden KI-Modells mit allgemeinem Verwendungszweck zur Verfügung. Ordnungsgemäß bevollmächtigte Rechtsanwälte können Informationen im Namen ihrer Mandanten erteilen. Die Mandanten bleiben jedoch in vollem Umfang dafür verantwortlich, dass die erteilten Auskünfte vollständig, sachlich richtig oder nicht irreführend sind."
    },
    {
      "chunk_idx": 296,
      "id": "63a159a9-e3dd-40b7-a414-2fb5c10817b4",
      "title": "Art 92",
      "relevantChunksIds": [
        "e83ab1ea-4c87-4599-978e-5912e4a9bda3",
        "7ce8047c-1e69-42c0-aba4-40614e1ff6b8"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 92: Power to conduct evaluations\n1. The AI Office, after consulting the Board, may conduct evaluations of the general-purpose AI model concerned:\n(a) to assess compliance of the provider with obligations under this Regulation, where the information gathered pursuant to Article 91 is insufficient; or\n(b) to investigate systemic risks at Union level of general-purpose AI models with systemic risk, in particular following a qualified alert from the scientific panel in accordance with Article 90(1), point (a).\n2. The Commission may decide to appoint independent experts to carry out evaluations on its behalf, including from the scientific panel established pursuant to Article 68. Independent experts appointed for this task shall meet the criteria outlined in Article 68(2).\n3. For the purposes of paragraph 1, the Commission may request access to the general-purpose AI model concerned through APIs or further appropriate technical means and tools, including source code.\n4. The request for access shall state the legal basis, the purpose and reasons of the request and set the period within which the access is to be provided, and the fines provided for in Article 101 for failure to provide access.\n5. The providers of the general-purpose AI model concerned or its representative shall supply the information requested. In the case of legal persons, companies or firms, or where the provider has no legal personality, the persons authorised to represent them by law or by their statutes, shall provide the access requested on behalf of the provider of the general-purpose AI model concerned.\n6. The Commission shall adopt implementing acts setting out the detailed arrangements and the conditions for the evaluations, including the detailed arrangements for involving independent experts, and the procedure for the selection thereof. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 98(2).\n7. Prior to requesting access to the general-purpose AI model concerned, the AI Office may initiate a structured dialogue with the provider of the general-purpose AI model to gather more information on the internal testing of the model, internal safeguards for preventing systemic risks, and other internal procedures and measures the provider has taken to mitigate such risks.",
      "original_content": "### Artikel 92: Befugnis zur Durchführung von Bewertungen\n(1) Das Büro für Künstliche Intelligenz kann nach Konsultation des KI-Gremiums Bewertungen des betreffenden KI-Modells mit allgemeinem Verwendungszweck durchführen, um\na) die Einhaltung der Pflichten aus dieser Verordnung durch den Anbieter zu beurteilen, wenn die gemäß Artikel 91 eingeholten Informationen unzureichend sind, oder\nb) systemische Risiken auf Unionsebene von KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko zu ermitteln, insbesondere im Anschluss an eine qualifizierte Warnung des wissenschaftlichen Gremiums gemäß Artikel 90 Absatz 1 Buchstabe a.\n(2) Die Kommission kann beschließen, unabhängige Sachverständige zu benennen, die in ihrem Namen Bewertungen durchführen, einschließlich aus dem gemäß Artikel 68 eingesetzten wissenschaftlichen Gremium. Die für diese Aufgabe benannten unabhängigen Sachverständigen erfüllen die in Artikel 68 Absatz 2 umrissenen Kriterien.\n(3) Für die Zwecke des Absatzes 1 kann die Kommission über API oder weitere geeignete technische Mittel und Instrumente, einschließlich Quellcode, Zugang zu dem betreffenden KI-Modell mit allgemeinem Verwendungszweck anfordern.\n(4) In der Anforderung des Zugangs sind die Rechtsgrundlage, der Zweck und die Gründe für die Anforderung zu nennen und die Frist für die Bereitstellung des Zugangs zu setzen und die Geldbußen gemäß Artikel 101 für den Fall, dass der Zugang nicht bereitgestellt wird, anzugeben.\n(5) Die Anbieter des betreffenden KI-Modells mit allgemeinem Verwendungszweck oder seine Vertreter stellen die angeforderten Informationen zur Verfügung. Im Falle juristischer Personen, Gesellschaften oder — wenn der Anbieter keine Rechtspersönlichkeit besitzt — die Personen, die nach Gesetz oder ihrer Satzung zur Vertretung dieser Personen befugt sind, stellen den angeforderten Zugang im Namen des Anbieters des betreffenden KI-Modells mit allgemeinem Verwendungszweck zur Verfügung.\n(6) Die Kommission erlässt Durchführungsrechtsakte, in denen die detaillierten Regelungen und Voraussetzungen für die Bewertungen, einschließlich der detaillierten Regelungen für die Einbeziehung unabhängiger Sachverständiger, und das Verfahren für deren Auswahl festgelegt werden. Diese Durchführungsrechtsakte werden gemäß dem in Artikel 98 Absatz 2 genannten Prüfverfahren erlassen.\n(7) Bevor es den Zugang zu dem betreffenden KI-Modell mit allgemeinem Verwendungszweck anfordert, kann das Büro für Künstliche Intelligenz einen strukturierten Dialog mit dem Anbieter des KI-Modells mit allgemeinem Verwendungszweck einleiten, um mehr Informationen über die interne Erprobung des Modells, interne Vorkehrungen zur Vermeidung systemischer Risiken und andere interne Verfahren und Maßnahmen, die der Anbieter zur Minderung dieser Risiken ergriffen hat, einzuholen."
    },
    {
      "chunk_idx": 297,
      "id": "16e69959-dc99-4627-8f3c-7ce956e1358e",
      "title": "Art 93",
      "relevantChunksIds": [
        "af44a0ad-8425-4b78-8b03-21093d6fc548",
        "a0c2ce4c-d906-4d3e-adb7-96e4be45e9bf"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 93: Power to request measures\n1. Where necessary and appropriate, the Commission may request providers to:\n(a) take appropriate measures to comply with the obligations set out in Articles 53 and 54;\n(b) implement mitigation measures, where the evaluation carried out in accordance with Article 92 has given rise to serious and substantiated concern of a systemic risk at Union level;\n(c) restrict the making available on the market, withdraw or recall the model.\n2. Before a measure is requested, the AI Office may initiate a structured dialogue with the provider of the general-purpose AI model.\n3. If, during the structured dialogue referred to in paragraph 2, the provider of the general-purpose AI model with systemic risk offers commitments to implement mitigation measures to address a systemic risk at Union level, the Commission may, by decision, make those commitments binding and declare that there are no further grounds for action.",
      "original_content": "### Artikel 93: Befugnis zur Aufforderung zu Maßnahmen\n(1) Soweit erforderlich und angemessen, kann die Kommission die Anbieter auffordern,\na) geeignete Maßnahmen zu ergreifen, um die Verpflichtungen gemäß den Artikeln 53 und 54 einzuhalten;\nb) Risikominderungsmaßnahmen durchzuführen, wenn die gemäß Artikel 92 durchgeführte Bewertung zu ernsthaften und begründeten Bedenken hinsichtlich eines systemischen Risikos auf Unionsebene geführt hat;\nc) die Bereitstellung des Modells auf dem Markt einzuschränken, es zurückzunehmen oder zurückzurufen.\n(2) Vor der Aufforderung zu einer Maßnahme kann das Büro für Künstliche Intelligenz einen strukturierten Dialog mit dem Anbieter des KI-Modells mit allgemeinem Verwendungszweck einleiten.\n(3) Wenn der Anbieter des KI-Modells mit allgemeinem Verwendungszweck im Rahmen des strukturierten Dialogs gemäß Absatz 2 Verpflichtungszusagen zur Durchführung von Risikominderungsmaßnahmen, um einem systemischen Risiko auf Unionsebene zu begegnen, anbietet, kann die Kommission diese Verpflichtungszusagen durch einen Beschluss für bindend erklären und feststellen, dass es keinen weiteren Anlass zum Handeln gibt."
    },
    {
      "chunk_idx": 298,
      "id": "250d8d85-cebe-415e-a198-35b61991c230",
      "title": "Art 94",
      "relevantChunksIds": [
        "af44a0ad-8425-4b78-8b03-21093d6fc548",
        "a0c2ce4c-d906-4d3e-adb7-96e4be45e9bf"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 94: Procedural rights of economic operators of the general-purpose AI model\n### Article 18 of Regulation (EU) 2019/1020 shall apply mutatis mutandis to the providers of the general-purpose AI model, without prejudice to more specific procedural rights provided for in this Regulation.",
      "original_content": "### Artikel 94: Verfahrensrechte der Wirtschaftsakteure des KI-Modells mit allgemeinem Verwendungszweck\nUnbeschadet der in dieser Verordnung enthaltenen spezifischeren Verfahrensrechte gilt für die Anbieter des KI-Modells mit allgemeinem Verwendungszweck Artikel 18 der Verordnung (EU) 2019/1020 sinngemäß."
    },
    {
      "chunk_idx": 299,
      "id": "b39b09ad-8deb-4ec1-bfae-a098c9425de2",
      "title": "Art 95",
      "relevantChunksIds": [
        "7cf70e13-b212-45e3-bf7b-66d860a91315",
        "1b90fb32-5b4f-463a-9396-217d47295c8d"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "# CHAPTER X: CODES OF CONDUCT AND GUIDELINES\n### Article 95: Codes of conduct for voluntary application of specific requirements\n1. The AI Office and the Member States shall encourage and facilitate the drawing up of codes of conduct, including related governance mechanisms, intended to foster the voluntary application to AI systems, other than high-risk AI systems, of some or all of the requirements set out in Chapter III, Section 2 taking into account the available technical solutions and industry best practices allowing for the application of such requirements.\n2. The AI Office and the Member States shall facilitate the drawing up of codes of conduct concerning the voluntary application, including by deployers, of specific requirements to all AI systems, on the basis of clear objectives and key performance indicators to measure the achievement of those objectives, including elements such as, but not limited to:\n(a) applicable elements provided for in Union ethical guidelines for trustworthy AI;\n(b) assessing and minimising the impact of AI systems on environmental sustainability, including as regards energy-efficient programming and techniques for the efficient design, training and use of AI;\n(c) promoting AI literacy, in particular that of persons dealing with the development, operation and use of AI;\n(d) facilitating an inclusive and diverse design of AI systems, including through the establishment of inclusive and diverse development teams and the promotion of stakeholders’ participation in that process;\n(e) assessing and preventing the negative impact of AI systems on vulnerable persons or groups of vulnerable persons, including as regards accessibility for persons with a disability, as well as on gender equality.\n3. Codes of conduct may be drawn up by individual providers or deployers of AI systems or by organisations representing them or by both, including with the involvement of any interested stakeholders and their representative organisations, including civil society organisations and academia. Codes of conduct may cover one or more AI systems taking into account the similarity of the intended purpose of the relevant systems.\n4. The AI Office and the Member States shall take into account the specific interests and needs of SMEs, including start-ups, when encouraging and facilitating the drawing up of codes of conduct.",
      "original_content": "# KAPITEL X: VERHALTENSKODIZES UND LEITLINIEN\n### Artikel 95: Verhaltenskodizes für die freiwillige Anwendung bestimmter Anforderungen\n(1) Das Büro für Künstliche Intelligenz und die Mitgliedstaaten fördern und erleichtern die Aufstellung von Verhaltenskodizes, einschließlich damit zusammenhängender Governance-Mechanismen, mit denen die freiwillige Anwendung einiger oder aller der in Kapitel III Abschnitt 2 genannten Anforderungen auf KI-Systeme, die kein hohes Risiko bergen, gefördert werden soll, wobei den verfügbaren technischen Lösungen und bewährten Verfahren der Branche, die die Anwendung dieser Anforderungen ermöglichen, Rechnung zu tragen ist.\n(2) Das Büro für Künstliche Intelligenz und die Mitgliedstaaten erleichtern die Aufstellung von Verhaltenskodizes in Bezug auf die freiwillige Anwendung spezifischer Anforderungen auf alle KI-Systeme, einschließlich durch Betreiber, auf der Grundlage klarer Zielsetzungen sowie wesentlicher Leistungsindikatoren zur Messung der Erfüllung dieser Zielsetzungen, einschließlich unter anderem folgender Elemente:\na) in den Ethik-Leitlinien der Union für eine vertrauenswürdige KI enthaltene anwendbare Elemente;\nb) Beurteilung und Minimierung der Auswirkungen von KI-Systemen auf die ökologische Nachhaltigkeit, einschließlich im Hinblick auf energieeffizientes Programmieren, und Techniken, um KI effizient zu gestalten, zu trainieren und zu nutzen;\nc) Förderung der KI-Kompetenz, insbesondere der von Personen, die mit der Entwicklung, dem Betrieb und der Nutzung von KI befasst sind;\nd) Erleichterung einer inklusiven und vielfältigen Gestaltung von KI-Systemen, unter anderem durch die Einsetzung inklusiver und vielfältiger Entwicklungsteams und die Förderung der Beteiligung der Interessenträger an diesem Prozess;\ne) Bewertung und Verhinderung der negativen Auswirkungen von KI-Systemen auf schutzbedürftige Personen oder Gruppen schutzbedürftiger Personen, einschließlich im Hinblick auf die Barrierefreiheit für Personen mit Behinderungen, sowie auf die Gleichstellung der Geschlechter.\n(3) Verhaltenskodizes können von einzelnen KI-System-Anbietern oder -Betreibern oder von Interessenvertretungen dieser Anbieter oder Betreiber oder von beiden aufgestellt werden, auch unter Einbeziehung von Interessenträgern sowie deren Interessenvertretungen einschließlich Organisationen der Zivilgesellschaft und Hochschulen. Verhaltenskodizes können sich auf ein oder mehrere KI-Systeme erstrecken, um ähnlichen Zweckbestimmungen der jeweiligen Systeme Rechnung zu tragen.\n(4) Das Büro für Künstliche Intelligenz und die Mitgliedstaaten berücksichtigen die besonderen Interessen und Bedürfnisse von KMU, einschließlich Startups, bei der Förderung und Erleichterung der Aufstellung von Verhaltenskodizes."
    },
    {
      "chunk_idx": 300,
      "id": "3d0fab99-63f1-4b12-97f4-6a481cd86228",
      "title": "Art 96",
      "relevantChunksIds": [
        "7cf70e13-b212-45e3-bf7b-66d860a91315",
        "1b90fb32-5b4f-463a-9396-217d47295c8d"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 96: Guidelines from the Commission on the implementation of this Regulation\n1. The Commission shall develop guidelines on the practical implementation of this Regulation, and in particular on:\n(a) the application of the requirements and obligations referred to in Articles 8 to 15 and in Article 25;\n(b) the prohibited practices referred to in Article 5;\n(c) the practical implementation of the provisions related to substantial modification;\n(d) the practical implementation of transparency obligations laid down in Article 50;\n(e) detailed information on the relationship of this Regulation with the Union harmonisation legislation listed in Annex I, as well as with other relevant Union law, including as regards consistency in their enforcement;\n\nThe guidelines referred to in the first subparagraph of this paragraph shall take due account of the generally acknowledged state of the art on AI, as well as of relevant harmonised standards and common specifications that are referred to in Articles 40 and 41, or of those harmonised standards or technical specifications that are set out pursuant to Union harmonisation law.\n2. At the request of the Member States or the AI Office, or on its own initiative, the Commission shall update guidelines previously adopted when deemed necessary.",
      "original_content": "### Artikel 96: Leitlinien der Kommission zur Durchführung dieser Verordnung\n(1) Die Kommission erarbeitet Leitlinien für die praktische Umsetzung dieser Verordnung, die sich insbesondere auf Folgendes beziehen:\na) die Anwendung der in den Artikeln 8 bis 15 und in Artikel 25 genannten Anforderungen und Pflichten;\nb) die in Artikel 5 genannten verbotenen Praktiken;\nc) die praktische Durchführung der Bestimmungen über wesentliche Veränderungen;\nd) die praktische Umsetzung der Transparenzpflichten gemäß Artikel 50;\ne) detaillierte Informationen über das Verhältnis dieser Verordnung zu den in Anhang I aufgeführten Harmonisierungsrechtsvorschriften der Union sowie zu anderen einschlägigen Rechtsvorschriften der Union, auch in Bezug auf deren kohärente Durchsetzung;\nf) die Anwendung der Definition eines KI-Systems gemäß Artikel 3 Nummer 1. Wenn die Kommission solche Leitlinien herausgibt, widmet sie den Bedürfnissen von KMU einschließlich Start-up-Unternehmen, von lokalen Behörden und von den am wahrscheinlichsten von dieser Verordnung betroffenen Sektoren besondere Aufmerksamkeit.\nDie Leitlinien gemäß Unterabsatz 1 dieses Absatzes tragen dem allgemein anerkannten Stand der Technik im Bereich KI sowie den einschlägigen harmonisierten Normen und gemeinsamen Spezifikationen, auf die in den Artikeln 40 und 41 Bezug genommen wird, oder den harmonisierten Normen oder technischen Spezifikationen, die gemäß den Harmonisierungsrechtsvorschriften der Union festgelegt wurden, gebührend Rechnung.\n(2) Auf Ersuchen der Mitgliedstaaten oder des Büros für Künstliche Intelligenz oder von sich aus aktualisiert die Kommission früher verabschiedete Leitlinien, wenn es als notwendig erachtet wird."
    },
    {
      "chunk_idx": 301,
      "id": "31a6ae2c-6df1-4c50-b88e-8956375096cd",
      "title": "Art 97",
      "relevantChunksIds": [
        "b1c54e3f-0856-463c-a767-339866de57a1"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "# CHAPTER XI: DELEGATION OF POWER AND COMMITTEE PROCEDURE\n### Article 97: Exercise of the delegation\n1. The power to adopt delegated acts is conferred on the Commission subject to the conditions laid down in this Article.\n2. The power to adopt delegated acts referred to in Article 6(6) and (7), Article 7(1) and (3), Article 11(3), Article 43(5) and (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) and (6) shall be conferred on the Commission for a period of five years from 1 August 2024. The Commission shall draw up a report in respect of the delegation of power not later than nine months before the end of the five-year period. The delegation of power shall be tacitly extended for periods of an identical duration, unless the European Parliament or the Council opposes such extension not later than three months before the end of each period.\n3. The delegation of power referred to in Article 6(6) and (7), Article 7(1) and (3), Article 11(3), Article 43(5) and (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) and (6) may be revoked at any time by the European Parliament or by the Council. A decision of revocation shall put an end to the delegation of power specified in that decision. It shall take effect the day following that of its publication in the Official Journal of the European Union or at a later date specified therein. It shall not affect the validity of any delegated acts already in force.\n4. Before adopting a delegated act, the Commission shall consult experts designated by each Member State in accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making.\n5. As soon as it adopts a delegated act, the Commission shall notify it simultaneously to the European Parliament and to the Council.\n6. Any delegated act adopted pursuant to Article 6(6) or (7), Article 7(1) or (3), Article 11(3), Article 43(5) or (6), Article 47(5), Article 51(3), Article 52(4) or Article 53(5) or (6) shall enter into force only if no objection has been expressed by either the European Parliament or the Council within a period of three months of notification of that act to the European Parliament and the Council or if, before the expiry of that period, the European Parliament and the Council have both informed the Commission that they will not object. That period shall be extended by three months at the initiative of the European Parliament or of the Council.",
      "original_content": "# KAPITEL XI: BEFUGNISÜBERTRAGUNG UND AUSSCHUSSVERFAHREN\n### Artikel 97: Ausübung der Befugnisübertragung\n(1) Die Befugnis zum Erlass delegierter Rechtsakte wird der Kommission unter den in diesem Artikel festgelegten Bedingungen übertragen.\n(2) Die in Artikel 6 Absätze 6 und 7, Artikel 7 Absätze 1 und 3, Artikel 11 Absatz 3, Artikel 43 Absätze 5 und 6, Artikel 47 Absatz 5, Artikel 51 Absatz 3, Artikel 52 Absatz 4 sowie Artikel 53 Absätze 5 und 6 genannte Befugnis zum Erlass delegierter Rechtsakte wird der Kommission für einen Zeitraum von fünf Jahren ab 1. August 2024 übertragen. Die Kommission erstellt spätestens neun Monate vor Ablauf des Zeitraums von fünf Jahren einen Bericht über die Befugnisübertragung. Die Befugnisübertragung verlängert sich stillschweigend um Zeiträume gleicher Länge, es sei denn, das Europäische Parlament oder der Rat widersprechen einer solchen Verlängerung spätestens drei Monate vor Ablauf des jeweiligen Zeitraums.\n(3) Die Befugnisübertragung gemäß Artikel 6 Absätze 6 und 7, Artikel 7 Absätze 1 und 3, Artikel 11 Absatz 3, Artikel 43 Absätze 5 und 6, Artikel 47 Absatz 5, Artikel 51 Absatz 3, Artikel 52 Absatz 4 sowie Artikel 53 Absätze 5 und 6 kann vom Europäischen Parlament oder vom Rat jederzeit widerrufen werden. Der Beschluss über den Widerruf beendet die Übertragung der in jenem Beschluss angegebenen Befugnis. Er wird am Tag nach seiner Veröffentlichung im Amtsblatt der Europäischen Union oder zu einem darin angegebenen späteren Zeitpunkt wirksam. Die Gültigkeit von delegierten Rechtsakten, die bereits in Kraft sind, wird von dem Beschluss über den Widerruf nicht berührt.\n(4) Vor dem Erlass eines delegierten Rechtsakts konsultiert die Kommission die von den einzelnen Mitgliedstaaten benannten Sachverständigen im Einklang mit den in der Interinstitutionellen Vereinbarung vom 13. April 2016 über bessere Rechtsetzung niedergelegten Grundsätzen.\n(5) Sobald die Kommission einen delegierten Rechtsakt erlässt, übermittelt sie ihn gleichzeitig dem Europäischen Parlament und dem Rat.\n(6) Ein delegierter Rechtsakt, der nach Artikel 6 Absatz 6 oder 7, Artikel 7 Absatz 1 oder 3, Artikel 11 Absatz 3, Artikel 43 Absatz 5 oder 6, Artikel 47 Absatz 5, Artikel 51 Absatz 3, Artikel 52 Absatz 4 sowie Artikel 53 Absatz 5 oder 6 erlassen wurde, tritt nur in Kraft, wenn weder das Europäische Parlament noch der Rat innerhalb einer Frist von drei Monaten nach Übermittlung jenes Rechtsakts an das Europäische Parlament und den Rat Einwände erhoben haben oder wenn vor Ablauf dieser Frist das Europäische Parlament und der Rat beide der Kommission mitgeteilt haben, dass sie keine Einwände erheben werden. Auf Initiative des Europäischen Parlaments oder des Rates wird diese Frist um drei Monate verlängert."
    },
    {
      "chunk_idx": 302,
      "id": "316b96d1-61c7-4a5f-8ddd-a8337fc3ae86",
      "title": "Art 98",
      "relevantChunksIds": [
        "b1c54e3f-0856-463c-a767-339866de57a1"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 98: Committee procedure\n",
      "original_content": "### Artikel 98: Ausschussverfahren\n(1) Die Kommission wird von einem Ausschuss unterstützt. Dieser Ausschuss ist ein Ausschuss im Sinne der Verordnung (EU) Nr. 182/2011. (2) Wird auf diesen Absatz Bezug genommen, so gilt Artikel 5 der Verordnung (EU) Nr. 182/2011."
    },
    {
      "chunk_idx": 303,
      "id": "61300139-468e-4f24-871e-61a39036b15f",
      "title": "Art 99",
      "relevantChunksIds": [
        "984548c3-63d4-4d21-99dc-c4f542c22ab3",
        "c837084f-afdb-431e-b95a-b1d3bdacff9e",
        "48edb908-8ded-493b-aa2d-f7864af2bd26"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "# CHAPTER XII: PENALTIES\n### Article 99: Penalties\n1. In accordance with the terms and conditions laid down in this Regulation, Member States shall lay down the rules on penalties and other enforcement measures, which may also include warnings and non-monetary measures, applicable to infringements of this Regulation by operators, and shall take all measures necessary to ensure that they are properly and effectively implemented, thereby taking into account the guidelines issued by the Commission pursuant to Article 96. The penalties provided for shall be effective, proportionate and dissuasive. They shall take into account the interests of SMEs, including start-ups, and their economic viability.\n2. The Member States shall, without delay and at the latest by the date of entry into application, notify the Commission of the rules on penalties and of other enforcement measures referred to in paragraph 1, and shall notify it, without delay, of any subsequent amendment to them.\n3. Non-compliance with the prohibition of the AI practices referred to in Article 5 shall be subject to administrative fines of up to EUR 35 000 000 or, if the offender is an undertaking, up to 7 % of its total worldwide annual turnover for the preceding financial year, whichever is higher.\n4. Non-compliance with any of the following provisions related to operators or notified bodies, other than those laid down in Articles 5, shall be subject to administrative fines of up to EUR 15 000 000 or, if the offender is an undertaking, up to 3 % of its total worldwide annual turnover for the preceding financial year, whichever is higher:\n(a) obligations of providers pursuant to Article 16;\n(b) obligations of authorised representatives pursuant to Article 22;\n(c) obligations of importers pursuant to Article 23;\n(d) obligations of distributors pursuant to Article 24;\n(e) obligations of deployers pursuant to Article 26;\n(f) requirements and obligations of notified bodies pursuant to Article 31, Article 33(1), (3) and (4) or Article 34;\n\n6. In the case of SMEs, including start-ups, each fine referred to in this Article shall be up to the percentages or amount referred to in paragraphs 3, 4 and 5, whichever thereof is lower.\n7. When deciding whether to impose an administrative fine and when deciding on the amount of the administrative fine in each individual case, all relevant circumstances of the specific situation shall be taken into account and, as appropriate, regard shall be given to the following:\n(a) the nature, gravity and duration of the infringement and of its consequences, taking into account the purpose of the AI system, as well as, where appropriate, the number of affected persons and the level of damage suffered by them;\n(b) whether administrative fines have already been applied by other market surveillance authorities to the same operator for the same infringement;\n(c) whether administrative fines have already been applied by other authorities to the same operator for infringements of other Union or national law, when such infringements result from the same activity or omission constituting a relevant infringement of this Regulation;\n(d) the size, the annual turnover and market share of the operator committing the infringement;\n(e) any other aggravating or mitigating factor applicable to the circumstances of the case, such as financial benefits gained, or losses avoided, directly or indirectly, from the infringement;\n(f) the degree of cooperation with the national competent authorities, in order to remedy the infringement and mitigate the possible adverse effects of the infringement;\n(g) the degree of responsibility of the operator taking into account the technical and organisational measures implemented by it;\n(h) the manner in which the infringement became known to the national competent authorities, in particular whether, and if so to what extent, the operator notified the infringement;\n(i) the intentional or negligent character of the infringement;\n(j) any action taken by the operator to mitigate the harm suffered by the affected persons.\n8. Each Member State shall lay down rules on to what extent administrative fines may be imposed on public authorities and bodies established in that Member State.\n9. Depending on the legal system of the Member States, the rules on administrative fines may be applied in such a manner that the fines are imposed by competent national courts or by other bodies, as applicable in those Member States. The application of such rules in those Member States shall have an equivalent effect.\n10. The exercise of powers under this Article shall be subject to appropriate procedural safeguards in accordance with Union and national law, including effective judicial remedies and due process.\n11. Member States shall, on an annual basis, report to the Commission about the administrative fines they have issued during that year, in accordance with this Article, and about any related litigation or judicial proceedings.",
      "original_content": "# KAPITEL XII: SANKTIONEN\n### Artikel 99: Sanktionen\n(1) Entsprechend den Vorgaben dieser Verordnung erlassen die Mitgliedstaaten Vorschriften für Sanktionen und andere Durchsetzungsmaßnahmen, zu denen auch Verwarnungen und nichtmonetäre Maßnahmen gehören können, die bei Verstößen gegen diese Verordnung durch Akteure Anwendung finden, und ergreifen alle Maßnahmen, die für deren ordnungsgemäße und wirksame Durchsetzung notwendig sind, wobei die von der Kommission gemäß Artikel 96 erteilten Leitlinien zu berücksichtigen sind. Die vorgesehenen Sanktionen müssen wirksam, verhältnismäßig und abschreckend sein. Sie berücksichtigen die Interessen von KMU, einschließlich Start-up-Unternehmen, sowie deren wirtschaftliches Überleben.\n(2) Die Mitgliedstaaten teilen der Kommission die Vorschriften für Sanktionen und andere Durchsetzungsmaßnahmen gemäß Absatz 1 unverzüglich und spätestens zum Zeitpunkt ihres Inkrafttretens mit und melden ihr unverzüglich etwaige spätere Änderungen.\n(3) Bei Missachtung des Verbots der in Artikel 5 genannten KI-Praktiken werden Geldbußen von bis zu 35 000 000 EUR oder — im Falle von Unternehmen — von bis zu 7 % des gesamten weltweiten Jahresumsatzes des vorangegangenen Geschäftsjahres verhängt, je nachdem, welcher Betrag höher ist.\n(4) Für Verstöße gegen folgende für Akteure oder notifizierte Stellen geltende Bestimmungen, mit Ausnahme der in Artikel 5 genannten, werden Geldbußen von bis zu 15 000 000 EUR oder — im Falle von Unternehmen — von bis zu 3 % des gesamten weltweiten Jahresumsatzes des vorangegangenen Geschäftsjahres verhängt, je nachdem, welcher Betrag höher ist:\na) Pflichten der Anbieter gemäß Artikel 16;\nb) Pflichten der Bevollmächtigten gemäß Artikel 22;\nc) Pflichten der Einführer gemäß Artikel 23;\nd) Pflichten der Händler gemäß Artikel 24;\ne) Pflichten der Betreiber gemäß Artikel 26;\nf) für notifizierte Stellen geltende Anforderungen und Pflichten gemäß Artikel 31, Artikel 33 Absätze 1, 3 und 4 bzw. Artikel 34;\ng) Transparenzpflichten für Anbieter und Betreiber gemäß Artikel 50. (5) Werden notifizierten Stellen oder zuständigen nationalen Behörden auf deren Auskunftsersuchen hin falsche, unvollständige oder irreführende Informationen bereitgestellt, so werden Geldbußen von bis zu 7 500 000 EUR oder — im Falle von Unternehmen — von bis zu 1 % des gesamten weltweiten Jahresumsatzes des vorangegangenen Geschäftsjahres verhängt, je nachdem, welcher Betrag höher ist.\n(6) Im Falle von KMU, einschließlich Start-up-Unternehmen, gilt für jede in diesem Artikel genannte Geldbuße der jeweils niedrigere Betrag aus den in den Absätzen 3, 4 und 5 genannten Prozentsätzen oder Summen.\n(7) Bei der Entscheidung, ob eine Geldbuße verhängt wird, und bei der Festsetzung der Höhe der Geldbuße werden in jedem Einzelfall alle relevanten Umstände der konkreten Situation sowie gegebenenfalls Folgendes berücksichtigt:\na) Art, Schwere und Dauer des Verstoßes und seiner Folgen, unter Berücksichtigung des Zwecks des KI-Systems sowie gegebenenfalls der Zahl der betroffenen Personen und des Ausmaßes des von ihnen erlittenen Schadens;\nb) ob demselben Akteur bereits von anderen Marktüberwachungsbehörden für denselben Verstoß Geldbußen auferlegt wurden;\nc) ob demselben Akteur bereits von anderen Behörden für Verstöße gegen das Unionsrecht oder das nationale Recht Geldbußen auferlegt wurden, wenn diese Verstöße auf dieselbe Handlung oder Unterlassung zurückzuführen sind, die einen einschlägigen Verstoß gegen diese Verordnung darstellt;\nd) Größe, Jahresumsatz und Marktanteil des Akteurs, der den Verstoß begangen hat;\ne) jegliche anderen erschwerenden oder mildernden Umstände im jeweiligen Fall, wie etwa unmittelbar oder mittelbar durch den Verstoß erlangte finanzielle Vorteile oder vermiedene Verluste;\nf) Grad der Zusammenarbeit mit den zuständigen nationalen Behörden, um den Verstoß abzustellen und die möglichen nachteiligen Auswirkungen des Verstoßes abzumildern;\ng) Grad an Verantwortung des Akteurs unter Berücksichtigung der von ihm ergriffenen technischen und organisatorischen Maßnahmen;\nh) Art und Weise, wie der Verstoß den zuständigen nationalen Behörden bekannt wurde, insbesondere ob und gegebenenfalls in welchem Umfang der Akteur den Verstoß gemeldet hat;\ni) Vorsätzlichkeit oder Fahrlässigkeit des Verstoßes;\nj) alle Maßnahmen, die der Akteur ergriffen hat, um den Schaden, der den betroffenen Personen zugefügt wird, zu mindern.\n(8) Jeder Mitgliedstaat erlässt Vorschriften darüber, in welchem Umfang gegen Behörden und öffentliche Stellen, die in dem betreffenden Mitgliedstaat niedergelassen sind, Geldbußen verhängt werden können.\n(9) In Abhängigkeit vom Rechtssystem der Mitgliedstaaten können die Vorschriften über Geldbußen je nach den dort geltenden Regeln so angewandt werden, dass die Geldbußen von den zuständigen nationalen Gerichten oder von sonstigen Stellen verhängt werden. Die Anwendung dieser Vorschriften in diesen Mitgliedstaaten muss eine gleichwertige Wirkung haben.\n(10) Die Ausübung der Befugnisse gemäß diesem Artikel muss angemessenen Verfahrensgarantien gemäß dem Unionsrecht und dem nationalen Recht, einschließlich wirksamer gerichtlicher Rechtsbehelfe und ordnungsgemäßer Verfahren, unterliegen.\n(11) Die Mitgliedstaaten erstatten der Kommission jährlich Bericht über die Geldbußen, die sie in dem betreffenden Jahr gemäß diesem Artikel verhängt haben, und über damit zusammenhängende Rechtsstreitigkeiten oder Gerichtsverfahren."
    },
    {
      "chunk_idx": 304,
      "id": "09a1468c-9fc4-44ee-a8c1-fd07084ea0d8",
      "title": "Art 100",
      "relevantChunksIds": [
        "c837084f-afdb-431e-b95a-b1d3bdacff9e",
        "48edb908-8ded-493b-aa2d-f7864af2bd26"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 100: Administrative fines on Union institutions, bodies, offices and agencies\n1. The European Data Protection Supervisor may impose administrative fines on Union institutions, bodies, offices and agencies falling within the scope of this Regulation. When deciding whether to impose an administrative fine and when deciding on the amount of the administrative fine in each individual case, all relevant circumstances of the specific situation shall be taken into account and due regard shall be given to the following:\n(a) the nature, gravity and duration of the infringement and of its consequences, taking into account the purpose of the AI system concerned, as well as, where appropriate, the number of affected persons and the level of damage suffered by them;\n(b) the degree of responsibility of the Union institution, body, office or agency, taking into account technical and organisational measures implemented by them;\n(c) any action taken by the Union institution, body, office or agency to mitigate the damage suffered by affected persons;\n(d) the degree of cooperation with the European Data Protection Supervisor in order to remedy the infringement and mitigate the possible adverse effects of the infringement, including compliance with any of the measures previously ordered by the European Data Protection Supervisor against the Union institution, body, office or agency concerned with regard to the same subject matter;\n(e) any similar previous infringements by the Union institution, body, office or agency;\n(f) the manner in which the infringement became known to the European Data Protection Supervisor, in particular whether, and if so to what extent, the Union institution, body, office or agency notified the infringement;\n(g) the annual budget of the Union institution, body, office or agency.\n2. Non-compliance with the prohibition of the AI practices referred to in Article 5 shall be subject to administrative fines of up to EUR 1 500 000.\n3. The non-compliance of the AI system with any requirements or obligations under this Regulation, other than those laid down in Article 5, shall be subject to administrative fines of up to EUR 750 000.\n4. Before taking decisions pursuant to this Article, the European Data Protection Supervisor shall give the Union institution, body, office or agency which is the subject of the proceedings conducted by the European Data Protection Supervisor the opportunity of being heard on the matter regarding the possible infringement. The European Data Protection Supervisor shall base his or her decisions only on elements and circumstances on which the parties concerned have been able to comment. Complainants, if any, shall be associated closely with the proceedings.\n5. The rights of defence of the parties concerned shall be fully respected in the proceedings. They shall be entitled to have access to the European Data Protection Supervisor’s file, subject to the legitimate interest of individuals or undertakings in the protection of their personal data or business secrets.\n6. Funds collected by imposition of fines in this Article shall contribute to the general budget of the Union. The fines shall not affect the effective operation of the Union institution, body, office or agency fined.\n7. The European Data Protection Supervisor shall, on an annual basis, notify the Commission of the administrative fines it has imposed pursuant to this Article and of any litigation or judicial proceedings it has initiated.",
      "original_content": "### Artikel 100: Verhängung von Geldbußen gegen Organe, Einrichtungen und sonstige Stellen der Union\n(1) Der Europäische Datenschutzbeauftragte kann gegen Organe, Einrichtungen und sonstige Stellen der Union, die in den Anwendungsbereich dieser Verordnung fallen, Geldbußen verhängen. Bei der Entscheidung, ob eine Geldbuße verhängt wird, und bei der Festsetzung der Höhe der Geldbuße werden in jedem Einzelfall alle relevanten Umstände der konkreten Situation sowie Folgendes gebührend berücksichtigt:\na) Art, Schwere und Dauer des Verstoßes und dessen Folgen, unter Berücksichtigung des Zwecks des betreffenden KI-Systems sowie gegebenenfalls der Zahl der betroffenen Personen und des Ausmaßes des von ihnen erlittenen Schadens;\nb) Grad der Verantwortung des Organs, der Einrichtung oder der sonstigen Stelle der Union unter Berücksichtigung der von diesem bzw. dieser ergriffenen technischen und organisatorischen Maßnahmen;\nc) alle Maßnahmen, die das Organ, die Einrichtung oder die sonstige Stelle der Union zur Minderung des von den betroffenen Personen erlittenen Schadens ergriffen hat;\nd) das Maß der Zusammenarbeit mit dem Europäischen Datenschutzbeauftragten bei der Behebung des Verstoßes und der Minderung seiner möglichen nachteiligen Auswirkungen, einschließlich der Befolgung von Maßnahmen, die der Europäische Datenschutzbeauftragte dem Organ, der Einrichtung oder der sonstigen Stelle der Union im Hinblick auf denselben Gegenstand zuvor bereits auferlegt hatte;\ne) ähnliche frühere Verstöße des Organs, der Einrichtung oder der sonstigen Stelle der Union;\nf) Art und Weise, wie der Verstoß dem Europäischen Datenschutzbeauftragten bekannt wurde, insbesondere ob und gegebenenfalls in welchem Umfang das Organ, die Einrichtung oder die sonstige Stelle der Union den Verstoß gemeldet hat;\ng) der Jahreshaushalt des Organs, der Einrichtung oder der sonstigen Stelle der Union.\n(2) Bei Missachtung des Verbots der in Artikel 5 genannten KI-Praktiken werden Geldbußen von bis zu 1 500 000 EUR verhängt.\n(3) Bei Nichtkonformität des KI-Systems mit in dieser Verordnung festgelegten Anforderungen oder Pflichten, mit Ausnahme der in Artikel 5 festgelegten, werden Geldbußen von bis zu 750 000 EUR verhängt.\n(4) Bevor der Europäische Datenschutzbeauftragte Entscheidungen nach dem vorliegenden Artikel trifft, gibt er dem Organ, der Einrichtung oder der sonstigen Stelle der Union, gegen das bzw. die sich das von ihm geführte Verfahren richtet, Gelegenheit, sich zum Vorwurf des Verstoßes zu äußern. Der Europäische Datenschutzbeauftragte stützt seine Entscheidungen nur auf die Elemente und Umstände, zu denen sich die betreffenden Parteien äußern können. Beschwerdeführer, soweit vorhanden, müssen in das Verfahren eng einbezogen werden.\n(5) Die Verteidigungsrechte der betroffenen Parteien werden während des Verfahrens in vollem Umfang gewahrt. Vorbehaltlich der legitimen Interessen von Einzelpersonen oder Unternehmen im Hinblick auf den Schutz ihrer personenbezogenen Daten oder Geschäftsgeheimnisse haben die betroffenen Parteien Anspruch auf Einsicht in die Unterlagen des Europäischen Datenschutzbeauftragten.\n(6) Das Aufkommen aus den nach diesem Artikel verhängten Geldbußen trägt zum Gesamthaushalt der Union bei. Die Geldbußen dürfen nicht den wirksamen Betrieb des Organs, der Einrichtung oder der sonstigen Stelle der Union beeinträchtigen, dem bzw. der die Geldbuße auferlegt wurde.\n(7) Der Europäische Datenschutzbeauftragte macht der Kommission jährlich Mitteilung über die Geldbußen, die er nach Maßgabe dieses Artikels verhängt hat, und über die von ihm eingeleiteten Rechtsstreitigkeiten oder Gerichtsverfahren."
    },
    {
      "chunk_idx": 305,
      "id": "de889fca-779f-4b70-8ae2-166b1f752102",
      "title": "Art 101",
      "relevantChunksIds": [
        "c837084f-afdb-431e-b95a-b1d3bdacff9e",
        "48edb908-8ded-493b-aa2d-f7864af2bd26"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 101: Fines for providers of general-purpose AI models\n1. The Commission may impose on providers of general-purpose AI models fines not exceeding 3 % of their annual total worldwide turnover in the preceding financial year or EUR 15 000 000, whichever is higher., when the Commission finds that the provider intentionally or negligently:\n(a) infringed the relevant provisions of this Regulation;\n(b) failed to comply with a request for a document or for information pursuant to Article 91, or supplied incorrect, incomplete or misleading information;\n(c) failed to comply with a measure requested under Article 93;\n(d) failed to make available to the Commission access to the general-purpose AI model or general-purpose AI model with systemic risk with a view to conducting an evaluation pursuant to Article 92.\nIn fixing the amount of the fine or periodic penalty payment, regard shall be had to the nature, gravity and duration of the infringement, taking due account of the principles of proportionality and appropriateness. The Commission shall also into account commitments made in accordance with Article 93(3) or made in relevant codes of practice in accordance with Article 56.\n2. Before adopting the decision pursuant to paragraph 1, the Commission shall communicate its preliminary findings to the provider of the general-purpose AI model and give it an opportunity to be heard.\n3. Fines imposed in accordance with this Article shall be effective, proportionate and dissuasive.\n4. Information on fines imposed under this Article shall also be communicated to the Board as appropriate.\n5. The Court of Justice of the European Union shall have unlimited jurisdiction to review decisions of the Commission fixing a fine under this Article. It may cancel, reduce or increase the fine imposed.\n6. The Commission shall adopt implementing acts containing detailed arrangements and procedural safeguards for proceedings in view of the possible adoption of decisions pursuant to paragraph 1 of this Article. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 98(2).",
      "original_content": "### Artikel 101: Geldbußen für Anbieter von KI-Modellen mit allgemeinem Verwendungszweck\n(1) Die Kommission kann gegen Anbieter von KI-Modellen mit allgemeinem Verwendungszweck Geldbußen von bis zu 3 % ihres gesamten weltweiten Jahresumsatzes im vorangegangenen Geschäftsjahr oder 15 000 000 EUR verhängen, je nachdem, welcher Betrag höher ist, wenn sie feststellt, dass der Anbieter vorsätzlich oder fahrlässig\na) gegen die einschlägigen Bestimmungen dieser Verordnung verstoßen hat;\nb) der Anforderung eines Dokuments oder von Informationen gemäß Artikel 91 nicht nachgekommen ist oder falsche, unvollständige oder irreführende Informationen bereitgestellt hat;\nc) einer gemäß Artikel 93 geforderten Maßnahme nicht nachgekommen ist;\nd) der Kommission keinen Zugang zu dem KI-Modell mit allgemeinem Verwendungszweck oder dem KI-Modell mit allgemeinem Verwendungszweck mit systemischem Risiko gewährt hat, um eine Bewertung gemäß Artikel 92 durchzuführen.\nBei der Festsetzung der Höhe der Geldbuße oder des Zwangsgelds wird der Art, der Schwere und der Dauer des Verstoßes sowie den Grundsätzen der Verhältnismäßigkeit und der Angemessenheit gebührend Rechnung getragen. Die Kommission berücksichtigt außerdem Verpflichtungen, die gemäß Artikel 93 Absatz 3 oder in den einschlägigen Praxisleitfäden nach Artikel 56 gemacht wurden.\n(2) Vor der Annahme einer Entscheidung nach Absatz 1 teilt die Kommission dem Anbieter des KI-Modells mit allgemeinem Verwendungszweck ihre vorläufige Beurteilung mit und gibt ihm Gelegenheit, Stellung zu nehmen.\n(3) Die gemäß diesem Artikel verhängten Geldbußen müssen wirksam, verhältnismäßig und abschreckend sein.\n(4) Informationen über gemäß diesem Artikel verhängte Geldbußen werden gegebenenfalls dem KI-Gremium mitgeteilt.\n(5) Der Gerichtshof der Europäischen Union hat die unbeschränkte Befugnis zur Überprüfung der Entscheidungen der Kommission über die Festsetzung einer Geldbuße gemäß diesem Artikel. Er kann die verhängte Geldbuße aufheben, herabsetzen oder erhöhen.\n(6) Die Kommission erlässt Durchführungsrechtsakte mit detaillierten Regelungen und Verfahrensgarantien für die Verfahren im Hinblick auf den möglichen Erlass von Beschlüssen gemäß Absatz 1 dieses Artikels. Diese Durchführungsrechtsakte werden gemäß dem in Artikel 98 Absatz 2 genannten Prüfverfahren erlassen."
    },
    {
      "chunk_idx": 306,
      "id": "78ec7e62-bc3c-4239-afad-86ca34ac4746",
      "title": "Art 102",
      "relevantChunksIds": [
        "0b3d80d6-3316-4d5d-85a8-6477c568dfbe"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "# CHAPTER XIII: FINAL PROVISIONS\n### Article 102: Amendment to Regulation (EC) No 300/2008\nIn Article 4(3) of Regulation (EC) No 300/2008, the following subparagraph is added:\n‘When adopting detailed measures related to technical specifications and procedures for approval and use of security equipment concerning Artificial Intelligence systems within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.",
      "original_content": "# KAPITEL XIII: SCHLUSSBESTIMMUNGEN\n### Artikel 102: Änderung der Verordnung (EG) Nr. 300/2008\nIn Artikel 4 Absatz 3 der Verordnung (EG) Nr. 300/2008 wird folgender Unterabsatz angefügt:\n„Beim Erlass detaillierter Maßnahmen, die technische Spezifikationen und Verfahren für die Genehmigung und den Einsatz von Sicherheitsausrüstung betreffen, bei der auch Systeme der künstlichen Intelligenz im Sinne der Verordnung (EU) 2024/1689 des Europäischen Parlaments und des Rates (*) zum Einsatz kommen, werden die in Kapitel III Abschnitt 2 jener Verordnung festgelegten Anforderungen berücksichtigt."
    },
    {
      "chunk_idx": 307,
      "id": "1b724be9-cb85-4ad6-97f8-2e4f22c482d3",
      "title": "Art 103",
      "relevantChunksIds": [
        "0b3d80d6-3316-4d5d-85a8-6477c568dfbe"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 103: Amendment to Regulation (EU) No 167/2013\nIn Article 17(5) of Regulation (EU) No 167/2013, the following subparagraph is added:\n‘When adopting delegated acts pursuant to the first subparagraph concerning artificial intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.",
      "original_content": "### Artikel 103: Änderung der Verordnung (EU) Nr. 167/2013\nIn Artikel 17 Absatz 5 der Verordnung (EU) Nr. 167/2013 wird folgender Unterabsatz angefügt:\n„Beim Erlass delegierter Rechtsakte nach Unterabsatz 1, die sich auf Systeme der künstlichen Intelligenz beziehen, bei denen es sich um Sicherheitsbauteile im Sinne der Verordnung (EU) 2024/1689 des Europäischen Parlaments und des Rates (*) handelt, werden die in Kapitel III Abschnitt 2 jener Verordnung festgelegten Anforderungen berücksichtigt."
    },
    {
      "chunk_idx": 308,
      "id": "0dc12b68-7c9a-435e-91e6-cede7d6adcb9",
      "title": "Art 104",
      "relevantChunksIds": [
        "0b3d80d6-3316-4d5d-85a8-6477c568dfbe"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 104: Amendment to Regulation (EU) No 168/2013\nIn Article 22(5) of Regulation (EU) No 168/2013, the following subparagraph is added:\n‘When adopting delegated acts pursuant to the first subparagraph concerning artificial intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.",
      "original_content": "### Artikel 104: Änderung der Verordnung (EU) Nr. 168/2013\nIn Artikel 22 Absatz 5 der Verordnung (EU) Nr. 168/2013 wird folgender Unterabsatz angefügt:\n„Beim Erlass delegierter Rechtsakte nach Unterabsatz 1, die sich auf Systeme der künstlichen Intelligenz beziehen, bei denen es sich um Sicherheitsbauteile im Sinne der Verordnung (EU) 2024/1689 des Europäischen Parlaments und des Rates (*) handelt, werden die in Kapitel III Abschnitt 2 jener Verordnung festgelegten Anforderungen berücksichtigt."
    },
    {
      "chunk_idx": 309,
      "id": "10ba704f-b5e6-4a7e-a2c8-7418525d1a4a",
      "title": "Art 105",
      "relevantChunksIds": [
        "0b3d80d6-3316-4d5d-85a8-6477c568dfbe"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 105: Amendment to Directive 2014/90/EU\nIn Article 8 of Directive 2014/90/EU, the following paragraph is added:\n‘5. For Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), when carrying out its activities pursuant to paragraph 1 and when adopting technical specifications and testing standards in accordance with paragraphs 2 and 3, the Commission shall take into account the requirements set out in Chapter III, Section 2, of that Regulation.",
      "original_content": "### Artikel 105: Änderung der Richtlinie 2014/90/EU\nIn Artikel 8 der Richtlinie 2014/90/EU wird folgender Absatz angefügt:\n„(5) Bei Systemen der künstlichen Intelligenz, bei denen es sich um Sicherheitsbauteile im Sinne der Verordnung (EU) 2024/1689 des Europäischen Parlaments und des Rates (*) handelt, berücksichtigt die Kommission bei der Ausübung ihrer Tätigkeiten nach Absatz 1 und bei Erlass technischer Spezifikationen und Prüfnormen nach den Absätzen 2 und 3 die in Kapitel III Abschnitt 2 jener Verordnung festgelegten Anforderungen."
    },
    {
      "chunk_idx": 310,
      "id": "b4d74881-6aab-4351-b7de-3b591441dd6f",
      "title": "Art 106",
      "relevantChunksIds": [
        "0b3d80d6-3316-4d5d-85a8-6477c568dfbe"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 106: Amendment to Directive (EU) 2016/797\nIn Article 5 of Directive (EU) 2016/797, the following paragraph is added:\n‘12. When adopting delegated acts pursuant to paragraph 1 and implementing acts pursuant to paragraph 11 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.",
      "original_content": "### Artikel 106: Änderung der Richtlinie (EU) 2016/797\nIn Artikel 5 der Richtlinie (EU) 2016/797 wird folgender Absatz angefügt:\n„(12) Beim Erlass von delegierten Rechtsakten nach Absatz 1 und von Durchführungsrechtsakten nach Absatz 11, die sich auf Systeme der künstlichen Intelligenz beziehen, bei denen es sich um Sicherheitsbauteile im Sinne der Verordnung (EU) 2024/1689 des Europäischen Parlaments und des Rates (*) handelt, werden die in Kapitel III Abschnitt 2 jener Verordnung festgelegten Anforderungen berücksichtigt."
    },
    {
      "chunk_idx": 311,
      "id": "261f7b87-d60b-4708-b420-d2552cf3e5ca",
      "title": "Art 107",
      "relevantChunksIds": [
        "0b3d80d6-3316-4d5d-85a8-6477c568dfbe"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 107: Amendment to Regulation (EU) 2018/858\nIn Article 5 of Regulation (EU) 2018/858 the following paragraph is added:\n‘4. When adopting delegated acts pursuant to paragraph 3 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.",
      "original_content": "### Artikel 107: Änderung der Verordnung (EU) 2018/858\nIn Artikel 5 der Verordnung (EU) 2018/858 wird folgender Absatz angefügt:\n„(4) Beim Erlass delegierter Rechtsakte nach Absatz 3, die sich auf Systeme der künstlichen Intelligenz beziehen, bei denen es sich um Sicherheitsbauteile im Sinne der Verordnung (EU) 2024/1689 des Europäischen Parlaments und des Rates (*) handelt, werden die in Kapitel III Abschnitt 2 jener Verordnung festgelegten Anforderungen berücksichtigt."
    },
    {
      "chunk_idx": 312,
      "id": "025462b7-9edf-4f37-b3a9-23f3c037f5db",
      "title": "Art 108",
      "relevantChunksIds": [
        "0b3d80d6-3316-4d5d-85a8-6477c568dfbe"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 108: Amendments to Regulation (EU) 2018/1139\nRegulation (EU) 2018/1139 is amended as follows:\n(1) in Article 17, the following paragraph is added:\n‘3. Without prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.\n(*)  Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/2024/1689/oj).’;\"\n(2) in Article 19, the following paragraph is added:\n‘4. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.’\n(3) in Article 43, the following paragraph is added:\n‘4. When adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.’\n(4) in Article 47, the following paragraph is added:\n‘3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.’\n(5) in Article 57, the following subparagraph is added:\n‘When adopting those implementing acts concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.’\n(6) in Article 58, the following paragraph is added:\n‘3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.’",
      "original_content": "### Artikel 108: Änderungen der Verordnung (EU) 2018/1139\nDie Verordnung (EU) 2018/1139 wird wie folgt geändert:\n1. In Artikel 17 wird folgender Absatz angefügt:\n„(3) Unbeschadet des Absatzes 2 werden beim Erlass von Durchführungsrechtsakten nach Absatz 1, die sich auf Systeme der künstlichen Intelligenz beziehen, bei denen es sich um Sicherheitsbauteile im Sinne der Verordnung (EU) 2024/1689 des Europäischen Parlaments und des Rates (*) handelt, die in Kapitel III Abschnitt 2 jener Verordnung festgelegten Anforderungen berücksichtigt.\n(*) Verordnung (EU) 2024/1689 des Europäischen Parlaments und des Rates vom 13. Juni 2024 zur Festlegung harmonisierter Vorschriften für künstliche Intelligenz und zur Änderung der Verordnungen (EG) Nr. 300/2008, (EU) Nr. 167/2013, (EU) Nr. 168/2013, (EU) 2018/858, (EU) 2018/1139 und (EU) 2019/2144 sowie der Richtlinien 2014/90/EU, (EU) 2016/797 und (EU) 2020/1828 (Verordnung über künstliche Intelligenz) (ABl. L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/2024/1689/oj).“ \"\n2. In Artikel 19 wird folgender Absatz angefügt:\n„(4) Beim Erlass delegierter Rechtsakte nach den Absätzen 1 und 2, die sich auf Systeme der künstlichen Intelligenz beziehen, bei denen es sich um Sicherheitsbauteile im Sinne der Verordnung (EU) 2024/1689 handelt, werden die in Kapitel III Abschnitt 2 jener Verordnung festgelegten Anforderungen berücksichtigt.“\n3. In Artikel 43 wird folgender Absatz angefügt:\n„(4) Beim Erlass von Durchführungsrechtsakten nach Absatz 1, die sich auf Systeme der künstlichen Intelligenz beziehen, bei denen es sich um Sicherheitsbauteile im Sinne der Verordnung (EU) 2024/1689 handelt, werden die in Kapitel III Abschnitt 2 jener Verordnung festgelegten Anforderungen berücksichtigt.“\n4. In Artikel 47 wird folgender Absatz angefügt:\n„(3) Beim Erlass delegierter Rechtsakte nach den Absätzen 1 und 2, die sich auf Systeme der künstlichen Intelligenz beziehen, bei denen es sich um Sicherheitsbauteile im Sinne der Verordnung (EU) 2024/1689 handelt, werden die in Kapitel III Abschnitt 2 jener Verordnung festgelegten Anforderungen berücksichtigt.“\n5. In Artikel 57 wird folgender Unterabsatz angefügt:\n„Beim Erlass solcher Durchführungsrechtsakte, die sich auf Systeme der künstlichen Intelligenz beziehen, bei denen es sich um Sicherheitsbauteile im Sinne der Verordnung (EU) 2024/1689 handelt, werden die in Kapitel III Abschnitt 2 jener Verordnung festgelegten Anforderungen berücksichtigt.“\n6. In Artikel 58 wird folgender Absatz angefügt:\n„(3) Beim Erlass delegierter Rechtsakte nach den Absätzen 1 und 2, die sich auf Systeme der künstlichen Intelligenz beziehen, bei denen es sich um Sicherheitsbauteile im Sinne der Verordnung (EU) 2024/1689 handelt, werden die in Kapitel III Abschnitt 2 jener Verordnung festgelegten Anforderungen berücksichtigt.“"
    },
    {
      "chunk_idx": 313,
      "id": "d788b690-5390-4f75-b1cb-08c2784a0b6f",
      "title": "Art 109",
      "relevantChunksIds": [
        "0b3d80d6-3316-4d5d-85a8-6477c568dfbe"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 109: Amendment to Regulation (EU) 2019/2144\nIn Article 11 of Regulation (EU) 2019/2144, the following paragraph is added:\n‘3. When adopting the implementing acts pursuant to paragraph 2, concerning artificial intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.",
      "original_content": "### Artikel 109: Änderung der Verordnung (EU) 2019/2144\nIn Artikel 11 der Verordnung (EU) 2019/2144 wird folgender Absatz angefügt:\n„(3) Beim Erlass von Durchführungsrechtsakten nach Absatz 2, die sich auf Systeme der künstlichen Intelligenz beziehen, bei denen es sich um Sicherheitsbauteile im Sinne der Verordnung (EU) 2024/1689 des Europäischen Parlaments und des Rates (*) handelt, werden die in Kapitel III Abschnitt 2 jener Verordnung festgelegten Anforderungen berücksichtigt."
    },
    {
      "chunk_idx": 314,
      "id": "b26862b2-50cc-4243-b8a6-00c623811356",
      "title": "Art 110",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 110: Amendment to Directive (EU) 2020/1828\nIn Annex I to Directive (EU) 2020/1828 of the European Parliament and of the Council (58), the following point is added:\n‘(68) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/2024/1689/oj).’.",
      "original_content": "### Artikel 110: Änderung der Richtlinie (EU) 2020/1828\nIn Anhang I der Richtlinie (EU) 2020/1828 des Europäischen Parlaments und des Rates (Fußnote 58) wird die folgende Nummer angefügt:\n„68. Verordnung (EU) 2024/1689 des Europäischen Parlaments und des Rates vom 13. Juni 2024 zur Festlegung harmonisierter Vorschriften für künstliche Intelligenz und zur Änderung der Verordnungen (EG) Nr. 300/2008, (EU) Nr. 167/2013, (EU) Nr. 168/2013, (EU) 2018/858, (EU) 2018/1139 und (EU) 2019/2144 sowie der Richtlinien 2014/90/EU, (EU) 2016/797 und (EU) 2020/1828 (Verordnung über künstliche Intelligenz) (ABl. L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/2024/1689/oj).“ Fußnote 58: Richtlinie (EU) 2020/1828 des Europäischen Parlaments und des Rates vom 25. November 2020 über Verbandsklagen zum Schutz der Kollektivinteressen der Verbraucher und zur Aufhebung der Richtlinie 2009/22/EG (ABl. L 409 vom 4.12.2020, S. 1)"
    },
    {
      "chunk_idx": 315,
      "id": "5a35c644-0096-47c7-88ed-c793b728774e",
      "title": "Art 111",
      "relevantChunksIds": [
        "3538822c-f1a4-4720-a75d-17a6b5e0c2e6",
        "6cb54c7b-96fd-468c-902d-7b11656ec6ad"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 111: AI systems already placed on the market or put into service and general-purpose AI models already placed on the marked\n1. Without prejudice to the application of Article 5 as referred to in Article 113(3), point (a), AI systems which are components of the large-scale IT systems established by the legal acts listed in Annex X that have been placed on the market or put into service before 2 August 2027 shall be brought into compliance with this Regulation by 31 December 2030.\nThe requirements laid down in this Regulation shall be taken into account in the evaluation of each large-scale IT system established by the legal acts listed in Annex X to be undertaken as provided for in those legal acts and where those legal acts are replaced or amended.\n",
      "original_content": "### Artikel 111: Bereits in Verkehr gebrachte oder in Betrieb genommene KI-Systeme und bereits in Verkehr gebrachte KI-Modelle mit allgemeinem Verwendungszweck\n(1) Unbeschadet der Anwendung des Artikels 5 gemäß Artikel 113 Absatz 3 Buchstabe a werden KI-Systeme, bei denen es sich um Komponenten von IT-Großsystemen handelt, die mit den in Anhang X aufgeführten Rechtsakten eingerichtet wurden und vor dem 2. August 2027 in Verkehr gebracht oder in Betrieb genommen wurden, bis zum 31. Dezember 2030 mit dieser Verordnung in Einklang gebracht.\nDie in dieser Verordnung festgelegten Anforderungen werden bei der Bewertung jedes IT-Großsystems, das mit den in Anhang X aufgeführten Rechtsakten eingerichtet wurde, berücksichtigt, wobei die Bewertung entsprechend den Vorgaben der jeweiligen Rechtsakte und bei Ersetzung oder Änderung dieser Rechtsakte erfolgt.\n(2) Unbeschadet der Anwendung des Artikels 5 gemäß Artikel 113 Absatz 3 Buchstabe a gilt diese Verordnung für Betreiber von Hochrisiko-KI-Systemen — mit Ausnahme der in Absatz 1 des vorliegenden Artikels genannten Systeme —, die vor dem 2. August 2026 in Verkehr gebracht oder in Betrieb genommen wurden, nur dann, wenn diese Systeme danach in ihrer Konzeption erheblich verändert wurden. In jedem Fall treffen die Anbieter und Betreiber von Hochrisiko-KI-Systemen, die bestimmungsgemäß von Behörden verwendet werden sollen, die erforderlichen Maßnahmen für die Erfüllung der Anforderungen und Pflichten dieser Verordnung bis zum 2. August 2030. (3) Anbieter von KI-Modellen mit allgemeinem Verwendungszweck, die vor dem 2. August 2025 in Verkehr gebracht wurden, treffen die erforderlichen Maßnahmen für die Erfüllung der in dieser Verordnung festgelegten Pflichten bis zum 2. August 2027."
    },
    {
      "chunk_idx": 316,
      "id": "6f871e54-e731-4009-a4cd-39bbd19b543e",
      "title": "Art 112",
      "relevantChunksIds": [
        "7ce8047c-1e69-42c0-aba4-40614e1ff6b8",
        "a0c2ce4c-d906-4d3e-adb7-96e4be45e9bf",
        "7cf70e13-b212-45e3-bf7b-66d860a91315",
        "9e485960-acdc-4f5b-9227-a02780abe54f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 112: Evaluation and review\n1. The Commission shall assess the need for amendment of the list set out in Annex III and of the list of prohibited AI practices laid down in Article 5, once a year following the entry into force of this Regulation, and until the end of the period of the delegation of power laid down in Article 97. The Commission shall submit the findings of that assessment to the European Parliament and the Council.\n2. By 2 August 2028 and every four years thereafter, the Commission shall evaluate and report to the European Parliament and to the Council on the following:\n(a) the need for amendments extending existing area headings or adding new area headings in Annex III;\n(b) amendments to the list of AI systems requiring additional transparency measures in Article 50;\n(c) amendments enhancing the effectiveness of the supervision and governance system.\n3. By 2 August 2029 and every four years thereafter, the Commission shall submit a report on the evaluation and review of this Regulation to the European Parliament and to the Council. The report shall include an assessment with regard to the structure of enforcement and the possible need for a Union agency to resolve any identified shortcomings. On the basis of the findings, that report shall, where appropriate, be accompanied by a proposal for amendment of this Regulation. The reports shall be made public.\n4. The reports referred to in paragraph 2 shall pay specific attention to the following:\n(a) the status of the financial, technical and human resources of the national competent authorities in order to effectively perform the tasks assigned to them under this Regulation;\n(b) the state of penalties, in particular administrative fines as referred to in Article 99(1), applied by Member States for infringements of this Regulation;\n(c) adopted harmonised standards and common specifications developed to support this Regulation;\n(d) the number of undertakings that enter the market after the entry into application of this Regulation, and how many of them are SMEs.\n5. By 2 August 2028, the Commission shall evaluate the functioning of the AI Office, whether the AI Office has been given sufficient powers and competences to fulfil its tasks, and whether it would be relevant and needed for the proper implementation and enforcement of this Regulation to upgrade the AI Office and its enforcement competences and to increase its resources. The Commission shall submit a report on its evaluation to the European Parliament and to the Council.\n6. By 2 August 2028 and every four years thereafter, the Commission shall submit a report on the review of the progress on the development of standardisation deliverables on the energy-efficient development of general-purpose AI models, and asses the need for further measures or actions, including binding measures or actions. The report shall be submitted to the European Parliament and to the Council, and it shall be made public.\n7. By 2 August 2028 and every three years thereafter, the Commission shall evaluate the impact and effectiveness of voluntary codes of conduct to foster the application of the requirements set out in Chapter III, Section 2 for AI systems other than high-risk AI systems and possibly other additional requirements for AI systems other than high-risk AI systems, including as regards environmental sustainability.\n8. For the purposes of paragraphs 1 to 7, the Board, the Member States and national competent authorities shall provide the Commission with information upon its request and without undue delay.\n9. In carrying out the evaluations and reviews referred to in paragraphs 1 to 7, the Commission shall take into account the positions and findings of the Board, of the European Parliament, of the Council, and of other relevant bodies or sources.\n10. The Commission shall, if necessary, submit appropriate proposals to amend this Regulation, in particular taking into account developments in technology, the effect of AI systems on health and safety, and on fundamental rights, and in light of the state of progress in the information society.\n11. To guide the evaluations and reviews referred to in paragraphs 1 to 7 of this Article, the AI Office shall undertake to develop an objective and participative methodology for the evaluation of risk levels based on the criteria outlined in the relevant Articles and the inclusion of new systems in:\n(a) the list set out in Annex III, including the extension of existing area headings or the addition of new area headings in that Annex;\n(b) the list of prohibited practices set out in Article 5; and\n\n13. By 2 August 2031, the Commission shall carry out an assessment of the enforcement of this Regulation and shall report on it to the European Parliament, the Council and the European Economic and Social Committee, taking into account the first years of application of this Regulation. On the basis of the findings, that report shall, where appropriate, be accompanied by a proposal for amendment of this Regulation with regard to the structure of enforcement and the need for a Union agency to resolve any identified shortcomings.",
      "original_content": "### Artikel 112: Bewertung und Überprüfung\n(1) Die Kommission prüft nach Inkrafttreten dieser Verordnung und bis zum Ende der Befugnisübertragung gemäß Artikel 97 einmal jährlich, ob eine Änderung der Liste in Anhang III und der Liste der verbotenen Praktiken im KI-Bereich gemäß Artikel 5 erforderlich ist. Die Kommission übermittelt die Ergebnisse dieser Bewertung dem Europäischen Parlament und dem Rat.\n(2) Bis zum 2. August 2028 und danach alle vier Jahre bewertet die Kommission Folgendes und erstattet dem Europäischen Parlament und dem Rat Bericht darüber:\na) Notwendigkeit von Änderungen zur Erweiterung bestehender Bereiche oder zur Aufnahme neuer Bereiche in Anhang III:\nb) Änderungen der Liste der KI-Systeme, die zusätzliche Transparenzmaßnahmen erfordern, in Artikel 50;\nc) Änderungen zur Verbesserung der Wirksamkeit des Überwachungs- und Governance-Systems.\n(3) Bis zum 2. August 2029 und danach alle vier Jahre legt die Kommission dem Europäischen Parlament und dem Rat einen Bericht über die Bewertung und Überprüfung dieser Verordnung vor. Der Bericht enthält eine Beurteilung hinsichtlich der Durchsetzungsstruktur und der etwaigen Notwendigkeit einer Agentur der Union zur Lösung der festgestellten Mängel. Auf der Grundlage der Ergebnisse wird diesem Bericht gegebenenfalls ein Vorschlag zur Änderung dieser Verordnung beigefügt. Die Berichte werden veröffentlicht.\n(4) In den in Absatz 2 genannten Berichten wird insbesondere auf folgende Aspekte eingegangen:\na) Sachstand bezüglich der finanziellen, technischen und personellen Ressourcen der zuständigen nationalen Behörden im Hinblick auf deren Fähigkeit, die ihnen auf der Grundlage dieser Verordnung übertragenen Aufgaben wirksam zu erfüllen;\nb) Stand der Sanktionen, insbesondere der Bußgelder nach Artikel 99 Absatz 1, die Mitgliedstaaten bei Verstößen gegen diese Verordnung verhängt haben;\nc) angenommene harmonisierte Normen und gemeinsame Spezifikationen, die zur Unterstützung dieser Verordnung erarbeitet wurden;\nd) Zahl der Unternehmen, die nach Inkrafttreten dieser Verordnung in den Markt eintreten, und wie viele davon KMU sind.\n(5) Bis zum 2. August 2028 bewertet die Kommission die Arbeitsweise des Büros für Künstliche Intelligenz und prüft, ob das Büro für Künstliche Intelligenz mit ausreichenden Befugnissen und Zuständigkeiten zur Erfüllung seiner Aufgaben ausgestattet wurde, und ob es für die ordnungsgemäße Durchführung und Durchsetzung dieser Verordnung zweckmäßig und erforderlich wäre, das Büro für Künstliche Intelligenz und seine Durchsetzungskompetenzen zu erweitern und seine Ressourcen aufzustocken. Die Kommission übermittelt dem Europäischen Parlament und dem Rat einen Bericht über ihre Bewertung.\n(6) Bis zum 2. August 2028 und danach alle vier Jahre legt die Kommission einen Bericht über die Überprüfung der Fortschritte bei der Entwicklung von Normungsdokumenten zur energieeffizienten Entwicklung von KI-Modellen mit allgemeinem Verwendungszweck vor und beurteilt die Notwendigkeit weiterer Maßnahmen oder Handlungen, einschließlich verbindlicher Maßnahmen oder Handlungen. Dieser Bericht wird dem Europäischen Parlament und dem Rat vorgelegt und veröffentlicht.\n(7) Bis zum 2. August 2028 und danach alle drei Jahre führt die Kommission eine Bewertung der Folgen und der Wirksamkeit der freiwilligen Verhaltenskodizes durch, mit denen die Anwendung der in Kapitel III Abschnitt 2 festgelegten Anforderungen an andere KI-Systeme als Hochrisiko-KI-Systeme und möglicherweise auch zusätzlicher Anforderungen an andere KI-Systeme als Hochrisiko-KI-Systeme, auch in Bezug auf deren ökologische Nachhaltigkeit, gefördert werden soll.\n(8) Für die Zwecke der Absätze 1 bis 7 übermitteln das KI-Gremium, die Mitgliedstaaten und die zuständigen nationalen Behörden der Kommission auf Anfrage unverzüglich die gewünschten Informationen.\n(9) Bei den in den Absätzen 1 bis 7 genannten Bewertungen und Überprüfungen berücksichtigt die Kommission die Standpunkte und Feststellungen des KI-Gremiums, des Europäischen Parlaments, des Rates und anderer einschlägiger Stellen oder Quellen.\n(10) Die Kommission legt erforderlichenfalls geeignete Vorschläge zur Änderung dieser Verordnung vor und berücksichtigt dabei insbesondere technologische Entwicklungen, die Auswirkungen von KI-Systemen auf die Gesundheit und Sicherheit und auf die Grundrechte und die Fortschritte in der Informationsgesellschaft.\n(11) Als Orientierung für die in den Absätzen 1 bis 7 genannten Bewertungen und Überprüfungen entwickelt das Büro für Künstliche Intelligenz ein Ziel und eine partizipative Methode für die Bewertung der Risikoniveaus anhand der in den jeweiligen Artikeln genannten Kriterien und für die Einbeziehung neuer Systeme in\na) die Liste gemäß Anhang III, einschließlich der Erweiterung bestehender Bereiche oder der Aufnahme neuer Bereiche in diesen Anhang;\nb) die Liste der verbotenen Praktiken gemäß Artikel 5; und\nc) die Liste der KI-Systeme, die zusätzliche Transparenzmaßnahmen erfordern, in Artikel 50. (12) Eine Änderung dieser Verordnung im Sinne des Absatzes 10 oder entsprechende delegierte Rechtsakte oder Durchführungsrechtsakte, die sektorspezifische Rechtsvorschriften für eine unionsweite Harmonisierung gemäß Anhang I Abschnitt B betreffen, berücksichtigen die regulatorischen Besonderheiten des jeweiligen Sektors und die in der Verordnung festgelegten bestehenden Governance-, Konformitätsbewertungs- und Durchsetzungsmechanismen und -behörden.\n(13) Bis zum 2. August 2031 nimmt die Kommission unter Berücksichtigung der ersten Jahre der Anwendung der Verordnung eine Bewertung der Durchsetzung dieser Verordnung vor und erstattet dem Europäischen Parlament, dem Rat und dem Europäischen Wirtschafts- und Sozialausschuss darüber Bericht. Auf Grundlage der Ergebnisse wird dem Bericht gegebenenfalls ein Vorschlag zur Änderung dieser Verordnung beigefügt, der die Struktur der Durchsetzung und die Notwendigkeit einer Agentur der Union für die Lösung festgestellter Mängel betrifft."
    },
    {
      "chunk_idx": 317,
      "id": "e0ff4862-edad-42af-8c72-7ac7ab5c25b7",
      "title": "Art 113",
      "relevantChunksIds": [
        "7c5fc4fc-05c2-4781-9924-fc7ef6ca50ef"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "### Article 113: Entry into force and application\nThis Regulation shall enter into force on the twentieth day following that of its publication in the Official Journal of the European Union.\n\n(a) Chapters I and II shall apply from 2 February 2025;\n(b) Chapter III Section 4, Chapter V, Chapter VII and Chapter XII and Article 78 shall apply from 2 August 2025, with the exception of Article 101;\n\n\nThe President\nR. METSOLA\nFor the Council\nThe President\nM. MICHEL",
      "original_content": "### Artikel 113: Inkrafttreten und Geltungsbeginn\nDiese Verordnung tritt am zwanzigsten Tag nach ihrer Veröffentlichung im Amtsblatt der Europäischen Union in Kraft.\nSie gilt ab dem 2. August 2026. Jedoch:\na) Die Kapitel I und II gelten ab dem 2. Februar 2025;\nb) Kapitel III Abschnitt 4, Kapitel V, Kapitel VII und Kapitel XII sowie Artikel 78 gelten ab dem 2. August 2025, mit Ausnahme des Artikels 101;\nc) ### Artikel 6 Absatz 1 und die entsprechenden Pflichten gemäß dieser Verordnung gelten ab dem 2. August 2027. Diese Verordnung ist in allen ihren Teilen verbindlich und gilt unmittelbar in jedem Mitgliedstaat.\nGeschehen zu Brüssel am 13. Juni 2024. Im Namen des Europäischen Parlaments\nDie Präsidentin\nR. METSOLA\nIm Namen des Rates\nDer Präsident\nM. MICHEL"
    },
    {
      "chunk_idx": 318,
      "id": "814206ed-2f5c-4c1c-91fd-616f0a128cd3",
      "title": "Annex I",
      "relevantChunksIds": [],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX I: List of Union harmonisation legislation\nSection A. List of Union harmonisation legislation based on the New Legislative Framework\n1. Directive 2006/42/EC of the European Parliament and of the Council of 17 May 2006 on machinery, and amending Directive 95/16/EC (OJ L 157, 9.6.2006, p. 24);\n2. Directive 2009/48/EC of the European Parliament and of the Council of 18 June 2009 on the safety of toys (OJ L 170, 30.6.2009, p. 1);\n3. Directive 2013/53/EU of the European Parliament and of the Council of 20 November 2013 on recreational craft and personal watercraft and repealing Directive 94/25/EC (OJ L 354, 28.12.2013, p. 90);\n4. Directive 2014/33/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of the laws of the Member States relating to lifts and safety components for lifts (OJ L 96, 29.3.2014, p. 251);\n5. Directive 2014/34/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of the laws of the Member States relating to equipment and protective systems intended for use in potentially explosive atmospheres (OJ L 96, 29.3.2014, p. 309);\n6. Directive 2014/53/EU of the European Parliament and of the Council of 16 April 2014 on the harmonisation of the laws of the Member States relating to the making available on the market of radio equipment and repealing Directive 1999/5/EC (OJ L 153, 22.5.2014, p. 62);\n7. Directive 2014/68/EU of the European Parliament and of the Council of 15 May 2014 on the harmonisation of the laws of the Member States relating to the making available on the market of pressure equipment (OJ L 189, 27.6.2014, p. 164);\n8. Regulation (EU) 2016/424 of the European Parliament and of the Council of 9 March 2016 on cableway installations and repealing Directive 2000/9/EC (OJ L 81, 31.3.2016, p. 1);\n9. Regulation (EU) 2016/425 of the European Parliament and of the Council of 9 March 2016 on personal protective equipment and repealing Council Directive 89/686/EEC (OJ L 81, 31.3.2016, p. 51);\n10. Regulation (EU) 2016/426 of the European Parliament and of the Council of 9 March 2016 on appliances burning gaseous fuels and repealing Directive 2009/142/EC (OJ L 81, 31.3.2016, p. 99);\n11. Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC (OJ L 117, 5.5.2017, p. 1);\n12. Regulation (EU) 2017/746 of the European Parliament and of the Council of 5 April 2017 on in vitro diagnostic medical devices and repealing Directive 98/79/EC and Commission Decision 2010/227/EU (OJ L 117, 5.5.2017, p. 176).\nSection B. List of other Union harmonisation legislation\n13. Regulation (EC) No 300/2008 of the European Parliament and of the Council of 11 March 2008 on common rules in the field of civil aviation security and repealing Regulation (EC) No 2320/2002 (OJ L 97, 9.4.2008, p. 72);\n14. Regulation (EU) No 168/2013 of the European Parliament and of the Council of 15 January 2013 on the approval and market surveillance of two- or three-wheel vehicles and quadricycles (OJ L 60, 2.3.2013, p. 52);\n15. Regulation (EU) No 167/2013 of the European Parliament and of the Council of 5 February 2013 on the approval and market surveillance of agricultural and forestry vehicles (OJ L 60, 2.3.2013, p. 1);\n16. Directive 2014/90/EU of the European Parliament and of the Council of 23 July 2014 on marine equipment and repealing Council Directive 96/98/EC (OJ L 257, 28.8.2014, p. 146);\n17. Directive (EU) 2016/797 of the European Parliament and of the Council of 11 May 2016 on the interoperability of the rail system within the European Union (OJ L 138, 26.5.2016, p. 44);\n18. Regulation (EU) 2018/858 of the European Parliament and of the Council of 30 May 2018 on the approval and market surveillance of motor vehicles and their trailers, and of systems, components and separate technical units intended for such vehicles, amending Regulations (EC) No 715/2007 and (EC) No 595/2009 and repealing Directive 2007/46/EC (OJ L 151, 14.6.2018, p. 1);\n19. Regulation (EU) 2019/2144 of the European Parliament and of the Council of 27 November 2019 on type-approval requirements for motor vehicles and their trailers, and systems, components and separate technical units intended for such vehicles, as regards their general safety and the protection of vehicle occupants and vulnerable road users, amending Regulation (EU) 2018/858 of the European Parliament and of the Council and repealing Regulations (EC) No 78/2009, (EC) No 79/2009 and (EC) No 661/2009 of the European Parliament and of the Council and Commission Regulations (EC) No 631/2009, (EU) No 406/2010, (EU) No 672/2010, (EU) No 1003/2010, (EU) No 1005/2010, (EU) No 1008/2010, (EU) No 1009/2010, (EU) No 19/2011, (EU) No 109/2011, (EU) No 458/2011, (EU) No 65/2012, (EU) No 130/2012, (EU) No 347/2012, (EU) No 351/2012, (EU) No 1230/2012 and (EU) 2015/166 (OJ L 325, 16.12.2019, p. 1);\n20. Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 on common rules in the field of civil aviation and establishing a European Union Aviation Safety Agency, and amending Regulations (EC) No 2111/2005, (EC) No 1008/2008, (EU) No 996/2010, (EU) No 376/2014 and Directives 2014/30/EU and 2014/53/EU of the European Parliament and of the Council, and repealing Regulations (EC) No 552/2004 and (EC) No 216/2008 of the European Parliament and of the Council and Council Regulation (EEC) No 3922/91 (OJ L 212, 22.8.2018, p. 1), in so far as the design, production and placing on the market of aircrafts referred to in Article 2(1), points (a) and (b) thereof, where it concerns unmanned aircraft and their engines, propellers, parts and equipment to control them remotely, are concerned.",
      "original_content": "ANHANG I: Liste der Harmonisierungsrechtsvorschriften der Union\nAbschnitt A — Liste der Harmonisierungsrechtsvorschriften der Union auf der Grundlage des neuen Rechtsrahmens\n1. Richtlinie 2006/42/EG des Europäischen Parlaments und des Rates vom 17. Mai 2006 über Maschinen und zur Änderung der Richtlinie 95/16/EG (ABl. L 157 vom 9.6.2006, S. 24)\n2. Richtlinie 2009/48/EG des Europäischen Parlaments und des Rates vom 18. Juni 2009 über die Sicherheit von Spielzeug (ABl. L 170 vom 30.6.2009, S. 1)\n3. Richtlinie 2013/53/EU des Europäischen Parlaments und des Rates vom 20. November 2013 über Sportboote und Wassermotorräder und zur Aufhebung der Richtlinie 94/25/EG (ABl. L 354 vom 28.12.2013, S. 90)\n4. Richtlinie 2014/33/EU des Europäischen Parlaments und des Rates vom 26. Februar 2014 zur Angleichung der Rechtsvorschriften der Mitgliedstaaten über Aufzüge und Sicherheitsbauteile für Aufzüge (ABl. L 96 vom 29.3.2014, S. 251)\n5. Richtlinie 2014/34/EU des Europäischen Parlaments und des Rates vom 26. Februar 2014 zur Harmonisierung der Rechtsvorschriften der Mitgliedstaaten für Geräte und Schutzsysteme zur bestimmungsgemäßen Verwendung in explosionsgefährdeten Bereichen (ABl. L 96 vom 29.3.2014, S. 309)\n6. Richtlinie 2014/53/EU des Europäischen Parlaments und des Rates vom 16. April 2014 über die Harmonisierung der Rechtsvorschriften der Mitgliedstaaten über die Bereitstellung von Funkanlagen auf dem Markt und zur Aufhebung der Richtlinie 1999/5/EG (ABl. L 153 vom 22.5.2014, S. 62)\n7. Richtlinie 2014/68/EU des Europäischen Parlaments und des Rates vom 15. Mai 2014 zur Harmonisierung der Rechtsvorschriften der Mitgliedstaaten über die Bereitstellung von Druckgeräten auf dem Markt (ABl. L 189 vom 27.6.2014, S. 164)\n8. Verordnung (EU) 2016/424 des Europäischen Parlaments und des Rates vom 9. März 2016 über Seilbahnen und zur Aufhebung der Richtlinie 2000/9/EG (ABl. L 81 vom 31.3.2016, S. 1)\n9. Verordnung (EU) 2016/425 des Europäischen Parlaments und des Rates vom 9. März 2016 über persönliche Schutzausrüstungen und zur Aufhebung der Richtlinie 89/686/EWG des Rates (ABl. L 81 vom 31.3.2016, S. 51)\n10. Verordnung (EU) 2016/426 des Europäischen Parlaments und des Rates vom 9. März 2016 über Geräte zur Verbrennung gasförmiger Brennstoffe und zur Aufhebung der Richtlinie 2009/142/EG (ABl. L 81 vom 31.3.2016, S. 99)\n11. Verordnung (EU) 2017/745 des Europäischen Parlaments und des Rates vom 5. April 2017 über Medizinprodukte, zur Änderung der Richtlinie 2001/83/EG, der Verordnung (EG) Nr. 178/2002 und der Verordnung (EG) Nr. 1223/2009 und zur Aufhebung der Richtlinien 90/385/EWG und 93/42/EWG des Rates (ABl. L 117 vom 5.5.2017, S. 1)\n12. Verordnung (EU) 2017/746 des Europäischen Parlaments und des Rates vom 5. April 2017 über In-vitro-Diagnostika und zur Aufhebung der Richtlinie 98/79/EG und des Beschlusses 2010/227/EU der Kommission (ABl. L 117 vom 5.5.2017, S. 176)\nAbschnitt B — Liste anderer Harmonisierungsrechtsvorschriften der Union\n13. Verordnung (EG) Nr. 300/2008 des Europäischen Parlaments und des Rates vom 11. März 2008 über gemeinsame Vorschriften für die Sicherheit in der Zivilluftfahrt und zur Aufhebung der Verordnung (EG) Nr. 2320/2002 (ABl. L 97 vom 9.4.2008, S. 72)\n14. Verordnung (EU) Nr. 168/2013 des Europäischen Parlaments und des Rates vom 15. Januar 2013 über die Genehmigung und Marktüberwachung von zwei- oder dreirädrigen und vierrädrigen Fahrzeugen (ABl. L 60 vom 2.3.2013, S. 52)\n15. Verordnung (EU) Nr. 167/2013 des Europäischen Parlaments und des Rates vom 5. Februar 2013 über die Genehmigung und Marktüberwachung von land- und forstwirtschaftlichen Fahrzeugen (ABl. L 60 vom 2.3.2013, S. 1)\n16. Richtlinie 2014/90/EU des Europäischen Parlaments und des Rates vom 23. Juli 2014 über Schiffsausrüstung und zur Aufhebung der Richtlinie 96/98/EG des Rates (ABl. L 257 vom 28.8.2014, S. 146)\n17. Richtlinie (EU) 2016/797 des Europäischen Parlaments und des Rates vom 11. Mai 2016 über die Interoperabilität des Eisenbahnsystems in der Europäischen Union (ABl. L 138 vom 26.5.2016, S. 44)\n18. Verordnung (EU) 2018/858 des Europäischen Parlaments und des Rates vom 30. Mai 2018 über die Genehmigung und die Marktüberwachung von Kraftfahrzeugen und Kraftfahrzeuganhängern sowie von Systemen, Bauteilen und selbstständigen technischen Einheiten für diese Fahrzeuge, zur Änderung der Verordnungen (EG) Nr. 715/2007 und (EG) Nr. 595/2009 und zur Aufhebung der Richtlinie 2007/46/EG (ABl. L 151 vom 14.6.2018, S. 1)\n19. Verordnung (EU) 2019/2144 des Europäischen Parlaments und des Rates vom 27. November 2019 über die Typgenehmigung von Kraftfahrzeugen und Kraftfahrzeuganhängern sowie von Systemen, Bauteilen und selbstständigen technischen Einheiten für diese Fahrzeuge im Hinblick auf ihre allgemeine Sicherheit und den Schutz der Fahrzeuginsassen und von ungeschützten Verkehrsteilnehmern, zur Änderung der Verordnung (EU) 2018/858 des Europäischen Parlaments und des Rates und zur Aufhebung der Verordnungen (EG) Nr. 78/2009, (EG) Nr. 79/2009 und (EG) Nr. 661/2009 des Europäischen Parlaments und des Rates sowie der Verordnungen (EG) Nr. 631/2009, (EU) Nr. 406/2010, (EU) Nr. 672/2010, (EU) Nr. 1003/2010, (EU) Nr. 1005/2010, (EU) Nr. 1008/2010, (EU) Nr. 1009/2010, (EU) Nr. 19/2011, (EU) Nr. 109/2011, (EU) Nr. 458/2011, (EU) Nr. 65/2012, (EU) Nr. 130/2012, (EU) Nr. 347/2012, (EU) Nr. 351/2012, (EU) Nr. 1230/2012 und (EU) 2015/166 der Kommission (ABl. L 325 vom 16.12.2019, S. 1)\n20. Verordnung (EU) 2018/1139 des Europäischen Parlaments und des Rates vom 4. Juli 2018 zur Festlegung gemeinsamer Vorschriften für die Zivilluftfahrt und zur Errichtung einer Agentur der Europäischen Union für Flugsicherheit sowie zur Änderung der Verordnungen (EG) Nr. 2111/2005, (EG) Nr. 1008/2008, (EU) Nr. 996/2010, (EU) Nr. 376/2014 und der Richtlinien 2014/30/EU und 2014/53/EU des Europäischen Parlaments und des Rates, und zur Aufhebung der Verordnungen (EG) Nr. 552/2004 und (EG) Nr. 216/2008 des Europäischen Parlaments und des Rates und der Verordnung (EWG) Nr. 3922/91 des Rates (ABl. L 212 vom 22.8.2018, S. 1), insoweit die Konstruktion, Herstellung und Vermarktung von Luftfahrzeugen gemäß Artikel 2 Absatz 1 Buchstaben a und b in Bezug auf unbemannte Luftfahrzeuge sowie deren Motoren, Propeller, Teile und Ausrüstung zur Fernsteuerung betroffen sind"
    },
    {
      "chunk_idx": 319,
      "id": "fa61592a-861e-4253-a4c6-2a62f0f9be2d",
      "title": "Annex II",
      "relevantChunksIds": [
        "c0d0abea-63e2-4ed1-b273-a1882356a3a7",
        "8c6356ea-0c7e-4fee-b210-97b4a9a18c77"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX II: List of criminal offences referred to in Article 5(1), first subparagraph, point (h)(iii) Criminal offences referred to in Article 5(1), first subparagraph, point (h)(iii):\n— terrorism,\n— trafficking in human beings,\n— sexual exploitation of children, and child pornography,\n— illicit trafficking in narcotic drugs or psychotropic substances,\n— illicit trafficking in weapons, munitions or explosives,\n— murder, grievous bodily injury,\n— illicit trade in human organs or tissue,\n— illicit trafficking in nuclear or radioactive materials,\n— kidnapping, illegal restraint or hostage-taking,\n— crimes within the jurisdiction of the International Criminal Court,\n— unlawful seizure of aircraft or ships,\n— rape,\n— environmental crime,\n— organised or armed robbery,\n— sabotage,\n— participation in a criminal organisation involved in one or more of the offences listed above.",
      "original_content": "ANHANG II: Liste der Straftaten gemäß Artikel 5 Absatz 1 Unterabsatz 1 Buchstabe h Ziffer iii Straftaten gemäß Artikel 5 Absatz 1 Unterabsatz 1 Buchstabe h Ziffer iii:\n- Terrorismus,\n- Menschenhandel,\n- sexuelle Ausbeutung von Kindern und Kinderpornografie,\n- illegaler Handel mit Drogen oder psychotropen Stoffen,\n- illegaler Handel mit Waffen, Munition oder Sprengstoffen,\n- Mord, schwere Körperverletzung,\n- illegaler Handel mit menschlichen Organen oder menschlichem Gewebe,\n- illegaler Handel mit nuklearen oder radioaktiven Substanzen,\n- Entführung, Freiheitsberaubung oder Geiselnahme,\n- Verbrechen, die in die Zuständigkeit des Internationalen Strafgerichtshofs fallen,\n- Flugzeug- oder Schiffsentführung,\n- Vergewaltigung,\n- Umweltkriminalität,\n- organisierter oder bewaffneter Raub,\n- Sabotage,\n- Beteiligung an einer kriminellen Vereinigung, die an einer oder mehreren der oben genannten Straftaten beteiligt ist."
    },
    {
      "chunk_idx": 320,
      "id": "416816a1-c29f-4c2c-b7f4-015e60a43042",
      "title": "Annex III: Z1",
      "relevantChunksIds": [
        "d1298add-efff-452d-ba5a-f730964d638a",
        "e3935b15-c152-49db-9b69-2166080067c1",
        "7f7015ca-f41b-45bb-97f0-d06734d9b6b0",
        "d605b91c-4f8d-4ba9-85fa-6dfbd07016e5",
        "f3f24184-1d9d-4f5c-af39-7853beef1c1b",
        "87c3670b-a636-4431-aadd-9cfe780035b0"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX III: High-risk AI systems referred to in Article 6(2) \nHigh-risk AI systems pursuant to Article 6(2) are the AI systems listed in any of the following areas:\n1. Biometrics, in so far as their use is permitted under relevant Union or national law:\n(a) remote biometric identification systems.\nThis shall not include AI systems intended to be used for biometric verification the sole purpose of which is to confirm that a specific natural person is the person he or she claims to be;\n(b) AI systems intended to be used for biometric categorisation, according to sensitive or protected attributes or characteristics based on the inference of those attributes or characteristics;\n(c) AI systems intended to be used for emotion recognition.",
      "original_content": "ANHANG III: Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 2\nAls Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 2 gelten die in folgenden Bereichen aufgeführten KI-Systeme:\n1. Biometrie, soweit ihr Einsatz nach einschlägigem Unionsrecht oder nationalem Recht zugelassen ist:\na) biometrische Fernidentifizierungssysteme.\nDazu gehören nicht KI-Systeme, die bestimmungsgemäß für die biometrische Verifizierung, deren einziger Zweck darin besteht, zu bestätigen, dass eine bestimmte natürliche Person die Person ist, für die sie sich ausgibt, verwendet werden sollen;\nb) KI-Systeme, die bestimmungsgemäß für die biometrische Kategorisierung nach sensiblen oder geschützten Attributen oder Merkmalen auf der Grundlage von Rückschlüssen auf diese Attribute oder Merkmale verwendet werden sollen;\nc) KI-Systeme, die bestimmungsgemäß zur Emotionserkennung verwendet werden sollen."
    },
    {
      "chunk_idx": 321,
      "id": "88111b7f-ede1-45cc-bedc-9aa57dc012a2",
      "title": "Annex III: Z2",
      "relevantChunksIds": [
        "0b34a175-21c7-4d02-9c4e-4380537fff16",
        "ee5c5bfc-ded7-4010-bdae-d396f7798630"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX III: High-risk AI systems referred to in Article 6(2) \nHigh-risk AI systems pursuant to Article 6(2) are the AI systems listed in any of the following areas:\n\n2. Critical infrastructure: AI systems intended to be used as safety components in the management and operation of critical digital infrastructure, road traffic, or in the supply of water, gas, heating or electricity.",
      "original_content": "ANHANG III: Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 2\nAls Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 2 gelten die in folgenden Bereichen aufgeführten KI-Systeme:\n[...]\n2. Kritische Infrastruktur: KI-Systeme, die bestimmungsgemäß als Sicherheitsbauteile im Rahmen der Verwaltung und des Betriebs kritischer digitaler Infrastruktur, des Straßenverkehrs oder der Wasser-, Gas-, Wärme- oder Stromversorgung verwendet werden sollen"
    },
    {
      "chunk_idx": 322,
      "id": "f6e922ce-0538-4d92-8c32-a0a750198c89",
      "title": "Annex III: Z3",
      "relevantChunksIds": [
        "0b34a175-21c7-4d02-9c4e-4380537fff16",
        "7401fefb-6095-47f0-a00e-78aac879499e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX III: High-risk AI systems referred to in Article 6(2) \nHigh-risk AI systems pursuant to Article 6(2) are the AI systems listed in any of the following areas:\n\n3. Education and vocational training:\n(a) AI systems intended to be used to determine access or admission or to assign natural persons to educational and vocational training institutions at all levels;\n(b) AI systems intended to be used to evaluate learning outcomes, including when those outcomes are used to steer the learning process of natural persons in educational and vocational training institutions at all levels;\n(c) AI systems intended to be used for the purpose of assessing the appropriate level of education that an individual will receive or will be able to access, in the context of or within educational and vocational training institutions at all levels;\n(d) AI systems intended to be used for monitoring and detecting prohibited behaviour of students during tests in the context of or within educational and vocational training institutions at all levels.",
      "original_content": "ANHANG III: Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 2\nAls Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 2 gelten die in folgenden Bereichen aufgeführten KI-Systeme:\n[...]\n3. Allgemeine und berufliche Bildung\na) KI-Systeme, die bestimmungsgemäß zur Feststellung des Zugangs oder der Zulassung oder zur Zuweisung natürlicher Personen zu Einrichtungen aller Ebenen der allgemeinen und beruflichen Bildung verwendet werden sollen;\nb) KI-Systeme, die bestimmungsgemäß für die Bewertung von Lernergebnissen verwendet werden sollen, einschließlich des Falles, dass diese Ergebnisse dazu dienen, den Lernprozess natürlicher Personen in Einrichtungen oder Programmen aller Ebenen der allgemeinen und beruflichen Bildung zu steuern;\nc) KI-Systeme, die bestimmungsgemäß zum Zweck der Bewertung des angemessenen Bildungsniveaus, das eine Person im Rahmen von oder innerhalb von Einrichtungen aller Ebenen der allgemeinen und beruflichen Bildung erhalten wird oder zu denen sie Zugang erhalten wird, verwendet werden sollen;\nd) KI-Systeme, die bestimmungsgemäß zur Überwachung und Erkennung von verbotenem Verhalten von Schülern bei Prüfungen im Rahmen von oder innerhalb von Einrichtungen aller Ebenen der allgemeinen und beruflichen Bildung verwendet werden sollen."
    },
    {
      "chunk_idx": 323,
      "id": "7e595036-3aa8-4edf-b26f-0627760fb44e",
      "title": "Annex III: Z4",
      "relevantChunksIds": [
        "0b34a175-21c7-4d02-9c4e-4380537fff16",
        "7401fefb-6095-47f0-a00e-78aac879499e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX III: High-risk AI systems referred to in Article 6(2) \nHigh-risk AI systems pursuant to Article 6(2) are the AI systems listed in any of the following areas:\n\n4. Employment, workers’ management and access to self-employment:\n(a) AI systems intended to be used for the recruitment or selection of natural persons, in particular to place targeted job advertisements, to analyse and filter job applications, and to evaluate candidates;\n(b) AI systems intended to be used to make decisions affecting terms of work-related relationships, the promotion or termination of work-related contractual relationships, to allocate tasks based on individual behaviour or personal traits or characteristics or to monitor and evaluate the performance and behaviour of persons in such relationships.",
      "original_content": "ANHANG III: Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 2\nAls Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 2 gelten die in folgenden Bereichen aufgeführten KI-Systeme:\n[...]\n4. Beschäftigung, Personalmanagement und Zugang zur Selbstständigkeit\na) KI-Systeme, die bestimmungsgemäß für die Einstellung oder Auswahl natürlicher Personen verwendet werden sollen, insbesondere um gezielte Stellenanzeigen zu schalten, Bewerbungen zu sichten oder zu filtern und Bewerber zu bewerten;\nb) KI-Systeme, die bestimmungsgemäß für Entscheidungen, die die Bedingungen von Arbeitsverhältnissen, Beförderungen und Kündigungen von Arbeitsvertragsverhältnissen beeinflussen, für die Zuweisung von Aufgaben aufgrund des individuellen Verhaltens oder persönlicher Merkmale oder Eigenschaften oder für die Beobachtung und Bewertung der Leistung und des Verhaltens von Personen in solchen Beschäftigungsverhältnissen verwendet werden soll."
    },
    {
      "chunk_idx": 324,
      "id": "b5274646-0b45-4227-ad44-f5afd431413c",
      "title": "Annex III: Z5",
      "relevantChunksIds": [
        "0b34a175-21c7-4d02-9c4e-4380537fff16",
        "7401fefb-6095-47f0-a00e-78aac879499e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX III: High-risk AI systems referred to in Article 6(2) \nHigh-risk AI systems pursuant to Article 6(2) are the AI systems listed in any of the following areas:\n\n5. Access to and enjoyment of essential private services and essential public services and benefits:\n(a) AI systems intended to be used by public authorities or on behalf of public authorities to evaluate the eligibility of natural persons for essential public assistance benefits and services, including healthcare services, as well as to grant, reduce, revoke, or reclaim such benefits and services;\n(b) AI systems intended to be used to evaluate the creditworthiness of natural persons or establish their credit score, with the exception of AI systems used for the purpose of detecting financial fraud;\n(c) AI systems intended to be used for risk assessment and pricing in relation to natural persons in the case of life and health insurance;\n(d) AI systems intended to evaluate and classify emergency calls by natural persons or to be used to dispatch, or to establish priority in the dispatching of, emergency first response services, including by police, firefighters and medical aid, as well as of emergency healthcare patient triage systems.",
      "original_content": "ANHANG III: Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 2\nAls Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 2 gelten die in folgenden Bereichen aufgeführten KI-Systeme:\n[...]\n5. Zugänglichkeit und Inanspruchnahme grundlegender privater und grundlegender öffentlicher Dienste und Leistungen:\na) KI-Systeme, die bestimmungsgemäß von Behörden oder im Namen von Behörden verwendet werden sollen, um zu beurteilen, ob natürliche Personen Anspruch auf grundlegende öffentliche Unterstützungsleistungen und -dienste, einschließlich Gesundheitsdiensten, haben und ob solche Leistungen und Dienste zu gewähren, einzuschränken, zu widerrufen oder zurückzufordern sind;\nb) KI-Systeme, die bestimmungsgemäß für die Kreditwürdigkeitsprüfung und Bonitätsbewertung natürlicher Personen verwendet werden sollen, mit Ausnahme von KI-Systemen, die zur Aufdeckung von Finanzbetrug verwendet werden;\nc) KI-Systeme, die bestimmungsgemäß für die Risikobewertung und Preisbildung in Bezug auf natürliche Personen im Fall von Lebens- und Krankenversicherungen verwendet werden sollen;\nd) KI-Systeme, die bestimmungsgemäß zur Bewertung und Klassifizierung von Notrufen von natürlichen Personen oder für die Entsendung oder Priorisierung des Einsatzes von Not- und Rettungsdiensten, einschließlich Polizei, Feuerwehr und medizinischer Nothilfe, sowie für Systeme für die Triage von Patienten bei der Notfallversorgung verwendet werden sollen."
    },
    {
      "chunk_idx": 325,
      "id": "24b540c7-6f65-42f3-80e6-2a3ec3e86999",
      "title": "Annex III: Z6",
      "relevantChunksIds": [
        "8a0af483-9e07-445f-8fa1-77555b40c583",
        "0b34a175-21c7-4d02-9c4e-4380537fff16",
        "7401fefb-6095-47f0-a00e-78aac879499e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX III: High-risk AI systems referred to in Article 6(2) \nHigh-risk AI systems pursuant to Article 6(2) are the AI systems listed in any of the following areas:\n\n6. Law enforcement, in so far as their use is permitted under relevant Union or national law:\n(a) AI systems intended to be used by or on behalf of law enforcement authorities, or by Union institutions, bodies, offices or agencies in support of law enforcement authorities or on their behalf to assess the risk of a natural person becoming the victim of criminal offences;\n(b) AI systems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies, offices or agencies in support of law enforcement authorities as polygraphs or similar tools;\n(c) AI systems intended to be used by or on behalf of law enforcement authorities, or by Union institutions, bodies, offices or agencies, in support of law enforcement authorities to evaluate the reliability of evidence in the course of the investigation or prosecution of criminal offences;\n(d) AI systems intended to be used by law enforcement authorities or on their behalf or by Union institutions, bodies, offices or agencies in support of law enforcement authorities for assessing the risk of a natural person offending or re-offending not solely on the basis of the profiling of natural persons as referred to in Article 3(4) of Directive (EU) 2016/680, or to assess personality traits and characteristics or past criminal behaviour of natural persons or groups;\n(e) AI systems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies, offices or agencies in support of law enforcement authorities for the profiling of natural persons as referred to in Article 3(4) of Directive (EU) 2016/680 in the course of the detection, investigation or prosecution of criminal offences.",
      "original_content": "ANHANG III: Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 2\nAls Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 2 gelten die in folgenden Bereichen aufgeführten KI-Systeme:\n[...]\n6. Strafverfolgung, soweit ihr Einsatz nach einschlägigem Unionsrecht oder nationalem Recht zugelassen ist:\na) KI-Systeme, die bestimmungsgemäß von Strafverfolgungsbehörden oder in deren Namen oder von Organen, Einrichtungen und sonstigen Stellen der Union zur Unterstützung von Strafverfolgungsbehörden oder in deren Namen zur Bewertung des Risikos einer natürlichen Person, zum Opfer von Straftaten zu werden, verwendet werden sollen;\nb) KI-Systeme, die bestimmungsgemäß von Strafverfolgungsbehörden oder in deren Namen oder von Organen, Einrichtungen und sonstigen Stellen der Union zur Unterstützung von Strafverfolgungsbehörden als Lügendetektoren oder ähnliche Instrumente verwendet werden sollen;\nc) KI-Systeme, die bestimmungsgemäß von Strafverfolgungsbehörden oder in deren Namen oder von Organen, Einrichtungen und sonstigen Stellen der Union zur Unterstützung von Strafverfolgungsbehörden zur Bewertung der Verlässlichkeit von Beweismitteln im Zuge der Ermittlung oder Verfolgung von Straftaten verwendet werden sollen;\nd) KI-Systeme, die bestimmungsgemäß von Strafverfolgungsbehörden oder in deren Namen oder von Organen, Einrichtungen und sonstigen Stellen der Union zur Unterstützung von Strafverfolgungsbehörden zur Bewertung des Risikos, dass eine natürliche Person eine Straftat begeht oder erneut begeht, nicht nur auf der Grundlage der Erstellung von Profilen natürlicher Personen gemäß Artikel 3 Absatz 4 der Richtlinie (EU) 2016/680 oder zur Bewertung persönlicher Merkmale und Eigenschaften oder vergangenen kriminellen Verhaltens von natürlichen Personen oder Gruppen verwendet werden sollen;\ne) KI-Systeme, die bestimmungsgemäß von Strafverfolgungsbehörden oder in deren Namen oder von Organen, Einrichtungen und sonstigen Stellen der Union zur Unterstützung von Strafverfolgungsbehörden zur Erstellung von Profilen natürlicher Personen gemäß Artikel 3 Absatz 4 der Richtlinie (EU) 2016/680 im Zuge der Aufdeckung, Ermittlung oder Verfolgung von Straftaten verwendet werden sollen."
    },
    {
      "chunk_idx": 326,
      "id": "56f9d88f-1824-4420-b01b-a3d855f3bbaf",
      "title": "Annex III: Z7",
      "relevantChunksIds": [
        "0b34a175-21c7-4d02-9c4e-4380537fff16",
        "7401fefb-6095-47f0-a00e-78aac879499e",
        "50b2ea2a-b39a-49b3-86b1-4b89a8b9dbf6"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX III: High-risk AI systems referred to in Article 6(2) \nHigh-risk AI systems pursuant to Article 6(2) are the AI systems listed in any of the following areas:\n\n7. Migration, asylum and border control management, in so far as their use is permitted under relevant Union or national law:\n(a) AI systems intended to be used by or on behalf of competent public authorities or by Union institutions, bodies, offices or agencies as polygraphs or similar tools;\n(b) AI systems intended to be used by or on behalf of competent public authorities or by Union institutions, bodies, offices or agencies to assess a risk, including a security risk, a risk of irregular migration, or a health risk, posed by a natural person who intends to enter or who has entered into the territory of a Member State;\n(c) AI systems intended to be used by or on behalf of competent public authorities or by Union institutions, bodies, offices or agencies to assist competent public authorities for the examination of applications for asylum, visa or residence permits and for associated complaints with regard to the eligibility of the natural persons applying for a status, including related assessments of the reliability of evidence;\n(d) AI systems intended to be used by or on behalf of competent public authorities, or by Union institutions, bodies, offices or agencies, in the context of migration, asylum or border control management, for the purpose of detecting, recognising or identifying natural persons, with the exception of the verification of travel documents.",
      "original_content": "ANHANG III: Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 2\nAls Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 2 gelten die in folgenden Bereichen aufgeführten KI-Systeme:\n[...]\n7. Migration, Asyl und Grenzkontrolle, soweit ihr Einsatz nach einschlägigem Unionsrecht oder nationalem Recht zugelassen ist:\na) KI-Systeme, die bestimmungsgemäß von zuständigen Behörden oder in deren Namen oder Organen, Einrichtungen und sonstigen Stellen der Union als Lügendetektoren verwendet werden sollen oder ähnliche Instrumente;\nb) KI-Systeme, die bestimmungsgemäß von zuständigen Behörden oder in deren Namen oder von Organen, Einrichtungen und sonstigen Stellen der Union zur Bewertung eines Risikos verwendet werden sollen, einschließlich eines Sicherheitsrisikos, eines Risikos der irregulären Einwanderung oder eines Gesundheitsrisikos, das von einer natürlichen Person ausgeht, die in das Hoheitsgebiet eines Mitgliedstaats einzureisen beabsichtigt oder eingereist ist;\nc) KI-Systeme, die bestimmungsgemäß von zuständigen Behörden oder in deren Namen oder von Organen, Einrichtungen und sonstigen Stellen der Union verwendet werden sollen, um zuständige Behörden bei der Prüfung von Asyl- und Visumanträgen sowie Aufenthaltstiteln und damit verbundenen Beschwerden im Hinblick auf die Feststellung der Berechtigung der den Antrag stellenden natürlichen Personen, einschließlich damit zusammenhängender Bewertungen der Verlässlichkeit von Beweismitteln, zu unterstützen;\nd) KI-Systeme, die bestimmungsgemäß von oder im Namen der zuständigen Behörden oder Organen, Einrichtungen und sonstigen Stellen der Union, im Zusammenhang mit Migration, Asyl oder Grenzkontrolle zum Zwecke der Aufdeckung, Anerkennung oder Identifizierung natürlicher Personen verwendet werden sollen, mit Ausnahme der Überprüfung von Reisedokumenten."
    },
    {
      "chunk_idx": 327,
      "id": "b740c960-f13f-4b09-95d9-3443b6e19b26",
      "title": "Annex III: Z8",
      "relevantChunksIds": [
        "0b34a175-21c7-4d02-9c4e-4380537fff16",
        "7401fefb-6095-47f0-a00e-78aac879499e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX III: High-risk AI systems referred to in Article 6(2) \nHigh-risk AI systems pursuant to Article 6(2) are the AI systems listed in any of the following areas:\n\n8. Administration of justice and democratic processes:\n(a) AI systems intended to be used by a judicial authority or on their behalf to assist a judicial authority in researching and interpreting facts and the law and in applying the law to a concrete set of facts, or to be used in a similar way in alternative dispute resolution;\n(b) AI systems intended to be used for influencing the outcome of an election or referendum or the voting behaviour of natural persons in the exercise of their vote in elections or referenda. This does not include AI systems to the output of which natural persons are not directly exposed, such as tools used to organise, optimise or structure political campaigns from an administrative or logistical point of view.",
      "original_content": "ANHANG III: Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 2\nAls Hochrisiko-KI-Systeme gemäß Artikel 6 Absatz 2 gelten die in folgenden Bereichen aufgeführten KI-Systeme:\n[...]\n8. Rechtspflege und demokratische Prozesse\na) KI-Systeme, die bestimmungsgemäß von einer oder im Namen einer Justizbehörde verwendet werden sollen, um eine Justizbehörde bei der Ermittlung und Auslegung von Sachverhalten und Rechtsvorschriften und bei der Anwendung des Rechts auf konkrete Sachverhalte zu unterstützen, oder die auf ähnliche Weise für die alternative Streitbeilegung genutzt werden sollen;\nb) KI-Systeme, die bestimmungsgemäß verwendet werden sollen, um das Ergebnis einer Wahl oder eines Referendums oder das Wahlverhalten natürlicher Personen bei der Ausübung ihres Wahlrechts bei einer Wahl oder einem Referendum zu beeinflussen. Dazu gehören nicht KI-Systeme, deren Ausgaben natürliche Personen nicht direkt ausgesetzt sind, wie Instrumente zur Organisation, Optimierung oder Strukturierung politischer Kampagnen in administrativer oder logistischer Hinsicht."
    },
    {
      "chunk_idx": 328,
      "id": "96aa4a38-ccae-4c7e-9f06-7c34fd418990",
      "title": "Annex IV",
      "relevantChunksIds": [
        "355be535-f654-47e6-9608-95369e856b37",
        "9d898b6d-782d-4626-8838-1296950b5b7c",
        "aeae4d53-6d81-4c49-843d-b93704615c05",
        "4095ee13-aa11-4440-bbb9-b3bd17715a47",
        "5b7ccaab-a361-4b2e-bf6c-67a90fd684eb",
        "bd393311-1589-4e5f-8a95-ca6421fe8583",
        "80dea4bd-1663-4cf8-a386-88b1788c23dd"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX IV: Technical documentation referred to in Article 11(1) \nThe technical documentation referred to in Article 11(1) shall contain at least the following information, as applicable to the relevant AI system:\n1. A general description of the AI system including:\n(a) its intended purpose, the name of the provider and the version of the system reflecting its relation to previous versions;\n(b) how the AI system interacts with, or can be used to interact with, hardware or software, including with other AI systems, that are not part of the AI system itself, where applicable;\n(c) the versions of relevant software or firmware, and any requirements related to version updates;\n(d) the description of all the forms in which the AI system is placed on the market or put into service, such as software packages embedded into hardware, downloads, or APIs;\n(e) the description of the hardware on which the AI system is intended to run;\n(f) where the AI system is a component of products, photographs or illustrations showing external features, the marking and internal layout of those products;\n(g) a basic description of the user-interface provided to the deployer;\n(h) instructions for use for the deployer, and a basic description of the user-interface provided to the deployer, where applicable;\n2. A detailed description of the elements of the AI system and of the process for its development, including:\n(a) the methods and steps performed for the development of the AI system, including, where relevant, recourse to pre-trained systems or tools provided by third parties and how those were used, integrated or modified by the provider;\n(b) the design specifications of the system, namely the general logic of the AI system and of the algorithms; the key design choices including the rationale and assumptions made, including with regard to persons or groups of persons in respect of who, the system is intended to be used; the main classification choices; what the system is designed to optimise for, and the relevance of the different parameters; the description of the expected output and output quality of the system; the decisions about any possible trade-off made regarding the technical solutions adopted to comply with the requirements set out in Chapter III, Section 2;\n(c) the description of the system architecture explaining how software components build on or feed into each other and integrate into the overall processing; the computational resources used to develop, train, test and validate the AI system;\n(d) where relevant, the data requirements in terms of datasheets describing the training methodologies and techniques and the training data sets used, including a general description of these data sets, information about their provenance, scope and main characteristics; how the data was obtained and selected; labelling procedures (e.g. for supervised learning), data cleaning methodologies (e.g. outliers detection);\n(e) assessment of the human oversight measures needed in accordance with Article 14, including an assessment of the technical measures needed to facilitate the interpretation of the outputs of AI systems by the deployers, in accordance with Article 13(3), point (d);\n(f) where applicable, a detailed description of pre-determined changes to the AI system and its performance, together with all the relevant information related to the technical solutions adopted to ensure continuous compliance of the AI system with the relevant requirements set out in Chapter III, Section 2;\n(g) the validation and testing procedures used, including information about the validation and testing data used and their main characteristics; metrics used to measure accuracy, robustness and compliance with other relevant requirements set out in Chapter III, Section 2, as well as potentially discriminatory impacts; test logs and all test reports dated and signed by the responsible persons, including with regard to pre-determined changes as referred to under point (f);\n(h) cybersecurity measures put in place;\n3. Detailed information about the monitoring, functioning and control of the AI system, in particular with regard to: its capabilities and limitations in performance, including the degrees of accuracy for specific persons or groups of persons on which the system is intended to be used and the overall expected level of accuracy in relation to its intended purpose; the foreseeable unintended outcomes and sources of risks to health and safety, fundamental rights and discrimination in view of the intended purpose of the AI system; the human oversight measures needed in accordance with Article 14, including the technical measures put in place to facilitate the interpretation of the outputs of AI systems by the deployers; specifications on input data, as appropriate;\n4. A description of the appropriateness of the performance metrics for the specific AI system;\n5. A detailed description of the risk management system in accordance with Article 9;\n6. A description of relevant changes made by the provider to the system through its lifecycle;\n7. A list of the harmonised standards applied in full or in part the references of which have been published in the Official Journal of the European Union; where no such harmonised standards have been applied, a detailed description of the solutions adopted to meet the requirements set out in Chapter III, Section 2, including a list of other relevant standards and technical specifications applied;\n8. A copy of the EU declaration of conformity referred to in Article 47;\n9. A detailed description of the system in place to evaluate the AI system performance in the post-market phase in accordance with Article 72, including the post-market monitoring plan referred to in Article 72(3).",
      "original_content": "ANHANG IV: Technische Dokumentation gemäß Artikel 11 Absatz 1\nDie in Artikel 11 Absatz 1 genannte technische Dokumentation muss mindestens die folgenden Informationen enthalten, soweit sie für das betreffende KI-System von Belang sind:\n1. Allgemeine Beschreibung des KI-Systems, darunter\na) Zweckbestimmung, Name des Anbieters und Version des Systems mit Angaben dazu, in welcher Beziehung sie zu vorherigen Versionen steht;\nb) gegebenenfalls Interaktion oder Verwendung des KI-Systems mit Hardware oder Software, einschließlich anderer KI-Systeme, die nicht Teil des KI-Systems selbst sind;\nc) Versionen der betreffenden Software oder Firmware und etwaige Anforderungen in Bezug auf Aktualisierungen der Versionen;\nd) Beschreibung aller Formen, in denen das KI-System in Verkehr gebracht oder in Betrieb genommen wird, zum Beispiel in Hardware eingebettete Softwarepakete, Herunterladen oder API;\ne) Beschreibung der Hardware, auf der das KI-System betrieben werden soll;\nf) falls das KI-System Bestandteil von Produkten ist: Fotografien oder Abbildungen, die äußere Merkmale, die Kennzeichnungen und den inneren Aufbau dieser Produkte zeigen;\ng) eine grundlegende Beschreibung der Benutzerschnittstelle, die dem Betreiber zur Verfügung gestellt wird;\nh) Betriebsanleitungen für den Betreiber und gegebenenfalls eine grundlegende Beschreibung der dem Betreiber zur Verfügung gestellten Benutzerschnittstelle;\n2. Detaillierte Beschreibung der Bestandteile des KI-Systems und seines Entwicklungsprozesses, darunter\na) Methoden und Schritte zur Entwicklung des KI-Systems, gegebenenfalls einschließlich des Einsatzes von durch Dritte bereitgestellten vortrainierten Systemen oder Instrumenten, und wie diese vom Anbieter verwendet, integriert oder verändert wurden;\nb) Entwurfsspezifikationen des Systems, insbesondere die allgemeine Logik des KI-Systems und der Algorithmen; die wichtigsten Entwurfsentscheidungen mit den Gründen und getroffenen Annahmen, einschließlich in Bezug auf Personen oder Personengruppen, bezüglich deren das System angewandt werden soll; hauptsächliche Einstufungsentscheidungen; was das System optimieren soll und welche Bedeutung den verschiedenen Parametern dabei zukommt; Beschreibung der erwarteten Ausgabe des Systems und der erwarteten Qualität dieser Ausgabe; die über mögliche Kompromisse in Bezug auf die technischen Lösungen, mit denen die in Kapitel III Abschnitt 2 festgelegten Anforderungen erfüllt werden sollen, getroffenen Entscheidungen;\nc) Beschreibung der Systemarchitektur, aus der hervorgeht, wie Softwarekomponenten aufeinander aufbauen oder einander zuarbeiten und in die Gesamtverarbeitung integriert sind; zum Entwickeln, Trainieren, Testen und Validieren des KI-Systems verwendete Rechenressourcen;\nd) gegebenenfalls Datenanforderungen in Form von Datenblättern, in denen die Trainingsmethoden und -techniken und die verwendeten Trainingsdatensätze beschrieben werden, einschließlich einer allgemeinen Beschreibung dieser Datensätze sowie Informationen zu deren Herkunft, Umfang und Hauptmerkmalen; Angaben zur Beschaffung und Auswahl der Daten; Kennzeichnungsverfahren (zum Beispiel für überwachtes Lernen) und Datenbereinigungsmethoden (zum Beispiel Erkennung von Ausreißern);\ne) Bewertung der nach Artikel 14 erforderlichen Maßnahmen der menschlichen Aufsicht, mit einer Bewertung der technischen Maßnahmen, die erforderlich sind, um den Betreibern gemäß Artikel 13 Absatz 3 Buchstabe d die Interpretation der Ausgaben von KI-Systemen zu erleichtern;\nf) gegebenenfalls detaillierte Beschreibung der vorab bestimmten Änderungen an dem KI-System und seiner Leistung mit allen einschlägigen Informationen zu den technischen Lösungen, mit denen sichergestellt wird, dass das KI-System die einschlägigen in Kapitel III Abschnitt 2 festgelegten Anforderungen weiterhin dauerhaft erfüllt;\ng) verwendete Validierungs- und Testverfahren, mit Informationen zu den verwendeten Validierungs- und Testdaten und deren Hauptmerkmalen; Parameter, die zur Messung der Genauigkeit, Robustheit und der Erfüllung anderer einschlägiger Anforderungen nach Kapitel III Abschnitt 2 sowie potenziell diskriminierender Auswirkungen verwendet werden; Testprotokolle und alle von den verantwortlichen Personen datierten und unterzeichneten Testberichte, auch in Bezug auf die unter Buchstabe f genannten vorab bestimmten Änderungen;\nh) ergriffene Cybersicherheitsmaßnahmen;\n3. Detaillierte Informationen über die Überwachung, Funktionsweise und Kontrolle des KI-Systems, insbesondere in Bezug auf Folgendes: die Fähigkeiten und Leistungsgrenzen des Systems, einschließlich der Genauigkeitsgrade bei bestimmten Personen oder Personengruppen, auf die es bestimmungsgemäß angewandt werden soll, sowie des in Bezug auf seine Zweckbestimmung insgesamt erwarteten Maßes an Genauigkeit; angesichts der Zweckbestimmung des KI-Systems vorhersehbare unbeabsichtigte Ergebnisse und Quellen von Risiken in Bezug auf Gesundheit und Sicherheit sowie Grundrechte und Diskriminierung; die nach Artikel 14 erforderlichen Maßnahmen der menschlichen Aufsicht, einschließlich der technischen Maßnahmen, die getroffen wurden, um den Betreibern die Interpretation der Ausgaben von KI-Systemen zu erleichtern; gegebenenfalls Spezifikationen zu Eingabedaten;\n4. Darlegungen zur Eignung der Leistungskennzahlen für das spezifische KI-System;\n5. Detaillierte Beschreibung des Risikomanagementsystems gemäß Artikel 9;\n6. Beschreibung einschlägiger Änderungen, die der Anbieter während des Lebenszyklus an dem System vorgenommen hat;\n7. Aufstellung der vollständig oder teilweise angewandten harmonisierten Normen, deren Fundstellen im Amtsblatt der Europäischen Union veröffentlicht wurden; falls keine solchen harmonisierten Normen angewandt wurden, eine detaillierte Beschreibung der Lösungen, mit denen die in Kapitel III Abschnitt 2 festgelegten Anforderungen erfüllt werden sollen, mit einer Aufstellung anderer angewandter einschlägiger Normen und technischer Spezifikationen;\n8. Kopie der EU-Konformitätserklärung gemäß Artikel 47;\n9. Detaillierte Beschreibung des Systems zur Bewertung der Leistung des KI-Systems in der Phase nach dem Inverkehrbringen gemäß Artikel 72, einschließlich des in Artikel 72 Absatz 3 genannten Plans für die Beobachtung nach dem Inverkehrbringen"
    },
    {
      "chunk_idx": 329,
      "id": "5b924767-eab8-4dfe-9f93-8d46e6945058",
      "title": "Annex V",
      "relevantChunksIds": [
        "53305eb0-be55-40af-8070-94d5214e877c"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX V: EU declaration of conformity\nThe EU declaration of conformity referred to in Article 47, shall contain all of the following information:\n1. AI system name and type and any additional unambiguous reference allowing the identification and traceability of the AI system;\n2. The name and address of the provider or, where applicable, of their authorised representative;\n3. A statement that the EU declaration of conformity referred to in Article 47 is issued under the sole responsibility of the provider;\n4. A statement that the AI system is in conformity with this Regulation and, if applicable, with any other relevant Union law that provides for the issuing of the EU declaration of conformity referred to in Article 47;\n5. Where an AI system involves the processing of personal data, a statement that that AI system complies with Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680;\n6. References to any relevant harmonised standards used or any other common specification in relation to which conformity is declared;\n7. Where applicable, the name and identification number of the notified body, a description of the conformity assessment procedure performed, and identification of the certificate issued;\n8. The place and date of issue of the declaration, the name and function of the person who signed it, as well as an indication for, or on behalf of whom, that person signed, a signature.",
      "original_content": "ANHANG V: EU-Konformitätserklärung\nDie EU-Konformitätserklärung gemäß Artikel 47 enthält alle folgenden Angaben:\n1. Name und Art des KI-Systems und etwaige zusätzliche eindeutige Angaben, die die Identifizierung und Rückverfolgbarkeit des KI-Systems ermöglichen;\n2. Name und Anschrift des Anbieters und gegebenenfalls seines Bevollmächtigten;\n3. Erklärung darüber, dass der Anbieter die alleinige Verantwortung für die Ausstellung der EU-Konformitätserklärung gemäß Artikel 47 trägt;\n4. Versicherung, dass das betreffende KI-System der vorliegenden Verordnung sowie gegebenenfalls weiteren einschlägigen Rechtsvorschriften der Union, in denen die Ausstellung der EU-Konformitätserklärung gemäß Artikel 47 vorgesehen ist, entspricht;\n5. wenn ein KI-System die Verarbeitung personenbezogener Daten erfordert, eine Erklärung darüber, dass das KI-System den Verordnungen (EU) 2016/679 und (EU) 2018/1725 sowie der Richtlinie (EU) 2016/680 entspricht;\n6. Verweise auf die verwendeten einschlägigen harmonisierten Normen oder sonstigen gemeinsamen Spezifikationen, für die die Konformität erklärt wird;\n7. gegebenenfalls Name und Identifizierungsnummer der notifizierten Stelle, Beschreibung des durchgeführten Konformitätsbewertungsverfahrens und Identifizierungsnummer der ausgestellten Bescheinigung;\n8. Ort und Datum der Ausstellung der Erklärung, den Namen und die Funktion des Unterzeichners sowie Angabe, für wen oder in wessen Namen diese Person unterzeichnet hat, eine Unterschrift."
    },
    {
      "chunk_idx": 330,
      "id": "071b4a84-31d9-4a02-ac82-9c257536812e",
      "title": "Annex VI",
      "relevantChunksIds": [
        "aeae4d53-6d81-4c49-843d-b93704615c05"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX VI: Conformity assessment procedure based on internal control\n\n3.  The provider examines the information contained in the technical documentation in order to assess the compliance of the AI system with the relevant essential requirements set out in Chapter III, Section 2.\n4.  The provider also verifies that the design and development process of the AI system and its post-market monitoring as referred to in Article 72 is consistent with the technical documentation.",
      "original_content": "ANHANG VI: Konformitätsbewertungsverfahren auf der Grundlage einer internen Kontrolle\n1. Das Konformitätsbewertungsverfahren auf der Grundlage einer internen Kontrolle ist das Konformitätsbewertungsverfahren gemäß den Nummern 2, 3 und 4. 2. Der Anbieter überprüft, ob das bestehende Qualitätsmanagementsystem den Anforderungen des Artikels 17 entspricht.\n3. Der Anbieter prüft die in der technischen Dokumentation enthaltenen Informationen, um zu beurteilen. ob das KI-System den einschlägigen grundlegenden Anforderungen in Kapitel III Abschnitt 2 entspricht.\n4. Der Anbieter überprüft ferner, ob der Entwurfs- und Entwicklungsprozess des KI-Systems und seine Beobachtung nach dem Inverkehrbringen gemäß Artikel 72 mit der technischen Dokumentation im Einklang stehen."
    },
    {
      "chunk_idx": 331,
      "id": "80dea4bd-1663-4cf8-a386-88b1788c23dd",
      "title": "Annex VII",
      "relevantChunksIds": [
        "aeae4d53-6d81-4c49-843d-b93704615c05",
        "2e2eba3d-b388-48a7-99a6-b31c4ea1f382"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX VII: Conformity based on an assessment of the quality management system and an assessment of the technical documentation\n1. Introduction \n\nThe approved quality management system for the design, development and testing of AI systems pursuant to Article 17 shall be examined in accordance with point 3 and shall be subject to surveillance as specified in point 5. The technical documentation of the AI system shall be examined in accordance with point 4.\n3. Quality management system\n3.1. The application of the provider shall include:\n(a) the name and address of the provider and, if the application is lodged by an authorised representative, also their name and address;\n(b) the list of AI systems covered under the same quality management system;\n(c) the technical documentation for each AI system covered under the same quality management system;\n(d) the documentation concerning the quality management system which shall cover all the aspects listed under Article 17;\n(e) a description of the procedures in place to ensure that the quality management system remains adequate and effective;\n(f) a written declaration that the same application has not been lodged with any other notified body.\n3.2. The quality management system shall be assessed by the notified body, which shall determine whether it satisfies the requirements referred to in Article 17.\nThe decision shall be notified to the provider or its authorised representative.\nThe notification shall contain the conclusions of the assessment of the quality management system and the reasoned assessment decision.\n3.3. The quality management system as approved shall continue to be implemented and maintained by the provider so that it remains adequate and efficient.\n3.4. Any intended change to the approved quality management system or the list of AI systems covered by the latter shall be brought to the attention of the notified body by the provider.\nThe proposed changes shall be examined by the notified body, which shall decide whether the modified quality management system continues to satisfy the requirements referred to in point 3.2 or whether a reassessment is necessary.\nThe notified body shall notify the provider of its decision. The notification shall contain the conclusions of the examination of the changes and the reasoned assessment decision.\n4. Control of the technical documentation.\n4.1. In addition to the application referred to in point 3, an application with a notified body of their choice shall be lodged by the provider for the assessment of the technical documentation relating to the AI system which the provider intends to place on the market or put into service and which is covered by the quality management system referred to under point 3.\n4.2. The application shall include:\n(a) the name and address of the provider;\n(b) a written declaration that the same application has not been lodged with any other notified body;\n(c) the technical documentation referred to in Annex IV.\n4.3. The technical documentation shall be examined by the notified body. Where relevant, and limited to what is necessary to fulfil its tasks, the notified body shall be granted full access to the training, validation, and testing data sets used, including, where appropriate and subject to security safeguards, through API or other relevant technical means and tools enabling remote access.\n4.4. In examining the technical documentation, the notified body may require that the provider supply further evidence or carry out further tests so as to enable a proper assessment of the conformity of the AI system with the requirements set out in Chapter III, Section 2. Where the notified body is not satisfied with the tests carried out by the provider, the notified body shall itself directly carry out adequate tests, as appropriate.\n4.5. Where necessary to assess the conformity of the high-risk AI system with the requirements set out in Chapter III, Section 2, after all other reasonable means to verify conformity have been exhausted and have proven to be insufficient, and upon a reasoned request, the notified body shall also be granted access to the training and trained models of the AI system, including its relevant parameters. Such access shall be subject to existing Union law on the protection of intellectual property and trade secrets.\n4.6. The decision of the notified body shall be notified to the provider or its authorised representative. The notification shall contain the conclusions of the assessment of the technical documentation and the reasoned assessment decision.\nWhere the AI system is in conformity with the requirements set out in Chapter III, Section 2, the notified body shall issue a Union technical documentation assessment certificate. The certificate shall indicate the name and address of the provider, the conclusions of the examination, the conditions (if any) for its validity and the data necessary for the identification of the AI system.\nThe certificate and its annexes shall contain all relevant information to allow the conformity of the AI system to be evaluated, and to allow for control of the AI system while in use, where applicable.\nWhere the AI system is not in conformity with the requirements set out in Chapter III, Section 2, the notified body shall refuse to issue a Union technical documentation assessment certificate and shall inform the applicant accordingly, giving detailed reasons for its refusal.\nWhere the AI system does not meet the requirement relating to the data used to train it, re-training of the AI system will be needed prior to the application for a new conformity assessment. In this case, the reasoned assessment decision of the notified body refusing to issue the Union technical documentation assessment certificate shall contain specific considerations on the quality data used to train the AI system, in particular on the reasons for non-compliance.\n4.7. Any change to the AI system that could affect the compliance of the AI system with the requirements or its intended purpose shall be assessed by the notified body which issued the Union technical documentation assessment certificate. The provider shall inform such notified body of its intention to introduce any of the abovementioned changes, or if it otherwise becomes aware of the occurrence of such changes. The intended changes shall be assessed by the notified body, which shall decide whether those changes require a new conformity assessment in accordance with Article 43(4) or whether they could be addressed by means of a supplement to the Union technical documentation assessment certificate. In the latter case, the notified body shall assess the changes, notify the provider of its decision and, where the changes are approved, issue to the provider a supplement to the Union technical documentation assessment certificate.\n5. Surveillance of the approved quality management system.\n5.1. The purpose of the surveillance carried out by the notified body referred to in Point 3 is to make sure that the provider duly complies with the terms and conditions of the approved quality management system.\n5.2. For assessment purposes, the provider shall allow the notified body to access the premises where the design, development, testing of the AI systems is taking place. The provider shall further share with the notified body all necessary information.\n5.3. The notified body shall carry out periodic audits to make sure that the provider maintains and applies the quality management system and shall provide the provider with an audit report. In the context of those audits, the notified body may carry out additional tests of the AI systems for which a Union technical documentation assessment certificate was issued.",
      "original_content": "ANHANG VII: Konformität auf der Grundlage einer Bewertung des Qualitätsmanagementsystems und einer Bewertung der technischen Dokumentation\n1. Einleitung\nDie Konformität auf der Grundlage einer Bewertung des Qualitätsmanagementsystems und einer Bewertung der technischen Dokumentation entspricht dem Konformitätsbewertungsverfahren gemäß den Nummern 2 bis 5. 2. Überblick\nDas genehmigte Qualitätsmanagementsystem für die Konzeption, die Entwicklung und das Testen von KI-Systemen nach Artikel 17 wird gemäß Nummer 3 geprüft und unterliegt der Überwachung gemäß Nummer 5. Die technische Dokumentation des KI-Systems wird gemäß Nummer 4 geprüft.\n3. Qualitätsmanagementsystem\n3.1. Der Antrag des Anbieters muss Folgendes enthalten:\na) Name und Anschrift des Anbieters sowie, wenn der Antrag von einem Bevollmächtigten eingereicht wird, auch dessen Namen und Anschrift;\nb) die Liste der unter dasselbe Qualitätsmanagementsystem fallenden KI-Systeme;\nc) die technische Dokumentation für jedes unter dasselbe Qualitätsmanagementsystem fallende KI-System;\nd) die Dokumentation über das Qualitätsmanagementsystem mit allen in Artikel 17 aufgeführten Aspekten;\ne) eine Beschreibung der bestehenden Verfahren, mit denen sichergestellt wird, dass das Qualitätsmanagementsystem angemessen und wirksam bleibt;\nf) eine schriftliche Erklärung, dass derselbe Antrag bei keiner anderen notifizierten Stelle eingereicht wurde.\n3.2. Das Qualitätssicherungssystem wird von der notifizierten Stelle bewertet, um festzustellen, ob es die in Artikel 17 genannten Anforderungen erfüllt.\nDie Entscheidung wird dem Anbieter oder dessen Bevollmächtigten mitgeteilt.\nDie Mitteilung enthält die Ergebnisse der Bewertung des Qualitätsmanagementsystems und die begründete Bewertungsentscheidung.\n3.3. Das genehmigte Qualitätsmanagementsystem wird vom Anbieter weiter angewandt und gepflegt, damit es stets angemessen und effizient funktioniert.\n3.4. Der Anbieter unterrichtet die notifizierte Stelle über jede beabsichtigte Änderung des genehmigten Qualitätsmanagementsystems oder der Liste der unter dieses System fallenden KI-Systeme.\nDie notifizierte Stelle prüft die vorgeschlagenen Änderungen und entscheidet, ob das geänderte Qualitätsmanagementsystem die unter Nummer 3.2 genannten Anforderungen weiterhin erfüllt oder ob eine erneute Bewertung erforderlich ist.\nDie notifizierte Stelle teilt dem Anbieter ihre Entscheidung mit. Die Mitteilung enthält die Ergebnisse der Prüfung der Änderungen und die begründete Bewertungsentscheidung.\n4. Kontrolle der technischen Dokumentation\n4.1. Zusätzlich zu dem unter Nummer 3 genannten Antrag stellt der Anbieter bei der notifizierten Stelle seiner Wahl einen Antrag auf Bewertung der technischen Dokumentation für das KI-System, das er in Verkehr zu bringen oder in Betrieb zu nehmen beabsichtigt und das unter das unter Nummer 3 genannte Qualitätsmanagementsystem fällt.\n4.2. Der Antrag enthält Folgendes:\na) den Namen und die Anschrift des Anbieters;\nb) eine schriftliche Erklärung, dass derselbe Antrag bei keiner anderen notifizierten Stelle eingereicht wurde;\nc) die in Anhang IV genannte technische Dokumentation.\n4.3. Die technische Dokumentation wird von der notifizierten Stelle geprüft. Sofern es relevant ist und beschränkt auf das zur Wahrnehmung ihrer Aufgaben erforderliche Maß erhält die notifizierte Stelle uneingeschränkten Zugang zu den verwendeten Trainings-, Validierungs- und Testdatensätzen, gegebenenfalls und unter Einhaltung von Sicherheitsvorkehrungen auch über API oder sonstige einschlägige technische Mittel und Instrumente, die den Fernzugriff ermöglichen.\n4.4. Bei der Prüfung der technischen Dokumentation kann die notifizierte Stelle vom Anbieter weitere Nachweise oder die Durchführung weiterer Tests verlangen, um eine ordnungsgemäße Bewertung der Konformität des KI-Systems mit den in Kapitel III Abschnitt 2 festgelegten Anforderungen zu ermöglichen. Ist die notifizierte Stelle mit den vom Anbieter durchgeführten Tests nicht zufrieden, so führt sie gegebenenfalls unmittelbar selbst angemessene Tests durch.\n4.5. Sofern es für die Bewertung der Konformität des Hochrisiko-KI-Systems mit den in Kapitel III Abschnitt 2 festgelegten Anforderungen notwendig ist, und nachdem alle anderen sinnvollen Möglichkeiten zur Überprüfung der Konformität ausgeschöpft sind oder sich als unzureichend erwiesen haben, wird der notifizierten Stelle auf begründeten Antrag Zugang zu den Trainingsmodellen und trainierten Modellen des KI-Systems, einschließlich seiner relevanten Parameter, gewährt. Ein solcher Zugang unterliegt dem bestehenden EU-Recht zum Schutz von geistigem Eigentum und Geschäftsgeheimnissen.\n4.6. Die Entscheidung der notifizierten Stelle wird dem Anbieter oder dessen Bevollmächtigten mitgeteilt. Die Mitteilung enthält die Ergebnisse der Bewertung der technischen Dokumentation und die begründete Bewertungsentscheidung.\nErfüllt das KI-System die Anforderungen in Kapitel III Abschnitt 2, so stellt die notifizierte Stelle eine Unionsbescheinigung über die Bewertung der technischen Dokumentation aus. Diese Bescheinigung enthält den Namen und die Anschrift des Anbieters, die Ergebnisse der Prüfung, etwaige Bedingungen für ihre Gültigkeit und die für die Identifizierung des KI-Systems notwendigen Daten.\nDie Bescheinigung und ihre Anhänge enthalten alle zweckdienlichen Informationen für die Beurteilung der Konformität des KI-Systems und gegebenenfalls für die Kontrolle des KI-Systems während seiner Verwendung.\nErfüllt das KI-System die in Kapitel III Abschnitt 2 festgelegten Anforderungen nicht, so verweigert die notifizierte Stelle die Ausstellung einer Unionsbescheinigung über die Bewertung der technischen Dokumentation und informiert den Antragsteller darüber, wobei sie ihre Verweigerung ausführlich begründet.\nErfüllt das KI-System nicht die Anforderung in Bezug auf die verwendeten Trainingsdaten, so muss das KI-System vor der Beantragung einer neuen Konformitätsbewertung erneut trainiert werden. In diesem Fall enthält die begründete Bewertungsentscheidung der notifizierten Stelle, mit der die Ausstellung der Unionsbescheinigung über die Bewertung der technischen Dokumentation verweigert wird, besondere Erläuterungen zu den zum Trainieren des KI-Systems verwendeten Qualitätsdaten und insbesondere zu den Gründen für die Nichtkonformität.\n4.7. Jede Änderung des KI-Systems, die sich auf die Konformität des KI-Systems mit den Anforderungen oder auf seine Zweckbestimmung auswirken könnte, bedarf der Bewertung durch die notifizierte Stelle, die die Unionsbescheinigung über die Bewertung der technischen Dokumentation ausgestellt hat. Der Anbieter informiert die notifizierte Stelle über seine Absicht, eine der oben genannten Änderungen vorzunehmen, oder wenn er auf andere Weise Kenntnis vom Eintreten solcher Änderungen erhält. Die notifizierte Stelle bewertet die beabsichtigten Änderungen und entscheidet, ob diese Änderungen eine neue Konformitätsbewertung gemäß Artikel 43 Absatz 4 erforderlich machen oder ob ein Nachtrag zu der Unionsbescheinigung über die Bewertung der technischen Dokumentation ausgestellt werden könnte. In letzterem Fall bewertet die notifizierte Stelle die Änderungen, teilt dem Anbieter ihre Entscheidung mit und stellt ihm, sofern die Änderungen genehmigt wurden, einen Nachtrag zu der Unionsbescheinigung über die Bewertung der technischen Dokumentation aus.\n5. Überwachung des genehmigten Qualitätsmanagementsystems\n5.1. Mit der unter Nummer 3 genannten Überwachung durch die notifizierte Stelle soll sichergestellt werden, dass der Anbieter sich ordnungsgemäß an die Anforderungen und Bedingungen des genehmigten Qualitätsmanagementsystems hält.\n5.2. Zu Bewertungszwecken gewährt der Anbieter der notifizierten Stelle Zugang zu den Räumlichkeiten, in denen die Konzeption, die Entwicklung und das Testen der KI-Systeme stattfindet. Außerdem übermittelt der Anbieter der notifizierten Stelle alle erforderlichen Informationen.\n5.3. Die notifizierte Stelle führt regelmäßig Audits durch, um sicherzustellen, dass der Anbieter das Qualitätsmanagementsystem pflegt und anwendet, und übermittelt ihm einen Prüfbericht. Im Rahmen dieser Audits kann die notifizierte Stelle die KI-Systeme, für die eine Unionsbescheinigung über die Bewertung der technischen Dokumentation ausgestellt wurde, zusätzlichen Tests unterziehen."
    },
    {
      "chunk_idx": 332,
      "id": "87917bb9-c211-4e3b-af13-9e472cea45ae",
      "title": "Annex VIII",
      "relevantChunksIds": [
        "19b32d95-81d7-4097-8ca6-ca2c6c0358df",
        "bea96906-be50-4eb8-9cbe-cf07816645c5",
        "e85b8add-b4ef-4422-bafa-b843f3f02662"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX VIII: Information to be submitted upon the registration of high-risk AI systems in accordance with Article 49\nSection A — Information to be submitted by providers of high-risk AI systems in accordance with Article 49(1) \nThe following information shall be provided and thereafter kept up to date with regard to high-risk AI systems to be registered in accordance with Article 49(1):\n1. The name, address and contact details of the provider;\n2. Where submission of information is carried out by another person on behalf of the provider, the name, address and contact details of that person;\n3. The name, address and contact details of the authorised representative, where applicable;\n4. The AI system trade name and any additional unambiguous reference allowing the identification and traceability of the AI system;\n5. A description of the intended purpose of the AI system and of the components and functions supported through this AI system;\n6. A basic and concise description of the information used by the system (data, inputs) and its operating logic;\n7. The status of the AI system (on the market, or in service; no longer placed on the market/in service, recalled);\n8. The type, number and expiry date of the certificate issued by the notified body and the name or identification number of that notified body, where applicable;\n9. A scanned copy of the certificate referred to in point 8, where applicable;\n10. Any Member States in which the AI system has been placed on the market, put into service or made available in the Union;\n11. A copy of the EU declaration of conformity referred to in Article 47;\n12. Electronic instructions for use; this information shall not be provided for high-risk AI systems in the areas of law enforcement or migration, asylum and border control management referred to in Annex III, points 1, 6 and 7;\n13. A URL for additional information (optional).\nSection B — Information to be submitted by providers of high-risk AI systems in accordance with Article 49(2) The following information shall be provided and thereafter kept up to date with regard to AI systems to be registered in accordance with Article 49(2):\n1. The name, address and contact details of the provider;\n1. The name, address and contact details of the provider;\n3. The name, address and contact details of the authorised representative, where applicable;\n3. The name, address and contact details of the authorised representative, where applicable;\n4. The AI system trade name and any additional unambiguous reference allowing the identification and traceability of the AI system;\n6. The condition or conditions under Article 6(3)based on which the AI system is considered to be not-high-risk;\n7. A short summary of the grounds on which the AI system is considered to be not-high-risk in application of the procedure under Article 6(3);\n8. The status of the AI system (on the market, or in service; no longer placed on the market/in service, recalled);\n9. Any Member States in which the AI system has been placed on the market, put into service or made available in the Union.\nSection C — Information to be submitted by deployers of high-risk AI systems in accordance with Article 49(3)\nThe following information shall be provided and thereafter kept up to date with regard to high-risk AI systems to be registered in accordance with Article 49(3):\n1. The name, address and contact details of the deployer;\n2. The name, address and contact details of the person submitting information on behalf of the deployer;\n3. The URL of the entry of the AI system in the EU database by its provider;\n4. A summary of the findings of the fundamental rights impact assessment conducted in accordance with Article 27;\n5. A summary of the data protection impact assessment carried out in accordance with Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680 as specified in Article 26(8) of this Regulation, where applicable.",
      "original_content": "ANHANG VIII: Bei der Registrierung des Hochrisiko-KI-Systems gemäß Artikel 49 bereitzustellende Informationen\nAbschnitt A — Von Anbietern von Hochrisiko-KI-Systemen gemäß Artikel 49 Absatz 1 bereitzustellende Informationen\nFür Hochrisiko-KI-Systeme, die gemäß Artikel 49 Absatz 1 zu registrieren sind, werden folgende Informationen bereitgestellt und danach auf dem neuesten Stand gehalten:\n1. der Name, die Anschrift und die Kontaktdaten des Anbieters;\n2. bei Vorlage von Informationen durch eine andere Person im Namen des Anbieters: der Name, die Anschrift und die Kontaktdaten dieser Person;\n3. gegebenenfalls der Name, die Anschrift und die Kontaktdaten des Bevollmächtigten;\n4. der Handelsname des KI-Systems und etwaige zusätzliche eindeutige Angaben, die die Identifizierung und Rückverfolgbarkeit des KI-Systems ermöglichen;\n5. eine Beschreibung der Zweckbestimmung des KI-Systems und der durch dieses KI-System unterstützten Komponenten und Funktionen;\n6. eine grundlegende und knappe Beschreibung der vom System verwendeten Informationen (Daten, Eingaben) und seiner Betriebslogik;\n7. der Status des KI-Systems (in Verkehr/in Betrieb; nicht mehr in Verkehr/in Betrieb, zurückgerufen);\n8. die Art, die Nummer und das Ablaufdatum der von der notifizierten Stelle ausgestellten Bescheinigung und gegebenenfalls Name oder Identifizierungsnummer dieser notifizierten Stelle;\n9. gegebenenfalls eine gescannte Kopie der in Nummer 8 genannten Bescheinigung;\n10. alle Mitgliedstaaten, in denen das KI-System in Verkehr gebracht, in Betrieb genommen oder in der Union bereitgestellt wurde;\n11. eine Kopie der in Artikel 47 genannten EU-Konformitätserklärung;\n12. elektronische Betriebsanleitungen; dies gilt nicht für Hochrisiko-KI-Systeme in den Bereichen Strafverfolgung oder Migration, Asyl und Grenzkontrolle gemäß Anhang III Nummern 1, 6 und 7;\n13. Eine URL-Adresse für zusätzliche Informationen (fakultativ).\nAbschnitt B — Von Anbietern von Hochrisiko-KI-Systemen gemäß Artikel 49 Absatz 2 bereitzustellende Informationen\nFür KI-Systeme, die gemäß Artikel 49 Absatz 2 zu registrieren sind, werden folgende Informationen bereitgestellt und danach auf dem neuesten Stand gehalten:\n1. der Name, die Anschrift und die Kontaktdaten des Anbieters;\n2. bei Vorlage von Informationen durch eine andere Person im Namen des Anbieters: Name, Anschrift und Kontaktdaten dieser Person;\n3. gegebenenfalls der Name, die Anschrift und die Kontaktdaten des Bevollmächtigten;\n4. der Handelsname des KI-Systems und etwaige zusätzliche eindeutige Angaben, die die Identifizierung und Rückverfolgbarkeit des KI-Systems ermöglichen;\n5. eine Beschreibung der Zweckbestimmung des KI-Systems;\n6. die Bedingung oder Bedingungen gemäß Artikel 6 Absatz 3, aufgrund derer das KI-System nicht als Hoch-Risiko-System eingestuft wird;\n7. eine kurze Zusammenfassung der Gründe, aus denen das KI-System in Anwendung des Verfahrens gemäß Artikel 6 Absatz 3 nicht als Hoch-Risiko-System eingestuft wird;\n8. der Status des KI-Systems (in Verkehr/in Betrieb; nicht mehr in Verkehr/in Betrieb, zurückgerufen) 9. alle Mitgliedstaaten, in denen das KI-System in der Union in Verkehr gebracht, in Betrieb genommen oder bereitgestellt wurde.\nAbschnitt C — Von Betreibern von Hochrisiko-KI-Systemen gemäß Artikel 49 Absatz 3 bereitzustellende Informationen\nFür Hochrisiko-KI-Systeme, die gemäß Artikel 49 Absatz 3 zu registrieren sind, werden folgende Informationen bereitgestellt und danach auf dem neuesten Stand gehalten:\n1. der Name, die Anschrift und die Kontaktdaten des Betreibers;\n2. der Name, die Anschrift und die Kontaktdaten der Person, die im Namen des Betreibers Informationen übermittelt;\n3. die URL des Eintrags des KI-Systems in der EU-Datenbank durch seinen Anbieter;\n4. eine Zusammenfassung der Ergebnisse der gemäß Artikel 27 durchgeführten Grundrechte-Folgenabschätzung;\n5. gegebenenfalls eine Zusammenfassung der im Einklang mit Artikel 35 der Verordnung (EU) 2016/679 oder Artikel 27 der Richtlinie (EU) 2016/680 gemäß Artikel 26 Absatz 8 der vorliegenden Verordnung durchgeführten Datenschutz-Folgenabschätzung."
    },
    {
      "chunk_idx": 333,
      "id": "2ea21537-bdf1-4fc3-9a4d-8be6d80aa4c0",
      "title": "Annex IX",
      "relevantChunksIds": [
        "bea96906-be50-4eb8-9cbe-cf07816645c5",
        "cbfbb51e-9675-4e61-b006-d153a588f1e1"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX IX: Information to be submitted upon the registration of high-risk AI systems listed in Annex III in relation to testing in real world conditions in accordance with Article 60\nThe following information shall be provided and thereafter kept up to date with regard to testing in real world conditions to be registered in accordance with Article 60:\n1. A Union-wide unique single identification number of the testing in real world conditions;\n2. The name and contact details of the provider or prospective provider and of the deployers involved in the testing in real world conditions;\n3. A brief description of the AI system, its intended purpose, and other information necessary for the identification of the system;\n4. A summary of the main characteristics of the plan for testing in real world conditions;\n5. Information on the suspension or termination of the testing in real world conditions.",
      "original_content": "ANHANG IX: Bezüglich Tests unter Realbedingungen gemäß Artikel 60 bei der Registrierung von in Anhang III aufgeführten Hochrisiko-KI-Systemen bereitzustellende Informationen\nBezüglich Tests unter Realbedingungen, die gemäß Artikel 60 zu registrieren sind, werden folgende Informationen bereitgestellt und danach auf dem aktuellen Stand gehalten:\n1. eine unionsweit einmalige Identifizierungsnummer des Tests unter Realbedingungen;\n2. der Name und die Kontaktdaten des Anbieters oder zukünftigen Anbieters und der Betreiber, die an dem Test unter Realbedingungen teilgenommen haben;\n3. eine kurze Beschreibung des KI-Systems, seine Zweckbestimmung und sonstige zu seiner Identifizierung erforderliche Informationen;\n4. eine Übersicht über die Hauptmerkmale des Plans für den Test unter Realbedingungen;\n5. Informationen über die Aussetzung oder den Abbruch des Tests unter Realbedingungen."
    },
    {
      "chunk_idx": 334,
      "id": "43ab68de-0530-41bb-a1b6-230ebfc984ec",
      "title": "Annex X",
      "relevantChunksIds": [
        "5a35c644-0096-47c7-88ed-c793b728774e"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX X: Union legislative acts on large-scale IT systems in the area of Freedom, Security and Justice\n1. Schengen Information System\n(a) Regulation (EU) 2018/1860 of the European Parliament and of the Council of 28 November 2018 on the use of the Schengen Information System for the return of illegally staying third-country nationals (OJ L 312, 7.12.2018, p. 1).\n(b) Regulation (EU) 2018/1861 of the European Parliament and of the Council of 28 November 2018 on the establishment, operation and use of the Schengen Information System (SIS) in the field of border checks, and amending the Convention implementing the Schengen Agreement, and amending and repealing Regulation (EC) No 1987/2006 (OJ L 312, 7.12.2018, p. 14).\n(c) Regulation (EU) 2018/1862 of the European Parliament and of the Council of 28 November 2018 on the establishment, operation and use of the Schengen Information System (SIS) in the field of police cooperation and judicial cooperation in criminal matters, amending and repealing Council Decision 2007/533/JHA, and repealing Regulation (EC) No 1986/2006 of the European Parliament and of the Council and Commission Decision 2010/261/EU (OJ L 312, 7.12.2018, p. 56).\n2. Visa Information System\n(a) Regulation (EU) 2021/1133 of the European Parliament and of the Council of 7 July 2021 amending Regulations (EU) No 603/2013, (EU) 2016/794, (EU) 2018/1862, (EU) 2019/816 and (EU) 2019/818 as regards the establishment of the conditions for accessing other EU information systems for the purposes of the Visa Information System (OJ L 248, 13.7.2021, p. 1).\n(b) Regulation (EU) 2021/1134 of the European Parliament and of the Council of 7 July 2021 amending Regulations (EC) No 767/2008, (EC) No 810/2009, (EU) 2016/399, (EU) 2017/2226, (EU) 2018/1240, (EU) 2018/1860, (EU) 2018/1861, (EU) 2019/817 and (EU) 2019/1896 of the European Parliament and of the Council and repealing Council Decisions 2004/512/EC and 2008/633/JHA, for the purpose of reforming the Visa Information System (OJ L 248, 13.7.2021, p. 11).\n3. Eurodac\n\nRegulation (EU) 2017/2226 of the European Parliament and of the Council of 30 November 2017 establishing an Entry/Exit System (EES) to register entry and exit data and refusal of entry data of third-country nationals crossing the external borders of the Member States and determining the conditions for access to the EES for law enforcement purposes, and amending the Convention implementing the Schengen Agreement and Regulations (EC) No 767/2008 and (EU) No 1077/2011 (OJ L 327, 9.12.2017, p. 20).\n5. European Travel Information and Authorisation System\n(a) Regulation (EU) 2018/1240 of the European Parliament and of the Council of 12 September 2018 establishing a European Travel Information and Authorisation System (ETIAS) and amending Regulations (EU) No 1077/2011, (EU) No 515/2014, (EU) 2016/399, (EU) 2016/1624 and (EU) 2017/2226 (OJ L 236, 19.9.2018, p. 1).\n(b) Regulation (EU) 2018/1241 of the European Parliament and of the Council of 12 September 2018 amending Regulation (EU) 2016/794 for the purpose of establishing a European Travel Information and Authorisation System (ETIAS) (OJ L 236, 19.9.2018, p. 72).\n6. European Criminal Records Information System on third-country nationals and stateless persons\nRegulation (EU) 2019/816 of the European Parliament and of the Council of 17 April 2019 establishing a centralised system for the identification of Member States holding conviction information on third-country nationals and stateless persons (ECRIS-TCN) to supplement the European Criminal Records Information System and amending Regulation (EU) 2018/1726 (OJ L 135, 22.5.2019, p. 1).\n7. Interoperability\n(a) Regulation (EU) 2019/817 of the European Parliament and of the Council of 20 May 2019 on establishing a framework for interoperability between EU information systems in the field of borders and visa and amending Regulations (EC) No 767/2008, (EU) 2016/399, (EU) 2017/2226, (EU) 2018/1240, (EU) 2018/1726 and (EU) 2018/1861 of the European Parliament and of the Council and Council Decisions 2004/512/EC and 2008/633/JHA (OJ L 135, 22.5.2019, p. 27).\n(b) Regulation (EU) 2019/818 of the European Parliament and of the Council of 20 May 2019 on establishing a framework for interoperability between EU information systems in the field of police and judicial cooperation, asylum and migration and amending Regulations (EU) 2018/1726, (EU) 2018/1862 and (EU) 2019/816 (OJ L 135, 22.5.2019, p. 85).",
      "original_content": "ANHANG X: Rechtsvorschriften der Union über IT-Großsysteme im Raum der Freiheit, der Sicherheit und des Rechts\n1. Schengener Informationssystem\na) Verordnung (EU) 2018/1860 des Europäischen Parlaments und des Rates vom 28. November 2018 über die Nutzung des Schengener Informationssystems für die Rückkehr illegal aufhältiger Drittstaatsangehöriger (ABl. L 312 vom 7.12.2018, S. 1)\nb) Verordnung (EU) 2018/1861 des Europäischen Parlaments und des Rates vom 28. November 2018 über die Einrichtung, den Betrieb und die Nutzung des Schengener Informationssystems (SIS) im Bereich der Grenzkontrollen, zur Änderung des Übereinkommens zur Durchführung des Übereinkommens von Schengen und zur Änderung und Aufhebung der Verordnung (EG) Nr. 1987/2006 (ABl. L 312 vom 7.12.2018, S. 14)\nc) Verordnung (EU) 2018/1862 des Europäischen Parlaments und des Rates vom 28. November 2018 über die Einrichtung, den Betrieb und die Nutzung des Schengener Informationssystems (SIS) im Bereich der polizeilichen Zusammenarbeit und der justiziellen Zusammenarbeit in Strafsachen, zur Änderung und Aufhebung des Beschlusses 2007/533/JI des Rates und zur Aufhebung der Verordnung (EG) Nr. 1986/2006 des Europäischen Parlaments und des Rates und des Beschlusses 2010/261/EU der Kommission (ABl. L 312 vom 7.12.2018, S. 56)\n2. Visa-Informationssystem\na) Verordnung (EU) 2021/1133 des Europäischen Parlaments und des Rates vom 7. Juli 2021 zur Änderung der Verordnungen (EU) Nr. 603/2013, (EU) 2016/794, (EU) 2018/1862, (EU) 2019/816 und (EU) 2019/818 hinsichtlich der Festlegung der Voraussetzungen für den Zugang zu anderen Informationssystemen der EU für Zwecke des Visa-Informationssystems (ABl. L 248 vom 13.7.2021, S. 1)\nb) Verordnung (EU) 2021/1134 des Europäischen Parlaments und des Rates vom 7. Juli 2021 zur Änderung der Verordnungen (EG) Nr. 767/2008, (EG) Nr. 810/2009, (EU) 2016/399, (EU) 2017/2226, (EU) 2018/1240, (EU) 2018/1860, (EU) 2018/1861, (EU) 2019/817 und (EU) 2019/1896 des Europäischen Parlaments und des Rates und zur Aufhebung der Entscheidung 2004/512/EG und des Beschlusses 2008/633/JI des Rates zum Zwecke der Reform des Visa-Informationssystems (ABl. L 248 vom 13.7.2021, S. 11)\n3. Eurodac\nVerordnung (EU) 2024/1358 des Europäischen Parlaments und des Rates vom 14. Mai 2024 über die Einrichtung von Eurodac für den Abgleich biometrischer Daten zum Zwecke der effektiven Anwendung der Verordnungen (EU) 2024/1315 und (EU) 2024/1350 des Europäischen Parlaments und des Rates und der Richtlinie 2001/55/EG des Rates und zur Feststellung der Identität illegal aufhältiger Drittstaatsangehöriger und Staatenloser und über der Gefahrenabwehr und Strafverfolgung dienende Anträge der Gefahrenabwehr- und Strafverfolgungsbehörden der Mitgliedstaaten und Europols auf den Abgleich mit Eurodac-Daten sowie zur Änderung der Verordnungen (EU) 2018/1240 und (EU) 2019/818 des Europäischen Parlaments und des Rates und zur Aufhebung der Verordnung (EU) Nr. 603/2013 des Europäischen Parlaments und des Rates (ABl. L, 2024/1358, 22.5.2024, ELI: http://data.europa.eu/eli/reg/2024/1358/oj) 4. Einreise-/Ausreisesystem\nVerordnung (EU) 2017/2226 des Europäischen Parlaments und des Rates vom 30. November 2017 über ein Einreise-/Ausreisesystem (EES) zur Erfassung der Ein- und Ausreisedaten sowie der Einreiseverweigerungsdaten von Drittstaatsangehörigen an den Außengrenzen der Mitgliedstaaten und zur Festlegung der Bedingungen für den Zugang zum EES zu Gefahrenabwehr- und Strafverfolgungszwecken und zur Änderung des Übereinkommens von Schengen sowie der Verordnungen (EG) Nr. 767/2008 und (EU) Nr. 1077/2011 (ABl. L 327 vom 9.12.2017, S. 20)\n5. Europäisches Reiseinformations- und -genehmigungssystem\na) Verordnung (EU) 2018/1240 des Europäischen Parlaments und des Rates vom 12. September 2018 über die Einrichtung eines Europäischen Reiseinformations- und -genehmigungssystems (ETIAS) und zur Änderung der Verordnungen (EU) Nr. 1077/2011, (EU) Nr. 515/2014, (EU) 2016/399, (EU) 2016/1624 und (EU) 2017/2226 (ABl. L 236 vom 19.9.2018, S. 1)\nb) Verordnung (EU) 2018/1241 des Europäischen Parlaments und des Rates vom 12. September 2018 zur Änderung der Verordnung (EU) 2016/794 für die Zwecke der Einrichtung eines Europäischen Reiseinformations- und -genehmigungssystems (ETIAS) (ABl. L 236 vom 19.9.2018, S. 72)\n6. Europäisches Strafregisterinformationssystem über Drittstaatsangehörige und Staatenlose\nVerordnung (EU) 2019/816 des Europäischen Parlaments und des Rates vom 17. April 2019 zur Einrichtung eines zentralisierten Systems für die Ermittlung der Mitgliedstaaten, in denen Informationen zu Verurteilungen von Drittstaatsangehörigen und Staatenlosen (ECRIS-TCN) vorliegen, zur Ergänzung des Europäischen Strafregisterinformationssystems und zur Änderung der Verordnung (EU) 2018/1726 (ABl. L 135 vom 22.5.2019, S. 1)\n7. Interoperabilität\na) Verordnung (EU) 2019/817 des Europäischen Parlaments und des Rates vom 20. Mai 2019 zur Errichtung eines Rahmens für die Interoperabilität zwischen EU-Informationssystemen in den Bereichen Grenzen und Visa und zur Änderung der Verordnungen (EG) Nr. 767/2008, (EU) 2016/399, (EU) 2017/2226, (EU) 2018/1240, (EU) 2018/1726 und (EU) 2018/1861 des Europäischen Parlaments und des Rates, der Entscheidung 2004/512/EG des Rates und des Beschlusses 2008/633/JI des Rates (ABl. L 135 vom 22.5.2019, S. 27)\nb) Verordnung (EU) 2019/818 des Europäischen Parlaments und des Rates vom 20. Mai 2019 zur Errichtung eines Rahmens für die Interoperabilität zwischen EU-Informationssystemen (polizeiliche und justizielle Zusammenarbeit, Asyl und Migration) und zur Änderung der Verordnungen (EU) 2018/1726, (EU) 2018/1862 und (EU) 2019/816 (ABl. L 135 vom 22.5.2019, S. 85)."
    },
    {
      "chunk_idx": 335,
      "id": "fe6a85d3-98f4-4ac0-9055-3b4dad052553",
      "title": "Annex XI",
      "relevantChunksIds": [
        "7bed8e5e-3585-48d6-889d-68089e0581a4",
        "49e88b0c-2141-4832-bc84-ee8b5fef7d4f"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX XI: Technical documentation referred to in Article 53(1), point (a) — technical documentation for providers of general-purpose AI models\nSection 1\nInformation to be provided by all providers of general-purpose AI models\nThe technical documentation referred to in Article 53(1), point (a) shall contain at least the following information as appropriate to the size and risk profile of the model:\n1. A general description of the general-purpose AI model including:\n(a) the tasks that the model is intended to perform and the type and nature of AI systems in which it can be integrated;\n(b) the acceptable use policies applicable;\n(c) the date of release and methods of distribution;\n(d) the architecture and number of parameters;\n(e) the modality (e.g. text, image) and format of inputs and outputs;\n(f) the licence.\n2. A detailed description of the elements of the model referred to in point 1, and relevant information of the process for the development, including the following elements:\n(a) the technical means (e.g. instructions of use, infrastructure, tools) required for the general-purpose AI model to be integrated in AI systems;\n(b) the design specifications of the model and training process, including training methodologies and techniques, the key design choices including the rationale and assumptions made; what the model is designed to optimise for and the relevance of the different parameters, as applicable;\n(c) information on the data used for training, testing and validation, where applicable, including the type and provenance of data and curation methodologies (e.g. cleaning, filtering, etc.), the number of data points, their scope and main characteristics; how the data was obtained and selected as well as all other measures to detect the unsuitability of data sources and methods to detect identifiable biases, where applicable;\n(d) the computational resources used to train the model (e.g. number of floating point operations), training time, and other relevant details related to the training;\n(e) known or estimated energy consumption of the model.\nWith regard to point (e), where the energy consumption of the model is unknown, the energy consumption may be based on information about computational resources used.\nSection 2\nAdditional information to be provided by providers of general-purpose AI models with systemic risk\n1. A detailed description of the evaluation strategies, including evaluation results, on the basis of available public evaluation protocols and tools or otherwise of other evaluation methodologies. Evaluation strategies shall include evaluation criteria, metrics and the methodology on the identification of limitations.\n2. Where applicable, a detailed description of the measures put in place for the purpose of conducting internal and/or external adversarial testing (e.g. red teaming), model adaptations, including alignment and fine-tuning.\n3. Where applicable, a detailed description of the system architecture explaining how software components build or feed into each other and integrate into the overall processing.",
      "original_content": "ANHANG XI: Technische Dokumentation gemäß Artikel 53 Absatz 1 Buchstabe a — technische Dokumentation für Anbieter von KI-Modellen mit allgemeinem Verwendungszweck\nAbschnitt 1\nVon allen Anbietern von KI-Modellen mit allgemeinem Verwendungszweck bereitzustellende Informationen\nDie in Artikel 53 Absatz 1 Buchstabe a genannte technische Dokumentation muss mindestens die folgenden Informationen enthalten, soweit es anhand der Größe und des Risikoprofils des betreffenden Modells angemessen ist:\n1. Eine allgemeine Beschreibung des KI-Modells einschließlich\na) der Aufgaben, die das Modell erfüllen soll, sowie der Art und des Wesens der KI-Systeme, in die es integriert werden kann;\nb) die anwendbaren Regelungen der akzeptablen Nutzung;\nc) das Datum der Freigabe und die Vertriebsmethoden;\nd) die Architektur und die Anzahl der Parameter;\ne) die Modalität (zum Beispiel Text, Bild) und das Format der Ein- und Ausgaben;\nf) die Lizenz.\n2. Eine ausführliche Beschreibung der Elemente des Modells gemäß Nummer 1 und relevante Informationen zum Entwicklungsverfahren, einschließlich der folgenden Elemente:\na) die technischen Mittel (zum Beispiel Betriebsanleitungen, Infrastruktur, Instrumente), die für die Integration des KI-Modells mit allgemeinem Verwendungszweck in KI-Systeme erforderlich sind;\nb) die Entwurfsspezifikationen des Modells und des Trainingsverfahrens einschließlich Trainingsmethoden und -techniken, die wichtigsten Entwurfsentscheidungen mit den Gründen und getroffenen Annahmen; gegebenenfalls, was das Modell optimieren soll und welche Bedeutung den verschiedenen Parametern dabei zukommt;\nc) gegebenenfalls Informationen über die für das Trainieren, Testen und Validieren verwendeten Daten, einschließlich der Art und Herkunft der Daten und der Aufbereitungsmethoden (zum Beispiel Bereinigung, Filterung usw.), der Zahl der Datenpunkte, ihres Umfangs und ihrer Hauptmerkmale; gegebenenfalls die Art und Weise, wie die Daten erlangt und ausgewählt wurden, sowie alle anderen Maßnahmen zur Feststellung, ob Datenquellen ungeeignet sind, und Methoden zur Erkennung ermittelbarer Verzerrungen;\nd) die für das Trainieren des Modells verwendeten Rechenressourcen (zum Beispiel Anzahl der Gleitkommaoperationen), die Trainingszeit und andere relevante Einzelheiten im Zusammenhang mit dem Trainieren;\ne) bekannter oder geschätzter Energieverbrauch des Modells.\nWenn der Energieverbrauch des Modells nicht bekannt ist, kann für Buchstabe e der Energieverbrauch auf Informationen über die eingesetzten Rechenressourcen gestützt werden.\nAbschnitt 2\nZusätzliche von Anbietern von KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko bereitzustellende Informationen\n1. Eine ausführliche Beschreibung der Prüfstrategien, einschließlich der Prüfungsergebnisse, auf der Grundlage öffentlich verfügbarer Prüfprotokolle und -instrumente oder anderer Prüfmethoden. Die Prüfstrategien umfassen Prüfkriterien und -metrik sowie die Methodik zur Ermittlung von Einschränkungen.\n2. Gegebenenfalls eine ausführliche Beschreibung der Maßnahmen, die ergriffen wurden, um interne und/oder externe Angriffstests durchzuführen (zum Beispiel Red Teaming), Modellanpassungen, einschließlich Ausrichtung und Feinabstimmung.\n3. Gegebenenfalls eine ausführliche Beschreibung der Systemarchitektur, aus der hervorgeht, wie Softwarekomponenten aufeinander aufbauen oder einander zuarbeiten und in die Gesamtverarbeitung integriert sind."
    },
    {
      "chunk_idx": 336,
      "id": "137f49d0-d9b0-4b55-a3dd-6c89ca9ca7b4",
      "title": "Annex XII",
      "relevantChunksIds": [
        "7bed8e5e-3585-48d6-889d-68089e0581a4"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX XII: Transparency information referred to in Article 53(1), point (b) — technical documentation for providers of general-purpose AI models to downstream providers that integrate the model into their AI system\nThe information referred to in Article 53(1), point (b) shall contain at least the following:\n1. A general description of the general-purpose AI model including:\n(a) the tasks that the model is intended to perform and the type and nature of AI systems in which it can be integrated;\n(b) the acceptable use policies applicable;\n(c) the date of release and methods of distribution;\n(d) how the model interacts, or can be used to interact, with hardware or software that is not part of the model itself, where applicable;\n(e) the versions of relevant software related to the use of the general-purpose AI model, where applicable;\n(f) the architecture and number of parameters;\n(g) the modality (e.g. text, image) and format of inputs and outputs;\n(h) the licence for the model.\n2. A description of the elements of the model and of the process for its development, including:\n(a) the technical means (e.g. instructions of use, infrastructure, tools) required for the general-purpose AI model to be integrated in AI systems;\n(b) the modality (e.g. text, image, etc.) and format of the inputs and outputs and their maximum size (e.g. context window length, etc.);\n(c) information on the data used for training, testing and validation, where applicable, including the type and provenance of data and curation methodologies.",
      "original_content": "ANHANG XII: Transparenzinformationen gemäß Artikel 53 Absatz 1 Buchstabe b — technische Dokumentation für Anbieter von KI-Modellen mit allgemeinem Verwendungszweck für nachgelagerte Anbieter, die das Modell in ihr KI-System integrieren\nDie in Artikel 53 Absatz 1 Buchstabe b genannten Informationen enthalten mindestens Folgendes:\n1. eine allgemeine Beschreibung des KI-Modells einschließlich\na) der Aufgaben, die das Modell erfüllen soll, sowie der Art und des Wesens der KI-Systeme, in die es integriert werden kann;\nb) die anwendbaren Regelungen der akzeptablen Nutzung;\nc) das Datum der Freigabe und die Vertriebsmethoden;\nd) gegebenenfalls wie das Modell mit Hardware oder Software interagiert, die nicht Teil des Modells selbst ist, oder wie es zu einer solchen Interaktion verwendet werden kann;\ne) gegebenenfalls die Versionen der einschlägigen Software im Zusammenhang mit der Verwendung des KI-Modells mit allgemeinem Verwendungszweck;\nf) die Architektur und die Anzahl der Parameter;\ng) die Modalität (zum Beispiel Text, Bild) und das Format der Ein- und Ausgaben;\nh) die Lizenz für das Modell.\n2. Eine Beschreibung der Bestandteile des Modells und seines Entwicklungsprozesses, einschließlich\na) die technischen Mittel (zum Beispiel Betriebsanleitungen, Infrastruktur, Instrumente), die für die Integration des KI-Modells mit allgemeinem Verwendungszweck in KI-Systeme erforderlich sind;\nb) Modalität (zum Beispiel Text, Bild usw.) und Format der Ein- und Ausgaben und deren maximale Größe (zum Beispiel Länge des Kontextfensters usw.);\nc) gegebenenfalls Informationen über die für das Trainieren, Testen und Validieren verwendeten Daten, einschließlich der Art und Herkunft der Daten und der Aufbereitungsmethoden."
    },
    {
      "chunk_idx": 337,
      "id": "58565563-a589-4b17-bca1-cd1220dd2302",
      "title": "Annex XIII",
      "relevantChunksIds": [
        "18f063a4-31d6-40a9-b45a-da3f602f920f",
        "e4d5fea4-38ce-4829-87ad-748c71e5bfd1"
      ],
      "keywords": [],
      "availableKeywords": [],
      "negativeKeywords": [],
      "parameters": [],
      "content": "ANNEX XIII: Criteria for the designation of general-purpose AI models with systemic risk referred to in Article 51\nFor the purpose of determining that a general-purpose AI model has capabilities or an impact equivalent to those set out in Article 51(1), point (a), the Commission shall take into account the following criteria:\n(a) the number of parameters of the model;\n(b) the quality or size of the data set, for example measured through tokens;\n(c) the amount of computation used for training the model, measured in floating point operations or indicated by a combination of other variables such as estimated cost of training, estimated time required for the training, or estimated energy consumption for the training;\n(d) the input and output modalities of the model, such as text to text (large language models), text to image, multi-modality, and the state of the art thresholds for determining high-impact capabilities for each modality, and the specific type of inputs and outputs (e.g. biological sequences);\n(e) the benchmarks and evaluations of capabilities of the model, including considering the number of tasks without additional training, adaptability to learn new, distinct tasks, its level of autonomy and scalability, the tools it has access to;\n(f) whether it has a high impact on the internal market due to its reach, which shall be presumed when it has been made available to at least 10 000 registered business users established in the Union;\n(g) the number of registered end-users.",
      "original_content": "ANHANG XIII: Kriterien für die Benennung von KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko gemäß Artikel 51\nUm festzustellen, ob ein KI-Modell mit allgemeinem Verwendungszweck über Fähigkeiten oder eine Wirkung verfügt, die den in Artikel 51 Absatz 1 Buchstabe a genannten gleichwertig sind, berücksichtigt die Kommission folgende Kriterien:\na) die Anzahl der Parameter des Modells;\nb) die Qualität oder Größe des Datensatzes, zum Beispiel durch Tokens gemessen;\nc) die Menge der für das Trainieren des Modells verwendeten Berechnungen, gemessen in Gleitkommaoperationen oder anhand einer Kombination anderer Variablen, wie geschätzte Trainingskosten, geschätzter Zeitaufwand für das Trainieren oder geschätzter Energieverbrauch für das Trainieren;\nd) die Ein- und Ausgabemodalitäten des Modells, wie Text-Text (Große Sprachmodelle), Text-Bild, Multimodalität, Schwellenwerte auf dem Stand der Technik für die Bestimmung der Fähigkeiten mit hoher Wirkkraft für jede Modalität und die spezifische Art der Ein- und Ausgaben (zum Beispiel biologische Sequenzen);\ne) die Benchmarks und Beurteilungen der Fähigkeiten des Modells, einschließlich unter Berücksichtigung der Zahl der Aufgaben ohne zusätzliches Training, der Anpassungsfähigkeit zum Erlernen neuer, unterschiedlicher Aufgaben, des Grades an Autonomie und Skalierbarkeit sowie der Instrumente, zu denen es Zugang hat;\nf) ob es aufgrund seiner Reichweite große Auswirkungen auf den Binnenmarkt hat — davon wird ausgegangen, wenn es mindestens 10 000 in der Union niedergelassenen registrierten gewerblichen Nutzern zur Verfügung gestellt wurde;\ng) die Zahl der registrierten Endnutzer."
    }
  ]
}