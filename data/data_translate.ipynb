{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T10:41:33.217676Z",
     "start_time": "2025-11-07T10:41:33.208733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load JSON data from combined_en.json\n",
    "with open('combined_en.json', 'r', encoding='utf-8') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Print basic structure information\n",
    "print(f\"JSON data type: {type(json_data)}\")\n",
    "print(f\"Top-level keys: {list(json_data.keys())}\")\n",
    "if 'chunks' in json_data:\n",
    "    print(f\"Number of chunks: {len(json_data['chunks'])}\")\n",
    "    print(f\"First chunk keys: {list(json_data['chunks'][0].keys())}\")"
   ],
   "id": "8bba2a7b8e2be8cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data type: <class 'dict'>\n",
      "Top-level keys: ['id', 'date_created', 'last_modified', 'chunks']\n",
      "Number of chunks: 477\n",
      "First chunk keys: ['id', 'title', 'content', 'keywords', 'availableKeywords', 'negativeKeywords', 'relevantChunksIds', 'parameters']\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T10:41:33.901820Z",
     "start_time": "2025-11-07T10:41:33.896314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load text file into linewise array\n",
    "with open('regulation-2024-1689.txt', 'r', encoding='utf-8') as f:\n",
    "    regulation_lines = f.readlines()\n",
    "\n",
    "# Remove newline characters\n",
    "regulation_lines = [line.rstrip('\\n') for line in regulation_lines]\n",
    "\n",
    "# Display basic information about the text file\n",
    "print(f\"Total number of lines: {len(regulation_lines)}\")\n",
    "print(\"\\nFirst 5 lines of the regulation:\")\n",
    "for i, line in enumerate(regulation_lines[:5]):\n",
    "    print(f\"{i + 1}: {line[:100]}{'...' if len(line) > 100 else ''}\")"
   ],
   "id": "b6563218afba34df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lines: 1734\n",
      "\n",
      "First 5 lines of the regulation:\n",
      "1: REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\n",
      "2: of 13 June 2024\n",
      "3: laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (...\n",
      "4: THE EUROPEAN PARLIAMENT AND THE COUNCIL OF THE EUROPEAN UNION,\n",
      "5: Having regard to the Treaty on the Functioning of the European Union, and in particular Articles 16 ...\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T10:41:34.859654Z",
     "start_time": "2025-11-07T10:41:34.853369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load text file into linewise array\n",
    "with open('verordnung-2024-1689_final_version.txt', 'r', encoding='utf-8') as f:\n",
    "    regulation_lines_de = f.readlines()\n",
    "\n",
    "# Remove newline characters\n",
    "regulation_lines_de = [line.rstrip('\\n') for line in regulation_lines_de]\n",
    "\n",
    "# Display basic information about the text file\n",
    "print(f\"Total number of lines: {len(regulation_lines_de)}\")\n",
    "print(\"\\nFirst 5 lines of the regulation:\")\n",
    "for i, line in enumerate(regulation_lines_de[:5]):\n",
    "    print(f\"{i + 1}: {line[:100]}{'...' if len(line) > 100 else ''}\")"
   ],
   "id": "7c725d7dbc91f7a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lines: 1676\n",
      "\n",
      "First 5 lines of the regulation:\n",
      "1: VERORDNUNG (EU) 2024/1689 DES EUROPÄISCHEN PARLAMENTS UND DES RATES\n",
      "2: vom 13. Juni 2024\n",
      "3: zur Festlegung harmonisierter Vorschriften für künstliche Intelligenz und zur Änderung der Verordnun...\n",
      "4: DAS EUROPÄISCHE PARLAMENT UND DER RAT DER EUROPÄISCHEN UNION —\n",
      "5: gestützt auf den Vertrag über die Arbeitsweise der Europäischen Union, insbesondere auf die Artikel ...\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T10:41:43.453497Z",
     "start_time": "2025-11-07T10:41:43.450459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Exploring the JSON structure a bit deeper\n",
    "if 'chunks' in json_data:\n",
    "    first_chunk = json_data['chunks'][0]\n",
    "    print(\"Sample chunk content structure:\")\n",
    "    print(f\"Title: {first_chunk.get('title', 'N/A')}\")\n",
    "    content_preview = first_chunk.get('content', 'N/A')[:150] + '...' if len(\n",
    "        first_chunk.get('content', 'N/A')) > 150 else first_chunk.get('content', 'N/A')\n",
    "    print(f\"Content preview: {content_preview}\")\n",
    "    print(f\"Keywords: {first_chunk.get('keywords', [])}\")\n",
    "    print(f\"Number of relevant chunks: {len(first_chunk.get('relevantChunksIds', []))}\")\n"
   ],
   "id": "39748d652c884417",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunk content structure:\n",
      "Title: KI-Servicestelle: FAQ - Was macht die KI-Servicestelle der RTR?\n",
      "Content preview: # Was macht die KI-Servicestelle der RTR?\n",
      "\n",
      "Die KI-Servicestelle bei der RTR, gilt als Ansprechpartner und Informationshub und steht dem österreichisch...\n",
      "Keywords: []\n",
      "Number of relevant chunks: 3\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fixing the Issue with Displaying Chunk Titles\n",
    "\n",
    "Let's debug and fix the code to properly display the titles of filtered chunks."
   ],
   "id": "3dd3ca88d5c15ab6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T10:41:51.495925Z",
     "start_time": "2025-11-07T10:41:51.491874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filter chunks that don't begin with \"KI-Servicestelle\" in their title\n",
    "filtered_chunks = []\n",
    "\n",
    "for chunk in json_data['chunks']:\n",
    "    title = chunk.get('title', '')\n",
    "    if title != \"\" and not title.startswith(\"KI-Servicestelle\"):\n",
    "        filtered_chunks.append(chunk)\n",
    "\n",
    "# Display information about the filtering\n",
    "print(f\"Original number of chunks: {len(json_data['chunks'])}\")\n",
    "print(f\"Number of chunks after filtering: {len(filtered_chunks)}\")\n",
    "print(f\"Removed {len(json_data['chunks']) - len(filtered_chunks)} chunks with 'KI-Servicestelle' in the title\")\n",
    "\n",
    "# Display the title attribute directly for the first few filtered chunks\n",
    "if filtered_chunks:\n",
    "    print(\"\\nSample filtered chunks (titles):\")\n",
    "    for i, chunk in enumerate(filtered_chunks[:5]):\n",
    "        print(f\"{i + 1}. {chunk['title']}\")\n"
   ],
   "id": "ac6ef4b3dda3851f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of chunks: 477\n",
      "Number of chunks after filtering: 338\n",
      "Removed 139 chunks with 'KI-Servicestelle' in the title\n",
      "\n",
      "Sample filtered chunks (titles):\n",
      "1. ErwG 1\n",
      "2. ErwG 2\n",
      "3. ErwG 3\n",
      "4. ErwG 4\n",
      "5. ErwG 5\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T10:41:57.523891Z",
     "start_time": "2025-11-07T10:41:57.521829Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c33792f02146db99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task Summary\n",
    "\n",
    "Process each chunk from the JSON data by:\n",
    "1. Splitting chunk content into individual lines\n",
    "2. For each line, normalize spaces and search for a match in `regulation_lines_de`\n",
    "3. Replace matched lines with corresponding lines from `regulation_lines` (same position)\n",
    "4. Track and output lines that couldn't be matched\n",
    "5. Build new content with replacements\n",
    "\n"
   ],
   "id": "31ed7ba80ef31da1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T13:16:46.584282Z",
     "start_time": "2025-11-07T13:15:40.590858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def normalize_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "# Normalize regulation_lines_de once at the beginning\n",
    "normalized_regulation_lines_de = [normalize_spaces(line) for line in regulation_lines_de]\n",
    "\n",
    "unmatched_lines = []\n",
    "translated_chunks = []\n",
    "\n",
    "for chunk_idx, chunk in enumerate(filtered_chunks):\n",
    "    content = chunk.get('content', '')\n",
    "    content_lines = content.split('\\n')\n",
    "\n",
    "    new_content_lines = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(content_lines):\n",
    "        line = content_lines[i]\n",
    "        normalized_line = normalize_spaces(line)\n",
    "\n",
    "        if normalized_line == '':\n",
    "            new_content_lines.append(line)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        found = False\n",
    "\n",
    "        # Try to match line beginning with regulation lines\n",
    "        for reg_idx, normalized_de_line in enumerate(normalized_regulation_lines_de):\n",
    "            if normalized_de_line.startswith(normalized_line):\n",
    "                # Found a match at the beginning\n",
    "                # Now check if subsequent lines are also in this regulation line\n",
    "                combined_text = normalized_line\n",
    "                lines_to_consume = 1\n",
    "                next_line_idx = i + 1\n",
    "\n",
    "                # Try to match subsequent lines from the chunk\n",
    "                while next_line_idx < len(content_lines):\n",
    "                    next_line = content_lines[next_line_idx]\n",
    "                    normalized_next_line = normalize_spaces(next_line)\n",
    "\n",
    "                    if normalized_next_line == '':\n",
    "                        next_line_idx += 1\n",
    "                        continue\n",
    "\n",
    "                    # Check if adding this line keeps us within the regulation line\n",
    "                    test_combined = combined_text + ' ' + normalized_next_line\n",
    "                    if normalized_de_line.startswith(test_combined) or normalized_de_line == test_combined:\n",
    "                        combined_text = test_combined\n",
    "                        lines_to_consume += 1\n",
    "                        next_line_idx += 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                # Use the matched regulation line (English version)\n",
    "                if reg_idx < len(regulation_lines):\n",
    "                    new_content_lines.append(regulation_lines[reg_idx])\n",
    "                    translated_chunks.append({\n",
    "                        'chunk_idx': chunk_idx,\n",
    "                        'chunk_title': chunk.get('title', 'N/A'),\n",
    "                        'original_content': normalized_regulation_lines_de[reg_idx],\n",
    "                        'content': regulation_lines[reg_idx]\n",
    "                    })\n",
    "                    found = True\n",
    "                    i += lines_to_consume\n",
    "                    break\n",
    "\n",
    "        if not found:\n",
    "            new_content_lines.append(line)\n",
    "            translated_chunks.append({\n",
    "                'chunk_idx': chunk_idx,\n",
    "                'chunk_title': chunk.get('title', 'N/A'),\n",
    "                'original_content': normalized_line,\n",
    "                'content': ''\n",
    "            })\n",
    "            unmatched_lines.append({\n",
    "                'chunk_idx': chunk_idx,\n",
    "                'chunk_title': chunk.get('title', 'N/A'),\n",
    "                'original_line': line,\n",
    "                'normalized_line': normalized_line\n",
    "            })\n",
    "            i += 1\n",
    "\n",
    "print(f\"Total unmatched lines: {len(unmatched_lines)}\\n\")\n",
    "print(\"Sample unmatched lines:\")\n",
    "for item in unmatched_lines[:10]:\n",
    "    print(f\"Chunk {item['chunk_idx']} ({item['chunk_title']}):\")\n",
    "    print(f\"  Original: {item['original_line'][:80]}...\")\n",
    "    print()\n"
   ],
   "id": "d5926b25ac3eeaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unmatched lines: 68\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[42]\u001B[39m\u001B[32m, line 89\u001B[39m\n\u001B[32m     86\u001B[39m             i += \u001B[32m1\u001B[39m\n\u001B[32m     88\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTotal unmatched lines: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(unmatched_lines)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m89\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mSample unmatched lines:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     90\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m unmatched_lines[:\u001B[32m10\u001B[39m]:\n\u001B[32m     91\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mChunk \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mitem[\u001B[33m'\u001B[39m\u001B[33mchunk_idx\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mitem[\u001B[33m'\u001B[39m\u001B[33mchunk_title\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m):\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m_pydevd_bundle/pydevd_cython_win32_312_64.pyx:1186\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython_win32_312_64.SafeCallWrapper.__call__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m_pydevd_bundle/pydevd_cython_win32_312_64.pyx:627\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython_win32_312_64.PyDBFrame.trace_dispatch\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m_pydevd_bundle/pydevd_cython_win32_312_64.pyx:937\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython_win32_312_64.PyDBFrame.trace_dispatch\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m_pydevd_bundle/pydevd_cython_win32_312_64.pyx:928\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython_win32_312_64.PyDBFrame.trace_dispatch\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m_pydevd_bundle/pydevd_cython_win32_312_64.pyx:585\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython_win32_312_64.PyDBFrame.do_wait_suspend\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\PyCharm Professional\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1196\u001B[39m, in \u001B[36mPyDB.do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[39m\n\u001B[32m   1193\u001B[39m         from_this_thread.append(frame_id)\n\u001B[32m   1195\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, stop_reason):\n\u001B[32m-> \u001B[39m\u001B[32m1196\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\PyCharm Professional\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1211\u001B[39m, in \u001B[36mPyDB._do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[39m\n\u001B[32m   1208\u001B[39m             \u001B[38;5;28mself\u001B[39m._call_mpl_hook()\n\u001B[32m   1210\u001B[39m         \u001B[38;5;28mself\u001B[39m.process_internal_commands()\n\u001B[32m-> \u001B[39m\u001B[32m1211\u001B[39m         \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1213\u001B[39m \u001B[38;5;28mself\u001B[39m.cancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[32m   1215\u001B[39m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task Summary\n",
    "\n",
    "The `translated_chunks` list currently contains one entry per line. You need to consolidate these back into the original chunk structure by:\n",
    "1. Grouping per-line entries by their `chunk_idx`\n",
    "2. Merging the `content` field of each line with `\\n` characters\n",
    "3. Creating consolidated chunks with the original chunk structure\n",
    "\n",
    "Here are the new cells:\n",
    "\n"
   ],
   "id": "b3c99c7f43352e65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T13:25:23.463706Z",
     "start_time": "2025-11-07T13:25:23.454640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "consolidated_chunks = {}\n",
    "\n",
    "for translated_chunk in translated_chunks:\n",
    "    chunk_idx = translated_chunk['chunk_idx']\n",
    "\n",
    "    if chunk_idx not in consolidated_chunks:\n",
    "        # Get the original chunk to preserve its attributes\n",
    "        original_chunk = filtered_chunks[chunk_idx]\n",
    "\n",
    "        consolidated_chunks[chunk_idx] = {\n",
    "            'chunk_idx': chunk_idx,\n",
    "            'id': original_chunk.get('id'),\n",
    "            'chunk_title': translated_chunk['chunk_title'],\n",
    "            'relevantChunksIds': original_chunk.get('relevantChunksIds', []),\n",
    "            'keywords': original_chunk.get('keywords', []),\n",
    "            'content_lines': [],\n",
    "            'original_content_lines': []\n",
    "        }\n",
    "\n",
    "    consolidated_chunks[chunk_idx]['content_lines'].append(translated_chunk['content'])\n",
    "    consolidated_chunks[chunk_idx]['original_content_lines'].append(translated_chunk['original_content'])\n",
    "\n",
    "consolidated_chunks_list = []\n",
    "for chunk_idx in sorted(consolidated_chunks.keys()):\n",
    "    chunk_data = consolidated_chunks[chunk_idx]\n",
    "    consolidated_chunks_list.append({\n",
    "        'chunk_idx': chunk_data['chunk_idx'],\n",
    "        'id': chunk_data['id'],\n",
    "        'chunk_title': chunk_data['chunk_title'],\n",
    "        'relevantChunksIds': chunk_data['relevantChunksIds'],\n",
    "        'keywords': chunk_data['keywords'],\n",
    "        'content': '\\n'.join(chunk_data['content_lines']),\n",
    "        'original_content': '\\n'.join(chunk_data['original_content_lines'])\n",
    "    })\n",
    "\n",
    "print(f\"Original per-line chunks: {len(translated_chunks)}\")\n",
    "print(f\"Consolidated chunks: {len(consolidated_chunks_list)}\")\n",
    "print(\"\\nFirst 3 consolidated chunks:\")\n",
    "for chunk in consolidated_chunks_list[:3]:\n",
    "    print(f\"\\nChunk {chunk['chunk_idx']}: {chunk['chunk_title']}\")\n",
    "    print(f\"ID: {chunk['id']}\")\n",
    "    print(f\"Relevant Chunks: {chunk['relevantChunksIds']}\")\n",
    "    print(f\"Content preview: {chunk['content'][:100]}...\")\n",
    "    print(f\"Original content preview: {chunk['original_content'][:100]}...\")\n"
   ],
   "id": "85566321665c5240",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original per-line chunks: 1746\n",
      "Consolidated chunks: 338\n",
      "\n",
      "First 3 consolidated chunks:\n",
      "\n",
      "Chunk 0: ErwG 1\n",
      "ID: b833c1d7-ad46-4548-a2c6-63f671c1d211\n",
      "Relevant Chunks: ['be26e0cd-4d28-42f6-8560-20e6911c4c4f']\n",
      "Content preview: (1) The purpose of this Regulation is to improve the functioning of the internal market by laying do...\n",
      "Original content preview: (1) Zweck dieser Verordnung ist es, das Funktionieren des Binnenmarkts zu verbessern, indem ein einh...\n",
      "\n",
      "Chunk 1: ErwG 2\n",
      "ID: aa44ef37-ff65-4237-9ed5-b34174aa9c6a\n",
      "Relevant Chunks: ['be26e0cd-4d28-42f6-8560-20e6911c4c4f']\n",
      "Content preview: (2) This Regulation should be applied in accordance with the values of the Union enshrined as in the...\n",
      "Original content preview: (2) Diese Verordnung sollte im Einklang mit den in der Charta verankerten Werten der Union angewandt...\n",
      "\n",
      "Chunk 2: ErwG 3\n",
      "ID: 10c70d26-f011-44f0-89db-011879d8401c\n",
      "Relevant Chunks: ['be26e0cd-4d28-42f6-8560-20e6911c4c4f']\n",
      "Content preview: (3) AI systems can be easily deployed in a large variety of sectors of the economy and many parts of...\n",
      "Original content preview: (3) KI-Systeme können problemlos in verschiedenen Bereichen der Wirtschaft und Gesellschaft, auch gr...\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T13:25:27.397589Z",
     "start_time": "2025-11-07T13:25:27.380440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "output_data = {\n",
    "    'chunks': consolidated_chunks_list\n",
    "}\n",
    "\n",
    "with open('combined_en_new.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Successfully wrote {len(consolidated_chunks_list)} consolidated chunks to combined_en_new.json\")\n"
   ],
   "id": "3686ebca984776a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote 338 consolidated chunks to combined_en_new.json\n"
     ]
    }
   ],
   "execution_count": 51
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
